{"2016": [["Appreciating Statistics", null], ["Improving and Evaluating Topic Models and Other Models of Text", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Model Comparison and Assessment for Single Particle Tracking in Biological Fluids", "State-of-the-art techniques in passive particle-tracking microscopy provide high-resolution path trajectories of diverse foreign particles in biological fluids. For particles on the order of 1 \u03bcm diameter, these paths are generally inconsistent with simple Brownian motion. Yet, despite an abundance of data confirming these findings and their wide-ranging scientific implications, stochastic modeling of the complex particle motion has received comparatively little attention. Even among posited models, there is virtually no literature on likelihood-based inference, model comparisons, and other quantitative assessments. In this article, we develop a rigorous and computationally efficient Bayesian methodology to address this gap. We analyze two of the most prevalent candidate models for 30-sec paths of 1 \u03bcm diameter tracer particles in human lung mucus: fractional Brownian motion (fBM) and a Generalized Langevin Equation (GLE) consistent with viscoelastic theory. Our model comparisons distinctly favor GLE over fBM, with the former describing the data remarkably well up to the timescales for which we have reliable information. Supplementary materials for this article are available online."], ["Hierarchical Feature Selection Incorporating Known and Novel Biological Information: Identifying Genomic Features Related to Prostate Cancer Recurrence", "Our work is motivated by a prostate cancer study aimed at identifying mRNA and miRNA biomarkers that are predictive of cancer recurrence after prostatectomy. It has been shown in the literature that incorporating known biological information on pathway memberships and interactions among biomarkers improves feature selection of high-dimensional biomarkers in relation to disease risk. Biological information is often represented by graphs or networks, in which biomarkers are represented by nodes and interactions among them are represented by edges; however, biological information is often not fully known. For example, the role of microRNAs (miRNAs) in regulating gene expression is not fully understood and the miRNA regulatory network is not fully established, in which case new strategies are needed for feature selection. To this end, we treat unknown biological information as missing data (i.e., missing edges in graphs), different from commonly encountered missing data problems where variable values are missing. We propose a new concept of imputing unknown biological information based on observed data and define the imputed information as the novel biological information. In addition, we propose a hierarchical group penalty to encourage sparsity and feature selection at both the pathway level and the within-pathway level, which, combined with the imputation step, allows for incorporation of known and novel biological information. While it is applicable to general regression settings, we develop and investigate the proposed approach in the context of semiparametric accelerated failure time models motivated by our data example. Data application and simulation studies show that incorporation of novel biological information improves performance in risk prediction and feature selection and the proposed penalty outperforms the extensions of several existing penalties. Supplementary materials for this article are available online."], ["Modeling the Evolution of Dynamic Brain Processes During an Associative Learning Experiment", "We develop a new time series model to investigate the dynamic interactions between the nucleus accumbens and the hippocampus during an associative learning experiment. Preliminary analyses indicated that the spectral properties of the local field potentials at these two regions changed over the trials of the experiment. While many models already take into account nonstationarity within a single trial, the evolution of the dynamics across trials is often ignored. Our proposed model, the slowly evolving locally stationary process (SEv-LSP), is designed to capture nonstationarity both within a trial and across trials. We rigorously define the evolving evolutionary spectral density matrix, which we estimate using a two-stage procedure. In the first stage, we compute the within-trial time-localized periodogram matrix. In the second stage, we develop a data-driven approach that combines information from trial-specific local periodogram matrices. Through simulation studies, we show the utility of our proposed method for analyzing time series data with different evolutionary structures. Finally, we use the SEv-LSP model to demonstrate the evolving dynamics between the hippocampus and the nucleus accumbens during an associative learning experiment. Supplementary materials for this article are available online."], ["Bayesian Methods for Nonignorable Dropout in Joint Models in Smoking Cessation Studies", "Inference on data with missingness can be challenging, particularly if the knowledge that a measurement was unobserved provides information about its distribution. Our work is motivated by the Commit to Quit II study, a smoking cessation trial that measured smoking status and weight change as weekly outcomes. It is expected that dropout in this study was informative and that patients with missed measurements are more likely to be smoking, even after conditioning on their observed smoking and weight history. We jointly model the categorical smoking status and continuous weight change outcomes by assuming normal latent variables for cessation and by extending the usual pattern mixture model to the bivariate case. The model includes a novel approach to sharing information across patterns through a Bayesian shrinkage framework to improve estimation stability for sparsely observed patterns. To accommodate the presumed informativeness of the missing data in a parsimonious manner, we model the unidentified components of the model under a nonfuture dependence assumption and specify departures from missing at random through sensitivity parameters, whose distributions are elicited from a subject-matter expert. Supplementary materials for this article are available online."], ["Multiple Imputation of Missing Categorical and Continuous Values via Bayesian Mixture Models With Local Dependence", "We present a nonparametric Bayesian joint model for multivariate continuous and categorical variables, with the intention of developing a flexible engine for multiple imputation of missing values. The model fuses Dirichlet process mixtures of multinomial distributions for categorical variables with Dirichlet process mixtures of multivariate normal distributions for continuous variables. We incorporate dependence between the continuous and categorical variables by (1) modeling the means of the normal distributions as component-specific functions of the categorical variables and (2) forming distinct mixture components for the categorical and continuous data with probabilities that are linked via a hierarchical model. This structure allows the model to capture complex dependencies between the categorical and continuous data with minimal tuning by the analyst. We apply the model to impute missing values due to item nonresponse in an evaluation of the redesign of the Survey of Income and Program Participation (SIPP). The goal is to compare estimates from a field test with the new design to estimates from selected individuals from a panel collected under the old design. We show that accounting for the missing data changes some conclusions about the comparability of the distributions in the two datasets. We also perform an extensive repeated sampling simulation using similar data from complete cases in an existing SIPP panel, comparing our proposed model to a default application of multiple imputation by chained equations. Imputations based on the proposed model tend to have better repeated sampling properties than the default application of chained equations in this realistic setting. Supplementary materials for this article are available online."], ["Low SNR in Diffusion MRI Models", "Noise is a common issue for all magnetic resonance imaging (MRI) techniques such as diffusion MRI and obviously leads to variability of the estimates in any model describing the data. Increasing spatial resolution in MR experiments further diminishes the signal-to-noise ratio (SNR). However, with low SNR the expected signal deviates from the true value. Common modeling approaches therefore lead to a bias in estimated model parameters. Adjustments require an analysis of the data generating process and a characterization of the resulting distribution of the imaging data. We provide an adequate quasi-likelihood approach that employs these characteristics. We elaborate on the effects of typical data preprocessing and analyze the bias effects related to low SNR for the example of the diffusion tensor model in diffusion MRI. We then demonstrate the relevance of the problem using data from the Human Connectome Project. Supplementary materials for this article are available online."], ["Localizing Temperature Risk", "On the temperature derivative market, modeling temperature volatility is an important issue for pricing and hedging. To apply the pricing tools of financial mathematics, one needs to isolate a Gaussian risk factor. A conventional model for temperature dynamics is a stochastic model with seasonality and intertemporal autocorrelation. Empirical work based on seasonality and autocorrelation correction reveals that the obtained residuals are heteroscedastic with a periodic pattern. The object of this research is to estimate this heteroscedastic function so that, after scale normalization, a pure standardized Gaussian variable appears. Earlier works investigated temperature risk in different locations and showed that neither parametric component functions nor a local linear smoother with constant smoothing parameter are flexible enough to generally describe the variance process well. Therefore, we consider a local adaptive modeling approach to find, at each time point, an optimal smoothing parameter to locally estimate the seasonality and volatility. Our approach provides a more flexible and accurate fitting procedure for localized temperature risk by achieving nearly normal risk factors. We also employ our model to forecast the temperaturein different cities and compare it to a model developed in 2005 by Campbell and Diebold. Supplementary materials for this article are available online."], ["Personalized Dose Finding Using Outcome Weighted Learning", "In dose-finding clinical trials, it is becoming increasingly important to account for individual-level heterogeneity while searching for optimal doses to ensure an optimal individualized dose rule (IDR) maximizes the expected beneficial clinical outcome for each individual. In this article, we advocate a randomized trial design where candidate dose levels assigned to study subjects are randomly chosen from a continuous distribution within a safe range. To estimate the optimal IDR using such data, we propose an outcome weighted learning method based on a nonconvex loss function, which can be solved efficiently using a difference of convex functions algorithm. The consistency and convergence rate for the estimated IDR are derived, and its small-sample performance is evaluated via simulation studies. We demonstrate that the proposed method outperforms competing approaches. Finally, we illustrate this method using data from a cohort study for warfarin (an anti-thrombotic drug) dosing. Supplementary materials for this article are available online."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Smoothing Parameter and Model Selection for General Smooth Models", "This article discusses a general framework for smoothing parameter estimation for models with regular likelihoods constructed in terms of unknown smooth functions of covariates. Gaussian random effects and parametric terms may also be present. By construction the method is numerically stable and convergent, and enables smoothing parameter uncertainty to be quantified. The latter enables us to fix a well known problem with AIC for such models, thereby improving the range of model selection tools available. The smooth functions are represented by reduced rank spline like smoothers, with associated quadratic penalties measuring function smoothness. Model estimation is by penalized likelihood maximization, where the smoothing parameters controlling the extent of penalization are estimated by Laplace approximate marginal likelihood. The methods cover, for example, generalized additive models for nonexponential family responses (e.g., beta, ordered categorical, scaled t distribution, negative binomial and Tweedie distributions), generalized additive models for location scale and shape (e.g., two stage zero inflation models, and Gaussian location-scale models), Cox proportional hazards models and multivariate additive models. The framework reduces the implementation of new model classes to the coding of some standard derivatives of the log-likelihood. Supplementary materials for this article are available online."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Using Ranked Set Sampling With Cluster Randomized Designs for Improved Inference on Treatment Effects", null], ["Accelerating Asymptotically Exact MCMC for Computationally Intensive Models via Local Approximations", null], ["Fast Bayesian Factor Analysis via Automatic Rotations to Sparsity", null], ["Modeling Probability Forecasts via Information Diversity", null], ["Inference for Monotone Functions Under Short- and Long-Range Dependence: Confidence Intervals and New Universal Limits", "We introduce new point-wise confidence interval estimates for monotone functions observed with additive, dependent noise. Our methodology applies to both short- and long-range dependence regimes for the errors. The interval estimates are obtained via the method of inversion of certain discrepancy statistics. This approach avoids the estimation of nuisance parameters such as the derivative of the unknown function, which previous methods are forced to deal with. The resulting estimates are therefore more accurate, stable, and widely applicable in practice under minimal assumptions on the trend and error structure. The dependence of the errors especially long-range dependence leads to new phenomena, where new universal limits based on convex minorant functionals of drifted fractional Brownian motion emerge. Some extensions to uniform confidence bands are also developed. Supplementary materials for this article are available online."], ["Robust Improper Maximum Likelihood: Tuning, Computation, and a Comparison With Other Methods for Robust Gaussian Clustering", null], ["A Bayesian Approach to Graphical Record Linkage and Deduplication", null], ["Identifiability of Normal and Normal Mixture Models with Nonignorable Missing Data", null], ["Testing the Predictor Effect on a Functional Response", null], ["On SURE-Type Double Shrinkage Estimation", "The article is concerned with empirical Bayes shrinkage estimators for the heteroscedastic hierarchical normal model using Stein's unbiased estimate of risk (SURE). Recently, Xie, Kou, and Brown proposed a class of estimators for this type of problems and established their asymptotic optimality properties under the assumption of known but unequal variances. In this article, we consider this problem with unequal and unknown variances, which may be more appropriate in real situations. By placing priors for both means and variances, we propose novel SURE-type double shrinkage estimators that shrink both means and variances. Optimal properties for these estimators are derived under certain regularity conditions. Extensive simulation studies are conducted to compare the newly developed methods with other shrinkage techniques. Finally, the methods are applied to the well-known baseball dataset and a gene expression dataset. Supplementary materials for this article are available online."], ["Extremal Depth for Functional Data and Applications", "We propose a new notion called \u201cextremal depth\u201d (ED) for functional data, discuss its properties, and compare its performance with existing concepts. The proposed notion is based on a measure of extreme \u201coutlyingness.\u201d ED has several desirable properties that are not shared by other notions and is especially well suited for obtaining central regions of functional data and function spaces. In particular: (a) the central region achieves the nominal (desired) simultaneous coverage probability; (b) there is a correspondence between ED-based (simultaneous) central regions and appropriate pointwise central regions; and (c) the method is resistant to certain classes of functional outliers. The article examines the performance of ED and compares it with other depth notions. Its usefulness is demonstrated through applications to constructing central regions, functional boxplots, outlier detection, and simultaneous confidence bands in regression problems. Supplementary materials for this article are available online."], ["Statistical Matching Analysis for Complex Survey Data With Applications", "The goal of statistical matching is the estimation of a joint distribution having observed only samples from its marginals. The lack of joint observations on the variables of interest is the reason of uncertainty about the joint population distribution function. In the present article, the notion of matching error is introduced, and upper-bounded via an appropriate measure of uncertainty. Then, an estimate of the distribution function for the variables not jointly observed is constructed on the basis of a modification of the conditional independence assumption in the presence of logical constraints. The corresponding measure of uncertainty is estimated via sample data. Finally, a simulation study is performed, and an application to a real case is provided. Supplementary materials for this article are available online."], ["Multitask Quantile Regression Under the Transnormal Model", null], [null, null], ["Sparse Approximate Inference for Spatio-Temporal Point Process Models", null], ["Composite Robust Estimators for Linear Mixed Models", null], ["Optimal Model Averaging Estimation for Generalized Linear Models and Generalized Linear Mixed-Effects Models", "Considering model averaging estimation in generalized linear models, we propose a weight choice criterion based on the Kullback\u2013Leibler (KL) loss with a penalty term. This criterion is different from that for continuous observations in principle, but reduces to the Mallows criterion in the situation. We prove that the corresponding model averaging estimator is asymptotically optimal under certain assumptions. We further extend our concern to the generalized linear mixed-effects model framework and establish associated theory. Numerical experiments illustrate that the proposed method is promising."], ["Bayesian Nonparametric Modeling of Higher Order Markov Chains", "We consider the problem of flexible modeling of higher order Markov chains when an upper bound on the order of the chain is known but the true order and nature of the serial dependence are unknown. We propose Bayesian nonparametric methodology based on conditional tensor factorizations, which can characterize any transition probability with a specified maximal order. The methodology selects the important lags and captures higher order interactions among the lags, while also facilitating calculation of Bayes factors for a variety of hypotheses of interest. We design efficient Markov chain Monte Carlo algorithms for posterior computation, allowing for uncertainty in the set of important lags to be included and in the nature and order of the serial dependence. The methods are illustrated using simulation experiments and real world applications. Supplementary materials for this article are available online."], ["Panel Data Models With Interactive Fixed Effects and Multiple Structural Breaks", "In this article, we consider estimation of common structural breaks in panel data models with unobservable interactive fixed effects. We introduce a penalized principal component (PPC) estimation procedure with an adaptive group fused LASSO to detect the multiple structural breaks in the models. Under some mild conditions, we show that with probability approaching one the proposed method can correctly determine the unknown number of breaks and consistently estimate the common break dates. Furthermore, we estimate the regression coefficients through the post-LASSO method and establish the asymptotic distribution theory for the resulting estimators. The developed methodology and theory are applicable to the case of dynamic panel data models. Simulation results demonstrate that the proposed method works well in finite samples with low false detection probability when there is no structural break and high probability of correctly estimating the break numbers when the structural breaks exist. We finally apply our method to study the environmental Kuznets curve for 74 countries over 40 years and detect two breaks in the data. Supplementary materials for this article are available online."], ["Sensitivity Analysis for Multiple Comparisons in Matched Observational Studies Through Quadratically Constrained Linear Programming", null], ["Matching and Weighting With Functions of Error-Prone Covariates for Causal Inference", "Matching estimators are commonly used to estimate causal effects in nonexperimental settings. Covariate measurement error can be problematic for matching estimators when observational treatment groups differ on latent quantities observed only through error-prone surrogates. We establish necessary and sufficient conditions for matching and weighting with functions of observed covariates to yield unconfounded causal effect estimators, generalizing results from the standard (i.e., no measurement error) case. We establish that in common covariate measurement error settings, including continuous variables with continuous measurement error, discrete variables with misclassification, and factor and item response theory models, no single function of the observed covariates computed for all units in a study is appropriate for matching. However, we demonstrate that in some circumstances, it is possible to create different functions of the observed covariates for treatment and control units to construct a variable appropriate for matching. We also demonstrate the counterintuitive result that in some settings, it is possible to selectively contaminate the covariates with additional measurement error to construct a variable appropriate for matching. We discuss the implications of our results for the choice between matching and weighting estimators with error-prone covariates. Supplementary materials for this article are available online."], ["Book Reviews", null], ["Correction", null], ["Editorial Collaborators", null], ["Editorial Board EOV", null], ["Bayesian Nonparametric Estimation for Dynamic Treatment Regimes With Sequential Transition Times", null], ["Comment", null], ["Comment", "Xu, M\u00fcller, Wahed, and Thall proposed a Bayesian model to analyze an acute leukemia study involving multi-stage chemotherapy regimes. We discuss two alternative methods, Q-learning and O-learning, to solve the same problem from the machine learning point of view. The numerical studies show that these methods can be flexible and have advantages in some situations to handle treatment heterogeneity while being robust to model misspecification."], ["Comment", null], ["Rejoinder", null], ["Analyzing Single-Molecule Protein Transportation Experiments via Hierarchical Hidden Markov Models", "To maintain proper cellular functions, over 50% of proteins encoded in the genome need to be transported to cellular membranes. The molecular mechanism behind such a process, often referred to as protein targeting, is not well understood. Single-molecule experiments are designed to unveil the detailed mechanisms and reveal the functions of different molecular machineries involved in the process. The experimental data consist of hundreds of stochastic time traces from the fluorescence recordings of the experimental system. We introduce a Bayesian hierarchical model on top of hidden Markov models (HMMs) to analyze these data and use the statistical results to answer the biological questions. In addition to resolving the biological puzzles and delineating the regulating roles of different molecular complexes, our statistical results enable us to propose a more detailed mechanism for the late stages of the protein targeting process."], ["Template-Based Models for Genome-Wide Analysis of Next-Generation Sequencing Data at Base-Pair Resolution", null], ["A Model of Text for Experimentation in the Social Sciences", null], ["Estimation of Directed Acyclic Graphs Through Two-Stage Adaptive Lasso for Gene Network Inference", null], ["Detecting and Localizing Differences in Functional Time Series Dynamics: A Case Study in Molecular Biophysics", "Motivated by the problem of inferring the molecular dynamics of DNA in solution, and linking them with its base-pair composition, we consider the problem of comparing the dynamics of functional time series (FTS), and of localizing any inferred differences in frequency and along curvelength. The approach we take is one of Fourier analysis, where the complete second-order structure of the FTS is encoded by its spectral density operator, indexed by frequency and curvelength. The comparison is broken down to a hierarchy of stages: at a global level, we compare the spectral density operators of the two FTS, across frequencies and curvelength, based on a Hilbert\u2013Schmidt criterion; then, we localize any differences to specific frequencies; and, finally, we further localize any differences along the length of the random curves, that is, in physical space. A hierarchical multiple testing approach guarantees control of the averaged false discovery rate over the selected frequencies. In this sense, we are able to attribute any differences to distinct dynamic (frequency) and spatial (curvelength) contributions. Our approach is presented and illustrated by means of a case study in molecular biophysics: how can one use molecular dynamics simulations of short strands of DNA to infer their temporal dynamics at the scaling limit, and probe whether these depend on the sequence encoded in these strands? Supplementary materials for this article are available online."], ["Probabilistic Cause-of-Death Assignment Using Verbal Autopsies", null], ["Parameterization of White Matter Manifold-Like Structures Using Principal Surfaces", "In this article, we are concerned with data generated from a diffusion tensor imaging (DTI) experiment. The goal is to parameterize manifold-like white matter tracts, such as the corpus callosum, using principal surfaces. The problem is approached by finding a geometrically motivated surface-based representation of the corpus callosum and visualized fractional anisotropy (FA) values projected onto the surface. The method also applies to any other diffusion summary. An algorithm is proposed that (a) constructs the principal surface of a corpus callosum; (b) flattens the surface into a parametric two-dimensional (2D) map; and (c) projects associated FA values on the map. The algorithm is applied to a longitudinal study containing 466 diffusion tensor images of 176 multiple sclerosis (MS) patients observed at multiple visits. For each subject and visit, the study contains a registered DTI scan of the corpus callosum at roughly 20,000 voxels. Extensive simulation studies demonstrate fast convergence and robust performance of the algorithm under a variety of challenging scenarios."], ["A Longitudinal Mixed Logit Model for Estimation of Push and Pull Effects in Residential Location Choice", "We develop a random effects discrete choice model for the analysis of households\u2019 choice of neighborhood over time. The model is parameterized in a way that exploits longitudinal data to separate the influence of neighborhood characteristics on the decision to move out of the current area (\u201cpush\u201d effects) and on the choice of one destination over another (\u201cpull\u201d effects). Random effects are included to allow for unobserved heterogeneity between households in their propensity to move, and in the importance placed on area characteristics. The model also includes area-level random effects. The combination of a large choice set, large sample size, and repeated observations mean that existing estimation approaches are often infeasible. We, therefore, propose an efficient MCMC algorithm for the analysis of large-scale datasets. The model is applied in an analysis of residential choice in England using data from the British Household Panel Survey linked to neighborhood-level census data. We consider how effects of area deprivation and distance from the current area depend on household characteristics and life course transitions in the previous year. We find substantial differences between households in the effects of deprivation on out-mobility and selection of destination, with evidence of severely constrained choices among less-advantaged households. Supplementary materials for this article are available online."], ["Hierarchical Models for Semicompeting Risks Data With Application to Quality of End-of-Life Care for Pancreatic Cancer", null], ["Model-Based Geostatistics for Prevalence Mapping in Low-Resource Settings", "In low-resource settings, prevalence mapping relies on empirical prevalence data from a finite, often spatially sparse, set of surveys of communities within the region of interest, possibly supplemented by remotely sensed images that can act as proxies for environmental risk factors. A standard geostatistical model for data of this kind is a generalized linear mixed model with binomial error distribution, logistic link, and a combination of explanatory variables and a Gaussian spatial stochastic process in the linear predictor. In this article, we first review statistical methods and software associated with this standard model, then consider several methodological extensions whose development has been motivated by the requirements of specific applications. These include: methods for combining randomized survey data with data from nonrandomized, and therefore potentially biased, surveys; spatio-temporal extensions; and spatially structured zero-inflation. Throughout, we illustrate the methods with disease mapping applications that have arisen through our involvement with a range of African public health programs."], ["Comment", null], ["Comment", null], ["Comment: Getting into Space with a Weight Problem", null], ["Rejoinder", null], ["Generalized Dynamic Principal Components", "Brillinger defined dynamic principal components (DPC) for time series based on a reconstruction criterion. He gave a very elegant theoretical solution and proposed an estimator which is consistent under stationarity. Here, we propose a new enterally empirical approach to DPC. The main differences with the existing methods\u2014mainly Brillinger procedure\u2014are (1) the DPC we propose need not be a linear combination of the observations and (2) it can be based on a variety of loss functions including robust ones. Unlike Brillinger, we do not establish any consistency results; however, contrary to Brillinger\u2019s, which has a very strong stationarity flavor, our concept aims at a better adaptation to possible nonstationary features of the series. We also present a robust version of our procedure that allows to estimate the DPC when the series have outlier contamination. We give iterative algorithms to compute the proposed procedures that can be used with a large number of variables. Our nonrobust and robust procedures are illustrated with real datasets. Supplementary materials for this article are available online."], ["Fast Estimation of Regression Parameters in a Broken-Stick Model for Longitudinal Data", null], ["Priors for Random Count Matrices Derived from a Family of Negative Binomial Processes", "We define a family of probability distributions for random count matrices with a potentially unbounded number of rows and columns. The three distributions we consider are derived from the gamma-Poisson, gamma-negative binomial, and beta-negative binomial processes, which we refer to generically as a family of negative-binomial processes. Because the models lead to closed-form update equations within the context of a Gibbs sampler, they are natural candidates for nonparametric Bayesian priors over count matrices. A key aspect of our analysis is the recognition that although the random count matrices within the family are defined by a row-wise construction, their columns can be shown to be independent and identically distributed (iid). This fact is used to derive explicit formulas for drawing all the columns at once. Moreover, by analyzing these matrices\u2019 combinatorial structure, we describe how to sequentially construct a column-iid\u00a0random count matrix one row at a time, and derive the predictive distribution of a new row count vector with previously unseen features. We describe the similarities and differences between the three priors, and argue that the greater flexibility of the gamma- and beta-negative binomial processes\u2014especially their ability to model over-dispersed, heavy-tailed count data\u2014makes these well suited to a wide variety of real-world applications. As an example of our framework, we construct a naive-Bayes text classifier to categorize a count vector to one of several existing random count matrices of different categories. The classifier supports an unbounded number of features and, unlike most existing methods, it does not require a predefined finite vocabulary to be shared by all the categories, and needs neither feature selection nor parameter tuning. Both the gamma- and beta-negative binomial processes are shown to significantly outperform the gamma-Poisson process when applied to document categorization, with comparable performance to other state-of-the-art supervised text classification algorithms. Supplementary materials for this article are available online."], ["Constructed Second Control Groups and Attenuation of Unmeasured Biases", "The informal folklore of observational studies claims that if an irrelevant observed covariate is left uncontrolled, say unmatched, then it will influence treatment assignment in haphazard ways, thereby diminishing the biases from unmeasured covariates. We prove a result along these lines: it is true, in a certain sense, to a limited degree, under certain conditions. Alas, the conditions are neither inconsequential nor easy to check in empirical work; indeed, they are often dubious, more often implausible. We suggest the result is most useful in the computerized construction of a second control group, where the investigator can see more in available data without necessarily believing the required conditions. One of the two control groups controls for the possibly irrelevant observed covariate, the other control group either leaves it uncontrolled or forces separation; therefore, the investigator views one situation from two angles under different assumptions. A pair of sensitivity analyses for the two control groups is coordinated by a weighted Holm or recycling procedure built around the possibility of slight attenuation of bias in one control group. Issues are illustrated using an observational study of the possible effects of cigarette smoking as a cause of increased homocysteine levels, a risk factor for cardiovascular disease. Supplementary materials for this article are available online."], ["Bayesian Nonparametric Longitudinal Data Analysis", "Practical Bayesian nonparametric methods have been developed across a wide variety of contexts. Here, we develop a novel statistical model that generalizes standard mixed models for longitudinal data that include flexible mean functions as well as combined compound symmetry (CS) and autoregressive (AR) covariance structures. AR structure is often specified through the use of a Gaussian process (GP) with covariance functions that allow longitudinal data to be more correlated if they are observed closer in time than if they are observed farther apart. We allow for AR structure by considering a broader class of models that incorporates a Dirichlet process mixture (DPM) over the covariance parameters of the GP. We are able to take advantage of modern Bayesian statistical methods in making full predictive inferences and about characteristics of longitudinal profiles and their differences across covariate combinations. We also take advantage of the generality of our model, which provides for estimation of a variety of covariance structures. We observe that models that fail to incorporate CS or AR structure can result in very poor estimation of a covariance or correlation matrix. In our illustration using hormone data observed on women through the menopausal transition, biology dictates the use of a generalized family of sigmoid functions as a model for time trends across subpopulation categories."], ["Brownian Integrated Covariance Functions for Gaussian Process Modeling: Sigmoidal Versus Localized Basis Functions", "Gaussian process modeling, or kriging, is a popular method for modeling data from deterministic computer simulations, and the most common choices of covariance function are Gaussian, power exponential, and Mat\u00e9rn. A characteristic of these covariance functions is that the basis functions associated with their corresponding response predictors are localized, in the sense that they decay to zero as the input location moves away from the simulated input sites. As a result, the predictors tend to revert to the prior mean, which can result in a bumpy fitted response surface. In contrast, a fractional Brownian field model results in a predictor with basis functions that are nonlocalized and more sigmoidal in shape, although it suffers from drawbacks such as inability to represent smooth response surfaces. We propose a class of Brownian integrated covariance functions obtained by incorporating an integrator (as in the white noise integral representation of a fractional Brownian field) into any stationary covariance function. Brownian integrated covariance models result in predictor basis functions that are nonlocalized and sigmoidal, but they are capable of modeling smooth response surfaces. We discuss fundamental differences between Brownian integrated and other covariance functions, and we illustrate by comparing Brownian integrated power exponential with regular power exponential kriging models in a number of examples. Supplementary materials for this article are available online."], ["Dynamic Covariance Models", null], ["Forward Variable Selection for Sparse Ultra-High Dimensional Varying Coefficient Models", "Varying coefficient models have numerous applications in a wide scope of scientific areas. While enjoying nice interpretability, they also allow for flexibility in modeling dynamic impacts of the covariates. But, in the new era of big data, it is challenging to select the relevant variables when the dimensionality is very large. Recently, several works are focused on this important problem based on sparsity assumptions; they are subject to some limitations, however. We introduce an appealing forward selection procedure. It selects important variables sequentially according to a reduction in sum of squares criterion and it employs a Bayesian information criterion (BIC)-based stopping rule. Clearly, it is simple to implement and fast to compute, and possesses many other desirable properties from theoretical and numerical viewpoints. The BIC is a special case of the extended BIC (EBIC) when an extra tuning parameter in the latter vanishes. We establish rigorous screening consistency results when either BIC or EBIC is used as the stopping criterion. The theoretical results depend on some conditions on the eigenvalues related to the design matrices, which can be relaxed in some situations. Results of an extensive simulation study and a real data example are also presented to show the efficacy and usefulness of our procedure. Supplementary materials for this article are available online."], ["A Subsampled Double Bootstrap for Massive Data", "The bootstrap is a popular and powerful method for assessing precision of estimators and inferential methods. However, for massive datasets that are increasingly prevalent, the bootstrap becomes prohibitively costly in computation and its feasibility is questionable even with modern parallel computing platforms. Recently, Kleiner and co-authors proposed a method called BLB (bag of little bootstraps) for massive data, which is more computationally scalable with little sacrifice of statistical accuracy. Building on BLB and the idea of fast double bootstrap, we propose a new resampling method, the subsampled double bootstrap, for both independent data and time series data. We establish consistency of the subsampled double bootstrap under mild conditions for both independent and dependent cases. Methodologically, the subsampled double bootstrap is superior to BLB in terms of running time, more sample coverage, and automatic implementation with less tuning parameters for a given time budget. Its advantage relative to BLB and bootstrap is also demonstrated in numerical simulations and a data illustration. Supplementary materials for this article are available online."], ["Coverage Inducing Priors in Nonstandard Inference Problems", null], ["Classification With Unstructured Predictors and an Application to Sentiment Analysis", "Unstructured data refer to information that lacks certain structures and cannot be organized in a predefined fashion. Unstructured data often involve words, texts, graphs, objects, or multimedia types of files that are difficult to process and analyze with traditional computational tools and statistical methods. This work explores ordinal classification for unstructured predictors with ordered class categories, where imprecise information concerning strengths of association between predictors is available for predicting class labels. However, imprecise information here is expressed in terms of a directed graph, with each node representing a predictor and a directed edge containing pairwise strengths of association between two nodes. One of the targeted applications for unstructured data arises from sentiment analysis, which identifies and extracts the relevant content or opinion of a document concerning a specific event of interest. We integrate the imprecise predictor relations into linear relational constraints over classification function coefficients, where large margin ordinal classifiers are introduced, subject to many quadratically linear constraints. The proposed classifiers are then applied in sentiment analysis using binary word predictors. Computationally, we implement ordinal support vector machines and \u03c8-learning through a scalable quadratic programming package based on sparse word representations. Theoretically, we show that using relationships among unstructured predictors improves prediction accuracy of classification significantly. We illustrate an application for sentiment analysis using consumer text reviews and movie review data. Supplementary materials for this article are available online."], ["Stabilized Nearest Neighbor Classifier and its Statistical Properties", null], ["Conditional Sure Independence Screening", "Independence screening is powerful for variable selection when the number of variables is massive. Commonly used independence screening methods are based on marginal correlations or its variants. When some prior knowledge on a certain important set of variables is available, a natural assessment on the relative importance of the other predictors is their conditional contributions to the response given the known set of variables. This results in conditional sure independence screening (CSIS). CSIS produces a rich family of alternative screening methods by different choices of the conditioning set and can help reduce the number of false positive and false negative selections when covariates are highly correlated. This article proposes and studies CSIS in generalized linear models. We give conditions under which sure screening is possible and derive an upper bound on the number of selected variables. We also spell out the situation under which CSIS yields model selection consistency and the properties of CSIS when a data-driven conditioning set is used. Moreover, we provide two data-driven methods to select the thresholding parameter of conditional screening. The utility of the procedure is illustrated by simulation studies and analysis of two real datasets. Supplementary materials for this article are available online."], ["Graphical Procedures for Multiple Comparisons Under General Dependence", "It has been more than half a century since Tukey first introduced graphical displays that relate nonoverlap of confidence intervals to statistically significant differences between parameter estimates. In this article, we show how Tukey\u2019s graphical overlap procedure can be modified to accommodate general forms of dependence within and across samples. We also develop a procedure that can be used to more effectively resolve rankings within the tails of the distributions of parameter values, thereby generalizing existing methods for \u201cmultiple comparisons with the best.\u201d We show that these new procedures retain the simplicity of Tukey\u2019s original procedure, while maintaining asymptotic control of the familywise error rate under very general conditions. Simple examples are used throughout to illustrate the procedures. Supplementary materials for this article are available online."], ["Joint Inference for Competing Risks Survival Data", "This article develops joint inferential methods for the cause-specific hazard function and the cumulative incidence function of a specific type of failure to assess the effects of a variable on the time to the type of failure of interest in the presence of competing risks. Joint inference for the two functions are needed in practice because (i) they describe different characteristics of a given type of failure, (ii) they do not uniquely determine each other, and (iii) the effects of a variable on the two functions can be different and one often does not know which effects are to be expected. We study both the group comparison problem and the regression problem. We also discuss joint inference for other related functions. Our simulation shows that our joint tests can be considerably more powerful than the Bonferroni method, which has important practical implications to the analysis and design of clinical studies with competing risks data. We illustrate our method using a Hodgkin disease data and a lymphoma data. Supplementary materials for this article are available online."], ["Analysis of Proportional Odds Models With Censoring and Errors-in-Covariates", "We propose a consistent method for estimating both the finite- and infinite-dimensional parameters of the proportional odds model when a covariate is subject to measurement error and time-to-events are subject to right censoring. The proposed method does not rely on the distributional assumption of the true covariate, which is not observed in the data. In addition, the proposed estimator does not require the measurement error to be normally distributed or to have any other specific distribution, and we do not attempt to assess the error distribution. Instead, we construct martingale-based estimators through inversion, using only the moment properties of the error distribution, estimable from multiple erroneous measurements of the true covariate. The theoretical properties of the estimators are established and the finite sample performance is demonstrated via simulations. We illustrate the usefulness of the method by analyzing a dataset from a clinical study on AIDS. Supplementary materials for this article are available online."], ["Sufficient Reductions in Regressions With Exponential Family Inverse Predictors", "We develop methodology for identifying and estimating sufficient reductions in regressions with predictors that, given the response, follow a multivariate exponential family distribution. This setup includes regressions where predictors are all continuous, all categorical, or mixtures of categorical and continuous. We derive the minimal sufficient reduction of the predictors and its maximum likelihood estimator by modeling the conditional distribution of the predictors given the response. Whereas nearly all extant estimators of sufficient reductions are linear and only partly capture the sufficient reduction, our method is not limited to linear reductions. It also provides the exact form of the sufficient reduction, which is exhaustive, its maximum likelihood (ML) estimates via an iterated reweighted least-square (IRLS) estimation algorithm, and asymptotic tests for the dimension of the regression. Supplementary materials for this article are available online."], ["How Much Information Does Dependence Between Wavelet Coefficients Contain?", null], ["Generalized Fiducial Inference: A Review and New Results", "R. A. Fisher, the father of modern statistics, proposed the idea of fiducial inference during the first half of the 20th century. While his proposal led to interesting methods for quantifying uncertainty, other prominent statisticians of the time did not accept Fisher\u2019s approach as it became apparent that some of Fisher\u2019s bold claims about the properties of fiducial distribution did not hold up for multi-parameter problems. Beginning around the year 2000, the authors and collaborators started to reinvestigate the idea of fiducial inference and discovered that Fisher\u2019s approach, when properly generalized, would open doors to solve many important and difficult inference problems. They termed their generalization of Fisher\u2019s idea as generalized fiducial inference (GFI). The main idea of GFI is to carefully transfer randomness from the data to the parameter space using an inverse of a data-generating equation without the use of Bayes\u2019 theorem. The resulting generalized fiducial distribution (GFD) can then be used for inference. After more than a decade of investigations, the authors and collaborators have developed a unifying theory for GFI, and provided GFI solutions to many challenging practical problems in different fields of science and industry. Overall, they have demonstrated that GFI is a valid, useful, and promising approach for conducting statistical inference. The goal of this article is to deliver a timely and concise introduction to GFI, to present some of the latest results, as well as to list some related open research problems. It is authors\u2019 hope that their contributions to GFI will stimulate the growth and usage of this exciting approach for statistical inference. Supplementary materials for this article are available online."], ["Book Reviews", null], ["Correction to Jin-Ting Zhang\u2019s \u201cApproximate and Asymptotic Distributions of Chi-Squared-Type Mixtures With Applications\u2019\u2019", null], ["Discrete Optimization for Interpretable Study Populations and Randomization Inference in an Observational Study of Severe Sepsis Mortality", null], ["A Dynamic Bayesian Model for Characterizing Cross-Neuronal Interactions During Decision-Making", "The goal of this article is to develop a novel statistical model for studying cross-neuronal spike train interactions during decision-making. For an individual to successfully complete the task of decision-making, a number of temporally organized events must occur: stimuli must be detected, potential outcomes must be evaluated, behaviors must be executed or inhibited, and outcomes (such as reward or no-reward) must be experienced. Due to the complexity of this process, it is likely the case that decision-making is encoded by the temporally precise interactions between large populations of neurons. Most existing statistical models, however, are inadequate for analyzing such a phenomenon because they provide only an aggregated measure of interactions over time. To address this considerable limitation, we propose a dynamic Bayesian model that captures the time-varying nature of neuronal activity (such as the time-varying strength of the interactions between neurons). The proposed method yielded results that reveal new insight into the dynamic nature of population coding in the prefrontal cortex during decision-making. In our analysis, we note that while some neurons in the prefrontal cortex do not synchronize their firing activity until the presence of a reward, a different set of neurons synchronizes their activity shortly after stimulus onset. These differentially synchronizing subpopulations of neurons suggest a continuum of population representation of the reward-seeking task. Second, our analyses also suggest that the degree of synchronization differs between the rewarded and nonrewarded conditions. Moreover, the proposed model is scalable to handle data on many simultaneously recorded neurons and is applicable to analyzing other types of multivariate time series data with latent structure. Supplementary materials (including computer codes) for our article are available online."], ["Bayesian Spatial Change of Support for Count-Valued Survey Data With Application to the American Community Survey", "We introduce Bayesian spatial change of support (COS) methodology for count-valued survey data with known survey variances. Our proposed methodology is motivated by the American Community Survey (ACS), an ongoing survey administered by the U.S. Census Bureau that provides timely information on several key demographic variables. Specifically, the ACS produces 1-year, 3-year, and 5-year \u201cperiod-estimates,\u201d and corresponding margins of errors, for published demographic and socio-economic variables recorded over predefined geographies within the United States. Despite the availability of these predefined geographies, it is often of interest to data-users to specify customized user-defined spatial supports. In particular, it is useful to estimate demographic variables defined on \u201cnew\u201d spatial supports in \u201creal-time.\u201d This problem is known as spatial COS, which is typically performed under the assumption that the data follow a Gaussian distribution. However, count-valued survey data is naturally non-Gaussian and, hence, we consider modeling these data using a Poisson distribution. Additionally, survey-data are often accompanied by estimates of error, which we incorporate into our analysis. We interpret Poisson count-valued data in small areas as an aggregation of events from a spatial point process. This approach provides us with the flexibility necessary to allow ACS users to consider a variety of spatial supports in \u201creal-time.\u201d We show the effectiveness of our approach through a simulated example as well as through an analysis using public-use ACS data."], ["Detection of Infectious Disease Outbreaks From Laboratory Data With Reporting Delays", null], ["Calibrating Functional Parameters in the Ion Channel Models of Cardiac Cells", "Computational modeling is a popular tool to understand a diverse set of complex systems. The output from a computational model depends on a set of parameters that are unknown to the designer, but a modeler can estimate them by collecting physical data. In the described study of the ion channels of ventricular myocytes, the parameter of interest is a function as opposed to a scalar or a set of scalars. This article develops a new modeling strategy to nonparametrically study the functional parameter using Bayesian inference with Gaussian process prior distributions. A new sampling scheme is devised to address this unique problem."], ["Identification and Estimation of Causal Mechanisms in Clustered Encouragement Designs: Disentangling Bed Nets Using Bayesian Principal Stratification", "Exploration of causal mechanisms is often important for researchers and policymakers to understand how an intervention works and how it can be improved. This task can be crucial in clustered encouragement designs (CEDs). Encouragement design studies arise frequently when the treatment cannot be enforced because of ethical or practical constraints and an encouragement intervention (information campaigns, incentives, etc.) is conceived with the purpose of increasing the uptake of the treatment of interest. By design, encouragements always entail the complication of noncompliance. Encouragements can also give rise to a variety of mechanisms, particularly when encouragement is assigned at the cluster level. Social interactions among units within the same cluster can result in spillover effects. Disentangling the effect of encouragement through spillover effects from that through the enhancement of the treatment would give better insight into the intervention and it could be compelling for planning the scaling-up phase of the program. Building on previous works on CEDs and noncompliance, we use the principal stratification framework to define stratum-specific causal effects, that is, effects for specific latent subpopulations, defined by the joint potential compliance statuses under both encouragement conditions. We show how the latter stratum-specific causal effects are related to the decomposition commonly used in the literature and provide flexible homogeneity assumptions under which an extrapolation across principal strata allows one to disentangle the effects. Estimation of causal estimands can be performed with Bayesian inferential methods using hierarchical models to account for clustering. We illustrate the proposed methodology by analyzing a cluster randomized experiment implemented in Zambia and designed to evaluate the impact on malaria prevalence of an agricultural loan program intended to increase the bed net coverage. Farmer households assigned to the program could take advantage of a deferred payment and a discount in the purchase of new bed nets. Our analysis shows a lack of evidence of an effect of the offering of the program to a cluster of households through spillover effects, that is, through a greater bed net coverage in the neighborhood. Supplementary materials for this article are available online."], ["Using Binary Paradata to Correct for Measurement Error in Survey Data Analysis", null], ["Joint Estimation of Treatment and Placebo Effects in Clinical Trials With Longitudinal Blinding Assessments", "In some therapeutic areas, treatment evaluation is frequently complicated by a possible placebo effect (i.e., the psychobiological effect of a patient's knowledge or belief of being treated). When a substantial placebo effect is likely to exist, it is important to distinguish the treatment and placebo effects in quantifying the clinical benefit of a new treatment. These causal effects can be formally defined in a joint causal model that includes treatment (e.g., new vs. placebo) and treatmentality (i.e., a patient's belief or mentality about which treatment she or he has received) as separate exposures. Information about the treatmentality exposure can be obtained from blinding assessments, which are increasingly common in clinical trials where blinding success is in question. Assuming that treatmentality has a lagged effect and is measured at multiple time points, this article is concerned with joint evaluation of treatment and placebo effects in clinical trials with longitudinal follow-up, possibly with monotone missing data. We describe and discuss several methods adapted from the longitudinal causal inference literature, apply them to a weight loss study, and compare them in simulation experiments that mimic the weight loss study. Supplementary materials for this article are available online."], ["Understanding the Impact of Stroke on Brain Motor Function: A Hierarchical Bayesian Approach", "Stroke is a disturbance in blood supply to the brain resulting in the loss of brain functions, particularly motor function. A study was conducted by the UCI Neurorehabilitation Lab to investigate the impact of stroke on motor-related brain regions. Functional MRI (fMRI) data were collected from stroke patients and healthy controls while the subjects performed a simple motor task. In addition to affecting local neuronal activation strength, stroke might also alter communications (i.e., connectivity) between brain regions. We develop a hierarchical Bayesian modeling approach for the analysis of multi-subject fMRI data that allows us to explore brain changes due to stroke. Our approach simultaneously estimates activation and condition-specific connectivity at the group level, and provides estimates for region/subject-specific hemodynamic response functions. Moreover, our model uses spike-and-slab priors to allow for direct posterior inference on the connectivity network. Our results indicate that motor-control regions show greater activation in the unaffected hemisphere and the midline surface in stroke patients than those same regions in healthy controls during the simple motor task. We also note increased connectivity within secondary motor regions in stroke subjects. These findings provide insight into altered neural correlates of movement in subjects who suffered a stroke. Supplementary materials for this article are available online."], ["Modeling E-mail Networks and Inferring Leadership Using Self-Exciting Point Processes", "We propose various self-exciting point process models for the times when e-mails are sent between individuals in a social network. Using an expectation\u2013maximization (EM)-type approach, we fit these models to an e-mail network dataset from West Point Military Academy and the Enron e-mail dataset. We argue that the self-exciting models adequately capture major temporal clustering features in the data and perform better than traditional stationary Poisson models. We also investigate how accounting for diurnal and weekly trends in e-mail activity improves the overall fit to the observed network data. A motivation and application for fitting these self-exciting models is to use parameter estimates to characterize important e-mail communication behaviors such as the baseline sending rates, average reply rates, and average response times. A primary goal is to use these features, estimated from the self-exciting models, to infer the underlying leadership status of users in the West Point and Enron networks. Supplementary materials for this article are available online."], ["A Multiresolution Stochastic Process Model for Predicting Basketball Possession Outcomes", null], ["Exact Post-Selection Inference for Sequential Regression Procedures", null], ["Comment", null], ["Rejoinder", null], ["Structured Matrix Completion with Applications to Genomic Data Integration", "Matrix completion has attracted significant recent attention in many fields including statistics, applied mathematics, and electrical engineering. Current literature on matrix completion focuses primarily on independent sampling models under which the individual observed entries are sampled independently. Motivated by applications in genomic data integration, we propose a new framework of structured matrix completion (SMC) to treat structured missingness by design. Specifically, our proposed method aims at efficient matrix recovery when a subset of the rows and columns of an approximately low-rank matrix are observed. We provide theoretical justification for the proposed SMC method and derive lower bound for the estimation errors, which together establish the optimal rate of recovery over certain classes of approximately low-rank matrices. Simulation studies show that the method performs well in finite sample under a variety of configurations. The method is applied to integrate several ovarian cancer genomic studies with different extent of genomic measurements, which enables us to construct more accurate prediction rules for ovarian cancer survival. Supplementary materials for this article are available online."], ["The Controlled Thermodynamic Integral for Bayesian Model Evidence Evaluation", "Approximation of the model evidence is well known to be challenging. One promising approach is based on thermodynamic integration, but a key concern is that the thermodynamic integral can suffer from high variability in many applications. This article considers the reduction of variance that can be achieved by exploiting control variates in this setting. Our methodology applies whenever the gradient of both the log-likelihood and the log-prior with respect to the parameters can be efficiently evaluated. Results obtained on regression models and popular benchmark datasets demonstrate a significant and sometimes dramatic reduction in estimator variance and provide insight into the wider applicability of control variates to evidence estimation. Supplementary materials for this article are available online."], ["Empirical Likelihood for Right Censored Lifetime Data", null], ["Bayesian Conditional Tensor Factorizations for High-Dimensional Classification", "In many application areas, data are collected on a categorical response and high-dimensional categorical predictors, with the goals being to build a parsimonious model for classification while doing inferences on the important predictors. In settings such as genomics, there can be complex interactions among the predictors. By using a carefully structured Tucker factorization, we define a model that can characterize any conditional probability, while facilitating variable selection and modeling of higher-order interactions. Following a Bayesian approach, we propose a Markov chain Monte Carlo algorithm for posterior computation accommodating uncertainty in the predictors to be included. Under near low-rank assumptions, the posterior distribution for the conditional probability is shown to achieve close to the parametric rate of contraction even in ultra high-dimensional settings. The methods are illustrated using simulation examples and biomedical applications. Supplementary materials for this article are available online."], ["A General Regression Changepoint Test for Time Series Data", "This article develops a test for a single changepoint in a general setting that allows for correlated time series regression errors, a seasonal cycle, time-varying regression factors, and covariate information. Within, a changepoint statistic is constructed from likelihood ratio principles and its asymptotic distribution is derived. The asymptotic distribution of the changepoint statistic is shown to be invariant of the seasonal cycle and the covariates should the latter obey some simple limit laws; however, the limit distribution depends on any time-varying factors. A new test based on ARMA residuals is developed and is shown to have favorable properties with finite samples. Driving our work is a changepoint analysis of the Mauna Loa record of monthly carbon dioxide concentrations. This series has a pronounced seasonal cycle, a nonlinear trend, heavily correlated regression errors, and covariate information in the form of climate oscillations. In the end, we find a prominent changepoint in the early 1990s, often attributed to the eruption of Mount Pinatubo, which cannot be explained by covariates. Supplementary materials for this article are available online."], ["A Class of Functional Methods for Error-Contaminated Survival Data Under Additive Hazards Models with Replicate Measurements", null], [null, null], ["Sparse Regression Incorporating Graphical Structure Among Predictors", "With the abundance of high-dimensional data in various disciplines, sparse regularized techniques are very popular these days. In this article, we make use of the structure information among predictors to improve sparse regression models. Typically, such structure information can be modeled by the connectivity of an undirected graph using all predictors as nodes of the graph. Most existing methods use this undirected graph edge-by-edge to encourage the regression coefficients of corresponding connected predictors to be similar. However, such methods do not directly use the neighborhood information of the graph. Furthermore, if there are more edges in the predictor graph, the corresponding regularization term will be more complicated. In this article, we incorporate the graph information node-by-node, instead of edge-by-edge as used in most existing methods. Our proposed method is very general and it includes adaptive Lasso, group Lasso, and ridge regression as special cases. Both theoretical and numerical studies demonstrate the effectiveness of the proposed method for simultaneous estimation, prediction, and model selection. Supplementary materials for this article are available online."], ["Multivariate-Sign-Based High-Dimensional Tests for the Two-Sample Location Problem", "This article concerns tests for the two-sample location problem when data dimension is larger than the sample size. Existing multivariate-sign-based procedures are not robust against high dimensionality, producing tests with Type I error rates far away from nominal levels. This is mainly due to the biases from estimating location parameters. We propose a novel test to overcome this issue by using the \u201cleave-one-out\u201d idea. The proposed test statistic is scalar-invariant and thus is particularly useful when different components have different scales in high-dimensional data. Asymptotic properties of the test statistic are studied. Compared with other existing approaches, simulation studies show that the proposed method behaves well in terms of sizes and power. Supplementary materials for this article are available online."], ["Generalized Quasi-Likelihood Ratio Tests for Semiparametric Analysis of Covariance Models in Longitudinal Data", "We model generalized longitudinal data from multiple treatment groups by a class of semiparametric analysis of covariance models, which take into account the parametric effects of time dependent covariates and the nonparametric time effects. In these models, the treatment effects are represented by nonparametric functions of time and we propose a generalized quasi-likelihood ratio test procedure to test if these functions are identical. Our estimation procedure is based on profile estimating equations combined with local linear smoothers. We find that the much celebrated Wilks phenomenon which is well established for independent data still holds for longitudinal data if a working independence correlation structure is assumed in the test statistic. However, this property does not hold in general, especially when the working variance function is misspecified. Our empirical study also shows that incorporating correlation into the test statistic does not necessarily improve the power of the test. The proposed methods are illustrated with simulation studies and a real application from opioid dependence treatments. Supplementary materials for this article are available online."], ["Approximations of the Optimal Importance Density Using Gaussian Particle Flow Importance Sampling", null], ["Robust Orthogonal Complement Principal Component Analysis", "Recently, the robustification of principal component analysis (PCA) has attracted lots of attention from statisticians, engineers, and computer scientists. In this work, we study the type of outliers that are not necessarily apparent in the original observation space but can seriously affect the principal subspace estimation. Based on a mathematical formulation of such transformed outliers, a novel robust orthogonal complement principal component analysis (ROC-PCA) is proposed. The framework combines the popular sparsity-enforcing and low-rank regularization techniques to deal with row-wise outliers as well as element-wise outliers. A nonasymptotic oracle inequality guarantees the accuracy and high breakdown performance of ROC-PCA in finite samples. To tackle the computational challenges, an efficient algorithm is developed on the basis of Stiefel manifold optimization and iterative thresholding. Furthermore, a batch variant is proposed to significantly reduce the cost in ultra high dimensions. The article also points out a pitfall of a common practice of singular value decomposition (SVD) reduction in robust PCA. Experiments show the effectiveness and efficiency of ROC-PCA in both synthetic and real data. Supplementary materials for this article are available online."], ["Functional CAR Models for Large Spatially Correlated Functional Datasets", null], ["Efficient Estimation of the Cox Model with Auxiliary Subgroup Survival Information", null], ["Hierarchical Nearest-Neighbor Gaussian Process Models for Large Geostatistical Datasets", "Spatial process models for analyzing geostatistical data entail computations that become prohibitive as the number of spatial locations become large. This article develops a class of highly scalable nearest-neighbor Gaussian process (NNGP) models to provide fully model-based inference for large geostatistical datasets. We establish that the NNGP is a well-defined spatial process providing legitimate finite-dimensional Gaussian densities with sparse precision matrices. We embed the NNGP as a sparsity-inducing prior within a rich hierarchical modeling framework and outline how computationally efficient Markov chain Monte Carlo (MCMC) algorithms can be executed without storing or decomposing large matrices. The floating point operations (flops) per iteration of this algorithm is linear in the number of spatial locations, thereby rendering substantial scalability. We illustrate the computational and inferential benefits of the NNGP over competing methods using simulation studies and also analyze forest biomass from a massive U.S. Forest Inventory dataset at a scale that precludes alternative dimension-reducing methods. Supplementary materials for this article are available online."], ["Trace Pursuit: A General Framework for Model-Free Variable Selection", null], ["Nonparametric Variance Estimation Under Fine Stratification: An Alternative to Collapsed Strata", "Fine stratification is commonly used to control the distribution of a sample from a finite population and to improve the precision of resulting estimators. One-per-stratum designs represent the finest possible stratification and occur in practice, but designs with very low numbers of elements per stratum (say, two or three) are also common. The classical variance estimator in this context is the collapsed stratum estimator, which relies on creating larger \u201cpseudo-strata\u201d and computing the sum of the squared differences between estimated stratum totals across the pseudo-strata. We propose here a nonparametric alternative that replaces the pseudo-strata by kernel-weighted stratum neighborhoods and uses deviations from a fitted mean function to estimate the variance. We establish the asymptotic behavior of the kernel-based estimator and show its superior practical performance relative to the collapsed stratum variance estimator in a simulation study. An application to data from the U.S. Consumer Expenditure Survey illustrates the potential of the method in practice."], ["Convex Banding of the Covariance Matrix", "We introduce a new sparse estimator of the covariance matrix for high-dimensional models in which the variables have a known ordering. Our estimator, which is the solution to a convex optimization problem, is equivalently expressed as an estimator that tapers the sample covariance matrix by a Toeplitz, sparsely banded, data-adaptive matrix. As a result of this adaptivity, the convex banding estimator enjoys theoretical optimality properties not attained by previous banding or tapered estimators. In particular, our convex banding estimator is minimax rate adaptive in Frobenius and operator norms, up to log factors, over commonly studied classes of covariance matrices, and over more general classes. Furthermore, it correctly recovers the bandwidth when the true covariance is exactly banded. Our convex formulation admits a simple and efficient algorithm. Empirical studies demonstrate its practical effectiveness and illustrate that our exactly banded estimator works well even when the true covariance matrix is only close to a banded matrix, confirming our theoretical results. Our method compares favorably with all existing methods, in terms of accuracy and speed. We illustrate the practical merits of the convex banding estimator by showing that it can be used to improve the performance of discriminant analysis for classifying sound recordings. Supplementary materials for this article are available online."], [null, null], ["Using a Monotonic Density Ratio Model to Find the Asymptotically Optimal Combination of Multiple Diagnostic Tests", null], ["Active Clinical Trials for Personalized Medicine", "Individualized treatment rules (ITRs) tailor treatments according to individual patient characteristics. They can significantly improve patient care and are thus becoming increasingly popular. The data collected during randomized clinical trials are often used to estimate the optimal ITRs. However, these trials are generally expensive to run, and, moreover, they are not designed to efficiently estimate ITRs. In this article, we propose a cost-effective estimation method from an active learning perspective. In particular, our method recruits only the \u201cmost informative\u201d patients (in terms of learning the optimal ITRs) from an ongoing clinical trial. Simulation studies and real-data examples show that our active clinical trial method significantly improves on competing methods. We derive risk bounds and show that they support these observed empirical advantages. Supplementary materials for this article are available online."], ["Spatio-Temporal Covariance and Cross-Covariance Functions of the Great Circle Distance on a Sphere", "In this article, we propose stationary covariance functions for processes that evolve temporally over a sphere, as well as cross-covariance functions for multivariate random fields defined over a sphere. For such processes, the great circle distance is the natural metric that should be used to describe spatial dependence. Given the mathematical difficulties for the construction of covariance functions for processes defined over spheres cross time, approximations of the state of nature have been proposed in the literature by using the Euclidean (based on map projections) and the chordal distances. We present several methods of construction based on the great circle distance and provide closed-form expressions for both spatio-temporal and multivariate cases. A simulation study assesses the discrepancy between the great circle distance, chordal distance, and Euclidean distance based on a map projection both in terms of estimation and prediction in a space-time and a bivariate spatial setting, where the space is in this case the Earth. We revisit the analysis of Total Ozone Mapping Spectrometer (TOMS) data and investigate differences in terms of estimation and prediction between the aforementioned distance-based approaches. Both simulation and real data highlight sensible differences in terms of estimation of the spatial scale parameter. As far as prediction is concerned, the differences can be appreciated only when the interpoint distances are large, as demonstrated by an illustrative example. Supplementary materials for this article are available online."], ["I-Optimal Design of Mixture Experiments", "In mixture experiments, the factors under study are proportions of the ingredients of a mixture. The special nature of the factors necessitates specific types of regression models, and specific types of experimental designs. Although mixture experiments usually are intended to predict the response(s) for all possible formulations of the mixture and to identify optimal proportions for each of the ingredients, little research has been done concerning their I-optimal design. This is surprising given that I-optimal designs minimize the average variance of prediction and, therefore, seem more appropriate for mixture experiments than the commonly used D-optimal designs, which focus on a precise model estimation rather than precise predictions. In this article, we provide the first detailed overview of the literature on the I-optimal design of mixture experiments and identify several contradictions. For the second-order and the special cubic model, we present continuous I-optimal designs and contrast them with the published results. We also study exact I-optimal designs, and compare them in detail to continuous I-optimal designs and to D-optimal designs. One striking result of our work is that the performance of D-optimal designs in terms of the I-optimality criterion very strongly depends on which of the D-optimal designs is considered. Supplemental materials for this article are available online."], ["Book Reviews", null], ["A Functional Approach to Deconvolve Dynamic Neuroimaging Data", "Positron emission tomography (PET) is an imaging technique which can be used to investigate chemical changes in human biological processes such as cancer development or neurochemical reactions. Most dynamic PET scans are currently analyzed based on the assumption that linear first-order kinetics can be used to adequately describe the system under observation. However, there has recently been strong evidence that this is not the case. To provide an analysis of PET data which is free from this compartmental assumption, we propose a nonparametric deconvolution and analysis model for dynamic PET data based on functional principal component analysis. This yields flexibility in the possible deconvolved functions while still performing well when a linear compartmental model setup is the true data generating mechanism. As the deconvolution needs to be performed on only a relative small number of basis functions rather than voxel by voxel in the entire three-dimensional volume, the methodology is both robust to typical brain imaging noise levels while also being computationally efficient. The new methodology is investigated through simulations in both one-dimensional functions and 2D images and also applied to a neuroimaging study whose goal is the quantification of opioid receptor concentration in the brain."], ["A Bayesian Partial Identification Approach to Inferring the Prevalence of Accounting Misconduct", "This article describes the use of flexible Bayesian regression models for estimating a partially identified probability function. Our approach permits efficient sensitivity analysis concerning the posterior impact of priors on the partially identified component of the regression model. The new methodology is illustrated on an important problem where only partially observed data are available\u2014inferring the prevalence of accounting misconduct among publicly traded U.S. businesses. Supplementary materials for this article are available online."], [null, null], ["Collective Estimation of Multiple Bivariate Density Functions With Application to Angular-Sampling-Based Protein Loop Modeling", "This article develops a method for simultaneous estimation of density functions for a collection of populations of protein backbone angle pairs using a data-driven, shared basis that is constructed by bivariate spline functions defined on a triangulation of the bivariate domain. The circular nature of angular data is taken into account by imposing appropriate smoothness constraints across boundaries of the triangles. Maximum penalized likelihood is used to fit the model and an alternating blockwise Newton-type algorithm is developed for computation. A simulation study shows that the collective estimation approach is statistically more efficient than estimating the densities individually. The proposed method was used to estimate neighbor-dependent distributions of protein backbone dihedral angles (i.e., Ramachandran distributions). The estimated distributions were applied to protein loop modeling, one of the most challenging open problems in protein structure prediction, by feeding them into an angular-sampling-based loop structure prediction framework. Our estimated distributions compared favorably to the Ramachandran distributions estimated by fitting a hierarchical Dirichlet process model; and in particular, our distributions showed significant improvements on the hard cases where existing methods do not work well."], ["Calibrating an Ice Sheet Model Using High-Dimensional Binary Spatial Data", "Rapid retreat of ice in the Amundsen Sea sector of West Antarctica may cause drastic sea level rise, posing significant risks to populations in low-lying coastal regions. Calibration of computer models representing the behavior of the West Antarctic Ice Sheet is key for informative projections of future sea level rise. However, both the relevant observations and the model output are high-dimensional binary spatial data; existing computer model calibration methods are unable to handle such data. Here we present a novel calibration method for computer models whose output is in the form of binary spatial data. To mitigate the computational and inferential challenges posed by our approach, we apply a generalized principal component based dimension reduction method. To demonstrate the utility of our method, we calibrate the PSU3D-ICE model by comparing the output from a 499-member perturbed-parameter ensemble with observations from the Amundsen Sea sector of the ice sheet. Our methods help rigorously characterize the parameter uncertainty even in the presence of systematic data-model discrepancies and dependence in the errors. Our method also helps inform environmental risk analyses by contributing to improved projections of sea level rise from the ice sheets. Supplementary materials for this article are available online."], ["Perturbation Detection Through Modeling of Gene Expression on a Latent Biological Pathway Network: A Bayesian Hierarchical Approach", "Cellular response to a perturbation is the result of a dynamic system of biological variables linked in a complex network. A major challenge in drug and disease studies is identifying the key factors of a biological network that are essential in determining the cell\u2019s fate. Here, our goal is the identification of perturbed pathways from high-throughput gene expression data. We develop a three-level hierarchical model, where (i) the first level captures the relationship between gene expression and biological pathways using confirmatory factor analysis, (ii) the second level models the behavior within an underlying network of pathways induced by an unknown perturbation using a conditional autoregressive model, and (iii) the third level is a spike-and-slab prior on the perturbations. We then identify perturbations through posterior-based variable selection. We illustrate our approach using gene transcription drug perturbation profiles from the DREAM7 drug sensitivity predication challenge dataset. Our proposed method identified regulatory pathways that are known to play a causative role and that were not readily resolved using gene set enrichment analysis or exploratory factor models. Simulation results are presented assessing the performance of this model relative to a network-free variant and its robustness to inaccuracies in biological databases. Supplementary materials for this article are available online."], ["A Model-Based Approach to Climate Reconstruction Using Tree-Ring Data", "Quantifying long-term historical climate is fundamental to understanding recent climate change. Most instrumentally recorded climate data are only available for the past 200 years, so proxy observations from natural archives are often considered. We describe a model-based approach to reconstructing climate defined in terms of raw tree-ring measurement data that simultaneously accounts for nonclimatic and climatic variability. In this approach, we specify a joint model for the tree-ring data and climate variable that we fit using Bayesian inference. We consider a range of prior densities and compare the modeling approach to current methodology using an example case of Scots pine from Tornetr\u00e4sk, Sweden, to reconstruct growing season temperature. We describe how current approaches translate into particular model assumptions. We explore how changes to various components in the model-based approach affect the resulting reconstruction. We show that minor changes in model specification can have little effect on model fit but lead to large changes in the predictions. In particular, the periods of relatively warmer and cooler temperatures are robust between models, but the magnitude of the resulting temperatures is highly model dependent. Such sensitivity may not be apparent with traditional approaches because the underlying statistical model is often hidden or poorly described. Supplementary materials for this article are available online."], ["Constrained Maximum Likelihood Estimation for Model Calibration Using Summary-Level Information From External Big Data Sources", "Information from various public and private data sources of extremely large sample sizes are now increasingly available for research purposes. Statistical methods are needed for using information from such big data sources while analyzing data from individual studies that may collect more detailed information required for addressing specific hypotheses of interest. In this article, we consider the problem of building regression models based on individual-level data from an \u201cinternal\u201d study while using summary-level information, such as information on parameters for reduced models, from an \u201cexternal\u201d big data source. We identify a set of very general constraints that link internal and external models. These constraints are used to develop a framework for semiparametric maximum likelihood inference that allows the distribution of covariates to be estimated using either the internal sample or an external reference sample. We develop extensions for handling complex stratified sampling designs, such as case-control sampling, for the internal study. Asymptotic theory and variance estimators are developed for each case. We use simulation studies and a real data application to assess the performance of the proposed methods in contrast to the generalized regression calibration methodology that is popular in the sample survey literature. Supplementary materials for this article are available online."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment: A Human Genetics Perspective", null], ["Comment: Addressing the Need for Portability in Big Data Model Building and Calibration", null], ["Rejoinder", null], ["Instrumental Variables Estimation With Some Invalid Instruments and its Application to Mendelian Randomization", null], ["Generalizing Quantile Regression for Counting Processes With Applications to Recurrent Events", "In survival analysis, quantile regression has become a useful approach to account for covariate effects on the distribution of an event time of interest. In this article, we discuss how quantile regression can be extended to model counting processes and thus lead to a broader regression framework for survival data. We specifically investigate the proposed modeling of counting processes for recurrent events data. We show that the new recurrent events model retains the desirable features of quantile regression such as easy interpretation and good model flexibility, while accommodating various observation schemes encountered in observational studies. We develop a general theoretical and inferential framework for the new counting process model, which unifies with an existing method for censored quantile regression. As another useful contribution of this work, we propose a sample-based covariance estimation procedure, which provides a useful complement to the prevailing bootstrapping approach. We demonstrate the utility of our proposals via simulation studies and an application to a dataset from the U.S. Cystic Fibrosis Foundation Patient Registry (CFFPR). Supplementary materials for this article are available online."], ["A Potential Tale of Two-by-Two Tables From Completely Randomized Experiments", "Causal inference in completely randomized treatment-control studies with binary outcomes is discussed from Fisherian, Neymanian, and Bayesian perspectives, using the potential outcomes model. A randomization-based justification of Fisher\u2019s exact test is provided. Arguing that the crucial assumption of constant causal effect is often unrealistic, and holds only for extreme cases, some new asymptotic and Bayesian inferential procedures are proposed. The proposed procedures exploit the intrinsic nonadditivity of unit-level causal effects, can be applied to linear and nonlinear estimands, and dominate the existing methods, as verified theoretically and also through simulation studies. Supplementary materials for this article are available online."], ["Ultrahigh-Dimensional Multiclass Linear Discriminant Analysis by Pairwise Sure Independence Screening", "This article is concerned with the problem of feature screening for multiclass linear discriminant analysis under ultrahigh-dimensional setting. We allow the number of classes to be relatively large. As a result, the total number of relevant features is larger than usual. This makes the related classification problem much more challenging than the conventional one, where the number of classes is small (very often two). To solve the problem, we propose a novel pairwise sure independence screening method for linear discriminant analysis with an ultrahigh-dimensional predictor. The proposed procedure is directly applicable to the situation with many classes. We further prove that the proposed method is screening consistent. Simulation studies are conducted to assess the finite sample performance of the new procedure. We also demonstrate the proposed methodology via an empirical analysis of a real life example on handwritten Chinese character recognition."], ["Constrained Estimators and Consistency of a Regression Model on a Lexis Diagram", null], [null, null], ["Pairwise Likelihood Inference for Nested Hidden Markov Chain Models for Multilevel Longitudinal Data", "In the context of multilevel longitudinal data, where sample units are collected in clusters, an important aspect that should be accounted for is the unobserved heterogeneity between sample units and between clusters. For this aim, we propose an approach based on nested hidden (latent) Markov chains, which are associated with every sample unit and with every cluster. The approach allows us to account for the previously mentioned forms of unobserved heterogeneity in a dynamic fashion; it also allows us to account for the correlation that may arise between the responses provided by the units belonging to the same cluster. Under the assumed model, computing the manifest distribution of these response variables is infeasible even with a few units per cluster. Therefore, we make inference on this model through a composite likelihood function based on all the possible pairs of subjects within each cluster. Properties of the composite likelihood estimator are assessed by simulation. The proposed approach is illustrated through an application to a dataset concerning a sample of Italian workers in which a binary response variable for the worker receiving an illness benefit was repeatedly observed. Supplementary materials for this article are available online."], ["Large-Scale Multiple Testing of Correlations", null], ["Personalized Prediction and Sparsity Pursuit in Latent Factor Models", "Personalized information filtering extracts the information specifically relevant to a user, predicting his/her preference over a large number of items, based on the opinions of users who think alike or its content. This problem is cast into the framework of regression and classification, where we integrate additional user-specific and content-specific predictors in partial latent models, for higher predictive accuracy. In particular, we factorize a user-over-item preference matrix into a product of two matrices, each representing a user\u2019s preference and an item preference by users. Then we propose a likelihood method to seek a sparsest latent factorization, from a class of overcomplete factorizations, possibly with a high percentage of missing values. This promotes additional sparsity beyond rank reduction. Computationally, we design methods based on a \u201cdecomposition and combination\u201d strategy, to break large-scale optimization into many small subproblems to solve in a recursive and parallel manner. On this basis, we implement the proposed methods through multi-platform shared-memory parallel programming, and through Mahout, a library for scalable machine learning and data mining, for mapReduce computation. For example, our methods are scalable to a dataset consisting of three billions of observations on a single machine with sufficient memory, having good timings. Both theoretical and numerical investigations show that the proposed methods exhibit a significant improvement in accuracy over state-of-the-art scalable methods. Supplementary materials for this article are available online."], ["Minimax and Adaptive Estimation of Covariance Operator for Random Variables Observed on a Lattice Graph", null], ["Estimation and Testing of Varying Coefficients in Quantile Regression", null], ["Feature Augmentation via Nonparametrics and Selection (FANS) in High-Dimensional Classification", "We propose a high-dimensional classification method that involves nonparametric feature augmentation. Knowing that marginal density ratios are the most powerful univariate classifiers, we use the ratio estimates to transform the original feature measurements. Subsequently, penalized logistic regression is invoked, taking as input the newly transformed or augmented features. This procedure trains models equipped with local complexity and global simplicity, thereby avoiding the curse of dimensionality while creating a flexible nonlinear decision boundary. The resulting method is called feature augmentation via nonparametrics and selection (FANS). We motivate FANS by generalizing the naive Bayes model, writing the log ratio of joint densities as a linear combination of those of marginal densities. It is related to generalized additive models, but has better interpretability and computability. Risk bounds are developed for FANS. In numerical analysis, FANS is compared with competing methods, so as to provide a guideline on its best application domain. Real data analysis demonstrates that FANS performs very competitively on benchmark email spam and gene expression datasets. Moreover, FANS is implemented by an extremely fast algorithm through parallel computing."], ["Spline-Lasso in High-Dimensional Linear Regression", null], ["Efficient Sequential Monte Carlo With Multiple Proposals and Control Variates", null], ["Stepwise Signal Extraction via Marginal Likelihood", "This article studies the estimation of a stepwise signal. To determine the number and locations of change-points of the stepwise signal, we formulate a maximum marginal likelihood estimator, which can be computed with a quadratic cost using dynamic programming. We carry out an extensive investigation on the choice of the prior distribution and study the asymptotic properties of the maximum marginal likelihood estimator. We propose to treat each possible set of change-points equally and adopt an empirical Bayes approach to specify the prior distribution of segment parameters. A detailed simulation study is performed to compare the effectiveness of this method with other existing methods. We demonstrate our method on single-molecule enzyme reaction data and on DNA array comparative genomic hybridization (CGH) data. Our study shows that this method is applicable to a wide range of models and offers appealing results in practice. Supplementary materials for this article are available online."], ["Hierarchical Testing in the High-Dimensional Setting With Correlated Variables", "We propose a method for testing whether hierarchically ordered groups of potentially correlated variables are significant for explaining a response in a high-dimensional linear model. In presence of highly correlated variables, as is very common in high-dimensional data, it seems indispensable to go beyond an approach of inferring individual regression coefficients, and we show that detecting smallest groups of variables (MTDs: minimal true detections) is realistic. Thanks to the hierarchy among the groups of variables, powerful multiple testing adjustment is possible which leads to a data-driven choice of the resolution level for the groups. Our procedure, based on repeated sample splitting, is shown to asymptotically control the familywise error rate and we provide empirical results for simulated and real data which complement the theoretical analysis. Supplementary materials for this article are available online."], ["Quantile Regression in the Secondary Analysis of Case\u2013Control Data", "Case\u2013control design is widely used in epidemiology and other fields to identify factors associated with a disease. Data collected from existing case\u2013control studies can also provide a cost-effective way to investigate the association of risk factors with secondary outcomes. When the secondary outcome is a continuous random variable, most of the existing methods focus on the statistical inference on the mean of the secondary outcome. In this article, we propose a quantile-based approach to facilitating a comprehensive investigation of covariates\u2019 effects on multiple quantiles of the secondary outcome. We construct a new family of estimating equations combining observed and pseudo outcomes, which lead to consistent estimation of conditional quantiles using case\u2013control data. Simulations are conducted to evaluate the performance of our proposed approach, and a case\u2013control study on genetic association with asthma is used to demonstrate the method. Supplementary materials for this article are available online."], ["Variable Selection With Prior Information for Generalized Linear Models via the Prior LASSO Method", "LASSO is a popular statistical tool often used in conjunction with generalized linear models that can simultaneously select variables and estimate parameters. When there are many variables of interest, as in current biological and biomedical studies, the power of LASSO can be limited. Fortunately, so much biological and biomedical data have been collected and they may contain useful information about the importance of certain variables. This article proposes an extension of LASSO, namely, prior LASSO (pLASSO), to incorporate that prior information into penalized generalized linear models. The goal is achieved by adding in the LASSO criterion function an additional measure of the discrepancy between the prior information and the model. For linear regression, the whole solution path of the pLASSO estimator can be found with a procedure similar to the least angle regression (LARS). Asymptotic theories and simulation results show that pLASSO provides significant improvement over LASSO when the prior information is relatively accurate. When the prior information is less reliable, pLASSO shows great robustness to the misspecification. We illustrate the application of pLASSO using a real dataset from a genome-wide association study. Supplementary materials for this article are available online."], ["An Adaptive Exchange Algorithm for Sampling From Distributions With Intractable Normalizing Constants", "Sampling from the posterior distribution for a model whose normalizing constant is intractable is a long-standing problem in statistical research. We propose a new algorithm, adaptive auxiliary variable exchange algorithm, or, in short, adaptive exchange (AEX) algorithm, to tackle this problem. The new algorithm can be viewed as a MCMC extension of the exchange algorithm, which generates auxiliary variables via an importance sampling procedure from a Markov chain running in parallel. The convergence of the algorithm is established under mild conditions. Compared to the exchange algorithm, the new algorithm removes the requirement that the auxiliary variables must be drawn using a perfect sampler, and thus can be applied to many models for which the perfect sampler is not available or very expensive. Compared to the approximate exchange algorithms, such as the double Metropolis-Hastings sampler, the new algorithm overcomes their theoretical difficulty in convergence. The new algorithm is tested on the spatial autologistic and autonormal models. The numerical results indicate that the new algorithm is particularly useful for the problems for which the underlying system is strongly dependent. Supplementary materials for this article are available online."], ["Asymptotically Normal and Efficient Estimation of Covariate-Adjusted Gaussian Graphical Model", null], ["Estimating Variance Components in Functional Linear Models With Applications to Genetic Heritability", "Quantifying heritability is the first step in understanding the contribution of genetic variation to the risk architecture of complex human diseases and traits. Heritability can be estimated for univariate phenotypes from nonfamily data using linear mixed effects models. There is, however, no fully developed methodology for defining or estimating heritability from longitudinal studies. By examining longitudinal studies, researchers have the opportunity to better understand the genetic influence on the temporal development of diseases, which can be vital for populations with rapidly changing phenotypes such as children or the elderly. To define and estimate heritability for longitudinally measured phenotypes, we present a framework based on functional data analysis, FDA. While our procedures have important genetic consequences, they also represent a substantial development for FDA. In particular, we present a very general methodology for constructing optimal, unbiased estimates of variance components in functional linear models. Such a problem is challenging as likelihoods and densities do not readily generalize to infinite-dimensional settings. Our procedure can be viewed as a functional generalization of the minimum norm quadratic unbiased estimation procedure, MINQUE, presented by C. R. Rao, and is equivalent to residual maximum likelihood, REML, in univariate settings. We apply our methodology to the Childhood Asthma Management Program, CAMP, a 4-year longitudinal study examining the long term effects of daily asthma medications on children."], ["Extended MaxT Tests of One-Sided Hypotheses", "In many statistical applications of one-sided tests of multiple hypotheses researchers are often concerned not only with global tests of the intersection of individual hypotheses, but also with multiple tests of individual hypotheses. For example, in clinical trial studies researchers often need to find out the efficacy of a treatment, as well as the significance of each outcome measurement (endpoint) of the treatment. This article proposes MaxT type tests aiming at improving the global power of existing MaxT tests. Our extended MaxT tests are constructed by adding an extra component to the maximand set of existing MaxT tests. The added component is a weighted sum of other components. Some power properties relating to choices of weight are studied. Our simulation study shows that the proposed tests can considerably improve the global power of existing MaxT tests and can also outperform many other global tests under some alternatives and/or some nonnormal distributions. Furthermore, it is shown that such global power improvement may involve little loss of power on multiple testing. Two real data examples on clinical trial studies reported in the literature are reexamined. The results of our tests suggest stronger evidence on treatment effects over MaxT tests and likelihood ratio tests while changing little on the evidence concerning endpoint testing. Supplementary materials for this article are available online."], ["Book Reviews", null]]}