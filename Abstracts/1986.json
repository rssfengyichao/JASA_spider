{"1986": [["John Craig and the Probability of History: From the Death of Christ to the Birth of Laplace", "In 1699 John Craig published an underappreciated book on the probability of historical events. A new interpretation of Craig's work is offered, and it is argued that his formula for the probability of testimony was tantamount to a logistic model for the posterior odds. A modern model for the transmission of evidence is given that is qualitatively similar to Craig's, and Craig's attempt to date the Second Coming of Christ is discussed. Finally, Craig's model is fit to more modern data (from 1882) on the birth date of Laplace."], ["Methods for National Population Forecasts: A Review", "Three widely used classes of methods for forecasting national populations are reviewed: demographic accounting/cohort-component methods for long-range projections, statistical time series methods for short-range forecasts, and structural modeling methods for the simulation and forecasting of the effects of policy changes. In each case, the major characteristics, strengths, and weaknesses of the methods are described. Factors that place intrinsic limits on the accuracy of population forecasts are articulated. Promising lines of additional research by statisticians and demographers are identified for each class of methods and for population forecasting generally."], ["Joint Forecasts of U.S. Marital Fertility, Nuptiality, Births, and Marriages Using Time Series Models", "This article presents a new approach to forecasting U.S. marital fertility, nuptiality, births, and marriages. The analysis represents a wedding of demographic and statistical time series in models amenable to Box-Jenkins techniques of model identification, estimation, diagnosis, and forecasting. The models demonstrate the advantages in this approach in forecasting both rates and events as opposed to the common practice of simply forecasting events. Using the best models of indexes of fertility and nuptiality, forecasts of births and first marriages are made for the U.S. for the years 1983\u20132000. Analyses of these forecasts are made with discussions of their demographic realism in terms of their forecast confidence intervals."], ["A Successive Differences Method for Growth Curves with Missing Data and Random Observation Times", "Incomplete growth curve data can be analyzed by the successive differences (SD) method, which uses the difference in consecutive pairs of observations for all subjects having two or more repeated measures. We have generalized it to handle varying observation times as well by partitioning the time interval spanned by all repeated measures into subintervals. The model is developed, including test statistics for the hypotheses of parallelism and of no change in response level over time, assuming parallelism. This generalized SD method is then applied to repeated serum cholesterol measurements on a subsample of 1,072 men from a prospective cohort study of 8,006 Hawaiian Japanese men living on Oahu."], ["Effect of Categorizing a Continuous Covariate on the Comparison of Survival Time", "The variance of the estimated hazard ratio between two groups when there is one categorized continuous gamma-distributed covariate is derived using exponential and Weibull regression models and asymptotic theory. Categorizing a continuous covariate increases the variance of the estimated hazard ratio and decreases the efficiency of the analysis. The efficiency of categorization is studied as a function of the strength of the relation between survival time and the covariate, the choice of cut points used in categorizing, and the number of categories. An application of the results to an advanced lung cancer clinical trial is given."], ["A Linear Model Approach to Backcalculation of Fish Length", "A common problem in fishery management is to infer the growth history of a population of fish from a single sample of fish. On each fish, one can measure its length at capture, age, and yearly increments between concentric annual rings in a scale. Linear models for growth from the scale increments are proposed. A method of inferring changes in length from the available data is then discussed. Results are illustrated by an example."], ["A Mean Squared Error Model for Dual Frame, Mixed Mode Survey Design", "An error model for dual frame survey designs is developed. It includes components of error for sampling variance, interviewer variance, and bias in each frame. A cost model that attempts to capture the complexity of a full scale dual frame survey is presented. The error and cost models are applied to a large national survey, the National Crime Survey, and the effect that alternative levels of bias in both frames have on the optimal allocation of sample to the two frames is examined for two types of crime."], ["The Use of Multiple Diaries in a Household Expenditure Survey in Hong Kong", "This article discusses two aspects of using daily expenditure records, commonly referred to as diaries, in a household expenditure survey, based on the experience of the 1979\u20131980 Hong Kong Household Expenditure Survey. In particular, the article investigates the determinants of the decision by individual household members to fill out diaries, conditional upon the household having agreed to participate in the survey. In addition, estimates are presented of the additional reporting that results from household members keeping individual diaries, as opposed to one member keeping one household diary book."], ["Statistics and Causal Inference", "Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Exchangeable Belief Structures", "The static features of beliefs, namely those structures that relate to one's beliefs at a particular moment, are described. It is argued that expectation (or prevision) should be the fundamental quantification of individual statements of uncertainty and that inner product spaces (or belief structures) should be the fundamental organizing structure for collections of such statements. Objections are given to the usual Bayesian justification of relative frequency-based statistical models via exchangeability. I offer an alternative approach, via exchangeable and co-exchangeable belief structures, and derive the representation theorem, and thus the implied statistical models, for these structures."], ["Residuals in Generalized Linear Models", "The excellent performance of the deviance-based residuals raises the question of why the Pearson goodness-of-fit statistic often has more nearly a chi-squared distribution than does the residual deviance. Some explanation and numerical results for this comparison are provided, including the suggestion that the residual deviance should provide a better basis for goodness-of-fit tests than the Pearson statistic, in spite of common assertions to the contrary."], ["Outliers and Residual Distributions in Logistic Regression", "The general intent of this article is to point out that direct application of linear regression techniques to logistic regression does not necessarily produce useful diagnostic tools. Thus careful consideration of each diagnostic technique is necessary. In addition, logistic regression might benefit from views and methods different from those applied to problems with continuous errors due to the 0\u20131 nature of the data."], ["Performance of Some Resistant Rules for Outlier Labeling", null], ["The Maximum Familywise Error Rate of Fisher's Least Significant Difference Test", null], ["Jackknifing and Bootstrapping Goodness-of-Fit Statistics in Sparse Multinomials", "In this article the use of nonparametric techniques to estimate these variances is examined. Simulations indicate (and heuristic arguments support) that although the bootstrap (Efron 1979) does not lead to a consistent variance estimate, the parametric bootstrap, the jackknife (Miller 1974) and a \u201ccategorical jackknife\u201d (in which cells are deleted rather than observations) each leads to a consistent estimate. Simulations indicate that the jackknife is the nonparametric estimator of choice, and it is superior to the usual asymptotic formula for sparse data. Although these comparisons are based on the unconditional variance of the statistics, it is shown that the unconditional variance and the variance conditional on fitted parameter estimates are asymptotically equal if the underlying probability vector is from the general exponential family. Simulations also indicate that the jackknife estimate of variance is the estimator of choice in general parametric models for multinomial data."], ["A Lower Confidence Bound on the Probability of a Correct Selection", null], [null, null], ["Asymptotically Chi-Squared Distributed Tests of Normality for Type II Censored Samples", null], ["Bootstrapping the Kaplan\u2014Meier Estimator", null], ["Estimating a Distribution Function Based on Nomination Sampling", null], ["Improved Estimation in Lognormal Models", null], ["The Gini Coefficient and Poverty Indexes: Some Reconciliations", null], ["Empirical Bayes Estimation in Finite Population Sampling", null], ["Outlier Robust Finite Population Estimation", "The article also contains some results from a comparative empirical study of the proposed robust estimator (Sec. 3). This study indicates that the use of this estimator leads to substantial gains over both conventional design-unbiased and \u201cstandard\u201d kernel model-based estimation strategies in a population with a significant number of outliers."], ["Completeness and Unbiased Estimation for Sum\u2014Quota Sampling", null], ["Estimators Based on Several Stratified Samples with Applications to Multiple Frame Surveys", "In this article, a new method for producing estimates from a multiple frame survey is presented. The discussion in this article is restricted to sample designs in which a stratified simple random sample is selected independently from each frame. The estimation technique outlined, however, can be applied to more complex sample designs. It is assumed that units selected in more than one sample can be identified. This estimation technique is based on the fact that a multiple frame sample can be viewed as a special case of selecting two or more samples independently from the same frame. As a result, standard techniques from the literature for estimating from a single frame, such as the Horvitz\u2014Thompson estimator or ratio estimation, can be applied to multiple frame samples. These standard techniques for estimating from a single frame are compared, based on data from a Statistics Canada survey, with the usual estimators suggested in the literature for estimating from a multiple frame sample design. These multiple frame estimators were first proposed by Hartley (1962). They have a common feature of averaging together separate estimates from two or more frames of the domains corresponding to the overlap of these frames. The numerical example given shows that the estimators suggested in this article provide lower variances in certain cases than the Hartley estimators. These estimators are also simpler to compute and extend more easily to three or more frames than the Hartley estimators. One of the single frame estimators considered is the raking ratio estimator. The complexity of its variance formula has discouraged its use in the past. In the numerical example, it is suggested that this problem can be circumvented by the simple technique of numerically linearizing the values of the observations repeatedly."], ["Kernel Regression Estimation Using Repeated Measurements Data", "In our data analysis, we choose the bandwidth that minimizes an estimate of the mean average squared error while taking into account the presence of correlated errors. Using the same data we show that ignoring correlation leads to an oversmoothed kernel estimate. An analytic result illustrates that this phenomenon is not necessarily an anomaly of the data."], ["Response Surface Designs in Flexible Regions", null], ["Incorporating Historical Controls in Testing for a Trend in Proportions", "The use of an exact conditional test is discussed, because for small beta-binomial means the approximations using asymptotic theory are somewhat unrealistic. Problems with the use of normality are illustrated numerically. Tables are given that compare the significance probability for selected outcomes for the exact and asymptotic tests. Also included are values for the Cochran\u2014Armitage test, using no historical information. These tables show that the incorporation of the historical information greatly improves the power of the test for small means of the beta-binomial. The second observation is that the asymptotic test appears to exceed its nominal size. These observations were made using a mean of 1% for the beta-binomial. For large values of the mean, that is, 20%, the tests generally agreed. Thus the incorporation of historical data in general and the exact conditional test in particular is especially important where the spontaneous rates are small."], ["A New ARMA Spectral Estimator", null], ["Book Reviews", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["An Analysis of Contaminated Well Water and Health Effects in Woburn, Massachusetts", "In 1979, two of the eight municipal wells servicing Woburn, Massachusetts, were discovered to be contaminated with several chlorinated organics. Shortly afterwards, the town was found to have an elevated rate of childhood leukemia. Using recent information about the space\u2014time distribution of water from the two contaminated wells, we find positive statistical associations between access to this water and the incidence rates of childhood leukemia, perinatal deaths (1970\u20131982), two of five categories of congenital anomalies, and two of nine categories of childhood disorders. We find no associations with spontaneous abortions, low birth weight, or the other categories of congenital anomalies and childhood disorders. This article discussed these results and other features of the data relevant to their interpretation."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["The Decomposition of Time-Varying Hazard into Phases, Each Incorporating a Separate Stream of Concomitant Information", "The hazard function of time-related events, such as death or reoperation following heart valve replacement, often is time-varying in a structured fashion, as is the influence of risk factors associated with the events. A completely parametric system is presented for the decomposition of time-varying patterns of risk into additive, overlapping phases, descriptively labeled as early, constant-hazard, and late. Each phase is shaped by a different generic function of time constituting a family of nested equations and is scaled by a separate logit-linear or log-linear function of concomitant information. Model building uses maximum likelihood estimation. The resulting parametric equations permit hazard function, survivorship function, and probability estimates and their confidence limits to be portrayed and adjusted for concomitant information. These provide a comprehensive analysis of time-related events from which inferences may be drawn to improve, for example, the management of patients with valvar heart disease."], ["Kriging Nonstationary Data", "Spatial data modeled to have come from a random function with a nonstationary mean are considered. The spatial prediction method known as kriging exploits second-order spatial correlation structure to obtain minimum variance predictions of certain average values of the random function. But to do so, it must be assumed that either the mean function (the drift) is known up to a constant or the second-order structure (the variogram) is known exactly. Knowledge of the drift allows the (stationary) variogram to be estimated and leads to ordinary kriging. Knowledge of the variogram allows the drift to be estimated and leads to universal kriging. More usually, neither is known. This article shows how median polish of gridded spatial data provides a resistant and relatively bias-free way of kriging in the presence of drift, yet yields results as good as the mathematically optimal (but operationally difficult) universal kriging. Comparisons are performed on two data sets."], ["Alternative Models for the Heterogeneity of Mortality Risks among the Aged", "To develop a model to estimate the degree of unobserved heterogeneity in morality risks in a population, it is necessary to specify two types of functions, one describing the age-specific rate of increase of mortality risks for individuals and the other describing the distribution of mortality risks across individuals. There has been considerable interest in the question of how sensitive the estimates of heterogeneity are to the choices of these functions. To explore this question, high-quality data were obtained from published Medicare mortality rates for the period 1968\u20131978 for analysis of total mortality among the aged. In addition, national vital statistics data for the period 1950\u20131977 were used to analyze adult lung cancer mortality. For these data, the estimates of structural parameters were less sensitive to reasonable choices of the heterogeneity distribution (gamma vs. inverse Gaussian) than to reasonable choices of the hazard rate function (Gompertz vs. Weibull)."], ["Birth Forecasting Based on Birth Order Probabilities, with Application to U.S. Data", "A model for birth forecasting based on prediction of the so-called \u201cbirth order probabilities\u201d is constructed. The relation between this model and recent models of fertility prediction is derived. Birth forecasts with approximate probability limits for the U.S. for the period 1983\u20131997 are generated. The performance of the proposed model in predicting future fertility is tested by fitting time series models to part of the available series (1917\u20131982) and ultimately generating birth forecasts for the remainder of the period, then comparing these forecasts with the actual data. Finally, the accuracy and precision of the birth forecasts in this study are compared with those of similar studies."], ["Estimation of Finite Population Properties When Sampling is without Replacement and Proportional to Magnitude", null], ["A Bayesian Procedure for Imputing Missing Values in Sample Surveys", "A new method is proposed for imputation of missing values in sample survey data. The procedure uses standard statistical methodology, permits a general specification of the nonresponse process, and does not impose specific model assumptions. Prior information from past similar surveys or from other sources may be incorporated in a routine manner."], ["Testing for Block Effects in Regression Models Based on Survey Data", "This article considers the problem of testing for intrablock or intracluster correlation in regression disturbances that may occur when cluster or two-stage sampling data is used in regression analysis. It points out that the one-sided Lagrange multiplier test is locally best invariant. An empirical power comparison suggests that if the block structure is known this test should be used. Otherwise the Durbin\u2014Watson test provides a useful test, especially in large samples."], ["Multiplicative Errors-in-Variables Models with Applications to Recent Data Released by the U.S. Department of Energy", null], ["Best Invariant Unbiased Estimators for the Mean Squared Error of Variance Component Estimators", "An unbiased estimator is derived for the mean squared error of any unbiased or biased estimator that is expressible as a linear combination of independent sums of squares. Further, it is shown that, for the classical balanced variance component models, this estimator is the best invariant unbiased estimator for the variance of the ANOVA estimator and for the mean squared error of the nonnegative minimum biased estimator. As an example, the balanced one-way classification model with random effects is considered."], ["The Existence of Asymptotically Unbiased Nonnegative Quadratic Estimates of Variance Components in ANOVA Models", "Conditions for the existence of asymptotically unbiased, nonnegative quadratic estimates of individual variance components in general ANOVA models are investigated. A simple necessary condition for general models is obtained. Necessary and sufficient conditions are obtained in the case of random, balanced nested classification models and the random unbalanced one-way model. The investigation demonstrates the need for exercising caution when employing nonnegative quadratic estimates of individual variance components in practice."], ["Improved Estimators for Ratios of Variance Components", "The problem of estimating a ratio of variance components in the balanced one-way random effects model is considered. It is shown that in terms of mean squared error, the ML, REML (or truncated ANOVA), and Bayes modal estimators (using the noninformative prior) are inadmissible. An estimator that dominates all three is derived. Two other estimators that are adaptive in nature are also introduced. The new estimators are shown to possess much-improved mean squared error properties. The results easily extend to balanced higher-way random or mixed effects models."], ["A Simple and Asymptotically Optimal Test for the Equality of Normal Populations: A Pragmatic Approach to One-Way Classification", "Snedecor and Cochran (1967, p. 324) observed that an application of different treatments to otherwise homogeneous experimental units often results in groups that are different not only in means but also in variances. The usual one-way classification procedure assumes a priori the homogeneity of variances among different groups and tests the equality of means only. Thus motivated by the problem of simultaneously testing the equality of means and the equality of variances of several normal populations, I suggest in this article a simple and an asymptotically optimal test. The suggested test is based on the combination of two independent tests, namely (a) the test for the equality of means given that all variances are equal but unspecified and (b) the test for the equality of variances when all means, not necessarily all equal, are unspecified. Tests (a) and (b) are combined by the Fisher method (1950, p. 99). Littell and Folks (1973) showed that the Fisher method of combining two or more independent tests is at least as good as any other optimal method. Using this fact and the results of Hsieh (1979) on the Bahadur optimality of the individual tests, the suggested test was found to be asymptotically optimal (details can be obtained from the author on request) in the sense of Bahadur (1960) efficiency. Further, the test can be applied easily using existing statistical tables. Examples where this test can be applied include the analysis of repeated measurements, the profile analysis of several groups, and, in general, the analysis of one-way classified experiments."], ["Confidence Bands for Percentiles in the Linear Regression Model", "Simultaneous confidence intervals for percentiles of the normal regression model similar to those given by Steinhorst and Bowden (1971) are considered. Kanofsky's (1968) confidence band for a single normal distribution is modified and extended to the regression model. The confidence bands that are simultaneous in all percentiles provide corresponding confidence bands for the cumulative conditional normal distribution functions. The various procedures are compared with respect to bandwidth."], ["Double Exponential Families and Their Use in Generalized Linear Regression", null], ["Regression Analysis with Censored Autocorrelated Data", "For many studies in which data are collected sequentially in time, the sensitivity of the measurement is limited and an exact value can be recorded only if it falls within a specified range. This gives rise to a censored time series. In this article, we present a methodology for regression analysis of censored time series data. We fit autoregressive models to account for the time dependence. Two numerical methods for full likelihood estimation and an approximate method are discussed. The methods are illustrated with air pollution data subject to lower limits of detection."], ["Efficiencies of Weighted Averages in Stationary Autoregressive Processes", "The criterion of second-order efficiency is used to distinguish among estimators, which have the same asymptotic variance, of the mean of a stationary autoregressive process. The best linear unbiased estimator is typically unknown, since it depends on the parameters of the process. It is demonstrated by second-order efficiency that the sample mean performs poorly under certain conditions, whereas some weighted averages maintain a more consistent performance as the parameters of the underlying process are allowed to vary. Numerical examples are shown for second-and third-order autoregressive processes."], ["Revisions in ARIMA Signal Extraction", "The problem of decomposing an observed series, assumed to follow an ARIMA process, into signal plus noise is considered. It is well known that the preliminary estimates of the signal will be subject to revisions as more data become available. For a general ARIMA process, the revision in the concurrent estimate of the signal is seen to follow a stationary ARMA process, easily derived from the overall series model. The results are extended to non-concurrent preliminary estimates. Finally, it is found that, except for a scale factor, the revisions are the same for all admissible decompositions and the canonical decomposition maximizes the variance of the revision."], ["Monitoring and Adaptation in Bayesian Forecasting Models", "Practical aspects of a new technique for monitoring and controlling the predictive performance of Bayesian forecasting models are discussed. The basic features of the approach to model monitoring introduced in a general setting in West (1986) are described and extended to a wide class of dynamic, nonnormal, and nonlinear Bayesian forecasting models. An associated method of automatically detecting and rejecting outliers and adapting models to abrupt structural changes in the time series is also discussed. The resulting forecast monitoring and control scheme is simply constructed and applied and is illustrated in two applications."], ["Estimation, Prediction, and Interpolation for ARIMA Models with Missing Data", "We show how to define and then compute efficiently the marginal likelihood of an ARIMA model with missing observations. The computation is carried out by using the univariate version of the modified Kalman filter introduced by Ansley and Kohn (1985a), which allows a partially diffuse initial state vector. We also show how to predict and interpolate missing observations and obtain the mean squared error of the estimate."], ["Estimation and Identification of Space-Time ARMAX Models in the Presence of Missing Data", "A method for modeling and fitting multivariate spatial time series data based on current spatial methodology coupled with the parameterization of the ARMAX model is presented. Because of the physical constraints imposed on multivariate data collection in both space and time, the estimation and identification procedures tolerate general patterns of missing or incomplete data."], ["Grouping and Association in Contingency Tables: An Exploratory Canonical Correlation Approach", "The criteria of homogeneity and structure were proposed by Goodman (1981a) for determining whether certain rows or columns of a contingency table should be grouped. A data-based procedure (using the canonical form of bivariate distributions) is presented in this article to guide exploratory analysis to determine which rows or columns of a table may be grouped. This procedure facilitates the application of the homogeneity criterion. Relationships between the proposed method of grouping and the structural criterion are discussed as well as simultaneous inference for grouped tables. The grouping method is extended to multiway tables. The use of canonical forms as a model exploratory tool is addressed. Examples are discussed in detail."], ["Canonical Analysis of Contingency Tables by Maximum Likelihood", "Canonical analysis has often been employed instead of log-linear models to analyze the relationship of two polytomous random variables; however, until the last few years, analysis has been informal. In this article, models are examined that place nontrivial restrictions on the values of the canonical parameters so that a parsimonious description of association is obtained. Maximum likelihood is used to obtain parameter estimates for these restricted models. Approximate confidence intervals are derived for parameters, and chi-squared tests are used to check adequacy of models. The resulting models may be used to determine the appropriateness of latent-class analysis or to determine whether a set of canonical scores has specified patterns. Results are illustrated through analysis of two tables previously analyzed in the statistical literature. Comparisons are made with alternate methods of analysis based on a log-linear parameterization of cell probabilities. It is shown that canonical analysis, which uses interpretations based on regression and correlation, is an alternative to log-linear parameterizations interpreted in terms of cross-product ratios."], ["The Effect of Sample Design on Principal Component Analysis", "Most sample surveys are multivariate and many lend themselves to multivariate methods of analysis. The most usual mode of such analysis is a standard statistical package, such as BMDP or SPSS, in which the multivariate analyses are based on the underlying assumption that the data are generated as independent observations from a common probability distribution. This assumption ignores the sample selection procedure involved in the survey, which leads to the following basic questions. What effects can the sample design have on methods of multivariate analysis? How should such effects be taken into account? This article considers the case of principal component analysis and, in particular, the point estimation of the eigenvalues and eigenvectors of a covariance matrix. It is assumed that the selection of the sample depends on the population values of auxiliary variables as, for example, in stratified sampling. The conventional estimators, based on the assumption of simple random sampling, are compared with alternative probability-weighted and maximum likelihood estimators. Under a multivariate normal model, simple expressions are presented for the approximate model bias of the different estimators. The validity of these results is assessed in a simulation study involving a disproportionate stratified design."], ["Multivariate Two-Sample Tests Based on Nearest Neighbors", null], ["Experimental Designs for Estimating the Correlation between Two Destructively Tested Variables", "A specimen can have two or more important strength properties. These could be its bending strength and tensile strength. Assuming joint normality, we study experimental designs for estimating the correlation coefficient. The specimen is proof loaded\u2014say, in bending. If it does not fail, it is loaded in tension to failure. Optimal proof loads are determined. Large sample properties of the maximum likelihood estimator are also given."], ["First-Order Deletion Designs and the Construction of Efficient Nearly Orthogonal Factorial Designs in Small Blocks", null], ["Locally Optimal Tests for Multiparameter Hypotheses", null], ["Modified Sequentially Rejective Multiple Test Procedures", null], ["Markov Graphs", "Log-linear statistical models are used to characterize random graphs with general dependence structure and with Markov dependence. Sufficient statistics for Markov graphs are shown to be given by counts of various triangles and stars. In particular, we show under which assumptions the triad counts are sufficient statistics. We discuss inference methodology for some simple Markov graphs."], ["Approximate Binomial Confidence Limits", null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Damned Liars and Expert Witnesses", "Until recently the applications of inferential statistics in legal proceedings have been minor and limited. With the advent of civil rights legislation, however, the courts have embraced statistical inference with enthusiasm. The needs of the courts are not well matched with the usual practice of statistics, and this mismatch has serious adverse consequences for both fields. The various sources of difficulty are outlined, and tentative proposals for their amelioration are put forward."], ["Statisticians, Econometricians, and Adversary Proceedings", "Statisticians acting as expert witnesses encounter special problems. These include (a) exposition of underlying methods and concepts, (b) the avoidance of data mining when attorneys wish to leave no stone unturned, and (c) the explanation of hypothesis testing versus prediction. Data management practices are particularly important. Appropriate and inappropriate forms of criticism are discussed, as is the problem of maintaining objectivity. Examples drawn from actual experience are given."], ["A Review and Evaluation of the Housing Unit Method of Population Estimation", "The housing unit (HU) method is used by public and private agencies throughout the United States to make local population estimates. This article describes many of the different types of data and techniques that can be used in applying the HU method, and it discusses the strengths and weaknesses of each. Empirical evidence from four different states is provided, comparing the accuracy of HU population estimates with the accuracy of other commonly used estimation techniques. Several conclusions are drawn regarding the usefulness of the HU method for local population estimation."], ["Child Health, Breast-Feeding, and Survival in Malaysia: A Random-Effects Logit Approach", "Evidence from clinical studies suggests that children who are healthier at birth are more likely to be breast-fed, raising the possibility of selection bias in the estimation of the effect of breast-feeding itself on subsequent survival. Several maximum likelihood models are estimated to assess the severity of selection bias. A child's health enters the models as an unobserved variable within the framework developed by Heckman and Singer. Birth weight is treated as an indicator for unmeasured variables; that is, the \u201cmixing distribution\u201d for the unobservables shifts with the level of birth weight. Results indicate that the direct influence of breast-feeding on survival remains of overwhelming importance even after corrections for selection bias are made."], ["Semiparametric Estimates of the Relation between Weather and Electricity Sales", "A nonlinear relationship between electricity sales and temperature is estimated using a semiparametric regression procedure that easily allows linear transformations of the data. This accommodates introduction of covariates, timing adjustments due to the actual billing schedules, and serial correlation. The procedure is an extension of smoothing splines with the smoothness parameter estimated from minimization of the generalized cross-validation criterion introduced by Craven and Wahba (1979). Estimates are presented for residential sales for four electric utilities and are compared with models that represent the weather using only heating and cooling degree days or with piecewise linear splines."], ["Binary Regression Using an Extended Beta-Binomial Distribution, with Discussion of Correlation Induced by Covariate Measurement Errors", "The beta-binomial distribution is extended to allow negative correlations among binary variates within an experimental unit. Regression models are proposed for both the binary variate response rate and for the pairwise correlation between binary variates, and corresponding likelihood estimation procedures are described. Binary regression problems are also considered, in which measurement errors in the regression variables represent the sole source of overdispersion. Some corresponding response rate estimation procedures are described and illustrated."], ["Nonparametric Prevalence and Mortality Estimators for Animal Experiments with Incomplete Cause-of-Death Data", "Age-specific prevalence and mortality estimators are important descriptors of disease development and the subsequent effects of a disease on longevity. Nonparametric maximum likelihood estimators for the prevalence and mortality functions are available for the case in which incidental and fatal occurrences of a disease can always be distinguished. This article generalizes these methods to allow the role of a disease in causing death to be uncertain for a subset of the animals dying with the disease. The proposed analysis makes no assumptions about the degree of disease lethality. Data from sacrificed animals can be incorporated easily, although sacrifices are not necessary if a certain representativeness assumption holds. No restrictions are imposed on the distribution of survival times, but the prevalence function is held constant over time intervals for stabilization purposes. Variances are estimated by inverting the observed information matrix. An EM algorithm simplifies the analysis when the prevalence function is constrained to be monotone or when cause of death is classified into ordered categories, according to how likely it was that the disease was responsible for death. The proposed methods are illustrated with some data on nonrenal vascular disease in female RFM mice."], ["Some Coverage Error Models for Census Data", "Alternative models are presented for representing coverage error in surveys and censuses of human populations. The models are related to the capture\u2014recapture models used in wildlife applications and to the dual-system models employed in the vital events literature. Estimation methodologies are discussed for one of the coverage error models. The theoretical foundations of the methodology are developed and distinctions are made between two kinds of error: (a) sampling error and (b) error associated with the model. An example involving data from the 1980 U.S. census is presented. The problem of adjusting census and survey data for coverage error is also discussed."], ["Capture\u2014Recapture Models When Both Sources Have Clustered Observations", "Capture\u2014recapture models assume that individuals in the population are captured one at a time and independently of each other. There are often situations, however, where individuals are captured in small clusters or groups. This article provides a model that allows individuals to be captured in groups; the EM algorithm is used to estimate parameters in the model that include capture probabilities and the size of the population under study."], ["Causal Models for Patterns of Nonresponse", "The problem of missing data for categorical variables is examined from the perspective of modeling the mechanisms of nonresponse. Log-linear causal models, as formulated by Goodman, are studied for the relationship of the survey variables to response; under some conditions several such models are estimable from the observed data. For nested patterns of nonresponse, a specific causal model represents exactly the assumption of ignorable response. Most causal models, however, imply nonignorable response mechanisms and yield alternative estimates for the distribution of the survey variables."], ["Multiple Imputation for Interval Estimation from Simple Random Samples with Ignorable Nonresponse", null], ["A Latent Markov Model Approach to the Estimation of Response Errors in Multiwave Panel Data", "When direct measures of response error are not available in multiwave panel data, latent Markov chain models can provide a useful framework for analysis of response errors. These models permit the estimation of real change and response error even when the observations are widely spaced. This methodology is illustrated with Social Security Administration panel data from the years 1971, 1972, and 1974 (see Frohlich 1975 and Social Security Administration 1979a,b) on self-reported disability status. The model shows much less real change than that indicated from inspection of raw turnover tables. Direct estimates of measurement error from a later survey (1978) give a proximate confirmation of the model results."], ["Response Bias and Reliability in Sensitive Topic Surveys", "Estimates of survey response bias and reliability are presented for six topics: receipt of welfare, income, alcohol use, drug use, criminal history, and embarrassing medical conditions. The estimates are derived from published full-design criterion validity studies. The common assumption that these characteristics are underreported is, in part, based on partial validity studies; the bias in estimating response parameters using partial designs is demonstrated. Evidence from the full-design studies suggests that the response biases for these topics center near zero but that the responses are unreliable or noisy. Implications for survey design and methodological research are considered."], ["Response Errors in the Measurement of Time Use", "The relation between (a) length of the recall period and (b) validity and quality of data on the use of time is examined. All of the data are based on 24-hour diary constructions or reconstructions. We find that recall periods of one day are essentially equivalent, in terms of validity and quality, to recall periods of up to one week for time diaries of Fridays, Saturdays, or Sundays, but that one-day recall is significantly better than more extended recall for Monday\u2014Thursday diaries. These findings suggest that a sample design using randomly designated rather than convenient diary dates would be of acceptable quality in the United States. We also find that technical change in telephone interviewing techniques significantly improves the quality of current data over data collected in the mid-1970s, and that longitudinal panel data have significantly higher quality than cross-sectional data."], ["The Quality of Survey Data as Related to Age of Respondent", "This research examines how the quality of survey measures varies with the age of the respondent. A structural modeling approach is applied to data from six surveys (five national surveys of American adults and one survey in a Canadian corporation\u20147,706 respondents in all) to generate measurement quality estimates for 106 survey measures. Results show that as respondent age increases (a) the percentage of true score variance in survey measures tends to decline, (b) the percentage of both random and correlated error variance tends to increase, and (c) people tend to have more interrelated\u2014or less differentiated\u2014views about their worlds. Most of these trends are highly replicable from survey to survey and are\u2014except for the level of differentiation in views\u2014not attributable to older respondents tending to have less education than younger people. The results suggest that data from older respondents tend to provide a somewhat less precise indication of the attitudes, behaviors, or other characteristics being measured than do data from younger respondents, but that relationships among survey measures will not necessarily be weaker."], ["Reporting Bias and Sampling Errors in a Survey of a Rare Population Using Multiplicity Counting Rules", "This article examines a methodology to improve the efficiency of surveys to locate and estimate characteristics of rare populations. Two multiplicity counting rules are compared with a conventional rule in terms of reporting errors, sample variances, and survey yields. The analysis is based on data from two survey field experiments with diagnosed cancer patients and a subsample of their siblings or children who reside outside their households. The article concludes with a brief discussion of the circumstances in which a multiplicity counting rule is the preferred methodology and those in which other approaches should be pursued."], [null, "Saddlepoint approximations are extended to general statistics. The technique is applied to derive approximations to the density of linear combinations of order statistics, including trimmed means. A comparison with exact results shows the accuracy of these approximations even in very small sample sizes."], ["Comparisons of Alternative Predictors under the Balanced One-Way Random Model", "Prediction of an arbitrary linear combination of the random effects of a balanced one-way random model is investigated. Alternative two-stage predictors are compared on the basis of their conditional (on the random effects) and unconditional bias and mean squared errors. When the true value of the ratio of expected mean squares is known, there exists a best linear unbiased predictor (BLUP). When the true value is unknown, a two-stage predictor, obtained from the BLUP by replacing the true value with an estimated value, can be used. When the ratio of expected mean squares is estimated by maximum likelihood, Bayesian methods, or various related methods, a two-stage predictor is obtained whose properties compare favorably with, for example, those of the least squares predictor and the positive-part James\u2014Stein predictor."], ["Combining Minimax Shrinkage Estimators", "When one estimates a multivariate normal mean, the use of Stein estimation entails both the grouping of coordinates and the selection of a set of targets toward which to shrink each group. In this article we propose new minimax multiple shrinkage estimators that allow for multiple specifications of these aspects. We provide examples that are evaluated on real and simulated data, including an estimator that adaptively resolves the issue of combining possibly related estimation problems and an adaptive clustering shrinkage estimator. The construction and properties of these estimators are shown to follow from the application of the multiple shrinkage results of George (1986a) to general partitioned shrinkage estimators."], ["Bayesian Estimation and Prediction Using Asymmetric Loss Functions", "Estimators and predictors that are optimal relative to Varian's asymmetric LINEX loss function are derived for a number of well-known models. Their risk functions and Bayes risks are derived and compared with those of usual estimators and predictors. It is shown that some usual estimators, for example, a scalar sample mean or a scalar least squares regression coefficient estimator, are inadmissible relative to asymmetric LINEX loss by providing alternative estimators that dominate them uniformly in terms of risk."], ["Aggregate Data, Ecological Regression, and Voting Transitions", "Voting data typically comprise the marginal distributions of votes cast at two successive elections. The fact that these obtain separately for many voting precincts or areas enables one to estimate the actual transition probabilities for movements between the options available to the voter at each election. An aggregated compound multinomial model is proposed. This allows log-linear dependence on various covariates and specifies a simple and illuminating structure of random effects. The information in such aggregated data is compared with that which would obtain had all of the transitions been observed. The model is applicable to many other types of aggregated data and meets many of the difficulties inherent in \u201cecological regression.\u201d It is illustrated with an analysis of the British European election of 1984."], ["How Biased is the Apparent Error Rate of a Prediction Rule?", null], ["Judging Inference Adequacy in Logistic Regression", "Inference for logistic regression based on the information matrix may be poor. This is noted in two examples in which confidence regions are examined. A measure to detect such inadequacies is presented; it judges the quadratic approximation to the likelihood surface, which justifies the usual procedure."], ["Maximum Likelihood Methods for Log-Linear Models When Expected Frequencies are Subject to Linear Constraints", "Hypotheses relating to log-linear models may be defined in terms of constraints on the frequencies or on the logarithms of the frequencies. A two-step algorithm is developed to allow fitting models containing constraints of both types. This algorithm is shown to yield maximum likelihood estimates of the log-linear parameters. Three examples are presented to illustrate cases in which these models are of interest."], ["Goodness-of-Fit Tests for Log-Linear Models in Sparse Contingency Tables", "The asymptotic normality of the likelihood ratio goodness-of-fit statistic is demonstrated for testing the fit of log-linear models with closed form maximum likelihood estimates in sparse contingency tables. Unlike the traditional chi-squared theory, the number of categories in the table increases as the sample size increases, but not all of the expected frequencies are required to become large. Some results of a small Monte Carlo study are presented. The traditional chi-squared approximation is reasonably accurate for the Pearson statistic for many sparse tables, but cases are presented for which it fails. The normal approximation can be much more accurate than the chi-squared approximation for the likelihood ratio statistic, but the bias of estimated moments is a potential problem for very sparse tables."], ["Variable Selection in Heteroscedastic Discriminant Analysis", null], ["Diagnostic Plots for Missing Data in Least Squares Regression", null], ["A Bounds Test of Equality between Sets of Coefficients in Two Linear Regressions When Disturbance Variances are Unequal", null], ["A Note on Smoothness Priors and Nonlinear Regression", "A generalization of a nonparametric estimator proposed by Shiller (1984) is considered. The estimator is shown to be directly related to penalized least squares estimation and spline smoothing. Some simplifications concerning the computation of Shiller's estimator and corresponding posterior covariances are indicated."], ["Robust Confidence Intervals for a Location Parameter: The Configural Approach", null], ["Union-Intersection Rank Tests for Ordered Alternatives in ANOCOVA", "For an analysis of covariance model (pertaining to completely randomized as well as complete block designs) based on the union-intersection principle, a general class of permutationally distribution-free rank tests for the ordered alternative problem is considered. The theory is supplemented by numerical studies."], ["A Method for Testing the Independence of Two Time Series That Accounts for a Potential Pattern in the Cross-Correlation Function", null], ["Testing for Deterministic Linear Trend in Time Series", "Most powerful tests are derived for the hypothesis that deterministic linear trend occurs in time series against the alternative that trend consists of random walk subject to drift. Comparisons of these tests and of the tests suggested by LaMotte and McWhorter (1978) are made in terms of exact powers and Pitman asymptotic relative efficiencies."], ["Combining One-Sided Binomial Tests", null], ["The Choice of Subsample Size in Two-Stage Sampling", null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Boundaries of Statistics\u2014 Sharp or Fuzzy?", null], ["Report by the Outgoing Editors", null], ["Disclosure-Limited Data Dissemination", "Statistical agencies use a variety of disclosure control policies with ad hoc justification in disseminating data. The issues involved are clarified here by showing that several of these policies are special cases of a general disclosure-limiting (DL) approach based on predictive distributions and uncertainty functions. A user's information posture regarding a target is represented by one predictive distribution before data release and another predictive distribution after data release. A user's lack of knowledge about the target at any time is measured by an uncertainty function applied either to the current predictive distribution or to the current predictive distribution and the previously held predictive distribution. Common disclosure control policies, such as requiring released cell relative frequencies to be bounded away from both zero and one, are shown to be equivalent to disclosure rules that allow data release only if specific uncertainty functions at particular predictive distributions exceed a limit. Data transformations, such as aggregation and cell suppression, that are intended to reduce the extent of disclosure are analyzed in simple but realistic scenarios."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Alternative Methods for CPS Income Imputation", "The U.S. Bureau of the Census imputes missing income items in the income supplement of the Current Population Survey (CPS) by a technique commonly known as the CPS hot deck. This article compares CPS hot deck imputations of wages and salary amounts with alternatives based on regression models for the logarithm of wages and salary and for the wage rate. Comparisons are effected by comparing imputations with an Internal Revenue Service (IRS) wages and salary amount found by an exact match of CPS data to IRS records. Although limitations in the matching and in the comparison variable preclude a definitive conclusion, we find that (a) the CPS hot deck does not underestimate income aggregates to any serious extent; (b) model-based alternatives have slightly smaller mean absolute error than the hot deck, when comparable data bases of respondents are used to carry out imputations; and (c) multivariate models for imputing recipiency, weeks and hours worked, and earnings need to be developed to provide realistic competitors to the current hot deck method."], ["Estimating Gross Flows Using Panel Data with Nonresponse: An Example from the Canadian Labour Force Survey", "This article considers the problem of using categorical data from a panel survey in which there is nonrandom nonresponse to estimate gross flows. The methods are illustrated for the case of estimating gross flows in labor force participation using data from the Canadian Labour Force Survey. Three models are proposed that allow nonresponse to be related to employment classification, time, or both employment classification and time. Maximum likelihood estimation is used to fit the models to a single panel of Labour Force Survey data."], ["Parameterized Multistate Population Dynamics and Projections", "This article reports progress on the development of a population projection process that emphasizes model selection over demographic accounting. Transparent multiregional/multistate population projections that rely on parameterized model schedules are illustrated, together with simple techniques that extrapolate the recent trends exhibited by the parameters of such schedules. The parameterized schedules condense the amount of demographic information, expressing it in a language and variables that are more readily understood by the users of the projections. In addition, they permit a concise specification of the expected temporal patterns of variation among these variables, and they allow a disaggregated focus on demographic change that otherwise would not be feasible."], ["Improving the Accuracy of Intercensal Estimates and Postcensal Projections of the Civilian Noninstitutional Population", "Estimates of the civilian noninstitutional population define the sample frame of most major national sample surveys and are used as denominators in the computation of prevalence rates for various sociodemographic phenomena, such as labor force participation and school enrollment. Current U.S. Bureau of the Census methods for making intercensal estimates and postcensal projections of the civilian noninstitutional population rest on the assumption that the age-, sex-, and race-specific proportions of the population that are institutionalized\u2014as estimated by the last census\u2014remain constant until the next census. This article examines the empirical validity of this assumption by using data from the decennial censuses for 1940\u20131980 and, in light of substantial decade to decade changes in the age patterns of the institutional proportions for sex- and race-specific populations, seeks to develop alternative methods. To pursue the latter objective, parametric curves are fit to the age-specific institutional proportions for each population for each decade. A study of the observed historical variation in the parameters of these curves then leads to some suggestions about how their shapes can be estimated between censuses and projected beyond the latest available census to provide more accurate estimates and projections of the civilian noninstitutional population."], ["Adjusting for Errors in Classification and Measurement in the Analysis of Partly and Purely Categorical Data", "This article examines a method of adjusting subgroup means when there are errors both in the allocation of elements to the subgroups and in the measurement of the response. The advantages with the method are that it is simple, it is also applicable when there are many subgroups, and it requires no distributional assumptions for the response variable. The method is compared to the maximum likelihood method for a dichotomous response, and an example from the Swedish Level of Living Survey is given."], ["Accurate Approximations for Posterior Moments and Marginal Densities", "This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal."], ["Reliability (and Fault Tree) Analysis Using Expert Opinions", "In this article we introduce a formal procedure for the use of expert opinions in reliability (and fault tree) analysis. We consider the case of multicomponent parallel redundant systems for which there could be a single expert or a group of experts giving us opinions about each component. Inherent in our approach are a procedure for reflecting our judgment of the experts' expertise and our own opinions about the components' life lengths. An important issue here is the induced dependencies between the components' life lengths due to any common knowledge shared by the experts. Our final results are approximations that depend on our having small uncertainty about what the experts have to say. The approximations are easily computable, and they can be generalized to cover any coherent system."], ["A Class of Life Distributions for Aging", null], ["Automatic Smoothing of Regression Functions in Generalized Linear Models", "We consider the penalized likelihood method for estimating nonparametric regression functions in generalized linear models (Nelder and Wedderburn 1972) and present a generalized cross-validation procedure for empirically assessing an appropriate amount of smoothing in these estimates. Asymptotic arguments and numerical simulations are used to show that the generalized cross-validatory procedure preforms well from the point of view of a weighted mean squared error criterion. The methodology adds to the battery of graphical tools for model building and checking within the generalized linear model framework. Included are two examples motivated by medical and horticultural applications."], ["The Conditional Distribution of Goodness-of-Fit Statistics for Discrete Data", null], ["Cross-Validation, the Jackknife, and the Bootstrap: Excess Error Estimation in Forward Logistic Regression", "Given a prediction rule based on a set of patients, what is the probability of incorrectly predicting the outcome of a new patient? Call this probability the true error. An optimistic estimate is the apparent error, or the proportion of incorrect predictions on the original set of patients, and it is the goal of this article to study estimates of the excess error, or the difference between the true and apparent errors. I consider three estimates of the excess error: cross-validation, the jackknife, and the bootstrap. Using simulations and real data, the three estimates for a specific prediction rule are compared. When the prediction rule is allowed to be complicated, overfitting becomes a real danger, and excess error estimation becomes important. The prediction rule chosen here is moderately complicated, involving a variable-selection procedure based on forward logistic regression."], ["The Retransformed Mean after a Fitted Power Transformation", "An approximate method is given to estimate the mean of a dependent variable after a linear model is fitted to the Box\u2014Cox power transformation of this variable. The estimate is accurate except when the transformation power parameter is near zero. The properties of the estimate in the one-sample and regression cases are considered, by both asymptotic calculations and Monte Carlo simulations, and comparisons are made with the smearing estimate (Duan 1983). It is shown that there can be some cost due to estimating the power transformation, as opposed to assuming it is known; however, this cost is not severe."], ["The Prediction Approach to Robust Variance Estimation in Two-Stage Cluster Sampling", "Robust statistics are derived for estimating the error variance when a finite population total or a ratio of totals is estimated from a two-stage cluster sample in which the first-stage sampling fraction is small. The analysis is based on prediction (superpopulation) models that allow for very general correlation structure within clusters. Results are obtained for when all cluster sizes are known and for when sizes are known only for sample clusters. Ratio, regression, and mean-of-ratios statistics are in the class of population total estimators considered."], ["Bartlett's, Cochran's, and Hartley's Tests on Variances are Liberal When the Underlying Distribution is Long-Tailed", null], ["Counting by Weighing: An Approach Using Renewal Theory", "Renewal theory and Bayesian decision theory are used to solve a problem related to counting a large number of items by weighing them. Specifically, a batch is to be obtained containing a given number of items by adding items until their total weight reaches a critical value that can depend on the results of a preliminary sample. Furthermore, the optimal sample size for this preliminary sample is to be determined. The distribution of individual weights is assumed to be normal."], ["Time Series Model Specification in the Presence of Outliers", "Outliers are commonplace in data analysis. Time series analysis is no exception. Noting that the effect of outliers on model identification statistics could be serious, this article is concerned with the problem of time series model specification in the presence of outliers. An iterative procedure is proposed to identify the outliers, to remove their effects, and to specify a tentative model for the underlying process. The procedure is essentially based on the iterative estimation procedure of Chang and Tiao (1983) and the extended sample autocorrelation function (ESACF) model identification method of Tsay and Tiao (1984). An example is given. Properties of the proposed procedure are discussed."], ["Asymptotic Theory of Overparameterized Structural Models", "A theory of overparameterized structural models is presented. In such a model some \u201credundant\u201d parameters are involved; the parameter vector is not identified, and the information matrix is not nonsingular. The minimum discrepancy function (MDF) test statistic is shown to have an asymptotic chi-squared distribution almost everywhere for a wide class of discrepancy functions. Asymptotic distribution properties of the MDF estimators are investigated. The factor analysis model is discussed as an example."], ["Least Squares Regression When the Independent Variable Follows an ARIMA Process", null], ["Robust Estimates for ARMA Models", null], ["The Number of Observed Classes from a Multiple Hypergeometric Distribution", null], [null, null], ["Further Developments on the Robustness of Clevenson\u2014Zidek-Type Means Estimators", "Tsui (1984) showed that many Clevenson\u2014Zidek-type Poisson means estimators are better than the usual estimator under the normalized squared error loss (1.1), even when the underlying distributions are negative binomial. This article shows that this dominance result is still true when the underlying distributions belong to a much larger class of distributions. This class includes all mixtures of Poisson distributions, and hence the negative binomial result is a very special case."], ["Combining Information on Measurement Error in the Errors-in-Variables Model", "For the regression model in which an explanatory variable contains measurement error, the instrumental variable and the correction for attenuation estimators of slope are considered. Although each requires extra information that is rarely available in ideal form, examinations of asymptotic variances and simulation results suggest that the correction for attenuation performs as well as a fairly strong instrumental variable when the measurement errors are not too large, and this is often true even when the extra information acquired is rough. For larger measurement errors the correction method is of less value. Whenever both kinds of information are available, an estimator with smaller variance is possible by taking a weighted average of the two."], ["Confidence Bounds for Normal Means under Order Restrictions, with Application to Dose-Response Curves, Toxicology Experiments, and Low-Dose Extrapolation", null], ["An Optimal Prediction Function for the Normal Linear Model", null], ["Likelihood Ratio Tests for a Change in the Multivariate Normal Mean", null], [null, null], ["The Equivalence of Regression-Simple and Best-Linear-Unbiased Estimators with Type II Censored Data from a Location Scale Distribution", null], ["A Kernel-Type Estimator of a Quantile Function from Right-Censored Data", null], ["Minimum Hellinger Distance Estimation for Multivariate Location and Covariance", null], ["The Selection of Terms in an Orthogonal Series Density Estimator", "We show that Kronmal and Tarter's well-known rule for selecting the terms in an orthogonal series density estimator can lead to poor performance and even inconsistency in certain cases. These difficulties arise when the underlying density has a nonmonotone sequence of Fourier coefficients, as is likely to be the case with sharply peaked or multimodal distributions. We suggest a way of overcoming these shortcomings."], ["Mean Integrated Squared Error Sampling", null], ["Testing the Rank of a Matrix with Applications to the Analysis of Interaction in ANOVA", null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"]]}