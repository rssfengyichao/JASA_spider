{"1983": [["Empirical Bayes Estimation of Rates in Longitudinal Studies", "The usually irregular follow-up intervals in epidemiologic studies preclude the use of classical growth curve analysis. For short follow-up intervals, it is suggested that individual rates of change are useful for exploratory analysis. Empirical Bayes estimates of these rates of change are recommended and developed. An example of bone loss with age in women is also given."], ["Prediction When Both Variables are Subject to Error, with Application to Earthquake Magnitudes", "The structural relation between the amplitude of earthquake surface waves and the amplitude of earthquake body waves is estimated. The structural relation is used to construct an equation to predict surface waves from recorded body waves in a second population of earthquakes."], ["Evaluation of the Power of Rerandomization Tests, with Application to Weather Modification Experiments", "Computations of power of rerandomization tests by exact methods are known to be computationally exorbitant. We introduce a much cheaper naive method of evaluating power and show\u2014both by simulation and analytically for a special case\u2014that it very mildly overestimates true power. We further derive a normal approximation to re-randomization distributions of linear statistics and illustrate its closeness to the true distributions. This yields analytical formulas for calculating power approximately at negligible computational cost. The proposed methods therefore eliminate the previously prohibitive cost of calculating power for rerandomization tests and make it practical to evaluate the sample sizes needed for a randomized experiment to be analyzed by rerandomization (see also Gabriel and Hall 1983). Its application to weather modification experiments is illustrated."], ["An Evaluation of Model-Dependent and Probability-Sampling Inferences in Sample Surveys", "In this paper we are concerned with inferences from a sample survey to a finite population. We contrast inferences that are dependent on an assumed model with inferences based on the randomization induced by the sample selection plan. Randomization consistency for finite population estimators is defined and adopted as a requirement of probability sampling. A numerical example is examined to illustrate the dangers in the use of model-dependent estimators even when the model is apparently consonant with the sample data. The paper concludes with a summary of principles that we believe should guide the practitioner of sample surveys of finite populations."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Calibration, Sufficiency, and Domination Considerations for Bayesian Probability Assessors", null], ["The Prevision of a Prevision", null], ["Analysis of Two-Factor Experiments under an Inverse Gaussian Model", "This article treats the analysis of factorial experiments under an inverse Gaussian distribution for the failure times. A reciprocal-linear model for the factor effects is motivated from the context of the underlying Wiener process. Explicit solutions to the likelihood equations are derived, and important properties such as strong consistency and limiting normality are established. A least squares approach using the reciprocals of the sample cell means is also studied and compared with the maximum likelihood method. Other aspects of the investigation include likelihood ratio tests, an analysis-of-reciprocals analogue of the usual normal theory analysis of variance, and confidence intervals for contrasts. An application of the procedures is illustrated with a data set of strength measurements of an insulating material."], ["Rerandomization Inference on Regression and Shift Effects: Computationally Feasible Methods", null], ["An Algorithm for Restricted Least Squares Regression", "A commonly occurring problem in statistics is that of minimizing a least squares expression subject to side constraints. Here a simple iterative algorithm is presented and shown to converge to the desired solution. Several examples are presented, including finding the closest concave (convex) function to a set of points and other general quadratic programming problems. The dual problem to the basic problem is also discussed and a solution for it is given in terms of the algorithm. Finally, extensions to expressions other than least squares are given."], ["A Fast Estimation Method for the Vector Autoregressive Moving Average Model with Exogenous Variables", null], ["Inference about Multivariate Means for a Nonstationary Autoregressive Model", null], ["Comparisons of Tests for the Presence of Random Walk Coefficients in a Simple Linear Model", "The locally most powerful test is derived for the hypothesis that the regression coefficients are constant over time against the alternative that they vary according to the random walk process. When the regression equation contains the constant term only, comparisons are made with the tests suggested by LaMotte and McWhorter (1978). These are based on exact powers and on three different types of asymptotic efficiencies including the classical Pitman and Bahadur approaches and the new one due to Gregory (1980). The concept of the Bahadur efficiency is extended to cover also the random slopes. Suggestions are made for choosing the test."], ["On Truncation of Shrinkage Estimators in Simultaneous Estimation of Normal Means", null], ["Selection of Strata Sample Sizes for the Comparison of Domain Means", null], ["Finite Population Sampling with Multivariate Auxiliary Information", "This article examines strategies that are approximately design-unbiased and nearly optimal, assuming a large-sample survey and a regression superpopulation model. A new class of predictors is proposed to link certain features of optimal design-unbiased and model-unbiased predictors. Generalized regression predictors are shown to pervade the subclass of asymptotically design-unbiased (ADU) predictors. Generalized regression predictors are combined with model-based stratification to construct highly efficient ADU strategies."], ["A Geometric Interpretation of Inferences Based on Ranks in the Linear Model", null], ["Tensor Analysis of ANOVA Decomposition", null], ["Improved Factors for One-Sided Tolerance Limits for Balanced One-Way ANOVA Random Model", null], ["Comparing Scheff\u00e9-Type to Constant-Width Confidence Bounds in Regression", "Conditions are given for when the Scheff\u00e9-type bound has smaller average width than the constant-width bound when coverage probabilities are equal, for bounding a multilinear regression function with intercept over one of the ellipsoidal regions of Halperin and Gurian (1968). The conditions depend on the size of the region relative to the design, and they are shown to hold for the special case of an orthogonal design, when the regressors are restricted to the boundary of the region."], ["Tests, Point Estimations, and Confidence Sets for a Capture-Recapture Model", null], ["Hypothesis Testing for Marginal Probabilities in a 2 \u00d7 2 \u00d7 2 Contingency Table with Conditional Independence", null], [null, null], ["Nonparametric Prediction Intervals for Sample Medians in the General Case", "This article deals with nonparametric prediction intervals for the median(s) of a future sample of arbitrary size without any assumption about the parent distribution. The best possible distribution-free lower bounds for the relevant coverage probabilities are derived. Furthermore, these sharp lower bounds are given in terms of lower tail probabilities of hypergeometric and binomial distributions for which extensive tables and good approximations are available."], ["The Set of Weighted Regression Estimates", "We present in this article several results that characterize the set of all weighted regressions. The set of weighted regression estimates is the set of all generalized least squares estimates for covariance matrices that have positive weights on the diagonal and have zeroes elsewhere. The study of sets of estimates is a method for dealing with distributional uncertainty, which does not require the strong assumption of a particular distribution. We show that the set of weighted regressions is the union of the bounded convex polytopes formed by the hyperplanes on which residuals are zero. For the two-dimensional case, we provide a simple method for outlining the set and give an efficient method for selecting the weighted regressions that determine the convex hull of the set. Finally, we discuss two examples that illustrate the use of these results."], ["Multiple Comparisons by Rerandomization Tests", "Classical methods of simultaneous inference cannot be used with rerandomization tests without risking excessive type I errors. This is illustrated by examples that show some published methods to be invalid for multiple comparisons. However, the use of stepwise procedures is shown to remain valid with rerandomization tests. In particular, this article advocates an adaptation of Begun and Gabriel's (1981) technique and provides the algorithm for using it."], ["Sequential Multiple Comparisons with the Best", null], ["Multiple Comparisons with the Best Treatment", null], ["Robust Trend Tests Derived and Simulated: Analogs of the Welch and Brown-Forsythe Tests", null], ["A Note on Pooling Variances", "The estimation of variance in the normal distribution is studied under statistical inference based on conditional specification. When there are two samples available for estimating the variance and it is not certain whether the two samples are from the same population, the experimenter usually uses a test to resolve the uncertainty. When the test is not significant, the samples are pooled to obtain a pooled estimator; otherwise, the individual sample variance is used. The bias and mean squared error of such a preliminary test estimator are studied. It is shown that the preliminary test estimator has a smaller mean squared error than the usual unbiased estimator when the level of significance for the preliminary test is appropriately chosen."], ["Book Reviews", null], ["Corrigenda", null], ["Editorial Board Page", "This article has no abstract"], ["Determining Participation in Income-Tested Social Programs", "Estimates of the number of participants to expect in income-tested social programs may be made from data on income distributions alone. These simple estimates will be biased if the social program induces incentive effects or if some eligible participants do not pursue their application for benefits. This article brings these issues together and sets out a statistical framework for testing whether the simple estimates are likely to be adequate in practice. The data used for the empirical work come from the Seattle and Denver Income Maintenance Experiments, the largest of several similar experiments thus far undertaken. The results imply that the simple estimates of program participation may be adequate if a program of the experimental type investigated here is implemented nationally."], ["Modeling Time Series with Calendar Variation", "The modeling of time series data that include calendar variation is considered. Autocorrelation, trends, and seasonality are modeled by ARIMA models. Trading day variation and Easter holiday variation are modeled by regression-type models. The overall model is a sum of ARIMA and regression models. Methods of identification, estimation, inference, and diagnostic checking are discussed. The ideas are illustrated through actual examples."], ["Using Sample Survey Weights in Multiple Regression Analyses of Stratified Samples", "The rationale for the use of sample survey weights in a least squares regression analysis is examined with respect to four increasingly general specifications of the population regression model. The appropriateness of the weighted regression estimate depends on which model is chosen. A proposal is made to use the difference between the weighted and unweighted estimates as an aid in choosing the appropriate model and hence the appropriate estimator. When applied to an analysis of the familial and environmental determinants of the educational level attained by a sample of young adults, the methods lead to a revision of the initial additive model in which interaction terms between county unemployment and race, as well as between sex and mother's education, are included."], ["Juries Hearing Death Penalty Cases: Statistical Analysis of a Legal Procedure", "Potential jurors in capital cases are often queried on their attitudes toward capital punishment. The extreme groups say they would never or they would always approve capital punishment, given a guilty verdict. In many jurisdictions, these two groups are routinely excluded from juries deciding whether the defendant is guilty in capital cases. This exclusion persists even when the potential jurors say they could be fair and impartial in deciding guilt or innocence. The current study shows that this exclusion creates a bias that almost certainly works against the defendant."], ["A Method for Comparing Two Hierarchical Clusterings", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Generating Test Data with Independently Controllable Features for Multivariate General Linear Forms", "Experiments to assess the performance characteristics, both of statistical methods and of computer programs that implement them, are best performed using test data with controlled statistical and numerical properties. We describe an algorithm for constructing test data for any multivariate linear model that provides complete control (subject only to the requirement of mutual consistency) over the following factors: regression coefficients; regression and residual sums of squares and products matrices; means, standard deviations, and correlations of the independent and dependent variables, residuals, and predicted values; and canonical correlations or multiple correlation. The algorithm permits aspects of the underlying data structure including high-leverage points, outliers, and the residual distribution, to be controlled by specifying the components of the singular-value decompositions of the independent variables, the dependent variables, or the error space. Some features of the generated test data may be matched to real data, while others are experimentally controlled or randomly generated."], ["Estimating a Finite Population Mean from Unequal Probability Samples", "Two approaches to estimating a finite population mean from unequal probability samples are contrasted. In the prediction approach a model is specified for the population values and is used to predict the nonsampled values. In the generalized regression approach the Horvitz-Thompson estimator of the population mean is modified to allow the introduction of covariate information (S\u00e4rndal 1980). Generalized regression estimates have the desirable property of asymptotic design consistency, which is not always enjoyed by estimates in the prediction class. However, it is suggested that the prediction approach is the more principled method of estimation. If asymptotic design consistency is a desirable property in a given application, then only models that yield asymptotically design-consistent prediction estimates should be used. Two classes of models with this property are suggested, namely fixed and random-effects models that allow a separate intercept for subclasses of the population indexed by the probability of selection. Estimates based on a simple random-effects model perform well in a limited simulation study carried out to illustrate some of the compared estimators."], ["Smearing Estimate: A Nonparametric Retransformation Method", "The smearing estimate is proposed as a nonparametric estimate of the expected response on the untransformed scale after fitting a linear regression model on a transformed scale. The estimate is consistent under mild regularity conditions, and usually attains high efficiency relative to parametric estimates. It can be viewed as a low-premium insurance policy against departures from parametric distributional assumptions. A real-world example of predicting medical expenditures shows that the smearing estimate can outperform parametric estimates even when the parametric assumption is nearly satisfied."], ["User-Oriented Inference", null], ["General Variance Modifications for Linear Bayes Estimators", "We consider the problem of modifying the linear Bayes estimator for the mean of a distribution of unknown form by using an estimate of sample variance. The general conditions under which there is no advantageous variance modification to the linear Bayes rule are identified. These results are applied to the problem of sampling from a normal distribution with unknown mean and variance, and the implications are discussed."], ["Unbiasedness as the Dual of Being Bayes", "In this note we define the notion of unbiasedness for a decision function for an arbitrary loss function. This is a generalization of Lehmann's (1951) definition. We show that this notion of unbiasedness is a dual to the notion of being Bayes; that is, if the role of the random variable and the parameter is interchanged, then unbiasedness is equivalent to being Bayes and vice versa. Some consequences of this fact are discussed."], ["Estimation with Inadequate Information", "As a possible explanation for the existence of absurd UMVU estimators, a class of estimation problems is defined: estimating the probability of an \u201cunobservable\u201d event, for example, estimating the probability of an event of interest occurring over a long period of time from observations over a much shorter period. It is pointed out that in typical cases of this kind, no reasonable unbiased estimator can exist. Consideration of the maximum likelihood estimator suggests the possibility that in fact there may then often exist no reasonable estimators."], ["Multiple Hypergeometric Functions: Probabilistic Interpretations and Statistical Uses", null], ["Exact Confidence Bands for Linear Regression over Intervals", null], [null, null], ["On Distribution-Free Rank Tests for Two-Way Layouts", "A class of distribution-free statistics using scores based on ranks for testing main effects in two-way layouts, with no interactions and more than one observation per cell, is discussed. Asymptotic distributions are developed under the null and alternative hypotheses for the case in which cell size is fixed and the number of blocks is allowed to become large, and as cell size becomes large with the number of blocks fixed. Efficiencies are considered under both asymptotic situations. We show that under the case in which the number of blocks becomes large, it is possible to choose a set of scores that is optimal in the sense of achieving the highest possible asymptotic relative efficiency for a given distribution."], ["Uniformly Minimum Variance Unbiased Estimation for the Inverse Gaussian Distribution", null], ["Maximum Likelihood Estimation for Two-Parameter Decreasing Hazard Rate Distributions Using Censored Data", "Problems of maximum likelihood estimation are discussed for shape and scale parameters from certain decreasing hazard rate distributions, typically either mixed-exponential or \u201cwork-hardened.\u201d Sufficient conditions on the mixing distribution are given that guarantee regular behavior of the hazard rate and that ensure, even with highly censored data, that the MLE's exist from such DHR distributions whenever the sample satisfies a certain condition; otherwise a constant hazard rate is estimated as a limiting case. Some computational methods are given and applications made."], [null, null], ["The Random-Effects Model in Discriminant Analysis", null], ["Empirical Bayes Confidence Sets for the Mean of a Multivariate Normal Distribution", "Through the use of an empirical Bayes argument, a confidence set for the mean of a multivariate normal distribution is derived. The set is a recentered sphere, is easy to compute, and has uniformly smaller volume than the usual confidence set. An exact formula for the coverage probability is derived, and numerical evidence is presented which shows that the empirical Bayes set uniformly dominates the usual set in coverage probability."], ["Uniform Consistency of Kernel Estimators of a Regression Function under Generalized Conditions", "In this article we prove uniform consistency of kernel estimators of a multivariate regression function under various assumptions on the distribution of the data. In addition to the usual assumptions that the data are iid and that the distribution of the regressors is absolutely continuous, we consider the cases that some regressors are discrete and the data are either stationary \u03d5-mixing themselves or generated by a class of functions of one-sided infinite stationary \u03d5-mixing sequences. Moreover, we demonstrate the performance of the kernel estimation method under these generalized conditions by a numerical example."], ["Bayesian Estimation Methods for 2 \u00d7 2 Contingency Tables Using Mixtures of Dirichlet Distributions", "In the estimation of cell probabilities from a 2 \u00d7 2 table, a prior distribution is developed that can reflect prior beliefs about the cross-classification structure in the table. The posterior means using this prior shrink the classical estimates towards the association structure specified a priori by the user. Closed-form approximations to the posterior means and credible regions are developed in the special case where the two variables of the table are believed independent."], ["Estimation for Transients in the Frequency Domain", "We consider the frequency domain approach for analyzing data that have been contaminated by transients. After formulating an additive model for transients, we suggest frequency domain estimation procedures. Part of our approach is much like complex demodulation, and this approach enables us to estimate separately the parameters for the transient and noise parts. Some simulation experiments are conducted to examine the properties of our estimates."], ["Tests for Interchangeability with Incomplete Paired Observations", "A class of conditionally distribution-free rank order tests is developed for testing interchangeability of two correlated variates with incomplete data on both responses. Asymptotic properties of these tests are examined. In particular, we show that the proposed tests are also asymptotically distribution-free. For location alternatives, the Pitman efficiency of the rank order test relative to the parametric test based on the bivariate normal model is investigated."], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Combining Historical and Randomized Controls for Assessing Trends in Proportions", "A statistical method for incorporating historical control data in the analysis of proportions is proposed and illustrated. The method has as its extremes logistic regressions completely pooling and completely ignoring historical controls. The degree of pooling used is determined by the variability from experiment to experiment in the control incidences. The fit of historical control groups to an assumed normal logistic model is assessed using probability plotting techniques. Monte Carlo studies evaluate the adequacy of the asymptotic approximation used. Sensitivity analyses show that results are insensitive to alternative priors. The method is applied to several sets of tumor data from animal experiments."], ["An Estimation Procedure for the Contaminated Normal Distributions Arising in Clinical Chemistry", "Mixtures of normal distributions have been used in modeling a wide variety of experimental data in fields as different as fisheries biology and medicine. A problem in the field of nutrition concerning estimation of the population proportion of anemic individuals from a sample of hemoglobin values has led to consideration of a class of mixtures of two components, where only the predominant component is assumed to be normal. The second component is assumed to contaminate only one side of the normal component (the side of contamination being known) but is of otherwise unspecified form. An estimation procedure is outlined for this problem. Monte Carlo methods are used to compare this procedure with the maximum likelihood (ML) method for mixtures of two normal components when the true model is (a) a Beta-normal mixture and (b) a normal-normal mixture. The new procedure compared favorably with the much-discussed ML method in certain situations not previously studied. Application of the two methods to real data gave remarkably similar results."], ["Reproductive Response to Child Mortality: A Maximum Likelihood Estimation Model", "The individual household's fertility response to an experienced child death, the replacement probability, is estimated for Brazil using a maximum likelihood estimation model. This model, which treats births, deaths, and replacement births as endogenous and which assumes discrete distributions for these random variables, avoids the statistical problems of earlier estimates. The model allows the distribution of births, deaths, and replacement births to vary with included exogenous variables. The replacement probabilities estimated for Brazil were between .4 and .9 and varied systematically with women's education, rural-urban residence, and household electrification. Increased development was shown to be correlated with a lower number of surviving children."], ["Evaluating a Hospital Cost-Containment Program in a Paired Experiment", "The persistently high rate of increase in medical costs and the inadequacies of health service delivery have become the paramount concern of the health industry in this country for the past two decades. In response to the need for cost containment and better service delivery systems, the BC/BS in Kansas City instituted a trial Cooperative Utilization Review Program (CURP) among member hospitals in 1977. This study, employing a hospital behavior model and various statistical procedures, attempts to evaluate the cost-effectiveness of the private program, CURP, with a sample of 19,284 inpatients admitted to both \u201ccontrol\u201d and \u201cstudy\u201d hospitals during the observation period of 1977\u201378. The paired experiment indicated that CURP was highly cost-effective in deterring unnecessarily lengthy stays in the \u201cstudy\u201d hospitals as compared with those in the \u201ccontrol\u201d hospitals. Observed were some economies of scale in the operation of CURP."], ["Comment", null], ["Small-Sample Properties of Predictions from the Regression Model with Autoregressive Errors", "Monte Carlo methods are used to examine the small-sample properties of various estimators of asymptotic prediction variance (APV) from the regression model with autoregressive errors. Two practical estimators of APV, one of which includes terms reflecting parameter estimation and one of which excludes these terms, are compared to the mean squared error of prediction for regression models with different autocorrelation of the errors, different sample sizes, and different period-ahead predictions. The inclusion of terms reflecting the estimation of parameters is found to be worthwhile, particularly in small samples."], ["A Reanalysis of the Stanford Heart Transplant Data", "This article represents analyses of survival of patients in the Stanford Heart Transplantation Program. We model survival time as a function of patient covariates and transplant status, and compare the results obtained using various parametric representations for survival time, including the Weibull, lognormal, and piecewise exponential distributions. Pretransplant and posttransplant survival are considered separately, and the effect of transplantation on survival is examined by comparison of the separate hazard functions. Comparisons are made with previous analyses. Using the piecewise exponential models, we estimate a generally declining hazard before transplant; after transplant the hazard increases for about 60 days, then declines. The presence of heavy censoring before and after transplant means that many of the other parametric models with differing shapes for the hazard all give equally adequate fits to the data. Inferences about the effect of covariates are also relatively insensitive to the exact choice of parametric model, although important exceptions can occur if the parametric model is clearly contradicted by the data. Using a variety of models, we do find large differences in predicted survival of transplanted and nontransplanted patients as a function of patient age and calendar time of transplant. The variability in these estimated differences is correspondingly large, mainly due to a lack of information about the long-term survival of the nontransplanted group. As a result, analysis of these data leaves unresolved the issue of the effect of transplant on survival."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Bayes Methods for Combining the Results of Cancer Studies in Humans and other Species", "We propose a class of Bayesian statistical methods for interspecies extrapolation of dose-response functions. The methods distinguish formally between the conventional sampling error within each dose-response experiment and a novel error of uncertain relevance between experiments. Through a system of hierarchical prior distributions similar to that of Lindley and Smith (1972), the dose-response data from many substances and species are used to estimate the interexperimental error. The data, the estimated error of interspecies extrapolation, and prior biological information on the relations between species or between substances each contribute to the posterior densities of human dose-response. We apply our methods to an illustrative problem in the estimation of human lung cancer risk from various environmental emissions."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation", "We construct a prediction rule on the basis of some data, and then wish to estimate the error rate of this rule in classifying future observations. Cross-validation provides a nearly unbiased estimate, using only the original data. Cross-validation turns out to be related closely to the bootstrap estimate of the error rate. This article has two purposes: to understand better the theoretical basis of the prediction problem, and to investigate some related estimators, which seem to offer considerably improved estimation in small samples."], ["Testing for Random Pairing", "The analysis of contingency tables in which both margins have the same polychotomy is considered. These tables arise when members from categories form pairs. Pairs cannot be between members of the same category. Appropriate models for pairing are introduced. A maximum likelihood algorithm is developed and asymptotic covariance formulas are derived. An empirical example illustrates the procedures."], ["Estimating the Relative Risk with Censored Data", "We investigate a class of nonparametric estimators of relative risk in the two-sample case of the proportional hazards model for censored data. The asymptotic distribution of these estimators is derived using influence functions. The optimal estimator in this class has the same influence functions and the same asymptotic distribution as the maximum partial likelihood estimator of Cox (1972). The behavior of the influence functions is discussed briefly, and the last section presents two examples from the literature."], ["On Testing Monotone Tendencies", null], ["Splines in Statistics", "This is a survey article that attempts to synthesize a broad variety of work on splines in statistics. Splines are presented as a nonparametric function estimating technique. After a general introduction to the theory of interpolating and smoothing splines, splines are treated in the nonparametric regression setting. The method of cross-validation for choosing the smoothing parameter is discussed and the general multivariate regression/surface estimation problem is addressed. An extensive discussion of splines as nonparametric density estimators is followed by a discussion of their role in time series analysis. A comparison of the spline and isotonic regression methodologies leads to a formulation of a hybrid estimator. The closing section provides a brief overall summary and formulates a number of open/unsolved problems relating to splines in statistics."], ["The Hunter Method of Simultaneous Inference and its Recommended Use for Applications Having Large Known Correlation Structures", null], ["Low Median and Least Absolute Residual Analysis of Two-Way Tables", "Some properties of and extensions to Tukey's method of median polish, an exploratory robust additive decomposition of a two-way table, are presented using the low median. If the table entries are rational numbers, then the iteration process must stop after a finite number of steps. However, even for tables of bounded dimension the number of iterations can be arbitrarily large. For 3 by 3 tables, the sum of absolute residuals is often minimized by median polish. Minimization conditions are identified that are likely to occur in practice. A method to supplement the polishing process by increasing the number of zero residuals is developed."], ["A Unified Treatment of Locally Most Powerful Rank Tests under Type II Censoring", "For the derivation of locally most powerful (LMP) rank tests, a general method that encompasses a wide variety of testing problems and type II censoring schemes is formulated. Applications are illustrated in the context of the one-sample, two-sample, and regression problems. It is shown that the traditional derivations of the LMP rank tests are greatly simplified with the proposed method. The LMP criterion is also applied to some problems for which only ad hoc tests were previously proposed."], ["Nonparametric Estimation for a Scale-Change with Censored Observations", "Nonparametric point and interval estimators of the ratio of two scale parameters are given for arbitrarily right-censored data based on the idea of Hodges and Lehmann (1963). These estimators are defined in terms of rank test statistics for testing the equality of two survival distributions. The asymptotic properties and efficiencies of estimators corresponding to the tests studied by Gill (1980) are investigated."], ["On Sequential Detection of a Shift in the Probability of a Rare Event", "Suppose one is monitoring a sequence of observations for a possible increase in the probability of a rare event, and that it is not possible to immediately stop the process under observation or influence it to return to its normal state. One would then desire a scheme which takes advantage of observations occurring after a detection of a change is proclaimed. A modification of Page's CUSUM Procedure is developed, taking account of these additional observations. A table is given enabling one to select a modified Page procedure with a specified rate of false alarms. A comparison is made between the modified Page procedure and a procedure proposed by Chen (1978)."], ["Sequential Allocation and Optional Stopping in Bayesian Simultaneous Estimation", null], ["A Frequentistic Approach to Sequential Estimation in the General Linear Model", null], ["Information Measures and Bayesian Hierarchical Models", null], ["Statistical Tests Based on Transformed Data", null], ["Priors and Likelihood Ratios as Evidence", "Arguments based on diagnostic data concerning a particular case, and ones based on prior experience with like cases, can be represented by belief functions and combined by Dempster's Rule. In limiting cases, the belief functions depend on likelihood ratio or on prior odds, and when these limiting cases occur together, Bayes' Theorem is applicable as is Dempster's Rule. The resulting functional equations lead to a single-parameter family of functions relating belief strength to probability and to likelihood ratio. The parameter can be measured by studying the belief strength produced by conceptually independent dissonant arguments that point to a common conclusion. This provides one possible solution to the problem of fundamental measurement for Shafer's theory of belief."], ["Don't Bet on it: Contingent Agreements with Asymmetric Information", "If two people have different probability assessments about the realization of an uncertain event, they can design a contingent agreement such as a bet or gamble that offers each of them positive expected value. Yet, in the process of formulating this kind of agreement, information about the basis for each person's probabilities may be indirectly revealed to the other. The very willingness to accept a proposed bet conveys information. This article models a process by which private, asymmetrically-held information is progressively unveiled as a possible contingent agreement is discussed. If the two parties share priors and their information partitions are common knowledge, simple discussion of the acceptability of any proposed bet is shown to reveal enough about the parties' private information to render the bet unacceptable."], [null, null], ["On Obtaining Permutation Distributions in Polynomial Time", "Polynomial time algorithms are presented for finding the permutation distribution of any statistic that is a linear combination of some function of either the original observations or the ranks. This class of statistics includes the original Fisher two-sample location statistic and such common nonparametric statistics as the Wilcoxon, Ansari-Bradley, Savage, and many others. The algorithms are presented for the two-sample problem and it is shown how to extend them to the multisample problem\u2014for example, to find the distribution of the Kruskal-Wallis and other extensions of the Wilcoxon\u2014and to the single-sample situation. Stratification, ties, and censored observations are also easily handled by the algorithms. The algorithms require polynomial time as opposed to complete enumeration algorithms, which require exponential time. This savings is effected by first calculating and then inverting the characteristic function of the statistic."], ["On the Accuracy of Simulated Percentage Points", "Two methods are discussed of obtaining percentage points of distributions by Monte Carlo studies, one using one long run, the other using the average of several smaller runs. The single run has certain advantages that we describe. The two methods are illustrated on percentage points of the normal distribution."], ["Maximum Likelihood Estimation for a Discrete Multivariate Shock Model", null], ["Prediction of Future Random Events with the Condensed Negative Binomial Distribution", "The condensed negative binomial distribution (CNBD) has been proposed as a stochastic model that is more appropriate for consumer purchasing behavior than is the common negative binomial distribution. Since this model was motivated previously and shown to fit single-period purchasing data, the focus here turns to the dynamics of the process, as reflected in the aggregate characteristics of interpurchase times and period-to-period predictions. The theoretical results are useful for comparing the predictive effectiveness of the CNBD with the commonly used negative binomial distribution."], ["Estimating the Parameters of a Convolution by Maximum Likelihood", "In the present age of computer sophistication, computational difficulty is no longer a justification for seeking alternative and inefficient estimation procedures in place of maximum likelihood estimation, particularly when there are only two or three parameters to be estimated. Convolution densities are an example where this has occurred. It is shown that in a large class of such densities, the maximum likelihood equations can be reduced by one. Thus for a two-parameter family, only a single equation need be solved iteratively. The required formulas are derived. The problem of basing inferences on the resulting maximum likelihood estimates is briefly discussed."], ["A Note on the Modified Likelihood for Density Estimation", null], ["On the Small-Sample Properties of the Olkin-Sobel-Tong Estimator of the Probability of Correct Selection", null], ["Linear Estimation with an Incorrect Dispersion Matrix in Linear Models with a Common Linear Part", null], ["Distributions of a Class of Statistics Useful in Multivariate Analysis", "In this article, exact and asymptotic distributions of a class of statistics whose moments are of a particular form have been obtained. An application to Wilks's criterion for testing a linear hypothesis is given, and it is shown by numerical comparison that the first term of the asymptotic expansion of the distribution obtained in this article is better than Box's asymptotic expansion in terms of chisquared distributions. The exact distribution is easily computable and has the distinct advantage that stable recurrence relations exist between the coefficients involved."], ["A Method for Calculating MINQUE Estimators of Variance Components", "The original expression for MINQUE estimators of variance components involved the inverse of a matrix whose dimensions equalled the total sample size. Later papers described more practical methods of computing the estimators. In this article a method is described that involves matrices and vectors with dimensions no larger than the number of nonempty cells."], ["Kernel Estimators of the Failure-Rate Function and Density Estimation: An Analogy", null], [null, null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Multiplicative Models and Cohort Analysis", "Three methods of cohort analysis are presented for a statistical model wherein the explanatory or exposure variables act multiplicatively on age \u00d7 calendar year specific death rates. The first method, which assumes that the baseline rates are known from national vital statistics, is a multiple regression analysis of the standardized mortality ratio. The second method is a variant of Cox's proportional hazards analysis in which the baseline rates are treated as unknown nuisance parameters. The third method consists of case-control sampling from the risk sets formed in the course of applying Cox's model. It requires substantially less computation than do the other two. In illustrative analysis of respiratory cancer deaths among a cohort of smelter workers, all three approaches yield roughly equivalent estimates of the relative risk associated with arsenic exposure. The discussion centers on the tradeoff between efficiency and bias in the selection of a particular method of analysis, and on practical issues that arise in applications."], ["The Accuracy of Population Projections", "Population projections are key elements of many planning and policy studies but are inherently inaccurate. This study of past population projection errors provides a means for constructing confidence intervals for future projections. We first define a statistic to measure projection errors independently of the size of the population and the length of the projection period. A sample of U.S. Census Bureau and United Nations projections indicates that the distributions of components of the error statistic are relatively stable. We then use this information to construct confidence intervals for the total population of the United States through the year 2000. We find that for projections of total population size, simple projection techniques are more accurate than more complex techniques."], ["Capitalizing on Nonrandom Assignment to Treatments: A Regression-Discontinuity Evaluation of a Crime-Control Program", "Despite the enormous potential of regression-discontinuity, quasi-experimental procedures, they have to date rarely been used in the evaluation of large-scale social programs. In this article, we report an evaluation of such a program in which a regression-discontinuity analysis is employed. Through a change in legislation, the program in question extended eligibility for unemployment benefits to prisoners after their release from prison. The regression-discontinuity approach proved practical and effective; it revealed that the program cut recidivism rates by 13 percent."], ["Stable Distributions and the Mixtures of Distributions Hypotheses for Common Stock Returns", "The form of the distribution underlying common stock returns has many implications for financial modeling. Among the frequently proposed models for stock return distributions is the family of stable distributions. Numerous studies of stock return distributions have also proposed various types of mixtures of normal distributions. This article examines the characteristics of several different combinations of mixtures of normal and stable distributions, and compares them with actual stock price distributions. The principal tests applied are based on the stability-under-addition property of stable distributions."], ["A \u201cTrue\u201d Time Series and its Indicators", "A problem that economic statisticians frequently face is to estimate the true movement in a time series on the basis of two or more imperfect indicators. A well-known example is nonagricultural employment as indicated (a) by a monthly survey of households and (b) by a monthly survey of employers. This article describes a procedure for estimating true changes in a time series as a linear combination of two indicators, with weights for indicators chosen so as to minimize errors. It applies the procedure to two examples, nonagricultural employment and capital goods prices."], ["Parametric Empirical Bayes Inference: Theory and Applications", "This article reviews the state of multiparameter shrinkage estimators with emphasis on the empirical Bayes viewpoint, particularly in the case of parametric prior distributions. Some successful applications of major importance are considered. Recent results concerning estimates of error and confidence intervals are described and illustrated with data."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Minimax Aspects of Bounded-Influence Regression", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["The Signal Extraction Approach to Nonlinear Regression and Spline Smoothing", null], ["Experimental Design for Binary Data", "Models for binary data are usually such that the information matrix depends on the unknown parameters. Thus the standard criteria for optimality in regression experiments cannot be applied without modification. Methods of going around this difficulty include the use of initial point estimates, sequential methods, and Bayesian analysis. This article is mainly concerned with the robustness and the number of design points for methods involving initial estimates, and for sequential methods in a small number of stages. A final section discusses the criterion of constant information for models involving one or two parameters, and summarizes recent results in this field."], ["Small-Sample Quantal Response Methods for Estimating the Location Parameter for a Location-Scale Family of Dose-Response Curves", "When one uses a small number of quantal responses to estimate a location parameter in the presence of an unknown scale parameter for a logistic or normal dose-response curve, it is often possible to substantially improve upon the method of maximum likelihood. Some generalized method-of-moments estimators are proposed for an arbitrary dose-response curve known up to location and scale and a theorem is given that establishes existence and uniqueness properties. In a computer study of the exact distributions of these estimators, they proved to be far superior to maximum likelihood and minimum logit chi-squared estimators of location in the logistic case."], ["Binomial Confidence Intervals", null], ["Variance Estimation Using Auxiliary Information", "Variance estimators under several sample designs are proposed and compared when auxiliary information is available. Improvement of the bias and mean squared error over some estimators commonly used in practice is illustrated. A Monte Carlo comparison of the estimators is also presented."], ["Estimation and Moment Recursion Relations for Multimodal Distributions of the Exponential Family", "Multimodal generalizations of the normal, gamma, inverse gamma, and beta distributions are introduced within a unified framework. These multimodal distributions, belonging to the exponential family, require fewer parameters than corresponding mixture densities and have unique maximum likelihood estimators. Simple moment recursion relations, which make maximum likelihood estimation feasible, also yield easily computed estimators that themselves are shown to be consistent and asymptotically normal. Lastly, a statistic for bimodality, based on Cardan's discriminant for a cubic shape polynomial, is introduced."], ["How Many Variables Should Be Entered in a Regression Equation?", null], ["A Predictive View of the Detection and Characterization of Influential Observations in Regression Analysis", "Assuming a set of observations is available from the general linear model, and assuming prior information about parameters, we propose a method of assessing the influence of specified subsets of the data when the goal is to predict future observations."], ["Robust Multiple Comparisons", null], ["A Method for Determining Periods in Time Series", "The periods corresponding to peaks in the autoregressive and window spectral density estimators are shown to be consistent and asymptotically normal estimators of peak periods in the true spectral density under assumptions of known autoregressive order and under slight modifications of the assumptions often made to show the asymptotic normality of window estimators, respectively. The asymptotic variances are obtained, and the use of the theory is illustrated by obtaining an asymptotic confidence interval for a peak period using a set of hormone data."], ["Score Tests for Regression Models", "Many score statistics used for testing trends in means, either based on the observed values of the response variables or their ranks, can be expressed in terms of quadratic forms. This article explores systematically the robustness of optimality of these statistics, as well as the optimal properties of the associated tests."], ["Sub-Balanced Data and the Mixed Analysis of Variance", "In the mixed model, it is well known that balanced data are a sufficient condition for an ANOVA with terms that are independent multiples of chi-squared variables (called a proper ANOVA). In this article a weaker condition, which is both necessary and sufficient, is determined and shown to be equivalent to what is defined here as sub-balanced random effects, a condition easily checked in practice. The usual method of generating an ANOVA by fitting sums of squares for a given ordering of the effects in the model is sometimes inadequate with sub-balanced data, requiring instead a spectral decomposition of the covariance matrix. An example is given."], ["Run Probabilities in Sequences of Markov-Dependent Trials", null], ["The Decomposability and Monotonicity of Pearson's Chi-Square for Collapsed Contingency Tables with Applications", null], ["Unimodality of Likelihood Functions for the Binomial Distribution", "A maximum likelihood estimate exists for both parameters of the binomial distribution if and only if the sample mean exceeds the sample variance. The derivative of the log-likelihood function has at most one finite root; consequently the likelihood function is unimodular."], ["A Survey of Strategies for Modeling Cross-Classifications Having Ordinal Variables", "A survey is given of the main strategies for modeling cross-classifications that contain ordinal variables. Models are described for two-way tables in which one or both classifications are ordered and for multidimensional tables in which at least one classification is ordered. Primary emphasis is given to construction, interpretation, and implications of loglinear and logit models."], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"]]}