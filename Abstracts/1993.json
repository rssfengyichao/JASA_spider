{"1993": [["A Statistical Analysis of Hitting Streaks in Baseball", "This article presents a study of hitting streaks for individual batters in the game of baseball. All baseball fans know that players go through periods where they never seem to make an out and other periods where nothing will get them to base. There is no doubt that \u201chot streaks\u201d and \u201ccold streaks\u201d do occur. The question explored here is whether these streaks occur more (or less) frequently than would be predicted by a probabilistic model of randomness. I examined the records of many \u201cregular\u201d Major League players through four seasons, 1987\u20131990 and used several statistical methods to check for streakiness. These include standard methods such as the runs test, as well as a more complex logistic regression model with several explanatory variables. Based on all of these methods, there is no doubt that a certain number of players exhibited definite streakiness in certain years. But the evidence also suggests that the behavior of all players examined, taken as a whole, does not differ significantly from what would be expected under a model of randomness. Furthermore, none of the players examined in the study exhibited unusually streaky behavior over the entire four-year period."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Correcting for Nonavailability Bias in Surveys by Weighting Based on Number of Callbacks", null], ["Errors in Survey Reports of Earnings, Hours Worked, and Hourly Wages", "Using administrative records on earnings, hours worked, and hourly wages for hourly employees of a single manufacturing firm, we found standard assumptions about errors in survey measures to be violated to varying extents. Errors are correlated with true scores, and errors in reports of earnings and hours in different periods are generally positively correlated with one another, particularly for reports given in a single interview. Although a high proportion of reporting errors are from an approximately normal distribution, a small proportion comes from a distribution with a much greater variance, and these cases often have considerable influence on estimates of relationships between variables."], ["Consumer Purchase Behavior in a Frequently Bought Product Category: Estimation Issues and Managerial Insights from a Hazard Function Model with Heterogeneity", "We apply hazard function models to marketing data on household purchases of disposable diapers to calibrate repeat-purchase and brand-switching behaviors. In the model we allow for household-specific unobserved traits. Hazard function models with fixed-effects heterogeneity specification are computationally feasible. We directly compare the results of fixed- and random-effects heterogeneity specifications in the context of parametric and nonparametric duration-dependence functions. We find that a rigorous test fails to reject the random-effects specification in favor of the fixed-effects specification. The unimodal gamma distribution in the random-effects specification adequately captures unobserved heterogeneity."], ["Forecasting Hourly Electricity Demand Using Time-Varying Splines", "A method for modeling a changing periodic pattern is developed. The use of time-varying splines enables this to be done relatively parsimoniously. The method is applied in a model used to forecast hourly electricity demand, with the periodic movements being intradaily or intraweekly. The full model contains other components, including a temperature response, which is also modeled using splines."], ["Bayesian Nonparametrics for Compliance to Exposure Standards", "A Bayesian nonparametric view of compliance to occupational standards is achieved through predictive distributions. The common assumption of lognormality of environmental exposures is relaxed while recognizing the practicality of a finite number of possible samples. These probability of compliance calculations are conditional on observing some of the samples. Familiar binomial and normal modes are identified with the classical perspective as limits of Bayesian nonparametric and parametric strategies, when the number of observed samples increases. In this situation, extensive previous sample data provide a correspondence between the classical and Bayesian approaches, rather than little or no previous information. Using an example, alternative procedures are illustrated and compared. Currently used methodology can be anti-conservative for protecting employees."], ["The Fisher, Neyman-Pearson Theories of Testing Hypotheses: One Theory or Two?", "The Fisher and Neyman-Pearson approaches to testing statistical hypotheses are compared with respect to their attitudes to the interpretation of the outcome, to power, to conditioning, and to the use of fixed significance levels. It is argued that despite basic philosophical differences, in their main practical aspects the two theories are complementary rather than contradictory and that a unified approach is possible that combines the best features of both. As applications, the controversies about the Behrens-Fisher problem and the comparison of two binomials (2 \u00d7 2 tables) are considered from the present point of view."], ["Hodges-Lehmann Point Estimates of Treatment Effect in Observational Studies", "A Hodges-Lehmann point estimate of an additive treatment effect is a robust estimate derived from the randomization distribution of a rank test. This article shows how to carry out a sensitivity analysis for such an estimate in an observational study where treatments are not randomly assigned. Two cases are discussed in detail: matched pairs and the comparison of two unmatched groups. The method uses a model for the distribution of treatment assignments when hidden biases may be present. This model has previously been used for sensitivity analysis of significance levels and confidence intervals."], ["The Use and Interpretation of Residuals Based on Robust Estimation", "Residual plots and diagnostic techniques are important tools for examining the fit of a regression model. In the case of least squares fits, plots of residuals provide a visual assessment of the adequacy of various aspects of the fitted model. An important question is whether plots of robust residuals can be interpreted in the same manner as their least squares counterparts. This article addresses this problem for two popular classes of robust estimates: M estimates and GM estimates of the Mallows and Schweppe types. First-order properties of the residuals and fitted values are derived under correct and misspecified models. These properties are insightful on the general interpretability of robust residual plots and on their ability to detect curvature in misspecified models. The results of a simulation study consisting of tests for randomness and curvature in residual plots supports these theoretical properties. Standardization of robust residuals is also presented."], ["Procedures for the Identification of Multiple Outliers in Linear Models", "We consider the problem of identifying and testing multiple outliers in linear models. The available outlier identification methods often do not succeed in detecting multiple outliers because they are affected by the observations they are supposed to identify. We introduce two test procedures for the detection of multiple outliers that appear to be less sensitive to this problem. Both procedures attempt to separate the data into a set of \u201cclean\u201d data points and a set of points that contain the potential outliers. The potential outliers are then tested to see how extreme they are relative to the clean subset, using an appropriately scaled version of the prediction error. The procedures are illustrated and compared to various existing methods, using several data sets known to contain multiple outliers. Also, the performances of both procedures are investigated by a Monte Carlo study. The data sets and the Monte Carlo indicate that both procedures are effective in the detection of multiple outliers in linear models and are superior to other methods, including methods based on robust fits (e.g., least median of squares residuals). In particular, the methods do not require presetting numbers of outliers to test for, do not require the efficiency level of an estimator, do not require Monte Carlo to determine cutoff values, are not highly computationally intensive, and are relatively resistant to both masking and swamping effects."], ["Alternatives to the Median Absolute Deviation", null], ["Optimal Kernel Weights under a Power Criterion", "We develop an approach to choosing optimal kernel weights for nonparametric estimation of the slope of the regression function which uses maximization of power, rather than minimization of integrated mean squared error (IMSE), as its optimality criterion. This power criterion leads to optimal kernel weights whose derivation is simpler than under other criteria and which provides an intuitive understanding of the nature of the optimality."], ["Confidence Bands in Nonparametric Regression", "New bias-corrected confidence bands are proposed for nonparametric kernal regression. These bands are constructed using only a kernel estimator of the regression curve and its data-selected bandwidth. They are shown to have asymptotically correct coverage properties and to behave well in a small-sample study. One consequence of the large-sample developments is that Bonferroni-type bands for the regression curve at the design points also have conservative asymptotic coverage behavior with no bias correction."], ["Locally Adaptive Bandwidth Choice for Kernel Regression Estimators", "Kernel estimators with a global bandwidth are commonly used to estimate regression functions. On the other hand, it is obvious that the choice of a local bandwidth can lead to better results, because a larger class of kernel estimators is available. Evidently, this may in turn affect variability. The optimal bandwidths depend essentially on the regression function itself and on the residual variance, and it is desirable to estimate them from the data. In this article, a local bandwidth estimator is studied. A comparison with its global bandwidth equivalent is performed both in theory and in simulations. As the main result it is shown that the possible gain in mean integrated squared error of the resulting regression estimator must be paid for by a larger variability of the estimator. This may lead to worse results if the sample size is small. An algorithm has been devised that puts special weight on stability aspects. Our simulation study shows that improvements over a global bandwidth estimator often can be realized even at small or moderate sample sizes."], [null, null], ["On Estimating Distribution Functions Using Nomination Samples", null], ["Deconvolution, Bandwidth, and the Trispectrum", "In the largest application area of time series analysis\u2014geophysical exploration\u2014the underlying innovations sequence is of primary interest and must be estimated. This sequence is estimated by deconvolving the non-Gaussian, noninvertible time series. This involves estimation of a phase-shift correction from the time series, which can be carried out by maximizing the kurtosis of the series. Unfortunately, the method is hampered by the fact that the time series is typically deficient in power in certain bands of frequencies (\u201cband-limited\u201d). The consequences of this can be analyzed by studying the trispectrum\u2014the third of the polyspectra\u2014of the series. This reveals two important results. First, we are able to easily appreciate why for certain types of band-limitation, kurtosis cannot be used to determine a phase correction. Second, by looking at the inner and outer subvolumes of the support volume for the discrete-parameter trispectrum, we see that for the standard linear model the trispectrum is non-0 in both the inner and outer volumes, whereas the trispectrum of a series sampled finely enough to avoid aliasing from a continuous fourth-order stationary process is equal to the same continuous-parameter trispectrum value in the inner volumes, but always 0 in the outer volumes. Hence, when looking at higher-order structure, the standard linear model need not give results that accord with physical reality."], ["Confidence Bands for the Median Survival Time as a Function of the Covariates in the Cox Model", null], ["Cox Regression with Incomplete Covariate Measurements", "This article provides a general solution to the problem of missing covariate data under the Cox regression model. The estimating function for the vector of regression parameters is an approximation to the partial likelihood score function with full covariate measurements and reduces to the pseudolikelihood score function of Self and Prentice in the special setting of case-cohort designs. The resulting parameter estimator is consistent and asymptotically normal with a covariance matrix for which a simple and consistent estimator is provided. Extensive simulation studies show that the large-sample approximations are adequate for practical use. The proposed approach tends to be more efficient than the complete-case analysis, especially for large cohorts with infrequent failures. For case-cohort designs, the new methodology offers a variance-covariance estimator that is much easier to calculate than the existing ones and allows multiple subcohort augmentations to improve efficiency. Real data taken from clinical and epidemiologic studies are analyzed."], ["Linear Combinations of Multiple Diagnostic Markers", "The receiver operating characteristic (ROC) curve is a simple and meaningful measure to assess the usefulness of diagnostic markers. To use the information carried by multiple markers, we note that Fisher's linear discriminant function provides a linear combination of markers to maximize the sensitivity over the entire specificity range uniformly under the multivariate normal distribution model with proportional covariance matrices. With no restriction on covariance matrices, we also provide a solution of the best linear combination of markers in the sense that the area under the ROC curve of this combination is maximized among all possible linear combinations. We illustrate both situations discussed in the article with a cancer clinical trial data."], ["Clustering Objects Generated by Linear Regression Models", "This article describes a new clustering method designed for objects generated by a linear model with different states. The objective is to divide the objects into clusters, with each cluster containing only the objects taken when the system is in one particular state. The cluster centroids are matrices of predetermined rank and can be computed by the singular value decomposition (SVD) algorithm. The clustering problem is formulated as a combinatorial optimization problem. Two heuristic iterative algorithms that ensure the decrease of the objective function are then proposed. The simulation examples are given to show the effectiveness of the methodology."], ["Sign Tests in Multidimension: Inference Based on the Geometry of the Data Cloud", null], ["A Simple Approach to Testing Homogeneity of Order-Constrained Means", "The likelihood-ratio test for equality of partially ordered means, first proposed by Bartholomew, is known to possess power characteristics that are generally superior to those of competing procedures. A limitation of this test, however, is that the null distribution of the test statistic is heavily dependent on both the specific form of the order restriction and the group sample sizes. It is not readily available for most orderings, especially when the group sample sizes are unequal. Furthermore, computation of the test statistics often involves intricate algorithms. We propose a class of procedures based upon classical methods for combining independent tests. These procedures are generally applicable and easily implemented. A study of the power functions of the competing tests indicates that the power of the test based on Fisher's combination method compares reasonably with that of the likelihood-ratio test. It is concluded that the procedure based on Fisher's combination method is an effective alternative to the likelihood-ratio test, especially when the latter is difficult to implement."], ["Analysis of Contingency Tables by Correspondence Models Subject to Order Constraints", "Inferential correspondence analysis, which has gained much attention in recent years, is applied here to contingency tables with ordered categories. To reflect such order, the parameters of the underlying correspondence models are constrained to follow the order induced by the categories of the analyzed table. A reparameterization of the correspondence model in terms of a latent variable model is presented. This allows a simple and straightforward use of the EM algorithm to obtain efficient order-restricted estimates. A goodness-of-fit test is also discussed, and an example is analyzed. A small Monte Carlo example is presented."], [null, null], ["Orthogonal Array-Based Latin Hypercubes", null], ["On the Construction of Trend-Resistant Designs for Comparing a Set of Test Treatments with a Set of Controls", "In this article we consider the problem of comparing a set of standard treatments to a set of test treatments where the treatments are to be applied to experimental units sequentially over time or space and where there may be an unknown time or spatial trend. Methods are given here for constructing run orders of treatments for those situations that are \u201coptimal\u201d and robust against trends that can be expressed as a polynomial function of the order in which the observations are taken."], ["Optimal Experimental Design for Another's Analysis", "We consider the optimal design of experiments in which estimation and design are performed by different parties. The parties are assumed to share similar goals, as reflected by a common loss function, but they may have different prior beliefs. After presenting a few motivating examples, we examine the problem of optimal sample size selection under a normal likelihood with constant cost per observation. We also consider the problem of optimal allocation for given overall sample sizes. We present results under both squared-error loss and a logarithmic utility, paying attention to the differences between one- and two-prior optimal designs. An asymmetric discrepancy measure features repeatedly in our development, and we question the extent of its role in optimal two-prior design."], ["Bayesian Analysis for the Poly-Weibull Distribution", "In this article Bayesian analysis for a Poly-Weibull distribution using informative priors is discussed. This distribution typically arises when the data is the minimum of several Weibull failure times from competing risks. To perform the Bayesian computations, simulation using the Gibbs sampler is suggested. This can be used to find posterior moments, the marginal posterior probability density function, and the predictive risk or reliability."], ["Posterior Cumulant Relationships in Bayesian Inference Involving the Exponential Family", "For Bayesian inference in one-parameter contexts where either the likelihood or the prior has an exponential family form, relationships are derived for posterior moments and cumulants of (functions of) both the canonical and the expectation parameters. The identities exhibited generalize the simple relationships well known in the conjugate analysis case. Applications of these results are indicated in the areas of Bayesian robustness and approximation. In particular, results are obtained on the behavior of the posterior distribution for a large observation, generalizing work of Meeden and Isaacson."], ["Noninformative Priors and Nuisance Parameters", "We study the conflict between priors that are noninformative for a parameter of interest versus priors that are noninformative for the whole parameter. Our investigation leads us to maximize a functional that has two terms: an asymptotic approximation to a standardized expected Kullback-Leibler distance between the marginal prior and marginal posterior for a parameter of interest, and a penalty term measuring the distance of the prior from the Jeffreys prior. A positive constant multiplying the second terms determines the tradeoff between noninformativity for the parameter of interest and noninformativity for the entire parameter. As the constant increases, the prior tends to the Jeffreys prior. When the constant tends to 0, the prior becomes degenerate except in special cases. This prior does not have a closed-form solution, but we present a simple, numerical algorithm for finding the prior. We compare this prior to the Berger-Bernardo prior."], ["When is Conflict Normal?", null], ["Monotone Empirical Bayes Estimators Based on More Informative Samples", "An empirical Bayes estimator is constructed for the case of nonidentical components coming from an exponential family of distributions that is closed under convolution. Examples are the normal, Poisson, and binomial distributions. The construction is based on Robbins' approach to empirical Bayes estimation and generalizes results of O'Bryan and Stijnen and Van Houwelingen. The method is elaborated for the normal and Poisson case, and examples are given that show that this approach gives results comparable to those from the nonparametric maximum likelihood approach of Laird."], ["A Quasi-Empirical Bayes Method for Small Area Estimation", "This article develops an empirical Bayes-type approach for small area estimation based only on the specification of a set of conditionally independent hierarchical mean and variance functions describing the first two moments of the process generating the data. The objective of the analysis is to draw inference about the intermediate or the random means. We combine the quasi-likelihood functions to construct the quasi-posterior density of the random means conditional on the data and the marginal parameters. We describe a method for estimating the marginal parameters that are then substituted in the quasi-posterior density of the random means. The empirical quasi-posterior density so derived is used to obtain the quasi-empirical Bayes estimates of the random parameters. We apply the methodology to estimate the utilization rates of cancer chemotherapy for the selected counties in the state of Washington."], ["The Analysis of Current Status Data on Point Processes", "This article considers the analysis of event count data in which each subject is observed at only one time point and no information is available on subjects between their entry time and observation points. This type of data, often referred to as current status data, arises frequently\u2014for example, in demography, epidemiology, and reliability studies. Statistical methods for the analysis of current status data from point processes are proposed. Specifically, a statistic for testing the equality of the mean functions of point processes is presented and its asymptotic distribution obtained. For illustration, the proposed method is used to analyze multiple tumor data from a tumorgenicity experiment, with focus on the comparison of tumor growth rates in male and female rats. The adequacy of the asymptotic distribution of the test statistic is evaluated in a small simulation study. Power comparisons with the usual parametric model are also obtained. Finally, some possible directions for further research are discussed."], ["Sampling the Leaves of a Tree with Equal Probabilities", "In the computer sciences, trees are among the most common data structures. Knuth proposed a method of sampling the leaves of a tree with known but unequal probabilities; he used the sample to estimate the behavior of backtrack algorithms. This article proposes two methods for sampling leaves with equal probabilities."], ["An Extension of Marshall and Olkin's Bivariate Exponential Distribution", "This article extends Marshall and Olkin's bivariate exponential distribution such that it is absolutely continuous and need not be memoryless. The new marginal distribution has an increasing failure rate, and the joint distribution exhibits an aging pattern. It offers an advantage in separately identifying the shock arrival rates and their impacts. Regarding estimation of the model, both maximum likelihood and method-of-moments-type estimation are considered. The former is more efficient but computationally more demanding, whereas the latter is simpler in computation but less efficient. The trade-off between computational burden and efficiency is gauged through Monte Carlo simulations, and it turns out to be favorable for the method-of-moments-type estimation."], ["Book Reviews", null], ["Publications Received", null], ["Letters to the Editor: Extension of Surprising Covariances", null], ["Letters to the Editor: Comment on O'Sullivan", null], ["Editorial Board Page", "This article has no abstract"], ["Modeling Disease Marker Processes in AIDS", "The importance of disease markers in understanding the progression of acquired immune deficiency syndrome (AIDS) and devising treatment strategies is well recognized. This issue is usually addressed using cross-sectional data analysis, which tends to ignore the longitudinal data collected on the individuals. Available longitudinal data for nontransfusion-related AIDS raise some technical challenges to standard longitudinal analyses due to left and right censoring as well as left truncation. We describe a likelihood method to model the disease markers as a function of time by modeling the joint distribution of the markers, the time of infection, and the time to AIDS. We address the problems of censoring and truncation using standard survival analysis techniques. We also consider the prediction of time to AIDS given a series of disease marker measurements. An illustrative example, using data from the Toronto AIDS cohort study, is given. In particular, the analysis shows that the slope of the decline in T4 cell count measurements or T4/T8 ratio is associated with the time to AIDS. We compare the prediction of the time to AIDS for an individual with or without a series of T4/T8 measurements and with a known or unknown infection time."], ["Nursing Home Discharges and Exhaustion of Medicare Benefits", "The relationship between the utilization of nursing home care and its price is a subject of considerable policy interest. We assess price sensitivity by developing and applying \u201coverlap polynomials\u201d as a method to exploit the temporal price variation implicit in Medicare payment rules for nursing home care. Standard methods for measuring price responsiveness are inappropriate for measuring the effects of temporal price variation, and unmeasured quality variation often confounds cross-sectional price variation. Our empirical analysis assesses the magnitude of shifts in nursing home discharge rates attributable to the price changes that occur when Medicare coverage diminishes or ends. Our findings provide strong evidence that the duration of nursing home stays is sensitive to price in the population examined here."], ["The Effect of Marijuana Decriminalization on Hospital Emergency Room Drug Episodes: 1975\u20131978", "Between 1973 and 1978, 12 states with collectively over one-third of the total U.S. population enacted laws that decriminalized the possession of marijuana. This article uses standard metropolitan statistical area (SMSA) level data on hospital emergency room drug episodes collected by the Drug Abuse Warning Network to measure the effect of changes in drug penalties on substance abuse crises. The regression models demonstrate that marijuana decriminalization was accompanied by a significant reduction in episodes involving drugs other than marijuana and an increase in marijuana episodes. Although possible biases in the data preclude firm conclusions, the results suggest that some substitution occurs towards the less severely penalized drug when punishments are differentiated."], ["Quantity Discounts and Quality Premia for Illicit Drugs", "This article explores quantity discounts and quality (purity) premia in the prices of illicit drugs. It examines several models of how drug prices might depend on transaction size. A simple relation implied by a tree model of the domestic distribution network fits data provided by the Western States Information Network for 1984\u20131991 quite well for various illicit drugs. Quality premia are less well explained. It is observed that price is not a function of pure quantity alone; customers pay more for 2 grams at a given purity than they do for 1 gram at double that purity. Nevertheless, some purity premia are observed for white heroin, brown heroin, and powder cocaine, although not for methamphetamines, crack, or heroin tar. The estimated coefficients reflect known phenomena such as the collapses in the prices of cocaine and black tar heroin; intuitively reasonable but undocumented phenomena, such as discounts for brown heroin near the Mexican border; and some unexpected results, such as an apparent difference between the distribution of sinsemilla and that of other cannabis products."], ["Regression Analysis of Current-Status Data: An Application to Breast-Feeding", "Current-status data are often collected for use in survival analyses, because this type of data generally is considered to be more reliable than retrospective reports of when an event occurred. Although techniques for calculating mean survival time from current-status data are well known, their use in multiple regression models is somewhat troublesome. Using data on current breast-feeding behavior, this article considers a number of techniques that have been suggested in the literature, including parametric, nonparametric, and semiparametric models as well as the application of standard schedules. Models are tested in both proportional-odds and proportional-hazards frameworks. Although the choice of models does not strongly affect the conclusions that would be drawn, I recommend that a logistic regression in which the baseline log-odds of breast-feeding by child's age are represented by a natural cubic spline should be the preferred methodology. This methodology offers a reasonable compromise between the parsimony of parametric models and the flexibility and good fit of nonparametric models. The same methodology may be well suited to other applications in which there are no theoretical reasons to use a particular parametric form."], ["Choice Models for Predicting Divisional Winners in Major League Baseball", "Major league baseball in the United States is divided into two leagues and four divisions. Each team plays 162 games against teams in the same league. The winner in each division is the team winning the most games of the teams in that division. We wish to predict the division winners based on games played up to any specified time. We use a generalized choice model for the probability of a team winning a particular game that allows for different strengths for each team, different home advantages, and strengths varying randomly with time. Future strengths and the outcomes of future games are simulated using Markov chain sampling. The probability of a particular team winning the division is then estimated by counting the proportion of simulated seasons in which it wins the most games. The method is applied to the 1991 National League season."], ["Response and Sequencing Errors in Surveys: A Discrete Contagious Regression Analysis", "Skip sequencing in surveys reduces costs but also acts as a mechanism of contagion in transmitting error from one survey item to subsequent items. The error process is modeled as a contagious, stochastic one in which both the initiating and induced errors follow the Poisson distribution. The resulting compound distribution, first used by Thomas in 1949, is then used as the basis of a discrete contagious regression model in which characteristics of respondents, subject individuals, and interviewers are allowed to affect the intensity of both initiating and induced errors. The model is applied to data from the 1984 Survey of Income and Program Participation (SIPP) Reinterview Program. The results suggest that induced-sequencing errors are at least as important a source of losses in reliability as are initiating-response or spontaneous-procedural errors. The relative importance of induced-sequencing errors is directly proportional to the sequence length. Because the relationships between individual characteristics and the various types of error differ significantly, conclusions based on narrow definitions of response errors do not necessarily generalize when sequencing errors are included. The regression estimates suggest that older and less educated respondents provide less reliable data. Older female interviewers, however, are found to obtain more reliable data."], ["The Identification of Multiple Outliers", "One approach to identifying outliers is to assume that the outliers have a different distribution from the remaining observations. In this article we define outliers in terms of their position relative to the model for the good observations. The outlier identification problem is then the problem of identifying those observations that lie in a so-called outlier region. Methods based on robust statistics and outward testing are shown to have the highest possible breakdown points in a sense derived from Donoho and Huber. But a more detailed analysis shows that methods based on robust statistics perform better with respect to worst-case behavior. A concrete outlier identifier based on a suggestion of Hampel is given."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Computer-Intensive Methods for Tests about the Mean of an Asymmetrical Distribution", null], ["Some Graphical Displays and Marginal Regression Analyses for Recurrent Failure Times and Time Dependent Covariates", "Recurrent event time data are common in medical research; examples include infections in AIDS patients and seizures in epilepsy patients. In this context, as well as in the more usual context of a single failure time variable, time-dependent covariates are frequently of interest. We suggest some rate functions that might be displayed when analyzing recurrent failure time data or when the effect of a categorical time-dependent covariate is of interest. Estimators of these functions are provided along with two-sample test statistics. A new approach to regression modeling of these data is suggested and contrasted with existing methods. Our methods do not require that an explicit model be formulated for the probabilistic association between failure times within an individual. This is in line with the currently popular generalized estimating equation approach to longitudinal data. If the nature of such associations is known or is of particular interest, then alternative methods may be appropriate."], ["ATS Methods: Nonparametric Regression for Non-Gaussian Data", null], ["Exploring Regression Structure Using Nonparametric Functional Estimation", null], ["A Cross-Validatory Choice of Smoothing Parameter in Adaptive Location Estimation", "This article proposes a new data-driven method for selecting the smoothing parameter involved in constructing kernel-based adaptive location estimators. The method consists of minimizing a cross-validatory criterion with respect to the bandwidth occurring in the kernel-type estimators of the efficient score function. It is shown that the location estimator with a data-driven bandwidth selector is indeed an adaptive estimator. A simulation study reveals that the method is also practicable, showing that our estimator performs well in comparison with some other well-known location estimators. It also shows that our method has comparable finite sample performance with the bootstrap method of selecting the smoothing parameter and yet has great computational advantages."], ["Smoothing Bias in Density Derivative Estimation", "This article discusses a generic feature of density estimation by local smoothing, namely that estimated derivatives and location score vectors will display a systematic downward (attenuation) bias. We study the behavior of kernel estimators, indicating how the derivative bias arises and showing a simple result. We then consider the estimation of score vectors (negative log-density derivatives), which are motivated by the problem of estimating average derivatives and the adaptive estimation of regression models. Using \u201cfixed bandwidth\u201d limits, we show how scores are proportionally downward biased for normal densities and argue from normal mixture densities that proportional bias can be a reasonable approximation. We propose a simple diagnostic statistic for score bias."], ["Robustness of the Likelihood Ratio Test for a Change in Simple Linear Regression", "This article examines the robustness of the likelihood ratio tests for a change point in simple linear regression. We first summarize the normal theory of Kim and Siegmund, who have considered the likelihood ratio tests for no change in the regression coefficients versus the alternatives with a change in the intercept alone and with a change in the intercept and slope. We then discuss the robustness of these tests. Using the convergence theory of stochastic processes, we show that the test statistics converge to the same limiting distributions regardless of the underlying distribution. We perform simulations to assess the distributional insensitivity of the test statistics to a Weibull, a lognormal, and a contaminated normal distribution in two different cases: fixed and random independent variables. Numerical examples illustrate that the test has a correct size and retains its power when the distribution is nonnormal. We also study the effects of the independent variable's configuration with the aid of a numerical example."], ["A Bounded Influence, High Breakdown, Efficient Regression Estimator", null], ["Variable Selection via Gibbs Sampling", "A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability\u2014the promising ones\u2014can then be identified by their more frequent appearance in the Gibbs sample."], ["Sequential Analysis for Censored Regression Data", "Motivated by Buckley and James' modification of the least squares procedure for censored regression data, we derive a score process that incorporates the time evolution and handles staggered entry data in a natural way. By censored regression data we mean that the responses are subject to a possible right censorship. Such cases often arise from clinical trials and industrial life tests. The score process can be interpreted as a weighted comparison of (transformed) survival times. This is especially suitable for the accelerated failure time regression model. By expressing it in an appropriate form so that the counting process and its associate martingale theory can be applied, we show that the score process is approximated by a mean 0 multidimensional Gaussian process. A consistent estimator of its covariance matrix function is provided. Based on the covariance matrix estimator and the sequential test of Slud and Wei, a repeated significance test is then proposed. Usefulness of the procedure is illustrated with two well-known data sets: the beta-blocker heart attack trial data and the Stanford heart transplant data. Both data sets are also analyzed using the more familiar repeated log-rank test. Comparing results from the log-rank and the proposed tests shows that the two procedures tend to reach similar conclusions. Simulation studies are conducted to investigate the accuracy of the normal approximations when sample sizes are moderate and to compare efficiency with the commonly used log-rank statistic. The results indicate that the proposed test is superior when the underlying error distribution is normal, and the log-rank method is superior when the error distribution is extreme value. We also show how the proposed test can be modified to adjust for ancillary covariates."], ["Generalized Confidence Intervals", null], ["The Construction of Upper Confidence Bounds on the Range of Several Location Parameters", null], ["Some Statistical Procedures for Combining Independent Tests", "In many applications available data from several independent studies address the same question, and it is essential to have statistical methods for combining the results from the different studies. This article addresses this issue in two setups: (1) a testing hypothesis concerning the common mean vector of two independent linear models having different variances, and (2) a testing hypothesis concerning a common variance component in linear models involving two variance components. The interblock analysis of a balanced incomplete block design (BIBD) is a special case of (1) when we are interested in testing the equality of the treatment effects. Testing the significance of the treatment variance component in a BIBD with random effects is a special case of (2). We suggest some new test procedures for the testing problems in (1) and (2) and also give a review of the various existing tests. We numerically compare the powers of the various tests and make specific recommendations regarding the choice of the test to be used in practical applications."], ["On a Monotonicity Problem in Step-Down Multiple Test Procedures", null], ["Characterization of Regularity for Single Replicate Factorial Designs", "Single replicate designs are shown to be regular if and only if for each subset of factors there exists a partition of the corresponding treatment subcombinations such that, for each block, the treatment subcombinations in the block consist of the elements of one set of the partition equireplicated. Limitations of existing methods of confounding for single replicate designs are briefly discussed."], ["Optimal Data Augmentation Strategies for Additive Models", null], ["Exact Inference about the Within-Subject Variability in 2 \u00d7 2 Crossover Trials", null], ["Fitting Continuous ARMA Models to Unequally Spaced Spatial Data", "Methods for fitting continuous spatial autoregressive moving average (ARMA) models to unequally spaced observations in two dimensions are reviewed and extended. These are models with rational two-dimensional spectra. Assuming Gaussian input noise and observational errors, maximum likelihood methods are used to estimate the ARMA parameters and the regression coefficients of the deterministic trend. When the number of observations is too large for exact maximum likelihood estimation, approximate maximum likelihood estimation is used based on nearest neighbors. Comparisons of nearest-neighbor methods with exact likelihood methods are presented. Predictions of the height of the field at unobserved points can be calculated with confidence intervals."], ["Nonlinear Additive ARX Models", "We consider in this article a class of nonlinear additive autoregressive models with exogenous variables for nonlinear time series analysis and propose two modeling procedures for building such models. The procedures proposed use two backfitting techniques (the ACE and BRUTO algorithms) to identify the nonlinear functions involved and use the methods of best subset regression and variable selection in regression analysis to determine the final model. Simulated and real examples are used to illustrate the analysis."], ["Bayesian Inference and Prediction for Mean and Variance Shifts in Autoregressive Time Series", "This article is concerned with statistical inference and prediction of mean and variance changes in an autoregressive time series. We first extend the analysis of random mean-shift models to random variance-shift models. We then consider a method for predicting when a shift is about to occur. This involves appending to the autoregressive model a probit model for the probability that a shift occurs given a chosen set of explanatory variables. The basic computational tool we use in the proposed analysis is the Gibbs sampler. For illustration, we apply the analysis to several examples."], ["Semiparametric Bayesian Analysis of Multiple Event Time Data", null], ["Multiple Imputation in Mixture Models for Nonignorable Nonresponse with Follow-ups", "One approach to inference for means or linear regression parameters when the outcome is subject to nonignorable nonresponse is mixture modeling. Mixture models assume separate parameters for respondents and nonrespondents; implementation by multiple imputation consists of repeatedly filling in missing values for nonrespondents, estimating parameters using the filled-in data, and then adjusting for variability between imputations. We evaluated the performance of this scheme using simulated data with a 25% sample of nonrespondents followed up. We conclude that it provides a generally satisfactory and robust approach to inference for means and regression parameters in this case, although a greater number of imputations may be required for good performance compared to the number required for estimation when nonresponse is ignorable."], ["Empirical Bayes Estimation for the Finite Population Mean on the Current Occasion", null], ["Post-Stratification: A Modeler's Perspective", null], ["Generalized Raking Procedures in Survey Sampling", null], ["On the Accuracy of Fieller Intervals for Binary Response Data", "Finney proposed the use of a fiducial interval for the median response dose based on Fieller's theorem. An alternative is to use the asymptotic confidence interval. The simulations by Abdelbasit and Plackett suggest that the two intervals have similar coverage probabilities. We compare the two intervals theoretically and in an expanded simulation study. Our results show that Fieller intervals are generally superior. An attempt is made to characterize how and when the two intervals differ."], ["Improved Eaton Bounds for Linear Combinations of Bounded Random Variables, with Statistical Applications", null], ["Statistical Inference Procedures for Bivariate Archimedean Copulas", null], ["Undercount in the 1990 Census: Special Section", null], ["The 1990 Post-Enumeration Survey: Operations and Results", "The Census Bureau has struggled for decades with the problem of undercount in the population census. Although the net national undercount has been greatly reduced in recent censuses, it still tends to display important differences by race, ethnic origin, and geographic location. The 1990 Post-Enumeration Survey (PES) was designed to produce Census tabulation of states and local areas corrected for the undercount or overcount of population. The PES was the subject of litigation between the federal government and a coalition of states and local governments. Because of the litigation, the PES was conducted under specific guidelines concerning timing, prespecification, and quality. The PES measured Census omissions by independently interviewing a stratified sample of the population. It measured Census erroneous enumerations by a dependent reinterview of a sample of Census records and by searching the records for duplicates. A dual-system estimator (DSE) was used to prepare estimates of the population by post-strata. Adjustment factors were computed as the ratio of these estimates to the census count. These factors were smoothed using a generalized linear model and then applied to the census counts by block and post-strata to produce adjusted census estimates. Although the government decided not to release these numbers as the official census results, the Census Bureau has conducted further research to improve these estimates to incorporate them into the postcensal estimates program. The revisions have included new post-strata and corrections of errors found in the original estimates. The results of the PES show a differential undercount by race and ethnic group and by owner/nonowner status. They also demonstrate differences in undercount by geography."], ["Estimation of Population Coverage in the 1990 United States Census Based on Demographic Analysis", "This article presents estimates of net coverage of the national population in the 1990 census, based on the method of demographic analysis. The general techniques of demographic analysis as an analytic tool for coverage measurement are discussed, including use of the demographic accounting equation, data components, and strengths and limitations of the method. Patterns of coverage displayed by the 1990 estimates are described, along with similarities or differences from comparable demographic estimates for previous censuses. The estimated undercount in the 1990 census was 4.7 million, or 1.85%. The undercount of males (2.8%) was higher than for females (.9%), and the undercount of Blacks (5.7%) exceeded the undercount of Non-Blacks (1.3%). Black adult males were estimated to have the highest rate of undercounting of all groups. Race-sex-age patterns of net coverage in the 1990 census were broadly similar to patterns in the 1980 and 1970 censuses. A final section presents the results of the first statistical assessment of the uncertainty in the demographic coverage estimates for 1990."], ["Comment Uncertainty in Demographic Analysis", null], ["Comment", null], ["Rejoinder", null], ["Accuracy of the 1990 Census and Undercount Adjustments", "In July 1991 the Census Bureau recommended to its parent agency, the Department of Commerce, that the 1990 census be adjusted for undercount. The Secretary of Commerce decided not to adjust, however. Those decisions relied at least partly on the Census Bureau's analyses of the accuracy of the census and of the proposed undercount adjustments based on the Post-Enumeration Survey (PES). Error distributions for the nation, states, and smaller geographic units were estimated with extensions of methods applied to test censuses. To summarize and assess the relative importance of errors in different units, the Census Bureau used aggregate loss functions. This article describes the total error analysis and loss function analysis of the Census Bureau. In its decision not to adjust the census, the Department of Commerce cited different criteria than aggregate loss functions. Those criteria are identified and discussed."], ["Combining Census, Dual-System, and Evaluation Study Data to Estimate Population Shares", "The 1990 census and Post-Enumeration Survey produced census and dual system estimates (DSE) of population by domain, together with an estimated sampling covariance matrix of the DSE. Estimates of the bias of the DSE were derived from various PES evaluation programs. Of the three sources, the unadjusted census is the least variable but is believed to be the most biased, the DSE is less biased but more variable, and the bias estimates may be regarded as unbiased but are the most variable. This article addresses methods for combining the census, the DSE, and bias estimates obtained from the evaluation programs to produce accurate estimates of population shares, as measured by weighted squared- or absolute-error loss functions applied to estimated population shares of domains. Several procedures are reviewed that choose between the census and the DSE using the bias evaluation data or that average the two with weights that are constant across domains. A multivariate hierarchical Bayes model is proposed for the joint distribution of the undercount rates and the biases of the DSE in the various domains. The specification of the model is sufficiently flexible to incorporate prior information on factors likely to be associated with undercount and bias. When combined with data on undercount and bias estimates, the model yields posterior distributions for the true population shares of each domain. The performance of the estimators was compared through an extensive series of simulations. The hierarchical Bayes procedures are shown to outperform the other estimators over a wide range of conditions and to be robust against misspecification of the models. The various composite estimators, applied to preliminary data from the 1990 Census and evaluation programs, yield similar results that are closer to the DSE than to the census. Analysis of a revised data set yields qualitatively similar estimates but shows that the revised post-stratification improves on the original one."], ["Using Information from Demographic Analysis in Post-Enumeration Survey Estimation", "Population estimates from the 1990 Post-Enumeration Survey (PES), used to measure decennial census undercount, were obtained from dual system estimates (DSE's) that assumed independence within strata defined by age-race-sex-geography and other variables. We make this independence assumption for females, but develop methods to avoid the independence assumption for males within strata by using national level sex ratios from demographic analysis (DA). This is done by using DSE results for females and the DA sex ratios to determine national level control totals for male population by age-race groups. These control totals are then used to determine some function of the individual strata 2 \u00d7 2 table probabilities for males that is assumed constant across strata within age-race groups. One such candidate function is the cross-product ratio, but other functions can be used that lead to different DSEs. We consider several such alternative DSE's, and use DA results for 1990 to apply them to data from the 1990 U.S. census and PES."], ["Assessing Between-Block Heterogeneity within the Post-Strata of the 1990 Post-Enumeration Survey", "The 1990 Post-Enumeration Survey (PES) stratified the population into 1,392 subpopulations called post-strata based on location, race, tenure, sex and age, in the hope that these subpopulations were homogeneous in relation to factors affecting the Census coverage. Homogeneity is necessary to justify the use of the same adjustment factor for many, sometimes quite small, subgroups of the post-strata. With block-level data from the PES for sites around Detroit and Texas, we are able to examine empirically the extent to which this hope was realized. Using various measures, we find that between-block variation in erroneous enumeration and gross omission rates is about the same magnitude as, and largely in addition to, the corresponding between-post-stratum variation."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Estimating Heterogeneity in the Probabilities of Enumeration for Dual-System Estimation", "We show how conditional logistic regression can be used to estimate the probability of being enumerated in a census and apply the model to the 1990 Post-Enumeration Survey (PES) in the United States. The estimates can be used in the estimation of population size and the estimation of correlation bias, for example. Unlike the classical stratification approach, the logistic approach permits the use of continuous explanatory variables. Model choice can be based on the standard techniques of the generalized linear models. We discuss some special problems caused by the fact that the PES sample area is open to migration between the captures. We also consider the effect of data errors in estimation. We characterize hard-to-enumerate populations and give some tentative estimates of correlation bias."], ["A Three-Sample Multiple-Recapture Approach to Census Population Estimation with Heterogeneous Catchability", null], ["Hierarchical Logistic Regression Models for Imputation of Unresolved Enumeration Status in Undercount Estimation", "In the process of collecting Post-Enumeration Survey (PES) data to evaluate census coverage, it is inevitable that there will be some individuals whose enumeration status (outcome in the census-PES match) remains unresolved even after extensive field follow-up operations. Earlier work developed a logistic regression framework for imputing the probability that unresolved individuals were enumerated in the census, so that the probability of having been enumerated is allowed to depend on covariates. The covariates may include demographic characteristics, geographic information, and census codes that summarize information on the characteristics of the match (e.g., the before-follow-up match code assigned by clerks to describe the type of match between PES and census records). In the production of 1990 undercount estimates, the basic logistic regression model was expanded into a mixed hierarchical model to allow for the presence of group-specific effects, where groups are characterized by common before-follow-up match code. Parameter estimates for individual match-code groups thus \u201cborrow strength\u201d across groups by making use of observed relationships between group-specific parameter estimates in the various groups and the characteristics of the groups. This allows predictions to be made for groups for which there are few or no resolved cases to which to fit the model. The model was fitted by an approximate expectation-conditional-maximization (ECM) algorithm, using a large-sample approximation to the posterior distributions of group parameters. Uncertainty in estimation of model parameters was evaluated using a resampling procedure and became part of the evaluation of total error in PES estimates of population. Results from fitting the model in the 1990 Census and PES are described."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Book Reviews", null], ["Publications Received", null], ["Editorial Board Page", "This article has no abstract"], ["Editors' Report for 1992", null], ["Regional Trends in Sulfate Wet Deposition", "I propose a multiple time series model for data from a network of monitoring stations that have both temporal and spatial correlation. The model includes a separate mean and trend for each monitoring station and obtains spatial estimates of mean and trend by smoothing the observed values over a rectangular grid using a discrete smoothing prior. Smoothing parameters and covariance estimates can be chosen subjectively or selected using indirect generalized cross-validation. The gridded values and their standard errors can be used for several purposes, including inference on regional means or trends and improving monitoring networks via station rearrangement."], ["On the Use of Cause-Specific Failure and Conditional Failure Probabilities: Examples from Clinical Oncology Data", null], ["Experimental Design for Clonogenic Assays in Chemotherapy", "One approach to estimating the number of cells with high growth potential in a cancer patient is the assessment of tumor cell colony formation in semisolid medium. A potential indicator of the clinical response to a specific drug is the capacity of the drug to reduce the formation of cell colonies. Often, a Poisson log-linear model provides an adequate representation of the dose-response curve. This model is then summarized by the dose required to reduce the number of colonies to a predetermined percentage of the maximum growth. But more general models are needed to account for the overdispersion and the resistant subpopulations often present in these assays. This article explores alternative methods for selecting the dose levels to obtain precise estimates of the parameters of interest. Optimal dose allocations for a single patient or a group of patients are derived. Methods to incorporate the information from pilot studies are discussed. A study of drug sensitivity with leukemia patients provides the framework and motivation for the problem."], ["Statistical Modeling of Expert Ratings on Medical Treatment Appropriateness", "This article uses latent structure analysis to model ordered category ratings by multiple experts on the appropriateness of indications for the medical procedure carotid endarterectomy. The statistical method used is a form of located latent class analysis, which combines elements of latent class and latent trait analysis. It assumes that treatment indications fall into distinct latent classes, with each latent class corresponding to a different level of appropriateness. The appropriateness rating of a treatment indication by a rater is assumed determined by the latent class membership of the indication, rating category thresholds of the rater, and random measurement error. The located latent class model has two alternative forms: a normal ogive form, which derives from the assumption of normally distributed measurement error, and a logistic approximation to the normal form. The approach has the following advantages for the analysis of ordered category ratings by multiple experts: (1) it assesses whether different raters base ratings on the same or different criteria; (2) it assesses rater bias\u2014the tendency of some raters to make higher or lower ratings than others; (3) it characterizes rater differences in rating category definitions; (4) it provides theoretically based methods for combining the ratings of different raters; and (5) it provides a description of the distribution of the latent trait. The data examined are appropriateness ratings on 848 indications for carotid endarterectomy made by nine medical experts. The located latent class approach provides unique insights concerning the data. It identifies what appears to be a set of clear nonindications for carotid endarterectomy, but a corresponding set of clear indications is not evident. The results indicate that all raters measured a common latent trait of treatment appropriateness, but that some measured the trait better than others. Rater differences in overall bias and rating category definitions are evident. Two methods are used to combine raters' ratings. One uses ratings to calculate a continuous appropriateness score for each indication. The other uses ratings to assign indications to discrete outcome categories, each corresponding to a specific level of appropriateness. The located latent class approach for ordered category measures has possible applications besides the analysis of expert ratings, such as item analysis. Potential extensions of the model are discussed."], ["A Bayesian Sequential Experimental Study of Learning in Games", null], ["An Algorithm for the Detection and Measurement of Rail Surface Defects", "Defects on the surface of railroad tracks have been the cause of growing concern over the past three decades. The automated detection and classification of rail surface defects would be of great assistance to rail maintenance planners, who develop grinding strategies to prevent the development of potentially dangerous deterioration. Videotaped images of the surface of rail have been obtained, but they are subject to distortions due to the acquisition process as well as physical phenomena on the track itself. In this analysis, an algorithm is presented for the simultaneous restoration and segmentation of objects in a two-dimensional image. The algorithm relies on distributions that model the relationships between sites and neighbors in order to restore a distorted image to an estimate of its ideal form, and also obtain detailed information about the objects located in the image. The foundation of the algorithm is the Iterated Conditional Modes procedure for image restoration. The resulting extension is capable of providing detailed measurements of the geometric features of objects detected in an image. The extended algorithm is applied to an image distorted by simulated noise, and also to an image taken from a videotape of a rail surface. The results of the analysis demonstrate the potential for accurate detection, measurement, and classification of rail surface defects."], ["Predicting Shifts in the Mean of a Multivariate Time Series Process: An Application in Predicting Business Failures", "A firm in the early stages of financial distress exhibits characteristics different from those of healthy firms. As the economic condition of a firm worsens, its financial characteristics shift toward those of failed firms. Practitioners in the financial sector have long been interested in the early detection of a firm's slide toward insolvency. Several models have been developed with this purpose in mind, but these older models are static in nature. Therefore, a need exists for the development of business failure prediction models that assess the financial condition of firms sequentially over time. This article addresses this need by presenting a sequential business failure prediction model."], ["Integration of Multimodal Functions by Monte Carlo Importance Sampling", null], ["Survival Trees by Goodness of Split", "A tree-based method for censored survival data is developed, based on maximizing the difference in survival between groups of patients represented by nodes in a binary tree. The method includes a pruning algorithm with optimal properties analogous to the classification and regression tree (CART) pruning algorithm. Uniform convergence of the estimates of the conditional cumulative hazard and survival functions is discussed, and an example is given to show the utility of the algorithm for developing prognostic classifications for patients."], ["Multivariate Normal Mixtures: A Fast Consistent Method of Moments", "A longstanding difficulty in multivariate statistics is identifying and evaluating nonnormal data structures in high dimensions with high statistical efficiency and low search effort. Here the possibilities of using sample moments to identify mixtures of multivariate normals are investigated. A particular system of moment equations is devised and then shown to be one that identifies the true mixing distribution, with some limitations (indicated in the text), and thus provides consistent estimates. Moreover, the estimates are shown to be quickly calculated in any dimension and to be highly efficient in the sense of being close to the values of the parameters that maximize the likelihood function. This is shown by simulation and the application of the method to Fisher's iris data. While establishing these results, we discuss certain limitations associated with moment methods with regard to uniqueness and equivariance and explain how we addressed these problems."], ["A Semiparametric Bootstrap Technique for Simulating Extreme Order Statistics", null], ["Linear Model Selection by Cross-validation", null], ["Smoothing Spline Density Estimation: A Dimensionless Automatic Algorithm", "As a sequel to an earlier article by Gu and Qiu, this article describes and illustrates a dimensionless automatic algorithm for nonparametric probability density estimation using smoothing splines. The algorithm is designed to calculate an adaptive finite dimensional solution to the penalized likelihood problem, which was shown by Gu and Qiu to share the same asymptotic convergence rates as the nonadaptive infinite dimensional solution. The smoothing parameter is updated jointly with the estimate in a performance-oriented iteration via a cross-validation performance estimate, where the performance is measured by proxies of the symmetrized Kullback-Leibler distance between the true density and the estimate. Simulations of limited scale are conducted to examine the relative effectiveness of the automatic smoothing parameter selection procedure and to assess the practical statistical performance of the methodology in general. The method is also applied to some real data sets. The algorithm is implemented in a few Ratfor routines, which will be included in later versions of RKPACK for public access."], ["Robust Singular Value Decompositions: A New Approach to Projection Pursuit", null], ["Unmasking Outliers and Leverage Points: A Confirmation", "Identification of multiple outliers and leverage points is difficult because of the masking effect. Recently, Rousseeuw and van Zomeren suggested using high-breakdown robust estimation methods\u2014the least median of squares and minimum volume ellipsoid\u2014for unmasking these observations. These methods tend to declare too many observations as extreme, however. A stepwise analysis is proposed here for confirmation of outliers and leverage points detected using the robust methods. Diagnostic measures are constructed for observations added back to the reduced sample. They are shown graphically. The complementary use of robust and diagnostic methods gives satisfactory results in analyzing two data sets. One data set consists often bad and four good leverage points. Four (or 10, using a different cutoff) extreme observations of the other data set (of size 28) are identified using the robust methods, but the stepwise analysis confirms only one. The limitations of Atkinson's confirmatory approach are discussed and illustrated."], ["Comparison of Smoothing Parameterizations in Bivariate Kernel Density Estimation", "The basic kernel density estimator in one dimension has a single smoothing parameter, usually referred to as the bandwidth. For higher dimensions, however, there are several options for smoothing parameterization of the kernel estimator. For the bivariate case, there can be between one and three independent smoothing parameters in the estimator, which leads to a flexibility versus complexity trade-off when using this estimator in practice. In this article the performances of the different possible smoothing parameterizations are compared, using both the asymptotic and exact mean integrated squared error. Our results show that it is important to have independent smoothing parameters for each of the coordinate directions. Although this is enough for many situations, for densities with high amounts of curvature in directions different to those of the coordinate axes, substantial gains can be made by allowing the kernel mass to have arbitrary orientations. The \u201csphering\u201d approaches to choosing this orientation are shown to be detrimental in general, however."], ["Adaptive Smoothing and Density-Based Tests of Multivariate Normality", "Methods of adaptive smoothing of density estimates, where the amount of smoothing applied varies according to local features of the underlying density, are investigated. The difficulties of applying Taylor series arguments in this context are explored. Simple properties of the estimates are investigated by numerical integration and compared with the fixed kernel approach. Optimal smoothing strategies, based on the multivariate Normal distribution, are derived. As an application of these techniques, two tests of multivariate Normality\u2014one based on integrated squared error and one on entropy\u2014are developed, and some power calculations are carried out."], ["Nonlinear Experiments: Optimal Design and Inference Based on Likelihood", null], ["Assessing Influence in Variable Selection Problems", "Variable selection techniques are often used in combination with multiple linear regression to produce a parsimonious model that fits the data well. It is clearly undesirable for the final model to depend strongly on the inclusion of a few influential cases in the data set. This article discusses a measure of influence of single cases on the final model, based on a similar measure used in ordinary multiple regression. When variables are selected objectively, deletion of individual cases can strongly affect the choice of model. The influence of individual cases on the parameters of the selected model are often assessed as part of the model building process. However, such conditional measures fail to evaluate the influence of the cases on the variable selection process. Modern computing environments make it feasible to use an unconditional criterion to determine the influence of each case on the selection procedure. A number of examples are discussed to illustrate the differences between these approaches. Heuristics are developed to explain the examples. We conclude that, although the conditional approach gives valuable information about the selected model, the use of the unconditional approach can lead to greater insight about the influence of individual observations on the process of model selection."], ["Linear Regression Analysis for Highly Stratified Failure Time Data", "In this article we consider cases where data consist of many small and independent groups of correlated failure time observations. For each failure time, which may be censored, some important covariates are also recorded. Our goal is to examine the covariate effects on the individual failure time observations. We assume that the logarithm of each failure time is linearly related to its covariates. We then take a population-averaged model approach to obtain inference procedures for the regression parameters without specifying the joint distribution of the observations within the group. The new proposals do not need complicated and unstable nonparametric estimates for the hazard function of the error term. Their properties are extensively examined for practical sample sizes. Comparisons among various procedures are also performed. All the methods studied in this article are illustrated with examples."], ["On Estimating a Survival Curve Subject to a Uniform Stochastic Ordering Constraint", null], ["Identifiability of Bivariate Survival Curves from Censored Data", null], ["The Accuracy of Elemental Set Approximations for Regression", "The elemental set algorithm involves performing many fits to a data set, each fit made to a subsample of size just large enough to estimate the parameters in the model. Elemental sets have been proposed as a computational device to approximate estimators in the areas of high breakdown regression and multivariate location/scale estimation, where exact optimization of the criterion function is computationally intractable. Although elemental set algorithms are used widely and for a variety of problems, the quality of the approximation they give has not been studied. This article shows that they provide excellent approximations for the least median of squares, least trimmed squares, and ordinary least squares criteria. It is suggested that the approach likely will be equally effective in the other problem areas in which exact optimization of a criterion is difficult or impossible."], ["Maximum Likelihood Estimation of Regression Models with Stochastic Trend Components", "This article is concerned with the estimation of a regression model with a stochastic trend component. It is shown that the probability of estimating the trend to be deterministic is very sensitive to the type of likelihood function used as the basis of inference."], ["Testing for a Moving Average Unit Root in Autoregressive Integrated Moving Average Models", null], ["Nonparametric Estimation of Rate Equations for Nutrient Uptake", null], ["Shrinkage Estimators of Relative Potency", "This article examines the finite and infinite sample properties of the shrinkage estimator, motivated by a Bayesian argument, for the log relative potency, proposed in an earlier paper by Kim, Carter, and Hubert. This estimator can be written in closed form and is shown to have finite mean and finite variance in finite samples. As a consequence, this shrinkage estimator has finite frequentist risk, which is an improvement over the usual maximum likelihood estimator, for all finite sample sizes. Furthermore, it is shown that this estimator asymptotically behaves the same as the usual maximum likelihood estimator."], ["On Optimal Screening Ages", "Several chronic diseases are characterized by an initial asymptomatic stage during which, if detected by screening, they can be cured in a more effective way. This article considers two statistical design problems in screening for chronic disease: the choice of examination ages and the choice of the part of the population to be screened. One main goal is capturing the trade-off between the costs of examination and the losses due to late detection, while accounting for the effects of age on the incidence of the disease, on mortality, and on the relative advantages of early detection. The problem is posed in a decision theoretic way. The model adopted considers a single individual, whose history relative to the disease is represented by a discrete-valued stochastic process. The transition structure is general, but known. The decision space includes all sequences of examination times, as well as no examination. The optimality criterion accounts for the cost of examinations and, in a general way, for the goals of screening in terms of mortality and morbidity. So the optimality criterion may depend on survival, quality-adjusted life years, cost of care, and so on, as well as on combinations of these factors. A general solution and computational algorithms are derived by extending to this context methodologies developed in reliability theory. The case in which the test used for screening has high sensitivity is studied in detail; then the determination of the optimal schedule and stopping rule is reduced to a one-dimensional optimization problem by recursive dynamic methods. Moreover, sufficient conditions for screening to be increasingly worthwhile with age are derived. Under these conditions, the optimal number of planned examinations is either 0 or infinity, and there is a simple check to establish whether or not to screen without having to compute the optimal schedule. Under slightly stronger conditions, the times between examinations decrease and the optimal solution is unique and easy to compute. The conditions mentioned relate increasing times between checks to properties of the failure rate of the time to onset of the disease and of the relative incidence of the disease. Applications of the results include developing guidelines for screening for breast and cervical cancers\u2014currently a controversial issue."], ["Hypothesis Testing with Complex Survey Data: The Use of Classical Quadratic Test Statistics with Particular Reference to Regression Problems", null], ["Experimental Designs for Model Discrimination", null], ["Combining Independent Tests in Linear Models", null], ["A New Confidence Interval Method Based on the Normal Approximation for the Difference of Two Binomial Probabilities", "Using a general method for obtaining confidence intervals from samples from discrete distributions, this article introduces a new confidence interval method for estimating the difference of two binomial probabilities and compares it to three other confidence interval methods, one of which is the usual method with no continuity correction. Each of the other two confidence interval methods uses its own continuity correction; one combines it with an estimate of the standard error that is slightly different from that commonly used. Some values of the \u201cexact\u201d confidence interval limits are also derived. The four confidence interval methods, each of which is based on the normal approximation and can be carried out easily on a hand calculator, are compared in terms of their precision, the agreement of their coverage probabilities with nominal confidence level values, and the smallness of their sample sizes before the normal approximation can be considered appropriate. Coverage probabilities and measures of precision are computed exactly rather than estimated by simulation. On the basis of these comparisons, the new confidence interval method is recommended."], ["Estimating Hot Numbers and Testing Uniformity for the Lottery", "We consider the problem of estimating probabilities for the different numbers in a lottery when the winning numbers are selected sequentially without replacement. Numerical methods are developed to compute maximum likelihood estimates of the probabilities and their variance-covariance matrix based on observed outcomes. The generalized likelihood ratio statistic \u20132 ln(\u03bb) for testing equiprobable numbers is also given. The methods are applied to a data set from the Lotto America lottery. The properties of the procedure are studied by simulation."], ["Bayesian Analysis of Binary and Polychotomous Response Data", null], ["Saddlepoint Approximations to the CDF of Some Statistics with Nonnormal Limit Distributions", "In standard saddlepoint approximations to the cumulative distribution function of a random variable, the normal distribution has appeared to play a special role. In this article we consider what happens when the normal \u201cbase\u201d distribution is replaced by an arbitrary base distribution. Generalized versions of several standard formulas, are presented. The choice of a chi-squared base or an inverse Gaussian base is then considered in detail. The generalized approximations are compared in two examples: a linear combination of chi-squared variables and the first passage time distribution for a random walk. The former example considers approximations using the chi-squared base that are slightly more accurate than their normal-based counterparts. In the latter example, approximations based on the inverse Gaussian are considerably more accurate than their normal-based counterparts."], ["Bayes and Empirical Bayes Procedures for Comparing Parameters", null], ["Book Reviews", null], ["Publications Received", null], ["Comment on Hasofer and Wang", null], ["\u201cTilt\u201d", null], ["Comment on Potthoff, Woodbury, and Manton", null], ["Reply to Kott's Letter", null], ["Correction", null], ["Editorial Board Page", "This article has no abstract"], ["Editorial Board", null], ["Enhancing Statistical Literacy: Enriching Our Society", null], ["Approximate Inference in Generalized Linear Mixed Models", "Statistical approaches to overdispersion, correlated errors, shrinkage estimation, and smoothing of regression relationships may be encompassed within the framework of the generalized linear mixed model (GLMM). Given an unobserved vector of random effects, observations are assumed to be conditionally independent with means that depend on the linear predictor through a specified link function and conditional variances that are specified by a variance function, known prior weights and a scale factor. The random effects are assumed to be normally distributed with mean zero and dispersion matrix depending on unknown variance components. For problems involving time series, spatial aggregation and smoothing, the dispersion may be specified in terms of a rank deficient inverse covariance matrix. Approximation of the marginal quasi-likelihood using Laplace's method leads eventually to estimating equations based on penalized quasilikelihood or PQL for the mean parameters and pseudo-likelihood for the variances. Implementation involves repeated calls to normal theory procedures for REML estimation in variance components problems. By means of informal mathematical arguments, simulations and a series of worked examples, we conclude that PQL is of practical value for approximate inference on parameters and realizations of random effects in the hierarchical model. The applications cover overdispersion in binomial proportions of seed germination; longitudinal analysis of attack rates in epilepsy patients; smoothing of birth cohort effects in an age-cohort model of breast cancer incidence; evaluation of curvature of birth cohort effects in a case-control study of childhood cancer and obstetric radiation; spatial aggregation of lip cancer rates in Scottish counties; and the success of salamander matings in a complicated experiment involving crossing of male and female effects. PQL tends to underestimate somewhat the variance components and (in absolute value) fixed effects when applied to clustered binary data, but the situation improves rapidly for binomial observations having denominators greater than one."], ["The AIDS Epidemic: Estimating Survival After AIDS Diagnosis From Surveillance Data", null], ["Using Events From Dropouts in Nonparametric Survival Function Estimation With Application to Incubation of AIDS", null], ["Use of Compliance Measures in an Analysis of the Effect of Diltiazem on Mortality and Reinfarction After Myocardial Infarction", "A clinical trial efficacy analysis based on actual drug usage is described. The influence of diltiazem therapy on mortality and reinfarction in the multicenter diltiazem post-infarction trial (MDPIT) is analyzed using records for drug discontinuation and reinitiation; the results are then compared with the previously published \u201cintention to treat\u201d analysis. As expected, previously reported beneficial effects of diltiazem therapy in patients without pulmonary congestion and previously reported harmful effects in patients with pulmonary congestion are strengthened for patients while on study medication; both effects are weakened for those not on study medication. It is also shown that for patients assigned to placebo, being on or off study medication is a powerful prognostic indicator of subsequent outcome events, especially among patients with pulmonary congestion. Analysis of discontinuation rates suggested that patients assigned to diltiazem therapy were likely to discontinue trial medication earlier than were patients assigned to placebo, especially for those patients with pulmonary congestion."], ["Evaluating an Anti-Drinking and Driving Advertising Campaign With a Sample Survey and Time Series Intervention Analysis", "The impact of a paid advertising campaign targeted at reducing youthful male drinking-driving behavior is examined using (1) pretest and posttest sample surveys taken at both a campaign site and a control site and (2) time series intervention modeling of monthly traffic accident data from both sites. These compatible analyses provide collaborative evidence that the advertising campaign reduced youthful male drinking and driving behavior and, consequently, traffic accidents."], ["Is Pitman Closeness a Reasonable Criterion?", "The criterion of Pitman closeness has been proposed as an alternative comparison criterion to quadratic losses and, more generally, to decision theory. However, it may lead to quite paradoxical phenomena, the most dramatic being a possible nontransitivity. The criterion takes into consideration the joint distribution of the compared estimators, but this consideration may be misleading in the selection of the \u201cbest\u201d estimator. We show through examples that this criterion is not consistent with a decision theoretic analysis and that it should be used very cautiously, if ever."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["A Surprising Covariance Involving the Minimum Of Multivariate Normal Variables", null], ["Simpson's Paradox and Related Phenomena", null], ["Poststratification and Conditional Variance Estimation", "Poststratification estimation is a technique used in sample surveys to improve efficiency of estimators. Survey weights are adjusted to force the estimated numbers of units in each of a set of estimation cells to be equal to known population totals. The resulting weights are then used in forming estimates of means or totals of variables collected in the survey. For example, in a household survey the estimation cells may be based on age/race/sex categories of individuals, and the known totals may come from the most recent population census. Although the variance of a poststratified estimator can be computed over all possible sample configurations, inferences made conditionally on the achieved sample configuration are desirable. Theory and a simulation study using data from the U.S. Current Population Survey are presented to study both the conditional bias and variance of the poststratified estimator of a total. The linearization, balanced repeated replication, and jackknife variance estimators are also examined to determine whether they appropriately estimate the conditional variance."], ["A Unified Approach to Measures of Privacy in Randomized Response Models: A Utilitarian Perspective", null], ["On Recensoring for Censored Paired Data", null], [null, null], ["Pattern-Mixture Models for Multivariate Incomplete Data", null], ["Statistical Inference Based on Pseudo-Maximum Likelihood Estimators in Elliptical Populations", null], ["Maximum Weighted Partial Likelihood Estimators for the Cox Model", null], ["Testing Against Ordered Alternatives for Censored Survival Data", null], ["Bahadur Efficiency of Tests of Separate Hypotheses and Adaptive Test Statistics", "The notion of separability of hypotheses was first introduced by Cox (1961); two families of hypotheses are separate if no distribution in one family can be obtained as a limit of distributions from the other family. For testing separate hypotheses, Cox (1961, 1962) suggested the test based on the comparison of the logarithm of the likelihood ratio with its expected value (or estimate thereof) under each hypothesis. The tests of such hypotheses are required to have high power against the specified alternatives; when nuisance parameters are present, this concept leads to the notion of an adaptive test, which by definition must be asymptotically efficient for any value of the unknown nuisance parameter. A testing problem of separate hypotheses for an exponential family is studied from the standpoint of Bahadur asymptotic optimality. A formula for the Bahadur-exact slope of any smooth test statistic is obtained, and in the example of testing lognormality versus exponentiality, it is shown that Cox's test can have zero Bahadur efficiency. A necessary and sufficient condition for the existence of an adaptive test statistic is derived using a result of Rubin and Rukhin (1983). This condition is met if the test is required to be asymptotically optimal at the least favorable alternative; that is, at the point that is the \u201cclosest\u201d in terms of Kullback-Leibler divergence."], ["Loss in Efficiency Caused by Omitting Covariates and Misspecifying Exposure in Logistic Regression Models", "We examine the effects of omitting covariates and misspecifying the explanatory variable of interest on tests of association between the explanatory variable and response based on logistic regression models. We assume throughout that the covariates are statistically independent of the explanatory variable, as under simple randomization. A general expression for the asymptotic loss in efficiency from omitting covariates and misspecifying the explanatory variable is derived. The expression for the asymptotic efficiency of the misspecified test statistic relative to the correctly specified test can be factored into two parts, each of which is less than or equal to 1. The first part reflects the consequences of misspecifying exposure, whereas the other part captures the effect of omitting needed covariates. This result permits numerical evaluation of the approximate loss in efficiency and can be useful in developing guidelines for study design and model-fitting procedures. We discuss applications of the result for several special cases."], ["Prediction of Random Effects in the Generalized Linear Model", "This article develops an estimating function-based approach to component estimation in the two-stage generalized linear model with univariate random effects and a vector of fixed effects. The novelty and unifying feature of the method is the use of estimating functions in the estimation of both the random effects and their variance. Two separate estimating procedures based on the method are proposed that differ in the intensity of numerical computation required. The estimating function approach is especially valuable in the longitudinal setting where the response variable is discrete and the number of repeated observations on each unit is small. Other key features of this empirical Bayes technique are that it uses all available data, it yields familiar forms for the estimators as special cases, and it is less computationally intensive than other methods designed to address the same problem. An application to the estimation of trends in acquired immune deficiency syndrome (AIDS) incidence across risk groups and geographical regions is presented."], ["Iterative Weighted Least Squares Estimation in Heteroscedastic Linear Models", null], ["Case-Control Studies With Errors in Covariates", null], ["Inferences About Exposure-Disease Associations Using Probability-of-Exposure Information", "Unconditional and conditional likelihood methods are proposed for modeling odds and odds ratios relating a binary disease variable and an exposure variable that is only known probabilistically (e.g., via measurement of a Surrogate exposure variable), while adjusting appropriately for a vector of covariables. Assuming nondifferential errors, a probabilistic structure is developed that permits analysis of the marginal relation between disease and the Surrogate while maintaining important properties of the marginal relation between disease and true exposure. In particular, analyses conditioned on marginal totals in surrogate\u2013disease tables remove the same nuisance Parameters as are removed in a conditional analysis of true exposure-disease tables. In addition, a relation between the probability of exposure (POE) in the disease and disease-free groups is derived, permitting the use of information about exposure from populations with structure different from the study population. Examples are presented illustrating methods appropriate when the POE values are known or when they must be estimated using partial information on true exposure; in this latter case, the exposure data may be considered to be missing at random."], ["Mixture Estimation From Multichannel Image Data", null], ["Conjugate Gradient Acceleration of the EM Algorithm", "The EM algorithm is a very popular and widely applicable algorithm for the computation of maximum likelihood estimates. Although its implementation is generally simple, the EM algorithm often exhibits slow convergence and is costly in some areas of application. Past attempts to accelerate the EM algorithm have most commonly been based on some form of Aitken acceleration. Here we propose an alternative method based on conjugate gradients. The key, as we show, is that the EM step can be viewed (approximately at least) as a generalized gradient, making it natural to apply generalized conjugate gradient methods in an attempt to accelerate the EM algorithm. The proposed method is relatively simple to implement and can handle Problems with a large number of parameters, an important feature of most EM algorithms. To demonstrate the effectiveness of the proposed acceleration method, we consider its application to several Problems in each of the following areas: estimation of a covariance matrix from incomplete multivariate normal data, confirmatory factor analysis, and repeated measures analysis. The examples in these areas demonstrate promise for the new acceleration method. In terms of Operation counts, for all of the examples considered the accelerated EM algorithm increases the speed of the EM algorithm, in some cases by a factor of 10 or more. In the context of repeated measures analysis, we give a new EM algorithm that, compared to earlier algorithms, can have a considerably smaller cost per iteration. We have not, however, attempted to evaluate the performance of this latter algorithm here."], ["A Fast Algorithm for the Repeated Evaluation of the Likelihood of a General Linear Process for Long Series", null], ["Computation of High Breakdown Nonlinear Regression Parameters", null], ["Rank-Based Methods for Multivariate Linear Models", null], ["A Quality Index Based on Data Depth and Multivariate Rank Tests", null], ["Robust Estimation, Nonnormalities, and Generalized Exponential Distributions", null], ["Bias Robust Estimation in Finite Populations Using Nonparametric Calibration", null], [null, "This article gives sufficient conditions that guarantee the existence of minimizers of special classes of functions. Using these results, a certain dass of minimum distance (MD) estimators of the autoregressive parameters is shown to exist under the additive and innovative outliers; that is, autoregression (AO and IO) models. Through simulations, a certain subclass of MD estimators is shown to perform as good as or in some cases even better than other classical estimators like GM and functional least square estimators under the IO and AO models. Simulation results also show that MD estimators are robust against AO's."], ["Joint Estimation of Model Parameters and Outlier Effects in Time Series", "Time series data are often subject to uncontrolled or unexpected interventions, from which various types of outlying observations are produced. Outliers in time series, depending on their nature, may have a moderate to significant impact on the effectiveness of the Standard methodology for time series analysis with respect to model identification, estimation, and forecasting. In this article we use an iterative outlier detection and adjustment procedure to obtain joint estimates of model parameters and outlier effects. Four types of outliers are considered, and the issues of spurious and masking effects are discussed. The major differences between this procedure and those proposed in earlier literature include (a) the types and effects of outliers are obtained based on less contaminated estimates of model parameters, (b) the outlier effects are estimated simultaneously using multiple regression, and (c) the model parameters and the outlier effects are estimated jointly. The sampling behavior of the test statistics for cases of small to large sample sizes is investigated through a Simulation study. The performance of the procedure is examined over a representative set of outlier cases. We find that the proposed procedure performs well in terms of detecting outliers and obtaining unbiased Parameter estimates. An example is used to illustrate the application of the proposed procedure. It is demonstrated that this procedure performs effectively in avoiding spurious outliers and masking effects. The model parameter estimates obtained from the proposed procedure are typically very close to those estimated by the exact maximum likelihood method using an intervention model to incorporate the outliers."], ["Functional-Coefficient Autoregressive Models", "In this article we propose a new class of models for nonlinear time series analysis, investigate properties of the proposed model, and suggest a modeling procedure for building such a model. The proposed modeling procedure makes use of ideas from both parametric and nonparametric statistics. A consistency result is given to support the procedure. For illustration we apply the proposed model and procedure to several data sets and show that the resulting models substantially improve postsample multi-step ahead forecasts over other models."], ["A Bayesian Analysis for Change Point Problems", null], ["Asymptotic Behavior of the Gibbs Sampler", "Mild sufficient conditions for the ergodicity of the Gibbs sampler are obtained. The geometric ergodicity of the Gibbs sampler is then discussed, where the main tool is the \u201cdrift\u201d criterion. This criterion requires the construction of a generalized energy function so that the chain is dissipating energy, at a suitable rate, whenever it lies outside the \u201ccenter\u201d of the state-space. For the \u201ctwo-variable\u201d case, a useful recipe is provided for constructing appropriate energy functions. These results are illustrated with four examples."], ["Understanding Elongation: The Scale Contaminated Normal Family", null], ["Bivariate Distributions With Nonmonotone Dependence Structure", "A new ordering for nonmonotone dependence is proposed, and a method is presented for describing parametric families of bivariate distributions with fixed marginals and specified dependence structures. The method is easy to use and allows for arbitrary marginals and arbitrary dependence structures (e.g., structures of nonmonotone dependence). Any parametric family of bivariate distributions is ordered by the nonmonotone dependence ordering constructed from the desirable dependence structure. The construction is based on a generalization of the copula of distribution functions to a copula of probability measures on product spaces. Examples are shown for various bivariate distributions."], ["Stochastic Bifurcation Processes and Distributions of Fractions", null], ["On Testing the Validity of Sequential Probability Forecasts", "Events are observed sequentially, and at each stage a probability for the next event is assessed. We consider the relationship between the sequence of probability forecasts and the sequence of outcomes. We argue that the forecasts may be considered \u201cempirically valid\u201d when both these sequences are consistent with a common joint distribution for the events. To aid in assessing validity, we introduce various test statistics that measure, in a natural way, the empirical performance of the probability forecasts in the light of the outcomes obtained. It is shown that under the null hypothesis of forecast validity, such statistics will, after suitable normalization, have a Standard normal (or chi-squared) distribution, virtually irrespective of the common joint distribution supposed to underlie both sequences."], ["Reference Priors When the Stopping Rule Depends on the Parameter of Interest", "Priors related to Jeffreys's prior and the reference priors of Berger and Bernardo are developed for experiments where a stopping rule is given. These noninformative priors depend on a function of the expected stopping time. In a one-parameter experiment with a stopping rule, the Jeffreys prior is shown to be improved by taking the stopping rule into account, based on a criterion using frequentist coverage probabilities. In a multiparameter experiment, however, the Jeffreys prior depends on the stopping rule in an inappropriate fashion. In the latter case the reference prior effectively accounts for the stopping rule. The motivation for this work was in part the statistical importance of the problem and the lack of noninformative priors for experiments when a stopping rule is given, and in part a wish to develop an interesting application of a recently developed method of deriving reference prior by Berger and Bernardo."], ["Estimating the Number of Species: A Review", null], ["Book Reviews", null]]}