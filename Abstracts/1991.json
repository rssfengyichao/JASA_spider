{"1991": [["Total Error in PES Estimates of Population", "We describe a methodology for estimating the accuracy of dual systems estimates (DSE's) of population, census estimates of population, and estimates of undercount in the census. The DSE's are based on the census and a post-enumeration survey (PES). We apply the methodology to the 1988 dress rehearsal census of St. Louis and east-central Missouri and we discuss its applicability to the 1990 census and PES. The methodology is based on decompositions of the total (or net) error into components, such as sampling error, matching error, and other nonsampling errors. Limited information about the accuracy of certain components of error, notably failure of assumptions in the \u201ccapture\u2013recapture\u201d model, but others as well, lead us to offer tentative estimates of the errors of the census, DSE, and undercount estimates for 1988. Improved estimates are anticipated for 1990."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Nonlinear Modeling of Time Series Using Multivariate Adaptive Regression Splines (MARS)", "Multivariate Adaptive Regression Splines (MARS) is a new methodology, due to Friedman, for nonlinear regression modeling. MARS can be conceptualized as a generalization of recursive partitioning that uses spline fitting in lieu of other simple fitting functions. Given a set of predictor variables, MARS fits a model in the form of an expansion in product spline basis functions of predictors chosen during a forward and backward recursive partitioning strategy. MARS produces continuous models for high-dimensional data that can have multiple partitions and predictor variable interactions. Predictor variable contributions and interactions in a MARS model may be analyzed using an ANOVA style decomposition. By letting the predictor variables in MARS be lagged values of a time series, one obtains a new method for nonlinear autoregressive threshold modeling of time series. A significant feature of this extension of MARS is its ability to produce models with limit cycles when modeling time series data that exhibit periodic behavior. In a physical context, limit cycles represent a stationary state of sustained oscillations, a satisfying behavior for any model of a time series with periodic behavior. Analysis of the yearly Wolf sunspot numbers with MARS appears to give an improvement over existing nonlinear threshold and bilinear models. A graphical representation for the models is given."], ["A \u201cTrue\u201d Time Series and its Indicators: An Alternative Approach", "de Leeuw and McKelvey proposed a method for using two imperfect indicators to predict the values of an unobserved series. In this article we suggest an alternative method that uses the multiple indicator approach to model the true time series and its indicators. We examine the identification status of the model, estimate the parameters by maximum likelihood, and use the Kalman filter to obtain predictions of the unobserved series. The method is applied to the U.S. gross private saving rate using flow of funds and national income and product accounts data as indicators."], ["An Examination of the Time Series Properties of Beta in the Market Model", "Understanding the stochastic behavior of beta is fundamental to portfolio risk management. We evaluate the time series properties of beta for oil industry stocks, machinery industry stocks, and a random sample of 100 stocks. Daily returns for stocks in each of these industries and the random sample are obtained from the Center for Research in Security Prices tapes for the period July 1962 through December 1986 and are grouped into 98 quarter-year, and also 49 half-year, nonoverlapping time intervals. Beta is estimated for each interval. Our analysis indicates that the time series of beta is consistent with ARMA(1, 1). We found that the autoregressive parameter of the time series of beta estimated using a half-year interval is equal to the square root of the autoregressive parameter of the time series of beta using a quarter-year interval. Thus, as the length of time interval chosen to estimate beta becomes longer, the time series behavior of beta becomes less autoregressive and more random in nature."], ["Nonlinear Modeling of Serial Immunologic Data: A Case Study", "This article concerns the analysis of serial immunologic data from a clinical trial of immunosuppressive chemotherapy in the treatment of multiple sclerosis. The goal of the analysis is to relate levels of drug dose to levels of an immunologic outcome variable. I propose a new, nonlinear model for the analysis of such data. The model assumes that the mean function is the solution of an ordinary differential equation in time, parameters of which are related to the dose via a regression function. The defining differential equation is that which gives rise to the generalized logistic function, a flexible form that includes a number of popular growth models. We fit the model, which accounts for random effects and time series autocorrelation, by maximum likelihood. Results suggest strong treatment effects in two active-drug groups and a small but significant effect in a placebo group. These findings agree well with previously reported analyses of clinical outcomes from the trial. An empirical comparison suggests that nonlinear models of this kind can fit better than linear models of comparable complexity."], ["Using EM to Obtain Asymptotic Variance-Covariance Matrices: The SEM Algorithm", "The expectation maximization (EM) algorithm is a popular, and often remarkably simple, method for maximum likelihood estimation in incomplete-data problems. One criticism of EM in practice is that asymptotic variance\u2013covariance matrices for parameters (e.g., standard errors) are not automatic byproducts, as they are when using some other methods, such as Newton\u2013Raphson. In this article we define and illustrate a procedure that obtains numerically stable asymptotic variance\u2013covariance matrices using only the code for computing the complete-data variance\u2013covariance matrix, the code for EM itself, and code for standard matrix operations. The basic idea is to use the fact that the rate of convergence of EM is governed by the fractions of missing information to find the increased variability due to missing information to add to the complete-data variance\u2013covariance matrix. We call this supplemented EM algorithm the SEM algorithm. Theory and particular examples reinforce the conclusion that the SEM algorithm can be a practically important supplement to EM in many problems. SEM is especially useful in multiparameter problems where only a subset of the parameters are affected by missing information and in parallel computing environments. SEM can also be used as a tool for monitoring whether EM has converged to a (local) maximum."], ["Referendum Contingent Valuation Estimates: Sensitivity to the Assignment of Offered Values", "Contingent valuation methods (CVM) are becoming increasingly popular for assessing the value of nonmarket resources and public goods. In particular, CVM \u201cwillingness to pay\u201d estimates are gaining currency for the assessment of damages in environmental litigation. Several studies have compared the value estimates resulting from alternative formats used for CVM survey questions and have speculated on the reasons for observed discrepancies. These reasons now include a whole taxonomy of possible biases. We take a closer look at one CVM format\u2014the referendum\u2014and demonstrate that simply the \u201cluck of the draw\u201d in assigning the referendum thresholds on individual questionnaires can produce a surprisingly wide variety of value estimates. We control for the behavioral biases that confound other comparison studies by using one sample of \u201cpayment card\u201d CV data and simulating 200 samples of consistent referendum responses. Due to the inefficiency of the referendum format, we conclude that, where referendum questions have produced different value estimates than other formats, elaborate explanations for the apparent discrepancies may not be necessary."], ["Problems in Extrapolation Illustrated with Space Shuttle O-Ring Data", null], ["Comment", null], ["A Focus on Bayesian Methods: Special Section", null], ["What Bayesians Expect of Each Other", null], ["On the Evidence Needed to Reach Agreed Action between Adversaries, with Application to Acceptance Sampling", "Two decision makers disagree about a quantity of interest to them both. One of them, the \u201cconsumer,\u201d has a choice of two decisions that are affected by the quantity. The other, the \u201cmanufacturer,\u201d offers to perform an agreed type of experiment that it is hoped will change the consumer's view of the quantity and hence the decision. This article is devoted to the evaluation of how much experimentation should be done. Binomial, Poisson, and normal likelihoods, together with their conjugate utilities and probabilities, are considered and illustrated by numerical cases. The scenario considered here arises in applications to quality control, bidding, drug testing, marketing, and sales."], ["Likelihood and Bayesian Prediction of Chaotic Systems", "There has recently been considerable interest in both applied disciplines and in mathematics, as well as in the popular science literature, in the areas of nonlinear dynamical systems and chaotic processes. By a nonlinear, deterministic dynamical system, we mean a time series in which, starting at some initial condition, the values of the series are some fixed, nonlinear function of the previous states. One of the more intriguing aspects of these models is their propensity for displaying very complex, apparently random behavior, even when simple models are analyzed. A consequence of such chaotic behavior is that it is difficult to predict the exact behavior of a chaotic system. The difficulty in prediction stems from the fact that even the tiniest of errors, including computer roundoff, in either the specification of the function or the initial condition, can lead to huge errors in prediction. After a brief review of dynamical systems and the role of probability in dealing with uncertainty, a common example of a simple dynamical system, the logistic map, is introduced. In Section 2 statistical prediction for a dynamical system, based on observing a portion of a realization of the system with error, is considered. The emphasis is on likelihood and Bayesian approaches to the problem of prediction. Example computations, based on simulated data, suggest some novel characteristics of chaotic data analysis. Section 3 is an excursus into the relationships between ergodic theory and chaos, with specific application to Bayesian forecasting. In Section 4 a common numerical approach, which generates discrete-time dynamical systems, to the solution of differential equations is described briefly. An example involving the so-called Duffing equation is presented."], ["Bayesian Prediction of Deterministic Functions, with Applications to the Design and Analysis of Computer Experiments", null], ["Bayesian Inference with Specified Prior Marginals", null], ["A Noninformative Bayesian Approach to Interval Estimation in Finite Population Sampling", "A noninformative Bayesian approach to interval estimation in finite population sampling is discussed. Given the sample, this method introduces the Polya distribution as a pseudo posterior distribution over the unobserved members of the population. In many cases this distribution yields interval estimates similar to those of standard frequentist theory. In addition, it can be used in situations where the standard methods are difficult to apply, for example, in producing an interval estimate for the ratio of two medians. We also consider related point estimation problems and observe that estimators derived from the pseudo posterior often perform better than classical alternatives."], ["On Bayesian Analysis of Generalized Linear Models Using Jeffreys's Prior", "Generalized linear models (GLM's) have proved suitable for modeling various kinds of data consisting of exponential family response variables with covariates. Bayesian analysis of such data requires specification of a prior for the regression parameters in the model used. Uniform priors are very commonly used as conventional noninformative priors. We show, however, that uniform priors for GLM's can lead to improper posterior distributions thus making them undesirable. Alternative reference priors may be constructed from Jeffreys's rule. In this article, we give two theorems that support the use of Jeffreys's priors for GLM's with intrinsically fixed or known scale parameters. These theorems provide (i) sufficient and (ii) necessary and sufficient conditions for the propriety of the (i) posterior and (ii) prior distributions as well as for the existence of moments. Implications of these theorems for some commonly used GLM's are discussed. Finally, an illustrative example is given for the binomial model with canonical link (i.e., logistic regression), and results using Jeffreys's priors are compared with those based on other non-informative and informative priors."], ["Data Base Error Trapping and Prediction", "We develop and analyze models for a class of problems involving inferences about uncertain numbers of errors in data bases. In particular, we study two error detection methods. In the duplicate performance method, all items in a data base are processed by two individuals (or machines), and the resulting records are compared to find disagreements, which are then resolved. In the known errors method, a data base is first extended to include additional items known to be in error, and then the extended data base is checked by a single individual. For both methods, we lay out the underlying structure of the model and generate inferences in terms of predictive distributions for the numbers of undetected errors. The role of prior information is important in these problems of data base quality management. In the first method of error checking, for example, observed data are always equally consistent with small error rates and few remaining errors and with high error rates and many remaining errors. Most of our illustrative analyses use fairly conservative prior specifications, and the results are compared with those in the less formal development of Strayhorn. In practice, of course, appropriately realistic priors should be used, and some possibilities are mentioned. Models of the type studied here are applicable in a wide variety of important practical problems in data quality management, with examples in industrial quality control and reliability control being of particular note."], ["A Comparison of the Maximum Likelihood Estimator and the Posterior Mean in the Single-Parameter Case", null], ["A James-Stein Type Estimator for Combining Unbiased and Possibly Biased Estimators", "We present a method for combining unbiased sample data with possibly biased auxiliary information. The estimator we derive is similar in spirit to the James\u2013Stein estimator. We prove that the estimator dominates the sample mean under quadratic loss. When the auxiliary information is unbiased, our estimator has risk slightly greater than the usual combined estimator. As the bias increases, however, the risk of the usual estimator is unbounded, while the risk of our estimator is bounded by the risk of the sample mean. We show how our estimator can be considered an approximation to the best linear combination of the sample data and the auxiliary information, allude to how it can be derived as an empirical Bayes estimator, and suggest a method for constructing confidence sets. Finally, the performance of our estimator is compared to that of the sample mean and the usual combined estimator using real forestry data."], ["Empirical Bayes Analysis for Systems of Mixed Models with Linked Autocorrelated Random Effects", null], ["An Expected Utility Approach to Influence Diagnostics", "We consider the problem of defining the influence of a set of observations in a parametric modeling framework. An expected utility approach, motivated by the amount of information to be gained from an experiment, is developed with regard to the parameter of interest. In some linear model cases simple closed-form expressions for our criterion may be found. In more complicated settings an adaptive Monte Carlo integration technique known as the Gibbs sampler provides a natural framework for evaluating the influence diagnostic. We demonstrate that the influence diagnostic obtained performs well in flagging aberrant subsets of the data, exemplified in the cases of a two-stage linear model, a hierarchical model, and a nonlinear Michaelis-Menten model."], ["Bayes and Minimax Sample-Admissible Multivariate Selection Procedures", null], ["Bootstrapping State-Space Models: Gaussian Maximum Likelihood Estimation and the Kalman Filter", "The bootstrap is proposed as a method for assessing the precision of Gaussian maximum likelihood estimates of the parameters of linear state-space models. Our results also apply to autoregressive moving average models, since they are a special case of state-space models. It is shown that for a time-invariant, stable system, the bootstrap applied to the innovations yields asymptotically consistent standard errors. To investigate the performance of the bootstrap for finite sample lengths, simulation results are presented for a two-state model with 50 and 100 observations; two cases are investigated, one with real characteristic roots and one with complex characteristic roots. The bootstrap is then applied to two real data sets, one used in a test for efficient capital markets and one used to develop an autoregressive integrated moving average model for quarterly earnings data. We find the bootstrap to be of definite value over the conventional asymptotics."], ["A Signal Extraction Approach to the Estimation of Treatment and Control Curves", null], ["The Performance of Cross-Validation and Maximum Likelihood Estimators of Spline Smoothing Parameters", "An important aspect of nonparametric regression by spline smoothing is the estimation of the smoothing parameter. In this article we report on an extensive simulation study that investigates the finite-sample performance of generalized cross-validation, cross-validation, and marginal likelihood estimators of the smoothing parameter in splines of orders 2 and 3. The performance criterion for both the estimate of the function and its first derivative is measured by the square root of integrated squared error. Marginal likelihood using splines of degree 5 emerges as an attractive alternative to the other estimators in that it usually outperforms them and is also faster to compute."], ["Compromise Maximum Likelihood Estimators for Location", null], [null, null], ["Allocation of Effort in Monte Carlo Simulation for Power of Permutation Tests", null], ["Tests of Distributional Hypotheses with Nuisance Parameters Using Fourier Series Methods", null], ["Measures, Models, and Graphical Displays in the Analysis of Cross-Classified Data", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Book Reviews", null], ["Publications Received", null], ["Extension of Wolter and Causey's Evaluation of Procedures", null], ["Comment on Faraway and Jhun", null], ["Editorial Board Page", "This article has no abstract"], ["Some Bayesian and Non-Bayesian Procedures for the Analysis of Comparative Experiments and for Small-Area Estimation: Computational Aspects, Frequentist Properties, and Relationships", "The estimation of a treatment contrast from experimental data and the estimation of a small-area mean are special cases of the prediction of the realization of a linear combination of fixed and random effects in a possibly unbalanced two-part mixed linear model. In this article a Bayesian approach to point and interval prediction is presented and its computational requirements are examined. Differences between the Bayesian approach and the traditional (classical) approach are discussed in general terms and, in addition, in terms of two examples taken from the literature: (1) the comparison of drug formulations in a bioavailability trial (Westlake) and (2) the estimation of corn-crop areas using satellite data (Battese, Harter, and Fuller). Some deficiencies in the classical approach are pointed out, and the Bayesian approach is considered from a frequentist perspective. It is shown, via a Monte Carlo study, that, for certain (noninformative) choices of the prior distribution, the frequentist properties of the Bayesian prediction procedures compare favorably with those of their classical counterparts and that, in certain situations, they produce different and more sensible answers."], ["Modeling the Progression of HIV Infection", "Statistical modeling of the progression of markers of HIV infection is complicated by three problems: (1) the times of infection are generally unknown; (2) the follow-up of infected patients is generally much shorter than the average time from infection to AIDS; and (3) the repeated measures of the markers are correlated. The marker we study in this article is T-helper cell count. As a consequence of these three problems, it is difficult to distinguish between different models for the decline in T-helper cell count over time for HIV-infected individuals. Some investigators have proposed that the decline is gradual until shortly before the onset of AIDS, yet available data do not reject models of a fairly constant decline over the entire latency period between HIV infection and onset of AIDS. The ability to discriminate between models can be enhanced by incorporating information about the distributions of the times of seroconversion (development of measurable antibodies) among HIV seropositive individuals and of markers among the seronegative. This can be achieved through the use of growth curve methods that treat time of infection as a random variable whose distribution is estimable, either from the data on progression itself or from external cohorts. Estimation of growth curve model parameters requires a generalization of existing methods for the maximization of mixture likelihoods to accommodate three different components of stochastic variation: (1) the random and unobservable times of infection, (2) individual random effects, and (3) measurement errors. We apply our proposed method to data on progression of T-helper cell count in 490 HIV-infected men from the Men's Health Study in San Francisco and use an externally available estimate of the infection time distribution for the entire city. We conclude from our analysis that models that assume a steady linear decline of T-helper cell count on the square root scale and accommodate the three sources of variation mentioned previously provide adequate fits to the study data. We also note that the linear decline does not apply near the time of seroconversion; this event seems to be accompanied by a sharp drop in T-helper cell count."], ["Estimating Second-Order Parameters of Volcanicity from Historical Data", "To study patterns of volcanicity it is important to use historical data. These data are, however, frequently incomplete. We develop a nonparametric procedure for estimating second-order parameters of the point process of eruption starts from a catalog. This method requires an assumption of underlying stationarity, of smoothness of the probability of recording an eruption as a function of time, and of independence of this probability from the history of the process. We illustrate the procedure on the Smithsonian catalog of worldwide volcanic eruptions. We also look separately at data from Japan and Iceland. The global and the Japanese data sets show little structure, but the Icelandic data exhibits a tendency toward periodic behavior, with a period of about 40 years. We do not see any regularity corresponding to certain long tidal cycles, as has previously been suggested in the literature."], ["The Draft Lottery and Voluntary Enlistment in the Vietnam Era", "A combination of voluntary enlistment, armed forces eligibility criteria, and the failure of draftees to avoid conscription jointly determined the racial composition of the Vietnam era armed forces. Administrative data show that men with draft lottery numbers that put them at high risk of conscription are overrepresented among men who voluntarily enlisted in the military but that the effect of the lottery on enlistment is stronger for Whites than for non-Whites. Minimum chi-squared estimates of enlistment models for the 1971 draft lottery suggest that non-Whites were more likely than Whites to prefer enlistment to a civilian career. This finding appears to explain racial differences in the effect of the lottery on enlistment. Contrary to the findings of a recent congressional study, the Vietnam era estimates presented here suggest that conscription of a relatively small number of Whites and non-Whites in a manner proportional to their prevalence in the population might substantially reduce non-White representation in the armed forces."], ["State-Space Analysis of Wildlife Telemetry Data", null], ["Forecasting Output with the Composite Leading Index: A Real-Time Analysis", "We examine the ability of the composite index of leading economic indicators to predict future movements in aggregate economic activity. Previous examinations of predictive performance have evaluated either the in-sample residual errors from a forecasting equation fitted to the entire sample of data or the out-of-sample forecast errors from an equation fitted to a subsample of the data. Unlike previous evaluations, we perform a real-time analysis, which uses the provisional and partially revised data for the leading index that were actually available historically, along with recursive out-of-sample forecasts. We find a substantial deterioration of forecasting performance in the real-time framework."], ["Adjustment of Provisional Mortality Series: The Dynamic Linear Model with Structured Measurement Errors", "We consider the problem of adjusting provisional time series using a bivariate structural model with correlated measurement errors. Maximum likelihood estimators and a minimum mean squared error adjustment procedure are derived for a provisional and final series containing common trend and seasonal components. The model also includes measurement errors common to both series and errors that are specific to the provisional series. We illustrate the technique by using provisional data to forecast ischemic heart disease mortality."], ["Analysis of Variance for Replicated Spatial Point Patterns in Clinical Neuroanatomy", "Our application concerns the analysis of the spatial distributions of pyramidal neurons in the cingulate cortex of human subjects in three diagnostic groupings. Our purpose is to develop and apply several methods for the analysis of replicated spatial point patterns to identify anatomical differences between normal, schizoaffective, and schizophrenic persons at the time of death. We begin with a scaled Poisson analysis that uncovers significant differences between the groups in the mean numbers of neurons in the sampled region, as well as a high degree of extra-Poisson variation in the distribution of cell counts within these groups. We then proceed to calculate two different functional descriptors of pattern for each subject to investigate departures from competely random patterns, both between subjects and between groups, while adjusting for cell count differences. The distributions of our main functional pattern descriptor and of our derived test statistic are unknown. For nonparametric inference, we propose a bootstrap procedure in which we resample our data to estimate the null distribution of our test statistic, even though the null hypothesis is not supported by these data. A simulation study provides evidence of the validity and power of our procedure for bootstrap hypothesis testing in our context. We conclude with a summary of what has been learned through this study about the cellular structure of cingulate cortex in persons with psychotic disorders and with some suggested directions for future research."], ["On the Threshold Method for Rainfall Estimation: Choosing the Optimal Threshold Level", "Real data experiments show that, for large areas, the area average rain rate and the fraction of the area covered by rain rate above a fixed threshold are highly correlated. For the right choice of the threshold level the correlation can easily exceed 95% and may even reach 99%. This remarkable fact observed in nature is the basis for the so-called threshold method for measuring rainfall from space via satellite. The method depends critically on the threshold level, showing significant improvement in performance for optimal and nearly optimal thresholds. Under the assumption that the continuous part of the distribution of rain rate is lognormal, two theoretically derived optimal levels agree closely with the best level obtained empirically from a well-known data set of rain rate."], ["Discretized Laplacian Smoothing by Fourier Methods", "An approach to multidimensional smoothing is introduced that is based on a penalized likelihood with a modified discretized Laplacian penalty term. The choice of penalty simplifies computational difficulties associated with standard multidimensional Laplacian smoothing methods yet without compromising mean squared error characteristics, at least on the interior of the region of interest. For linear smoothing in hyper-rectangular domains, which has wide application in image reconstruction and restoration problems, computations are carried out using fast Fourier transforms. Nonlinear smoothing is accomplished by iterative application of the linear smoothing technique. The iterative procedure is shown to be convergent under general conditions. Adaptive choice of the amount of smoothing is based on approximate cross-validation type scores. An importance sampling technique is used to estimate the degrees of freedom of the smooth. The methods are implemented in one- and two-dimensional settings. Some illustrations are given relating to scatterplot smoothing, estimation of a logistic regression surface, and density estimation. The asymptotic mean squared error characteristics of the linear smoother are derived from first principles and shown to match those of standard Laplacian smoothing splines in the case where the target function is locally linear at the boundary. A one-dimensional Monte Carlo simulation indicates that the mean squared error properties of the linear smoother largely match those of smoothing splines even when these boundary conditions are not satisfied."], ["A Flexible and Fast Method for Automatic Smoothing", null], ["Choosing a Range for the Amount of Smoothing in Nonparametric Regression", null], ["A Geometrical Method for Removing Edge Effects from Kernel-Type Nonparametric Regression Estimators", "We introduce a simple geometric method for removing edge effects from kernel-type nonparametric regression estimators. It involves reflecting the data set in two estimated points, thereby generating a new data set with three times the range of the original data. The usual kernel-type estimator may be applied to the new, enlarged data set, without any danger of edge effects. This technique is applicable generally to both regularly spaced and randomly spaced designs and admits a natural analog of leave-one-out cross-validation. The new cross-validation algorithm may be extended to the very ends of the design interval, unlike its more conventional counterpart, which must be downweighted at the ends of the interval to avoid edge effects."], ["Computing Kernel-Smoothed Conditional Quantiles from Many Observations", null], ["Graphical Methods for Censored Data", "Various methods of graphically portraying censored data are discussed. These include smoothing and model checking. Many existing graphical methods may be expressed as functionals of the empirical distribution function. When the data are subject to censoring, the Kaplan\u2013Meier estimator of the distribution may replace the empirical distribution in these methods. The plots and problems that arise from such a substitution will be discussed."], ["Diagnostics for Assessing Regression Models", null], ["Influence Diagnostics for the Cross-Validated Smoothing Parameter in Spline Smoothing", "This article addresses the problem of influence in estimating the smoothing parameter when fitting a univariate smoothing spline. Diagnostics are presented that can identify observations that locally influence the choice of smoothing parameter by generalized cross-validation, either through their case weights or observed responses. Generalized cross-validation and the diagnostic methods are given a frequency interpretation and illustrated with examples."], [null, null], [null, null], ["Influence Functions of Iteratively Reweighted Least Squares Estimators", null], ["Constrained Maximum Likelihood Exemplified by Isotonic Convex Logistic Regression", null], ["Goodness-of-Fit Analysis for the Cox Regression Model Based on a Class of Parameter Estimators", "In this article we propose a class of estimation functions for the vector of regression parameters in the Cox proportional hazards model with possibly time-dependent covariates by incorporating the weight functions commonly used in weighted log-rank tests into the partial likelihood score function. The resulting estimators behave much like the conventional maximum partial likelihood estimator in that they are consistent and asymptotically normal. When the Cox model is inappropriate, however, the estimators with different weight functions generally converge to nonidentical constant vectors. For example, the magnitude of the parameter estimator using the Kaplan\u2013Meier survival estimator as the weight function will be stochastically larger than that of the maximum partial likelihood estimator if covariate effects diminish over time. Such facts motivate us to develop goodness-of-fit methods for the Cox regression model by comparing parameter estimators with different weight functions. Under the assumed model, the normalized difference between the maximum partial likelihood estimator and a weighted parameter estimator is shown to converge weakly to a multivariate normal with mean zero and with a covariance matrix for which a consistent estimator is proposed. The asymptotic properties of the weighted parameter estimators and those of the related goodness-of-fit tests under misspecified Cox models are also investigated. In particular, it is demonstrated that a goodness-of-fit test with a monotone weight function is consistent against monotone departures from the proportional hazards assumption. Versatile testing procedures with broad sensitivities can be developed based on simultaneous use of several weight functions. Three examples using real data are presented."], ["Efficient Calculation of the Permutation Distribution of Trimmed Means", "The permutation distribution of the trimmed mean in matched-pairs designs is studied. This statistic is important since it is robust to both outliers and distributional assumptions. A polynomial-time algorithm for this inherently exponential-time problem is presented. The characteristic function of the distribution provides both the basis for the algorithm and a vehicle for calculating the asymptotic permutation distribution."], ["Excess Mass Estimates and Tests for Multimodality", "We propose and study a method for analyzing the modality of a distribution. The method is based on the excess mass functional. This functional measures excessive empirical mass in comparison with multiples of uniform distribution. By the excess mass approach we separate the investigation about the number of modes from questions concerning their location. For distributions on the line, the excess mass functional can be estimated at a square-root rate. The asymptotic behavior of estimators is studied, and tests for multimodality based on the excess mass are derived."], ["A Test for a Specific Principal Component of a Correlation Matrix", null], ["Small-Sample Characterizations of near Replicate Lack-of-Fit Tests", "Several ad hoc tests based on near replicates have been proposed for testing lack of fit in regression analysis. Christensen characterized lack of fit as existing between clusters of near replicates, within clusters, or as a combination of these pure types. Of these, the between-cluster variety is the type commonly associated with the idea of lack of fit. Christensen examined a test that was new to the normal theory regression literature and established uniformly most powerful invariant (UMPI) properties of the test. In particular, the test is UMPI for orthogonal lack of fit within clusters. In this article, a new test is proposed that is UMPI for orthogonal lack of fit between clusters. The relationship of this optimal test to other proposed tests is examined, giving the first small-sample theoretical justification for these tests. The power of the new test is compared to that of others in the literature, and consistency results are discussed."], ["Sequential Comparison of Changes with Repeated Measurements Data", "There are many clinical trials in which the patients enter sequentially, and a response variable is measured repeatedly over time for each patient. A group sequential procedure is proposed for comparing the rates of change between two treatment groups. Some existing procedures for testing the equality of means between two treatment groups with repeated measurements data, such as those proposed by Armitage, Stratton, and Worthington and by Geary, can be interpreted as special cases of the proposed procedure. Under a linear mixed effects model, the asymptotic joint distribution of the sequentially computed statistics is derived. Construction of the group sequential boundaries is based on this distribution theory. The extension to nonnormal distributions of errors is also discussed. The proposed procedure allows for using either the sequential method of Slud and Wei or that of Lan and DeMets. This procedure is then applied to repeated bone density measurements on a sample of 74 middle-aged women and implemented by the method of Lan and DeMets."], ["Dynamic Linear Models with Switching", "The problem of modeling change in a vector time series is studied using a dynamic linear model with measurement matrices that switch according to a time-varying independent random process. We derive filtered estimators for the usual state vectors and also for the state occupancy probabilities of the underlying nonstationary measurement process. A maximum likelihood estimation procedure is given that uses a pseudo-expectation-maximization algorithm in the initial stages and nonlinear optimization. We relate the models to those considered previously in the literature and give an application involving the tracking of multiple targets."], ["Inference for Events with Dependent Risks in Multiple Endpoint Studies", "Kaplan\u2013Meier and cumulative incidence functions are not sufficient descriptive devices for studies that have multiple time-to-event endpoints. For example, in cancer treatment research the probability of tumor recurrence conditional on not having died from treatment-related toxicities and the prevalence of graft-versus-host disease among leukemia-free patients surviving a bone marrow transplant are of interest. These quantities can be estimated nonparametrically using simple functions of several Kaplan\u2013Meier and cumulative incidence estimates for events with possibly dependent risks. We derive asymptotic distribution theory for such functions by representing Kaplan\u2013Meier, cumulative incidence, and cumulative hazard estimators as sums of iid random variables. Variance estimation also follows directly from this representation. Two-sample test statistics with asymptotic null distribution theory are presented. Several examples illustrate the utility of these results."], ["On the Efficiency of Raking Ratio Estimation for Multiple Frame Surveys", "Multiple frame surveys involve the combination of samples selected from separate sampling frames. A topical example is a dual-frame survey that combines a sample of households interviewed by telephone with a sample drawn from a frame including nontelephone households. Various estimators of population totals and means have been proposed for multiple frame surveys. In particular Bankier proposed the application of raking ratio estimation and compared its performance to alternative estimators in a numerical study based on dual-frame Statistics Canada data. He concluded that the raking ratio estimator performed well but gave no theoretical results to support his empirical findings. This article provides a theoretical study of the efficiency of the raking ratio estimator for multiple-frame surveys. Attention is restricted mainly to the classical, unstratified two-frame case considered by Hartley. For the estimation of totals, we give a closed-form expression for the limiting form of the raking ratio estimator as the number of iterations increases. We show that this limiting estimator is consistent and obtain an expression for its asymptotic variance. Using this expression we show that this estimator is uniformly less efficient than an estimator proposed by Fuller and Burmeister. However, the loss of efficiency is small in some special cases. The raking ratio estimator has the advantage that it extends simply to more than two frames and to stratified sampling, whereas the corresponding extension of the Fuller\u2013Burmeister estimator is more problematic."], ["Slowly Decaying Correlations, Testing Normality, Nuisance Parameters", "Slowly decaying serial correlations can cause goodness-of-fit tests for a distribution to reject the null hypothesis with probability tending to one with increasing sample size. When the null distribution is completely specified (simple null hypothesis) the problem is actually worse than for the case where there are nuisance parameters to be estimated (composite null hypothesis). In particular, we consider here testing normality. We discuss limit theorems and propose correction terms to be incorporated in certain goodness-of-fit statistics to improve the rates of convergence for the simple hypothesis case. We show how the problem is solved automatically for the composite null hypothesis case when the nuisance parameters are estimated from the data. Simulations illustrate the results."], ["Intrablock Analysis of Designs with Multiple Blocking Criteria", null], ["Interpreting Blocks and Random Factors", "Can blocks be tested in a randomized blocks design? It is well known that two different formulations of the linear mixed model yield conflicting answers to this question. This article examines the model formulations from the point of view of statistical relevance. Viewing blocking as a device to increase efficiency leads to a hypothesis concerning blocks that is distinct from the hypothesis of no \u201cblock main effect.\u201d Relevant tests and interval estimates are reviewed. The merits of the two model formulations are compared, and recommendations are made to enhance the clarity and heuristic usefulness of mixed linear models."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Book Reviews", null], ["Publications Received", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["Editors' Report for 1990", null], ["Statistical Inference: Likelihood to Significance", "The concepts of likelihood and significance were defined and initially developed by R. A. Fisher, but followed almost separate and distinct routes. We suggest that a central function of statistical inference is in fact the conversion of the first, likelihood, into the second, significance: a linking of the Fisher concepts. A first-order asymptotic route for this is incorporated into most statistical packages. It uses the standardized maximum likelihood estimate, the standardized score, or the signed square root of the likelihood ratio statistic as arguments for the standard normal distribution function, thus giving approximate tail probabilities or observed levels of significance. Recent third-order asymptotic methods provide a substantial increase in accuracy but need the first derivative dependence of likelihood on the data value as an additional input. This can be envisaged as the effect on the likelihood function of dithering the data point. Extensions to the multivariate, multiparameter context are surveyed, indicating major areas for continuing research."], ["Nondetects, Detection Limits, and the Probability of Detection", null], ["Evaluation of Procedures for Improving Population Estimates for Small Areas", "We provide and illustrate methods for evaluating across-the-board ratio estimation and synthetic estimation, two techniques that might be used for improving population estimates for small areas. The methods emphasize determination of a break-even accuracy of knowledge concerning externally obtained population totals, which marks the point at which improvement occurs."], ["Using Medical Malpractice Data to Predict the Frequency of Claims: A Study of Poisson Process Models with Random Effects", "I use the Florida state data base (1975\u20131987) of settled malpractice claims to develop Poisson process models for the frequency of claims filed against individual physicians. These models incorporate random effects and covariates that represent physician attributes and are natural generalizations of the negative binomial model that is typically used to study claims frequency. I predict claims frequencies during 1981\u20131982 using models that are selected and estimated from claims data during 1975\u20131980 and then compare these predictions to the actual frequencies."], ["Hierarchical Models for the Probabilities of a Survey Classification and Nonresponse: An Example from the National Crime Survey", "A goal in many survey sampling problems is to estimate the probability that elements of the population within various small areas or domains have some characteristic or fall in some particular survey classification. The estimation problem is typically complicated by nonrandom nonresponse in that the probability that a unit responds to the survey may be related to the characteristic of interest. This article presents a random parameter or hierarchical model approach to modeling the small-domain probabilities of the characteristic of interest and the probabilities of nonresponse. The general model allows nonresponse probabilities to depend on a unit's survey classification. A special case of the model treats nonresponse as occurring at random. Empirical Bayes methods are used to obtain parameter estimates under the hierarchical models. The method is illustrated using data from the National Crime Survey."], ["Shrinkage Estimation of Price and Promotional Elasticities: Seemingly Unrelated Equations", "Consider the problem where a retailer or manufacturer wants to estimate product price and promotional elasticities based on supermarket scanner data. Classical linear modeling suffers from the following aggregation dilemma. Price and promotional elasticities appear to vary considerably among chains and brands so that one overall model is too restrictive. Alternatively, the use of a different model for each chain and brand leads to noisy and often nonsensical estimates of separate elasticities because of excessive data variation. To resolve this dilemma, shrinkage estimation procedures are proposed. By borrowing strength across chains and brands, these procedures reduce variability while providing flexibility that allows for separate elasticity estimates. Application of these procedures to a large data set yields not only more reasonable model estimates but also improved predictive power."], ["Sliced Inverse Regression for Dimension Reduction", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Transformations in Density Estimation", "For the density estimation problem the global window width kernel density estimator does not perform well when the underlying density has features that require different amounts of smoothing at different locations. In this article we propose to transform the data with the intention that a global window width is more appropriate for the density of the transformed data. The density estimate of the original data is the \u201cback-transform\u201d by change of variables of the global window width estimate of the transformed data's density. We explore choosing the transformation from suitable parametric families. Data-based selection rules for the choice of transformations and the window width are discussed. Application to real and simulated data demonstrates the usefulness of our proposals."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["On the Problem of Interactions in the Analysis of Variance", "An apology for the recommendation is that it can be easily improved by considering the power of the test and thus obtaining information on the interactions neglected. We consider this improved recommendation for the goals of (i) an interval estimate of one of the cell expectations, (ii) a simultaneous interval estimate of the cell expectations, and (iii) an estimate of the cell with the largest expectation. We show that the improved recommendation leads to methods inferior to methods based on the replacement of the two-way model by the one-way model in cases (ii) and (iii) and to a method not better in case (i). This refutes the apology for the original unwarranted recommendation: the improved recommendation is not easy to apply and does not always lead to good methods."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Structural Image Restoration through Deformable Templates", "Prior knowledge on the space of possible images is given in the form of a function or template in some domain. The set of all possible true images is assumed to be formed by a composition of that function with continuous mappings of the domain into itself. A prior Gaussian distribution is given on the set of continuous mappings. The observed image is assumed to be a degradation of the true image with additive noise. Given the observed image, a posterior distribution is then obtained and has the form of a nonlinear perturbation of the Gaussian measure on the space of mappings. We present simulations of the posterior distribution that lead to structural reconstructions of the true image in the sense that it enables us to determine landmarks and other characteristic features of the image, as well as to spot pathologies in it. Moreover, we show that the reconstruction algorithm is relatively robust when the images are degraded by noise that is not necessarily additive."], ["Least Squares Estimation of Covariance Matrices in Balanced Multivariate Variance Components Models", "The problem of estimating covariance matrices in balanced multivariate variance components models is discussed. As with univariate models, it is possible for the traditional estimators, based on differences of the mean square matrices, to produce estimates that are outside the parameter space. In fact, in many cases it is extremely likely that traditional estimates of the covariance matrices will not be nonnegative definite (nnd). In this article we develop an iterative estimation procedure, satisfying a least squares criterion, that is guaranteed to produce nnd estimates of the covariance matrices, discuss the speed of convergence, and provide an example to show how the estimates change."], ["Sensitivity in Bayesian Statistics: The Prior and the Likelihood", "One paradigm for sensitivity analyses in Bayesian statistics is to specify \u0393, a reasonable class of priors, and to compute the corresponding class of posterior inferences. The class \u0393 is chosen to represent uncertainty about the prior. There is often additional uncertainty, however, about the family of sampling distributions. This article introduces a method for computing ranges of posterior expectations over reasonable classes of sampling distributions that lie \u201cclose to\u201d a given parametric family. By treating the prior as a probability measure on the space of sampling distributions this article also gives a unified treatment to what are usually considered two separate problems\u2014sensitivity to the prior and sensitivity to the sampling model. First the notion of \u201cclose to\u201d is made explicit. Then, an algorithm is given for turning ratio-linear problems into sequences of linear problems. In addition to solving the problem at hand, the algorithm simplifies many other robust Bayesian computational problems. Finally, the method is illustrated with an example."], ["An Approach to Robust Bayesian Analysis for Multidimensional Parameter Spaces", "In some multiparameter Bayesian problems the prior is more naturally described as a joint density for all of the parameters and in others it is more naturally described as a product of marginal and conditional densities. Although ordinary quantile, \u03b5-contamination, and density-ratio classes of priors may be sensible choices for robust Bayesian analyses in the former case, they may be less satisfactory in the latter. This article shows how to construct product classes of priors for use in the second case, using one class of priors for the marginal and another for each conditional. Then the article introduces density-bounded classes as an alternative to the more familiar classes for representing uncertainty about the prior. It is often of interest to find tight bounds on posterior expectations as the prior ranges over the class of priors\u2014an apparently difficult nonlinear optimization problem regardless of which class of priors is used. This article describes a technique for turning the problem into a sequence of linear optimizations that can be much easier to handle. Then it shows how to solve the linear optimization problem for quantile, \u03b5-contamination, density-ratio, and density-bounded classes. The ideas are illustrated with an example from a clinical trial."], ["Fast Computation of Exact Confidence Limits for the Common Odds Ratio in a Series of 2 \u00d7 2 Tables", "The odds ratio is widely used as a measure of association in epidemiologic studies and clinical trials. We consider calculation of exact confidence limits for the common odds ratio in a series of independent 2 \u00d7 2 tables and propose three modifications of the network algorithm of Mehta, Patel and Gray: (1) formulating and dealing with the problem in algebraic instead of graph theoretic terms, (2) performing convolutions on the natural scale instead of the logarithmic scale, and (3) using the secant method instead of binary search to compute roots of polynomial equations. Enhancement of computational efficiency, exceeding an order of magnitude, afforded by these modifications is empirically demonstrated. We also compare the modified method with one based on the fast Fourier transform (FFT). Further, we show that the FFT method can also result in considerable loss of numerical accuracy. The modifications proposed in this article yield an algorithm that is not only fast and accurate but that combines conceptual simplicity with ease of implementation. Anyone with a rudimentary knowledge of computer programming can implement it and quickly compute exact confidence intervals for relatively large data sets even on microcomputers. Thus it should help make exact analysis of the common odds ratio more common."], ["A Unified Approach to Rank Tests for Multivariate and Repeated Measures Designs", null], ["A Lack-of-Fit Test for the Mean Function in a Generalized Linear Model", "A supremum-type statistic, based on partial sums of residuals, is proposed to test the validity of the mean function of the response variable in a generalized linear model. The proposed test does not need a partition of the space of covariates to handle the case of nonreplication. The new test is consistent against the alternative, under which the deterministic part of the model is not the one specified in the null hypothesis. The asymptotic null distribution of the test statistic can be approximated through simulations. This approximation is valid even if the variance of the response variable is misspecified in the model. For practical sample sizes, the adequacy of this large-sample approximation to the null distribution of the test statistic is carefully examined. Power comparisons with other lack-of-fit tests are also performed to show the advantage of the test. In particular, we find that the new procedure is rather sensitive to detect a misspecified mean function, if the assumed mean function and the true one do not intersect too frequently in the space of the independent variable. The new test is illustrated with examples for the logistic and linear regression models."], ["Smooth Goodness-of-Fit Tests: A Quantile Function Approach", "A quantile function based analog to Neyman's smooth tests for goodness-of-fit hypotheses is developed. These procedures possess asymptotic and small-sample power properties similar to those for smooth tests. They are, however, computationally simple, applicable to both complete and Type II censored data, and easily focused in the direction of any specific set of alternatives. These results are used to develop a new test for exponentiality. Only location/scale null hypotheses will be considered. The ideas are easily adapted to other types of nulls, however. Neyman first proposed smooth tests for the goodness-of-fit hypothesis. As stated by Kopecky and Pierce and by Rayner and Best, the basic idea behind these tests is to embed the null density in a larger exponential family. The procedure is then based upon the likelihood-ratio test statistic for testing that the additional parameters equal zero. Smooth type tests may be viewed as a compromise between omnibus test procedures, with generally low power in all directions, and procedures whose power is focused in the direction of a specific alternative. It is easily shown that any of the smooth type tests are inconsistent for some alternatives. Further, as stated by Koziol, one of the main criticisms of these tests has been that the larger exponential families employed seem to be selected more for mathematical convenience than as statistically reasonable alternative distributions. Results of Monte Carlo studies by Miller and Quesenberry and others, however, indicate that smooth tests perform well for a wide range of alternatives. The proposed tests are developed by embedding the quantile function associated with the null hypothesis into a larger linear model and then using typical regression techniques to test that the coefficients of the added terms in this model vanish. These statistics are interpreted and asymptotically have many of the properties of the corresponding regression tests. Further, as with Neyman's smooth tests, if the alternative model is properly selected, the resulting procedure provides a good omnibus test."], ["Optimal Sample Allocation for Normal Discrimination and Logistic Regression under Stratified Sampling", "For two multivariate normal populations with a common covariance matrix and stratified sampling, we consider two methods of estimation\u2014Fisher's linear discriminant function and logistic regression. Intuition suggests that taking half of the observations from each population is a reasonable design choice. Based on minimizing the expected error regret, asymptotic optimal sample allocations are found. The results indicate that the differences in the expected error regret for optimal versus balanced allocation are generally quite small. It is recommended that equal sample sizes for the two populations be used for these problems."], ["A Large Deviation-Type Approximation for the \u201cBox Class\u201d of Likelihood Ratio Criteria", "For a large class of tests related to the multivariate normal distribution a number of approximations to the distribution of the log-likelihood ratio statistic are discussed. Some of the approximations require the use of a small personal computer for their calculation, but this pays off in terms of giving very accurate approximations. Actually, one of the approximations, based on the use of a conjugate distribution, works well for all sample sizes and all significance levels."], ["An Unbiased Estimator of the Covariance Matrix of the Mixed Regression Estimator", "This article derives an unbiased estimator of the covariance matrix of the \u201cmixed regression\u201d estimator suggested by Theil and Goldberger for combining prior information with the sample information in regression analysis. This derivation facilitates the construction of finite-sample standard errors for the mixed estimators of the individual regression coefficients. Comparisons are made between the unbiased covariance estimator, the conventional consistent estimator based on the generalized least squares formula, and a simple modification of the latter, which is found to approximate the unbiased estimator well in practical situations."], ["Estimating a Population Total Using an Area Frame", "Land segments form the sampling frame in a variety of sampling problems. In this article, the class of linear homogeneous estimators is investigated for the area sampling frame used in agricultural surveys. It is shown that the commonly used estimators belong to this class, and results are obtained comparing these estimators with other estimators in the class. In previous work in agriculture surveys where the problem is to estimate the population total using an area sampling frame, the estimator has been formulated by assigning values to the sampling segments, so the segments are treated as the measurement units as well as the sampling units. We formulate the class of linear homogeneous estimators directly in terms of farms as the measurement unit and use this formulation, which contains the commonly used estimators, to provide a better understanding of the estimation problem. Alternatives to presently used estimators are given, practical aspects of the various estimators are discussed, and some variance inequalities are obtained."], ["An Approach to the Construction of Asymmetrical Orthogonal Arrays", null], ["Limitations of the Rank Transform Procedure: A Study of Repeated Measures Designs, Part I", null], ["Walsh-Fourier Analysis and its Statistical Applications", "The aim of this article is to acquaint statisticians and practitioners, whose research activities require statistical methodology, with the statistical theory and applications of Walsh-Fourier analysis. It has been suggested that Walsh spectral analysis is suited to (albeit not restricted to) the analysis of discrete-valued and categorical-valued time series, and of time series that contain sharp discontinuities. I explain the need for Walsh-Fourier analysis, review the history and properties of Walsh functions, and outline the existing Walsh-Fourier theory for real-time stationary time series. I discuss various statistical applications based on the Walsh-Fourier transform and provide an annotated bibliography."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Social Statistics and Public Policy for the 1990s Special Section", null], ["Contextually Specific Effects and other Generalizations of the Hierarchical Linear Model for Comparative Analysis", "Contextually specific differences among members of a context pose substantive and methodological problems in multilevel analysis. An important example is ethnic identity in comparative studies involving different societies. Ethnic (and religious) group membership in many societies is a basis not only for differentiation among members but also for the identification and maintenance of deeply rooted integrative ties. So important is ethnicity that the existence of different groups within societies sometimes delays or prohibits the taking of censuses. Moreover, even when ethnic information is available in national statistical data sources, researchers are sometimes prohibited from reporting the results of analyses that contain ethnic detail. Despite these constraints, information on ethnicity is often available for pluralistic societies. Where it is available, ethnicity poses a challenging analytic problem for comparative analysis. If researchers wish to compare different societies, how is ethnicity to be allowed for within a framework of quantitative comparative analysis? Using the notion of contextual specificity, we present an answer to this question within the multilevel analysis paradigm. Ethnicity is taken into account, but ethnic differences as such are not modeled across contexts. This strategy is effected through a generalization of the hierarchical linear model for multilevel analysis, using the method of restricted maximum likelihood in combination with empirical Bayes. The methodology proposed here is applicable to dimensions other than ethnicity. We illustrate the approach with an empirical example concerning the dependence of fertility on socioeconomic origins of women in 36 Third World countries, using data from the World Fertility Survey."], ["Social Statistics and an American Urban Underclass: Improving the Knowledge Base for Social Policy in the 1990s", "Social statistics have played a large role in describing the nature of the problem of an emerging urban underclass in the United States and in placing these issues on the public agenda and before the research community. Extant social statistics, however, are inadequate to the task of understanding the processes and mechanisms that create, maintain, or overcome the conditions and consequences of the urban underclass. Without theoretically informed social statistics of organizations and institutions and of geographic and contextual detail, social statistics will be unable to inform public policy in the 1990s."], ["National Surveys and the Health and Functioning of the Elderly: The Effects of Design and Content", "The rapid growth of the U.S. elderly (age 65+) and oldest-old (age 85+) populations, combined with their high per capita acute health and long-term care (LTC) service needs, raises concerns about existing health care payment systems. Adapting and designing new types of health insurance and existing health policies require accurate data on the elderly's health and functional characteristics. Strengths and weaknesses of five national health surveys in providing such data are evaluated. Methodological issues arising in surveying elderly populations and analyzing data from those surveys are discussed, with implications for designing private LTC insurance and for reducing future LTC service burden."], ["The Effects of Census Undercount Adjustment on Congressional Apportionment", "This article shows that undercount adjustment will probably reallocate one to three House seats across the states. The adjustment's impact may depend on the method used and the assumptions underlying undercount estimates. Using regression analysis to reduce sampling error in undercount estimates from dual-systems analysis, however, eliminates sensitivity to all but the most extreme changes in assumptions. Generally, adjustment will more likely affect large states than small ones, and large states with proportionately many urban Black and Hispanic residents will likely gain seats at the expense of large states with few such residents."], ["Book Reviews", null], ["Publications Received", null], ["Editorial Board Page", "This article has no abstract"], [null, null], ["Compliance as an Explanatory Variable in Clinical Trials", "The Lipid Research Clinics Coronary Primary Prevention Trial (LRC-CPPT) measured the effectiveness of the drug cholestyramine for lowering cholesterol levels. The patients in the study were measured for compliance (the proportion of the intended dose actually taken) and for cholesterol decrease. The compliance-response regression for the Treatment group shows a smooth increasing effect of the drug in cholesterol level with increasing compliance. However, a similar, though less dramatic, compliance-response regression is seen in the Control group. This article investigates the recovery of the true dose-response curve from the Treatment and Control compliance-response curves. A simple model is proposed, analyzed, and applied to the LRC-CPPT data. Under this model, part but not all of the true dose-response curve can be estimated."], ["Comment: Dose-Response Estimands", null], ["Comment", null], ["Comment: Dose-Response Estimands", null], ["Rejoinder", null], ["Some Statistical Problems in the Assessment of Inhomogeneities of DNA Sequence Data", null], ["Bayesian and Frequentist Predictive Inference for the Patterns of Care Studies", "The Patterns of Care Studies were conducted to determine the quality of care received by cancer patients whose primary treatment modality is radiation therapy. In this article, we propose and evaluate models which, if acceptable, permit Bayesian and frequentist model-based predictive inference for the desired finite population parameters. Using both hierarchical Bayesian and frequentist mixed linear models, we describe methodology for making the desired inferences, emphasizing the use of transformed random variables. Finally, we compare the frequentist, Bayes, and empirical Bayes approaches using data from one of the surveys. All three methods produce essentially the same value for the (finite population) mean. The standard empirical Bayes and frequentist measures of variability are very much smaller than those derived from the Bayesian approach, the latter reflecting uncertainty about the values of the scale parameters in the model."], ["An Exploratory Analysis of the Occurrence of Explosive Volcanism in the Northern Hemisphere, 1851-1985", "Studies of regional climate following volcanic eruptions suggest that explosive volcanism exerts a short-term cooling effect. To understand the effects of explosive volcanism on historic temperatures, it is necessary to identify changes through time in the frequency of explosive eruptions. This article presents an exploratory analysis of a record of explosive eruptions in the Northern Hemisphere over the period 1851\u20131985. This record is modeled as a nonstationary Poisson process and the intensity function is estimated by kernel smoothing. Approximate confidence bands are constructed and diagnostics for checking the Poisson assumption are described. A comparison with a record of Northern Hemisphere temperatures indicates that part of the variability in temperature can be explained by variations in explosive volcanic activity."], ["A Linear Spatial Correlation Model, with Applications to Positron Emission Tomography", "A simple spatial-correlation model is presented for repeated measures data. Correlation between observations on the same subject is assumed to decay as a linear function of the squared distance separating the regions in three-dimensional space where the observations are made. This quadratic decay (QD) model has many attractive theoretical properties. The covariance structure of the \u201cnormalized\u201d data, formed by subtracting the subject average, is the same as that generated by random linear trends plus centered white noise, which leads to a simple method for simulating the original data. It also implies that the first three principal components of the normalized observations are the spatial coordinates of the regions. Thus principal components analysis can be used as an exploratory tool to verify the QD model, provided the white noise component is small. Certain simple predictions can be made, such as the fact that the variances of the normalized observations increase linearly with the squared distance from the centroid of the regions, even though the original observations are assumed to have equal variance. This implies that it is harder to detect abnormal regional measurements in the outlying regions using normalized observations. Assuming multivariate normality, generalized least squares can be used to find maximum likelihood estimates of the QD model; calculations can be reduced using an expression for the inverse of the covariance matrix that only involves the inverse of a 3 \u00d7 3 matrix. It is shown, however, that although normalization simplifies the model by removing the random subject effect, it reduces information about the spatial correlation effect, making it harder to detect. This may have implications for other spatial-correlation models that are fitted to residuals from a sample mean. The QD model is applied to 30 cerebral regional glucose metabolism measurements from positron emission tomography (PET) images on a group of 20 normal subjects."], ["Multiple Imputation of Industry and Occupation Codes in Census Public-use Samples Using Bayesian Logistic Regression", "We describe methods used to create a new Census data base that can be used to study comparability of industry and occupation classification systems. This project represents the most extensive application of multiple imputation to date, and the modeling effort was considerable as well\u2014hundreds of logistic regressions were estimated. One goal of this article is to summarize the strategies used in the project so that researchers can better understand how the new data bases were created. Another goal is to show how modifications of maximum likelihood methods were made for the modeling and imputation phases of the project. To multiply-impute 1980 census-comparable codes for industries and occupations in two 1970 census public-use samples, logistic regression models were estimated with flattening constants. For many of the regression models considered, the data were too sparse to support conventional maximum likelihood analysis, so some alternative had to be employed. These methods solve existence and related computational problems often encountered with maximum likelihood methods. Inferences pertaining to effects of predictor variables and inferences regarding predictions from logit models are also more satisfactory. The Bayesian strategy used in this project can be applied in other sparse-data settings where logistic regression is used because the approach can be implemented easily with any standard computer program for logit regression or log-linear analysis."], ["Generalized Linear Models with Random Effects; a Gibbs Sampling Approach", "Generalized linear models have unified the approach to regression for a wide variety of discrete, continuous, and censored response variables that can be assumed to be independent across experimental units. In applications such as longitudinal studies, genetic studies of families, and survey sampling, observations may be obtained in clusters. Responses from the same cluster cannot be assumed to be independent. With linear models, correlation has been effectively modeled by assuming there are cluster-specific random effects that derive from an underlying mixing distribution. Extensions of generalized linear models to include random effects has, thus far, been hampered by the need for numerical integration to evaluate likelihoods. In this article, we cast the generalized linear random effects model in a Bayesian framework and use a Monte Carlo method, the Gibbs sampler, to overcome the current computational limitations. The resulting algorithm is flexible to easily accommodate changes in the number of random effects and in their assumed distribution when warranted. The methodology is illustrated through a simulation study and an analysis of infectious disease data."], ["Models for Contingency Tables with Known Margins when Target and Sampled Populations Differ", "The analysis of two-way contingency tables with known margins is considered. Four methods for estimating the cell probabilities are compared, namely, raking (RAKE), maximum likelihood under random sampling (MLRS), minimum chi-squared (MCSQ), and least squares (LSQ). Assuming random sampling from the target population, these methods are known to be asymptotically equivalent, and small-sample studies have suggested that MCSQ is slightly better than the other methods in average root mean squared error. We consider properties of the methods when the sampled population differs from the target population, through deficiencies in the sampling frame or defects in the implementation of the sample. We show that each method is in fact maximum likelihood for a particular model relating the target and sampled populations. Expressions for the standard errors of the estimates are developed under these alternative models. The methods are compared on data from a health survey and in a simulation study where each of the methods is assessed using data generated in a variety of ways. The results suggest that LSQ is inferior to the other three methods, and RAKE and MLRS dominate MCSQ."], ["Semiparametric Estimation in the Rasch Model and Related Exponential Response Models, Including a Simple Latent Class Model for Item Analysis", null], ["A Nonparametric Method for Dealing with Mismeasured Covariate Data", null], ["Weighted Least Squares Estimation for Aalen's Additive Risk Model", null], ["Nonparametric Estimation from Cross-Sectional Survival Data", "In many follow-up studies survival data are often observed according to a cross-sectional sampling scheme. Data of this type are subject to left truncation in addition to the usual right censoring. A number of characteristics and properties of the product-limit estimate, for left-truncated and right-censored data, have been explored and found to be similar to those of the Kaplan-Meier estimate. Under the stationarity assumption, however, it is believed that an alternative estimate has much better efficiency. In this article the conditional maximum likelihood estimate (MLE) property of the product-limit estimate is visited. The non-parametric MLE of the truncation distribution is derived. Use of this estimate includes testing the stationarity assumption, estimating the proportion of truncated data, and other applications in prevalent cohort studies. The analysis of the estimation is based on a \u201cworking data\u201d approach. The asymptotic properties of the proposed estimates are developed through nonparametric score functions. It is presented that nonparametric conditional score functions possess properties similar to those of parametric conditional score functions. This observation leads to simplification of the asymptotic results. The nonparametric MLE of the joint distribution of truncation and censoring variables is also derived. This nonparametric estimate together with the product-limit estimate are used to generalize Efron's \u201cobvious method\u201d of bootstrapping, for right-censored data, to data that are both left truncated and right censored."], ["\u201cOptimal\u201d Estimation of Correlated Response Variance under Additive Models", null], [null, null], ["A Test of Homogeneity of Odds Ratios against Order Restrictions", null], ["A General Method for Approximating Tail Probabilities", null], ["Homogeneity of Subpopulations and Simpson's Paradox", "Pooling of information from subpopulations achieves data compactification, but some characteristics of the data are lost in the process. The greatest danger in amalgamating contingency tables is the possibility of a resulting paradox. Many such have been noted since 1903, when Yule first noticed this phenomenon. We note three types of paradoxes. The definition of homogeneity proposed here is a sufficient condition to avoid Yule's Reversal Paradox. We look at interpretations of this definition and some examples. We also explore the connection of this definition with Lindley and Novick's necessary condition for the paradox."], ["On Weak Convergence of an Estimator of the Survival Function when New is Better than Used of a Specified Age", null], ["Some Alternatives to Wald's Confidence Interval and Test", null], ["Use of Nonnull Models for Rank Statistics in Bivariate, Two-Sample, and Analysis of Variance Problems", null], [null, null], ["Review Papers: Recent Developments in Nonparametric Density Estimation", "Advances in computation and the fast and cheap computational facilities now available to statisticians have had a significant impact upon statistical research, and especially the development of nonparametric data analysis procedures. In particular, theoretical and applied research on nonparametric density estimation has had a noticeable influence on related topics, such as nonparametric regression, nonparametric discrimination, and nonparametric pattern recognition. This article reviews recent developments in nonparametric density estimation and includes topics that have been omitted from review articles and books on the subject. The early density estimation methods, such as the histogram, kernel estimators, and orthogonal series estimators are still very popular, and recent research on them is described. Different types of restricted maximum likelihood density estimators, including order-restricted estimators, maximum penalized likelihood estimators, and sieve estimators, are discussed, where restrictions are imposed upon the class of densities or on the form of the likelihood function. Nonparametric density estimators that are data-adaptive and lead to locally smoothed estimators are also discussed; these include variable partition histograms, estimators based on statistically equivalent blocks, nearest-neighbor estimators, variable kernel estimators, and adaptive kernel estimators. For the multivariate case, extensions of methods of univariate density estimation are usually straightforward but can be computationally expensive. A method of multivariate density estimation that did not spring from a univariate generalization is described, namely, projection pursuit density estimation, in which both dimensionality reduction and density estimation can be pursued at the same time. Finally, some areas of related research are mentioned, such as nonparametric estimation of functionals of a density, robust parametric estimation, semiparametric models, and density estimation for censored and incomplete data, directional and spherical data, and density estimation for dependent sequences of observations."], ["Review Papers: Modeling Capture, Recapture, and Removal Statistics for Estimation of Demographic Parameters for Fish and Wildlife Populations: Past, Present, and Future", "In this article I review the modeling of capture, recapture, and removal statistics for the purpose of estimating demographic parameters of fish and wildlife populations. Topics considered include capture-recapture models, band or tag return models, removal and catch per unit effort models, selective removal or change-in-ratio models, radio-tagging survival models, and nest survival models. The purpose is to present important concepts in a general manner for the benefit of a wide audience of statisticians. I will not attempt to be comprehensive, and I indulge in speculation about future directions. I indicate the importance of different statistical tools to this subject, such as Bayesian inference, \u201cboot strapping,\u201d robustness studies, goodness-of-fit tests. I also emphasize connections to other application areas of statistics. Capture-recapture methods, for example, are being considered for estimation of a variety of elusive human populations, such as the homeless and people missed in the census. Survival analysis, widely used in medicine and engineering, can also be applied to radio-tagged animals."], ["Book Reviews", null], ["Publications Received", null], ["Comment on Zeger and Brookmeyer", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"]]}