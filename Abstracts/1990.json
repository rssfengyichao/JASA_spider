{"1990": [["Reporting Delays and the Incidence of AIDS", "It can take several months, and often years, for case reports of acquired immunodeficiency syndrome (AIDS) to be received by the Centers for Disease Control (CDC). As a result, the cumulative number of AIDS cases reported by the CDC at a given date may fall considerably below the actual number thus far diagnosed. Methods are described for estimating both the probability distribution of reporting delays and the actual incidence of AIDS. An estimated 62% of AIDS cases are reported more than 2 months after diagnosis, and 17% are reported with a delay of 3 years or more. An estimated 130,000 AIDS cases were actually diagnosed through March 1989, compared to about 91,000 cases reported by that time. There has been an increase in reporting delays during 1982\u20131989, as well as significant geographic variation in reporting delays. The actual incidence of AIDS is found to be rising most rapidly in nonurban regions and metropolitan areas with less than one million population. A model of AIDS incidence based upon the incubation of human immunodeficiency virus (HIV) is applied to data on reported AIDS cases among non-drug-using homosexual men. In this group, the estimated incidence of HIV infection peaked at about 8,900 cases per month in early 1983, and the resulting incidence of AIDS will peak at an estimated 3,200 cases per month in early 1993. The latter estimates were very sensitive to the specification of the incubation density for HIV infection."], ["A Statistical Analysis of Adverse Impact of Employer Decisions", "Federal law prohibits discrimination in employment decisions against persons 40 years old and older. This article uses data from an actual case to illustrate several methods of showing adverse impact, a legal doctrine under which only the effects of the employer's acts are at issue and not the motives with which they were done. The strengths and weaknesses of the Fisher exact test of significance, a Bayesian analysis, and a method of paired observations inspired by the Mann-Whitney-Wilcoxon statistic are assessed. One important conclusion from this analysis is that it is useful to have several different kinds of analysis bearing on the same issue. To the extent that the analyses agree, credibility is added to each."], ["The Use of Intentions Data to Predict Behavior: A Best-Case Analysis", "In surveys individuals are routinely asked to predict their future behavior, that is, to state their intentions. This article studies the relationship between stated intentions and subsequent behavior under the \u201cbest-case\u201d hypothesis that individuals have rational expectations and that their responses to intentions questions are best predictions of their future behavior. The objective is to place an upper bound on the behavioral information contained in intentions data and to determine whether prevailing approaches to the analysis of intentions data respect the bound. The analysis focuses on the simplest form of intentions questions, those that call for yes/no predictions of binary outcomes. The article also discusses \u201cforced-choice\u201d questions, which are distinct from, but are sometimes confused with, intentions questions. A primary lesson is that not too much should be expected of intentions data. It is shown that intentions data bound but do not identify the probability that a person will behave in a given way. The derived bounds are nonparametrically estimable and may be used to test the best-case hypothesis. Contrary to assertions in the literature, there is no reason to think that individual-level differences between intentions and behavior should \u201caverage out\u201d in the aggregate."], ["Regression Splines in the Cox Model with Application to Covariate Effects in Liver Disease", null], ["Lognormal and Moving Window Methods of Estimating Acid Deposition", "The deposition of heightened levels of sulfuric and nitric acid through rainfall in the United States may adversely affect the environment. For example, soils may become toxic to native tree species because of soil acidification. Ecological effects models being built to study these potential problems have a need for regional deposition estimates with associated measures of uncertainty. However, statistical estimation of the deposition process is complicated by a strong spatial trend (mean nonstationarity) and apparently a spatial covariance structure dependent on location (covariance nonstationarity). The available data for calculating deposition estimates consist of several hundred point observations at irregularly spaced sampling locations across the United States. The spatial estimation technique of kriging is the foundation of four deposition estimation methods evaluated in this study. These are lognormal kriging with a single model of the spatial covariance structure (the variogram); single-estimate lognormal kriging within a moving window using a local model of the variogram; ordinary kriging within a moving window, also with a local variogram model; and planar regression with spatially correlated errors within a moving window used to estimate residuals that are then input to the moving window ordinary kriging algorithm to calculate a (local) trend-adjusted estimate. Each of these four methods has some capacity for accommodating mean and covariance nonstationarity. The moving window methods are new. It is shown that the log transform may not be the correct transform for the deposition process. However, confidence intervals found from a lognormal kriging method do not include a negative interval, as can frequently occur with intervals found from ordinary kriging. Assuming that a partially negative confidence interval for deposition is interpretable, the method of local planar regressions followed by residual kriging gives confidence interval widths that are the most resistant to inflation due to trend effects on the variogram and trend-dependent variance."], ["Predicting Rodent Carcinogenicity from Four in vitro Genetic Toxicity Assays: An Evaluation of 114 Chemicals Studied by the National Toxicology Program", null], ["Illustration of Bayesian Inference in Normal Data Models Using Gibbs Sampling", "The use of the Gibbs sampler as a method for calculating Bayesian marginal posterior and predictive densities is reviewed and illustrated with a range of normal data models, including variance components, unordered and ordered means, hierarchical growth curves, and missing data in a crossover trial. In all cases the approach is straightforward to specify distributionally and to implement computationally, with output readily adapted for required inference summaries."], ["Sample Weighting Methods and Estimation of Totals in the Consumer Expenditure Survey", "The widely used Principal Person method of weighting households in federal government surveys uses external post-Censal information on population to improve survey sample weights by a form of poststratification. While the Principal Person Methodology can be viewed as part of a procedure to adjust for nonresponse and undercoverage, it is not oriented for efficiently incorporating ancillary information or combining information from multiple surveys into survey estimates of subdomain totals. In this article a generalized least squares adjustment algorithm is shown to incorporate ancillary information in a way that, in principle, reduces the design variance of estimated survey totals. The flexibility of the method is exploited in an application to the Consumer Expenditure Survey that makes use of its \u201cweighting control\u201d and \u201ccomposition\u201d features."], ["Estimating the Incubation Period of AIDS by Comparing Population Infection and Diagnosis Patterns", "The incidence of AIDS virus infections over time among gay men in San Francisco is nonparametrically estimated from interval-censored data by using an EM algorithm to maximize a roughness-penalized likelihood. Because the distribution of AIDS diagnoses is the convolution of the infection and incubation distributions, the incubation distribution can be estimated by comparing the estimated infection distribution and the observed pattern of diagnoses. This is again accomplished by nonparametrically maximizing a roughness-penalized likelihood using the EM algorithm. The optimal degree of smoothness for the estimates is chosen using external data and subjective assessments of plausibility. Three prospective studies of initially uninfected men produce comparable estimated infection rates and are merged to produce an overall estimate, which shows rates increasing until late 1981 and then falling sharply. The estimated incubation period hazard function is near 0 for two years following infection and then increases until it flattens out at about seven years after infection. Bootstrap simulations are used to gauge the variability of the estimates. The incubation estimate is as accurate as other published estimates. Inclusion of the roughness penalty in the criteria to be optimized greatly reduces the variability of the estimates while also greatly speeding the convergence of the algorithms."], ["Modeling American Marriage Patterns", "This article investigates the application of the three-parameter, Coale-McNeil marriage model and some related hyperparameterized specifications to data on the first marriage patterns of American women. Because the model is parametric, it can be used to estimate the parameters of the marriage process for cohorts that have yet to complete their first marriage experience. Empirical evidence from three surveys is reported on the ability of the model to replicate and project observed marriage behavior. The results indicate that the model can be a useful tool for analyzing cohort marriage data and that recent cohorts are showing relatively strong proclivities to both delay and forego marriage. Consistent with earlier work, the results also indicate that education is a powerful covariate of the timing of first marriage and that race is a powerful covariate of its incidence."], ["Regression Diagnostics for Rank-Based Methods", "Residual plots and diagnostic techniques have become important tools in examining the least squares fit of a linear model. In this article we explore the properties of the residuals from a rank-based fit of the model. We present diagnostic techniques that detect outlying cases and cases that have an influential effect on the rank-based fit. We show that the residuals from this fit can be used to detect curvature not accounted for by the fitted model. Furthermore, our diagnostic techniques inherit the excellent efficiency properties of the rank-based fit over a wide class of error distributions, including asymmetric distributions. We illustrate these techniques with several examples."], ["The Adjoint Projection Pursuit Regression", null], ["Bootstrap Test for Difference between Means in Nonparametric Regression", null], ["Adaptive Cluster Sampling", "In many real-world sampling situations, researchers would like to be able to adaptively increase sampling effort in the vicinity of observed values that are high or otherwise interesting. This article describes sampling designs in which, whenever an observed value of a selected unit satisfies a condition of interest, additional units are added to the sample from the neighborhood of that unit. If any of these additional units satisfies the condition, still more units may be added. Sampling designs such as these, in which the selection procedure is allowed to depend on observed values of the variable of interest, are in contrast to conventional designs, in which the entire selection of units to be included in the sample may be determined prior to making any observations. Because the adaptive selection procedure introduces biases into conventional estimators, several estimators are given that are design unbiased for the population mean with the adaptive cluster designs of this article; that is, the unbiasedness does not depend on any assumptions about the population. The Rao-Blackwell method is used to obtain improved unbiased estimators; because of the incompleteness of the minimal sufficient statistic, more than one of these improved estimators are obtained. Simple criteria are given determining when adaptive cluster sampling strategies are more efficient than simple random sampling of equivalent sample size. Motivation for the designs in this article is provided by a wide variety of sampling situations in fields such as ecology, geology, and epidemiology. For example, in a survey of a rare bird species, once individuals of the species are detected, additional observations at nearby sites often reveal more individuals. In a study of a contagious disease, the addition to the sample of close contacts of infected individuals reveals a higher than average incidence rate. The results and examples in this article show that adaptive cluster sampling strategies give lower variance than conventional strategies for certain types of populations and, in particular, provide an extremely effective way of sampling rare, clustered populations."], [null, null], ["Inference for Near-Integrated Time Series with Infinite Variance", null], ["Double Sampling for Exact Values in Some Multivariate Measurement Error Problems", "Increasing attention is being given to measurement error models in which the dimension of the proxy or surrogate values is different from that of the missing true values. Even if they are of the same dimension, the error model may not be the simple additive one of observed = true + error, where the error has mean 0. The use of broader models relating true and observed values requires the use of external or internal data containing some true values to calibrate/validate the measurement error model. This article considers a multivariate normal framework in which measurement error is allowed in any subset of the variables with a broad class of multivariate regression models relating true and observed values. The classical additive model is a special case. Multiple regression with random regressors (the structural case) is treated within this framework. Correcting for measurement error is made possible through double sampling in which true values are obtained for a randomly chosen subset of the main study units. Maximum likelihood estimators and their asymptotic properties are developed for both unrestricted and restricted models, where the latter arise through specific assumptions about the nature of the measurement error. Detailed results are given for simple linear regression in which case optimal double sampling rates are determined for estimating the slope with minimum variance subject to cost considerations. An example is presented based on the use of infrared measurements as surrogates for characteristics of wheat."], ["Comparing Classical and Resistant Outlier Rules", null], ["Bias of Autoregressive Spectral Estimators", null], ["Needed Data Expenditure for an Ambiguous Decision Problem", null], ["On Bootstrap Iteration for Coverage Correction in Confidence Intervals", null], ["Bootstrap Choice of Bandwidth for Density Estimation", "A bootstrap-based choice of bandwidth for kernel density estimation is introduced. The method works by estimating the integrated mean squared error (IMSE) for any given bandwidth and then minimizing over all bandwidths. A straightforward application of the bootstrap method to estimate the IMSE fails because it does not capture the bias component. A smoothed bootstrap method based on an initial density estimate is described that solves this problem. It is possible to construct pointwise and simultaneous confidence intervals for the density. The simulation study compares cross-validation and the bootstrap method over a wide range of densities\u2014a long-tailed, a short-tailed, an asymmetric, and a bimodal, among others. The bootstrap method uniformly outperforms cross-validation. The accuracy of the constructed confidence bands improves as the sample size increases."], ["Estimation following a Sequentially Designed Experiment", "We investigate the design of experiments for the nonparametric estimation of the root of an unknown regression function. Approaches to this problem include the Robbins-Monro (1951), Venter (1967), and Lai-Robbins (1979, 1981) stochastic approximation procedures and Wu's (1985, 1986) sequential maximum likelihood estimators. Because the regression function is not assumed to belong to a parametric family, only experimentation near to the root is informative and the sequential design should converge to the root. After the sequential design has been generated, there are at least two distinct methods for estimation of the root: (a) estimate the root by the last design point, or (b) fit a parametric model. Except for Ruppert (1988) and simulation studies by Bodt (1985) and Bodt and Tingey (1990), all studies known to us of stochastic approximation procedures used the last design point as the estimator. Wu (1985, 1986) fit a generalized linear model. When the last design point is the estimator, then clearly the design should converge to the root as rapidly as possible. Wu also used designs converging as rapidly as possible. The main new idea in this article is that, when one fits a parametric model, it is not necessary for the design to converge rapidly to the root, and there is an important advantage to slow convergence\u2014it improves the precision with which one can estimate the derivative of the regression function at the root. This derivative must be estimated, at least implicitly, to estimate the root efficiently, and the derivative is itself an important scale measure. Using techniques from Wei (1985), it is relatively easy to establish the asymptotic distribution of the least squares estimator of the root when one fits a linear model. We establish sufficient conditions for the least squares estimate of the root to be asymptotically efficient. We also show that sequential designs satisfying these conditions can be generated by the Robbins-Monro, Lai-Robbins, and Venter stochastic approximation procedures. The latter is especially convenient, since it allows the design to converge to the root at an easily controlled rate. Besides fitting a linear model by least squares, we discuss fitting generalized linear models by a one-step approximate maximum likelihood algorithm. These results differ from those of Wu (1985, 1986) in that a much wider class of designs is considered."], ["A Distribution-Free Test for Symmetry Based on a Runs Statistic", "I present a simple test, based on a runs statistic, for symmetry of a continuous distribution about a known median. The statistic has a binomial sampling distribution and desirable invariance properties. Monte Carlo studies demonstrate that, for a wide variety of alternative asymmetric distributions, the test is more powerful than tests proposed by Butler (1969), Rothman and Woodroofe (1972), or Hill and Rao (1977)."], ["Allocation Proportional to Coefficients of Variation When Estimating the Product of Parameters", "Consider the problem of allocating a fixed total number of observations or a fixed budget to several populations with the goal of minimizing the variance of the product of sample means, where this product of sample means estimates the product of population means. The main application of this problem is to reliability, where the population mean is the Bernoulli success probability that the respective system component will function and the product is the probability that a series system composed of these components will function. The solution of the problem is shown to be allocation approximately proportional to population coefficients of variation or to the square root of the odds of failure in the reliability case. Furthermore, balanced allocation is shown to have a maximin property in terms of its asymptotic relative efficiency to optimal allocation. Finally, guidelines are given for constructing allocations that improve on balanced allocation."], ["Nonparametric Analysis of Unbalanced Paired-Comparison or Ranked Data", null], ["A Lower Bound for the Risk in Estimating the Value of a Probability Density", null], ["Confidence Intervals from the Difference between Two Correlated Proportions", "When data arise as matched binary pairs, it is often of interest to test whether or not the probabilities of an odd pair of each type are equal. Construction of a small-sample confidence interval for the difference in these probabilities is more difficult and either approximate or conservative methods need be resorted to. Two methods are suggested in this article, one a natural asymptotic method and the other a more generally applicable conservative method. The coverage probability and average width are studied for small and moderate sample sizes."], ["Bayesian Inference in Models with Euclidean Structures", "When a parameter space is endowed with a Euclidean structure, there is a Euclidean measure, called the structural (or geometric) prior, which is an obvious candidate to represent ignorance concerning an unknown parameter. The rationale for using this prior is strengthened when the unknown parameter is the canonical parameter of an exponential family. Then the structural prior is the uniform prior for the canonical parameter. It is shown also that Euclidean structures can be combined in different ways with group structures. The structural (or geometric) prior is then the product of the invariant prior for the group structure and the structural (or geometric) prior for the Euclidean structure."], ["Book Reviews", null], ["Publications Received", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["Error Models for Official Mortality Forecasts", "The Office of the Actuary, U.S. Social Security Administration, produces alternative forecasts of mortality to reflect uncertainty about the future. Appropriate probabilistic interpretations of the intervals have not been provided, however, because explicit stochastic models are not used. In this article we identify the components and assumptions of the official forecasts and approximate them by stochastic parametric models. We estimate parameters of the models from past data, derive statistical intervals for the forecasts, and compare them with the official high-low intervals. We use the models to evaluate the forecasts rather than to develop different predictions of the future. Analysis of data from 1972 to 1985 shows that the official intervals for mortality forecasts for males or females aged 45\u201370 have approximately a 95% chance of including the true mortality rate in any year. For other ages the chances are much less than 95%. The Office of the Actuary's interval estimates of age- and cause-specific mortality for males and females have no consistent probabilistic interpretation. The ranges are too large for some diseases and too small for others."], ["Density Estimation with Confidence Sets Exemplified by Superclusters and Voids in the Galaxies", "A method is presented for forming both a point estimate and a confidence set of semiparametric densities. The final product is a three-dimensional figure that displays a selection of density estimates for a plausible range of smoothing parameters. The boundaries of the smoothing parameter are determined by a nonparametric goodness-of-fit test that is based on the sample spacings. For each value of the smoothing parameter our estimator is selected by choosing the normal mixture that maximizes a function of the sample spacings. A point estimate is selected from this confidence set by using the method of cross-validation. An algorithm to find the mixing distribution that maximizes the spacings functional is presented. These methods are illustrated with a data set from the astronomy literature. The measurements are velocities at which galaxies in the Corona Borealis region are moving away from our galaxy. If the galaxies are clustered, the velocity density will be multimodal, with clusters corresponding to modes. Natural candidates for examining the distribution of the data are finite normal mixtures and histograms. The shortcomings of these methods become apparent from the analysis of these data. By finding a confidence set of densities a set of estimates is obtained, ranging from smooth to rough; the number of modes ranges from three to seven. The confidence set of densities is further substantiated by performing nonparametric tests for the number of modes."], ["Forecasting Technological Substitutions with Concurrent Short Time Series", "It has been shown in the literature that the data of technological substitutions exhibit a strong correlation across different time periods, that is, a strong serial correlation. Significant improvement in predicting such substitutions has been achieved by incorporating serial correlation and power transformation parameters into four growth curve models. The modified models, or data-based transformed models, however, break down when the number of time points is small. This article proposes a generalized growth curve model for forecasting technological substitutions with concurrent short time series. The model combines the concepts of power transformations and repeated measurements with a common serial covariance structure. Concurrent time series for several cases provide the repeated measurement requirement of the model. Improvement in forecasting accuracy by using this model is demonstrated with a set of telephone switching data."], ["Unmasking Multivariate Outliers and Leverage Points", "In the case of regression data, the classical least squares approach masks outliers in a similar way. Also here, the outliers may be unmasked by using a highly robust regression method. Finally, a new display is proposed in which the robust regression residuals are plotted versus the robust distances. This plot classifies the data into regular observations, vertical outliers, good leverage points, and bad leverage points. Several examples are discussed."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Approximate Quasi-likelihood Estimation in Models with Surrogate Predictors", "We consider quasi-likelihood estimation with estimated parameters in the variance function when some of the predictors are measured with error. We review and extend four approaches to estimation in this problem, all of them based on small measurement error approximations. A taxonomy of the data sets likely to be available in measurement error studies is developed. An asymptotic theory based on this taxonomy is obtained and includes measurement error and Berkson error models as special cases."], ["Hyperdimensional Data Analysis Using Parallel Coordinates", "This article presents the basic results of using the parallel coordinate representation as a high-dimensional data analysis tool. Several alternatives are reviewed. The basic algorithm for parallel coordinates is laid out and a discussion of its properties as a projective transformation is given. Several duality results are discussed along with their interpretations as data analysis tools. Permutations of the parallel coordinate axes are discussed, and some examples are given. Some extensions of the parallel coordinate idea are given. The article closes with a discussion of implementation and some of my experiences."], ["Fast Evaluation of the Distribution of the Durbin-Watson and other Invariant Test Statistics in Time Series Regression", null], ["On the Behavior of Randomization Tests without a Group Invariance Assumption", null], ["Construction of Permutation Tests", "A general method for constructing permutation tests for various experimental designs follows from invariance and sufficiency. In this framework, randomization (or rerandomization) tests are just special cases of permutation tests. The methodology extends the applicability of permutation tests: An example demonstrates a test for an interaction effect in a factorial experiment, a problem thought to be unapproachable by permutation or randomization methods. Tests are constructed as follows. The modeling of block and treatment effects closely parallels classical analysis of variance (ANOVA). The ANOVA assumptions of iid normal random errors are replaced, however, by the much weaker assumption of exchangeability of subsets of random errors. Distributional dependences on the unknown nuisance parameters for blocks, covariates, and untested treatments and on the unknown error distribution are eliminated by invariance and sufficiency, respectively. Initial reduction of the data by invariance appears to extend considerably the applicability of permutation tests. Because of the assumed exchangeability of the errors, the sufficiency step leads to data permutations when computing the test. The method also addresses some philosophical problems associated with randomization tests. For instance, in this article's framework, it is the design, and not the data, which is always regarded as fixed, so ancillarity of the design causes no difficulty."], ["A Monte Carlo Implementation of the EM Algorithm and the Poor Man's Data Augmentation Algorithms", null], ["A Frequency Domain Selection Criterion for Regression with Autocorrelated Errors", null], ["Calibrating Prediction Regions", null], ["Forecast Error Symmetry in ARIMA Models", null], [null, null], [null, null], ["Kernel Smoothing of Data with Correlated Errors", null], ["Density Estimation Using Exponentials of Orthogonal Series", "If a density is represented as an exponential of a series of orthogonal polynomials, it has a simple likelihood with a complete set of sufficient statistics. To ensure that the maximum likelihood equations are well conditioned, the polynomials should be orthogonal with respect to the estimated density. The optimal number of terms in the series can be determined by minimizing an estimate of a loss function. The penalized likelihood equations can be solved with little extra effort. Computer simulations show that the resulting density estimator has a smaller mean integrated squared error than the linear orthogonal series estimator."], ["Incomplete Data in Generalized Linear Models", "This article examines incomplete data for the class of generalized linear models, in which incompleteness is due to partially missing covariates on some observations. Under the assumption that the missing data are missing at random, it is shown that the E step of the EM algorithm for any generalized linear model can be expressed as a weighted complete data log-likelihood when the unobserved covariates are assumed to come from a discrete distribution with finite range. Expressing the E step in this manner allows for a straightforward maximization in the M step, thus leading to maximum likelihood estimates (MLE's) for the parameters. Asymptotic variances of the MLE's are also derived, and results are illustrated with two examples."], ["Diagnostics for Assessing the Accuracy of Normal Approximations in Exponential Family Nonlinear Models", "Diagnostics are investigated for assessing the agreement between likelihood and standard large-sample confidence regions for parameters from an exponential family nonlinear model (Cordeiro and Paula 1989). The development connects the contour methods proposed by Hodges (1985, 1987) with the curvature measures for normal, nonlinear regression proposed by Bates and Watts (1980) and Jennings's (1982, 1986) diagnostics for logistic regression. The proposed methodology is illustrated with several examples."], ["A One-sided Studentized Range Test for lesting against a Simple Ordered Alternative", null], ["Assessing the Effects of Multiple Rows on the Condition Number of a Matrix", "The methods presented in this article deal with the computational as well as the data analytic aspects of problems arising in multivariate data analysis. These methods are applicable in situations where the eigenstructure of a matrix is of interest."], ["Confidence Intervals for Probabilities and Tolerance Regions Based on a Generalization of the Mann-Whitney Statistic", null], ["Adaptive Spline Smoothing in Non-Gaussian Regression Models", "A convenient and efficient algorithm for penalized likelihood smoothing spline regression is proposed and illustrated. The amount of smoothing is tuned adaptively via the generalized cross-validation method. Under certain conditions, the method is shown to approximately minimize the expected Kullback-Leibler discrepancy in modeling one-parameter exponential family observations through the canonical parameter. The algorithm is applied to fit logistic models using interaction splines. This research extends the recent algorithmic development of computing smoothing splines, provides further justification for the heuristics of O'Sullivan, Yandell, and Raynor (1986) on the property of generalized cross-validation in this setting, and experiments with interaction splines."], ["On the Power of Tests for Multiple Comparison of Three Normal Means", null], ["Estimation for Partially Nonstationary Multivariate Autoregressive Models", null], ["An Ancillarity Paradox in the Estimation of Multinomial Probabilities", null], ["Posterior Computations for Censored Regression Data", "This article describes the computation of and sampling from the posterior density for censored regression problems with normal and generalized log-gamma errors. The data augmentation algorithm (Tanner and Wong 1987) is facilitated in the normal error case because of the form of the augmented posterior. In the generalized log-gamma context, this simplicity is absent. The work of Sweeting (1981) is used as a motivation to develop an importance sampling scheme to sample from an augmented posterior. It is shown how the predictive distribution for a new observation may be computed and sampled from. The methodology is illustrated with two examples."], ["State-Dependent Utilities", null], ["Quality Control of Welfare Programs Special Section", null], ["Statistics and Policy in Welfare Quality Control: A Basis for Understanding and Assessing Competing Views", "Designing quality control (QC) systems in public welfare programs exposes competing policy interests. Statistical design choices are not exclusively technical choices. Rather, they balance competing interests in assigning and collecting penalties and can influence how the programs themselves are run and to what ends. Thus the recent controversy around the application of financial penalties for overpayments in the nation's three family assistance programs joins statistical issues with deeper public policy concerns. This article describes the policy context in which the statistical issues are raised and the design, measurement, and estimation questions that expose competing policy interests. It describes the administration of the family assistance programs, the operation and limitations of their QC systems for addressing problems of administration, the statistical and policy issues inherent in the major QC design questions, and National Academy of Sciences and federal study findings and recent legislative reforms as they relate to the issues raised."], ["Regression Estimates in Federal Welfare Quality Control Programs", "States that administer federal family welfare programs review samples of the beneficiaries to guide corrective actions. Federal agencies review subsamples of the state samples to compute overpayment error rates using a regression estimator. These federal estimates make state-to-state comparisons of quality possible and are also used for fiscal sanctions. Such applications of the regression estimator have been widely challenged. We show that the regression estimates and estimates of their variances are closely unbiased, and we examine their statistical characteristics. We also comment on the recent reports of the National Research Council Panel on Quality Control of Family Assistance Programs."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Inference for Welfare Quality Control Programs", "Federal and state agencies estimate rates of payment error made by state caseworkers in determining benefits under the Aid to Families with Dependent Children, Food Stamp, and Medicaid programs. Estimated payment error rates are used in turn to estimate program penalties that are a function of the difference between payment error rates and target rates. This article first outlines the statistical methodology currently used in welfare quality control systems, namely, the double sampling scheme for data collection, the regression estimator of the payment error rate, and the penalty system. Stratification of the data is suggested as a method for addressing obvious modeling problems with the regression estimator, and the notion of penalty bias and its properties is discussed. Alternative estimators of payment error rates, such as hierarchical Bayes and empirical Bayes estimators, are next proposed, which may reduce bias and error through combining information. The article is illustrated by real-data examples. Directions for future research are also pointed out."], ["Food Stamp Payment Error Rates: Can State-Specific Performance Standards Be Developed?", null], ["Book Reviews", null], ["Publications Received", null], ["Editorial Board Page", "This article has no abstract"], ["Editors' Report for 1989", null], ["Estimating the Electoral Consequences of Legislative Redistricting", "We analyze the effects of redistricting as revealed in the votes received by the Democratic and Republican candidates for state legislature. We develop measures of partisan bias and the responsiveness of the composition of the legislature to changes in statewide votes. Our statistical model incorporates a mixed hierarchical Bayesian and non-Bayesian estimation, requiring simulation along the lines of Tanner and Wong (1987). This model provides reliable estimates of partisan bias and responsiveness along with measures of their variabilities from only a single year of electoral data. This allows one to distinguish systematic changes in the underlying electoral system from typical election-to-election variability."], ["Estimating Fecundability from Data on Waiting Times to First Conception", "This article tests assumptions invoked in the demographic literature to estimate the population distribution of fecundability from data on waiting times to first conception. In continuous time, the key assumption is that waiting times are realizations from a mixture of exponentials distribution. In discrete time, the key assumption is that waiting times are realizations from a mixture of geometrics distribution. The Hutterite data analyzed by Sheps (1965) are consistent with this assumption. Various models, however, have one representation in mixture of exponentials form. A fundamental identification problem plagues the conventional estimation procedure. Our analysis calls into question the conventional practice of checking model specification by using goodness-of-fit tests. The practical importance of the identification problem in duration models is demonstrated."], ["The Deterrent Effect of Capital Punishment: An Analysis of Daily Homicide Counts", "This article analyzes daily homicide data to address whether the occurrence of executions exerts a short-term deterrent effect. The data analyzed are drawn from computerized death certificates from California during the early 1960s, the last period during which frequent executions were carried out there. The analysis of data from one legal jurisdiction helps overcome criticisms of earlier studies based on data aggregated over executing and nonexecuting jurisdictions. Furthermore, the detailed information in the death certificates allows one to analyze the murders of victims of different race and sex separately, which in principle should allow one to detect possible race- or gender-specific effects too small to be discerned in aggregate data. Poisson and compound-Poisson regression models are estimated, to control for seasonal fluctuations and to measure the effect of executions on daily homicide counts. Two-week and four-week periods surrounding the dates on which executions took place are examined for possible deterrent effects, and a large-sample variant of Scheffe's procedure is used to construct the appropriate critical regions for the resulting multiple comparisons. The data are seen to provide no support for the notion that executions deter homicides in the short term."], ["Inference from Coarse Data via Multiple Imputation with Application to Age Heaping", "Multiple imputation is applied to a demographic data set with coarse age measurements for Tanzanian children. The heaped ages are multiply imputed with plausible true ages using (a) a simple naive model and (b) a new, relatively complex model that relates true age to the observed values of heaped age, sex, and anthropometric variables. The imputed true ages are used to create valid inferences under the models and compare inferences across models, thereby revealing sensitivity of inferences to prior specifications, from naive to complex. In addition, diagnostic analyses applied to the imputed data are used to suggest which models appear most appropriate. Because it is not clear just what set of heaping intervals should be used, the models are applied under various assumptions about the heaping: rounding (to the nearest year or half year) versus a combination of rounding and truncation as practiced in the United States, and medium versus wide heaping interval sizes. The most striking conclusions are the following: (a) inferences are very sensitive to the assumption of strict rounding versus rounding combined with truncation, yet judging from the diagnostics, the data cannot distinguish between such models; and (b) the diagnostics consistently favor the new, more complex model, which, although theoretically more satisfactory, can lead to inferences very similar to those obtained with the naive model. It is concluded that knowledge of the interval widths and heaping process sharpens valid inferences from data of this kind, and that given a specified process, simple and easily programmed multiple-imputation methods can lead to valid inferences."], ["Multinomial Runs Tests to Detect Clustering in Constrained Free Recall", "Psychologists often want to detect category structure in subjects' free recall protocols. Although runs tests based on the binomial distribution are commonly used to detect nonrandomness within a sequence, many research situations require tests based on the multinomial distribution. We propose a test of randomness versus clustering based on the number of runs in multinomial data. We illustrate its use with data from a mass communication experiment using a constrained free recall procedure."], ["Evaluating Screening for the Early Detection and Treatment of Cancer without Using a Randomized Control Group", "Sometimes the only data available for evaluating screening for the early detection and treatment of cancer come from studies lacking a randomized control group. To evaluate these data, various approaches have been proposed. They include constructing a model to estimate the duration of preclinical cancer and the sensitivity of the screen, formulating a model to estimate rates of cancer mortality, and using a nonrandomized control group to estimate age-specific incidence and mortality rates. These methods have various limitations: Either the estimated quantities are insufficient for a complete evaluation, strong parametric assumptions are required, or an assumption must be made that the nonrandomized control group yields unbiased estimates of cancer incidence. To bypass some of these limitations, a new method has been developed for analyzing screening data without a randomized control group. The method is called periodic screening evaluation (PSE) because it requires data from periodic screens rather than a single screen. Using weakly structured parametric models, PSE estimates the effect of screening on cancer mortality, using only data from persons offered screening. PSE's major innovation is a technique for estimating age-specific cancer incidence using older screened persons as controls for younger screened persons. The limitations of PSE are often preferable to those associated with other methods. PSE's principal limitations are (a) use of a nonrandomized control group to estimate case-fatality rates in the absence of screening and (b) an assumption that, given a person's age, the year of birth adds no information for predicting cancer incidence. PSE was applied to data on persons offered screening for the early detection and treatment of breast cancer in which a randomized control group not offered screening was available for validation. Using only data from persons offered screening, the probability of averting death from cancer due to starting periodic screening at various ages was estimated. Confidence intervals were large. Nevertheless, a fivefold increase in sample size, which probably would have been feasible without the constraints of a randomized trial, would likely have yielded adequate precision. For validation purposes the cumulative number of observed deaths from cancer in the randomized control group was compared with the estimated cumulative number based on applying PSE only to women offered screening. The agreement was good."], ["Modeling and Monitoring Biomedical Time Series", "A framework for modeling and monitoring medical time series is developed, based on extensions to the linear dynamic model. The approach is designed to provide for on-line, recursive updating and inference for time series that are subject to several forms of potential discontinuous change, and that may have missing values. The models and methods introduced are illustrated with three biomedical series."], ["An Application of the Seasonal Fractionally Differenced Model to the Monetary Aggregates", "In this article, three significant variables used by the U.S. Federal Reserve as targets to shape monetary policy, the monetary aggregates M1, M2, and M3, are examined using a seasonal fractionally differenced model. The sample autocorrelation functions of these monetary variables exhibit a decay pattern at the seasonal lags that is typical of a fractional model. The seasonal fractionally differenced model is found to remove a great deal of the autocorrelation at the seasonal lags, especially when a series is extended by splicing together earlier monetary data. Some Monte Carlo evidence as to the efficacy of this technique is presented. Finally, one-year-ahead out-of-sample forecasts of M1 are made using both the Box\u2014Jenkins airline model and the seasonal fractionally differenced model."], ["Sliding-Spans Diagnostics for Seasonal and Related Adjustments", "When are the results of a seasonal adjustment procedure (or another smoothing procedure) likely to be of little value? The diagnostic approach presented in this article offers an answer to this question and to other questions concerned with the comparison of competing adjustments. It is based on a straightforward idea. A minimal requirement of the output of any smoothing or adjustment procedure is stability: Appending or deleting a small number of series values should not substantially change the smoothed values\u2014otherwise, what reliable interpretation can they have? An important related principle is that, for a given series, if only one of several plausible signal-extraction procedures has a stable output, then this procedure should be the preferred one for the series. To implement these principles successfully, the definition of stability must be made precise in an appropriate way. The implementation described in this article is focused on multiplicative adjustments produced by the widely used X-11 seasonal adjustment procedure, but it is clear that the basic ideas are more widely applicable. The discussion addresses decisions about direct and indirect seasonal adjustment, trading-day adjustment, trends, forecast extension prior to adjustment, and other common adjustment issues."], ["Homophily and Social Distance in the Choice of Multiple Friends an Analysis Based on Conditionally Symmetric Log-Bilinear Association Models", "This article presents an analysis of friendship choice data by focusing on homophily (or inbreeding bias) and social distance revealed in the patterns of both association between subjects' and friends' statuses and association among friends' statuses. These two aspects of inbreeding bias and social distance are simultaneously taken into account in modeling the data of friendship choice from subjects with different numbers of friends. The statuses of friends are expressed in terms of their combinations rather than their full cross-classifications. Conditionally symmetric log-bilinear partial association models are usefully employed for the analysis. The structural characteristics of inbreeding bias and social distance are identified by comparing nested models and through the interpretation of parameters estimated from models that adequately fit the data."], ["The Relationship between the Length of the Base Period and Population Forecast Errors", "The base period of a population forecast is the time period from which historical data are collected for the purpose of forecasting future population values. The length of the base period is one of the fundamental decisions made in preparing population forecasts, yet very few studies have investigated the effects of this decision on population forecast errors. In this article the relationship between the length of the base period and population forecast errors is analyzed, using three simple forecasting techniques and data from 1900 to 1980 for states in the United States. It is found that increasing the length of the base period up to 10 years improves forecast accuracy, but that further increases generally have little additional effect. The only exception to this finding is long-range forecasts of rapidly growing states, in which a longer base period substantially improves forecast accuracy for two of the forecasting techniques."], ["A Multivariate Generalization of Quantile-Quantile Plots", null], ["Testing the Goodness of Fit of a Linear Model via Nonparametric Regression Techniques", null], ["Influence on Confidence Regions for Regression Coefficients in Generalized Linear Models", "Influence on confidence regions has received less attention than the problem of influence on regression coefficients in generalized linear models. Diagnostics are given for examining influence on confidence intervals or the confidence region for all of the regression coefficients. These diagnostics permit one to check whether the standard errors of the coefficients are sensitive to the exact specification of case weights, or the observed values of the explanatory variables or the response. The diagnostics are illustrated with an example and compared to standard diagnostic methods."], ["Sampling-Based Approaches to Calculating Marginal Densities", "Stochastic substitution, the Gibbs sampler, and the sampling-importance-resampling algorithm can be viewed as three alternative sampling- (or Monte Carlo-) based approaches to the calculation of numerical estimates of marginal probability distributions. The three approaches will be reviewed, compared, and contrasted in relation to various joint probability structures frequently encountered in applications. In particular, the relevance of the approaches to calculating Bayesian posterior densities for a variety of structured models will be discussed and illustrated."], ["Kernel Quantile Estimators", null], ["Refining Bootstrap Simultaneous Confidence Sets", null], ["Small-Sample Confidence Intervals", null], ["Extension of the Stein Estimating Procedure through the Use of Estimating Functions", null], [null, null], ["Breakdown Robustness of Tests", null], ["Exact Inference for Contingency Tables with Ordered Categories", null], ["The Relative Efficiency of Goodness-of-Fit Statistics in the Simple and Composite Hypothesis-Testing Problem", null], [null, "Logistic-normal distributions and related functions arise in a variety of statistical applications of current interest, including binary measurement-error models and the analysis of teratogenicity experiments. Analytic intractability has led to the development of numerous approximations to the desired forms, often with consequences that have not been well studied. A method is developed to compute these forms to arbitrary accuracy, and comparative calculations are made that show when the common numerical alternative, 20-point Gaussian quadrature, begins to fail. By using a simple matrix transformation, this method can be used with multiple covariate regression models of the logistic-normal form. We conducted a simulation study that compares the ability of 20-point Gaussian quadrature and our new method to obtain the maximum likelihood estimator of relative risk in the logistic-normal measurement-error model. Using standard subroutines to maximize the likelihood equations, 27 of 50 trials failed to converge with 20-point Gaussian quadrature, whereas the new method allowed convergence in all but one case."], ["The Maximal Smoothing Principle in Density Estimation", null], ["Signed-Rank Tests for Censored Matched Pairs", null], ["Bootstrap Prediction Intervals for Autoregression", null], ["Sensitivity of Two-Sample Permutation Inferences in Observational Studies", "In observational studies, subjects are not randomly assigned to treatment or control, so they may differ in their chances of receiving the treatment. A simple method is developed and demonstrated for displaying the sensitivity of conventional two-group permutation inferences to departures from random assignment of treatments. The unmatched case, discussed here, differs in certain technical and computational details from the matched case, discussed previously; however, the underlying model and the method for quantifying departures from randomization are the same. The method may be applied to Wilcoxon's rank sum test, the Gehan and log-rank tests for censored outcomes, Mantel's test for scored categories, and Fisher's exact test for binary responses. The method embeds the usual randomization reference distribution in a one-parameter family of departures involving an unobserved covariate that would have been controlled by adjustments had it been observed. As this parameter is varied, the sensitivity of permutational significance levels and confidence intervals is displayed. Brief discussion of an example shows that comparisons vary considerably in their degree of sensitivity to unobserved biases."], ["Modeling Time-Varying Dynamical Systems", "A global methodology of identification, estimation and forecasting of transfer function (Box-Jenkins) models with deterministically varying parameters is provided. First, properties of stability and forecasting algorithms are investigated by means of Markovian representations and methods of solution of nonstationary difference equations. Next, the degree of the polynomials of the system is specified with typical off-line methods, and the shape of the coefficients (parameter functions) is identified by means of recursive (on-line) algorithms. Finally, the identified parameter functions are inserted in the model and their coefficients are estimated (off-line) on the original data by means of pseudolinear regression techniques."], ["Bounded-Influence Rank Regression: A One-Step Estimator Based on Wilcoxon Scores", null], ["The Accuracy of Approximate Intervals for a Binomial Parameter", null], ["Efficiencies of Interblock Rank Statistics for Repeated Measures Designs", null], ["Two-Sample Inference for Median Survival Times Based on One-Sample Procedures for Censored Survival Data", null], ["A Simulation Study of the Analysis of Sets of 2 \u00d7 2 Contingency Tables under Cluster Sampling: Estimation of a Common Odds Ratio", null], ["Confidence Curves in Nonlinear Regression", "Standard Wald confidence regions for parameters in a normal nonlinear regression model often fail to capture accurately the uncertainty of estimation as reflected by the corresponding profile log-likelihood. We present a graphical method, along with a stable computational algorithm, for inference on scalar parameters in a nonlinear regression model."], ["A Multivariate Signed-Rank Test for the One-Sample Location Problem", null], ["Models for Distributions on Permutations", null], ["Tests of Hypotheses in Overdispersed Poisson Regression and other Quasi-Likelihood Models", null], [null, null], ["Comparison of Linear Estimators Using Pitman's Measure of Closeness", "A method is given for the comparison of two linear forms of a common random vector under the criterion of Pitman's measure of closeness. Assuming multivariate normality of the random vector, one can determine exact expressions for the closeness probabilities. The applicability of the theory is illustrated on the comparison of ridge regression estimators."], ["Testing the Mixture of Exponentials Hypothesis and Estimating the Mixing Distribution by the Method of Moments", "This article presents nonparametric methods for testing the hypothesis that duration data can be represented by a mixture of exponential distributions. Both Bayesian and classical tests are developed. A variety of apparently distinct models can be written in mixture of exponentials form. This raises a fundamental identification problem. A consistent estimator for the number of points of support of a discrete mixture is developed. A consistent method-of-moments estimator for the mixing distribution is derived from the testing criteria and is evaluated in a Monte Carlo study."], ["Book Reviews", null], ["Publications Received", null], ["Letters to the Editor", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["Statistics and Public Policy: Reflections of a Changing World", null], ["Probabilistic Methods in Crystal Structure Analysis", "One of the main goals of modern crystallography is the determination of the detailed internal structure of crystalline matter, at the atomic level. Statistical analyses and, in particular, random-walk models play a central role in inferring structural information from crystallographic data. Such methods are routinely employed by crystallographers in the determination of crystal symmetry from the experimental data, and in the solution of the outstandingly important problem for this discipline, the phase problem. Three classes of approaches are discussed: (a) methods based entirely on the central limit theorem; (b) approximate expansions in terms of orthogonal polynomials that have the central-limit-theorem pdf as their weight function\u2014that is, Gram\u2013Charlier and Edgeworth expansions; and (c) pdf's that are exactly formulated and reduced to computable forms, represented as Fourier and Fourier\u2013Bessel series. Both univariate and multivariate pdf's of crystallographic interest are derived and discussed. Some other approximate probabilistic approaches that have been applied to crystallographic problems are also briefly reviewed."], ["Censored Survival Data with Misclassified Covariates: A Case Study of Breast-Cancer Mortality", null], ["Modular Nonparametric Subsurvival Estimation", "This article describes new methods for estimating survival distributions based on nonparametric curve estimators. One approach improves the estimation of long-term survival rates. Simulation studies using Weibull and lognormal data show that even in the case found to be least favorable, the new method has less than one-seventh the prediction error of all conventional life-table (LT) or Kaplan\u2013Meier (KM) estimators, even when the LT and KM techniques are optimized for the purpose of long-term survival estimation. In addition to conventional survival applications, one can also estimate the probability of being disease-free at different ages and following different exposures to possibly harmful environmental contaminants. This approach is particularly useful in situations where the effects of a confounding, nuisance, or effect-modifying variable cannot be confidently modeled in a parametric form. The new techniques are based on a closed-form nonparametric maximum likelihood curve estimator expressed in terms of separate curve estimates obtained from samples of randomly censored and uncensored times to failure\u2014that is, subsurvival populations."], ["Disclosure Control of Microdata", "National statistical offices meet an increasing demand for the dissemination of microdata. This trend can be at variance with the care taken not to disclose data about individuals. Therefore, the risk of disclosure of each microdata set to be released should be assessed. If this risk is too high, measures have to be taken to protect the data set. This article describes the disclosure problem, and explains why it is a real problem. Using the concept of uniqueness, some theory is developed that can help establish the risk of identification. It turns out that useful microdata sets can only be released if some of the disclosure risks are dealt with by legal arrangements, rather than by restrictions on the data to be released."], ["Deviations from Randomness in Children's Early Speech", "One notion of language development suggests that infants' and toddlers' vocalizations are produced at random with no particular patterns or organization. In this article two tests (the pairs test and the gap test) that are used to check the randomness properties of computer-generated pseudorandom numbers are modified to study the sounds produced by infants 13 months of age and toddlers 24 months of age. Deviations from the expected patterns that would be observed if the children did produce sound segments at random suggest features of the development of language skills. The information provided by the lack of fit of the model corresponding to the pairs test is used to develop models that capture the common patterns in transitions among sound classifications in children's early speech."], ["Measurement of Hispanic Ethnicity in the U.S. Census: An Evaluation Based on Latent-Class Analysis", "Two models, the U.S. census model and the latent-class model, are compared in their application to evaluating measurements of ethnicity. Although the census approach assumes that the response categories of a questionnaire item correspond to groups in the population, the latent-class approach seeks to assess whether any set of response categories can represent observed ethnic heterogeneity. Data collected using the 1990 census Hispanic-origin question and other instruments for measuring ethnicity suggest that the latent-class approach is superior whenever the response categories are not known to be valid. In particular, using the latent-class model, this article rejects the census model's assumption of a single dimension of meaning underlying responses to the Hispanic-origin question."], ["Comparison of Data-Driven Bandwidth Selectors", "This article compares several promising data-driven methods for selecting the bandwidth of a kernel density estimator. The methods compared are least squares cross-validation, biased cross-validation, and a plug-in rule. The comparison is done by asymptotic rate of convergence to the optimum and a simulation study. It is seen that the plug-in bandwidth is usually most efficient when the underlying density is sufficiently smooth, but is less robust when there is not enough smoothness present. We believe the plug-in rule is the best of those currently available, but there is still room for improvement."], ["The Rank Transform Method in Some Two-Factor Designs", null], ["More Efficient Bootstrap Computations", "This article concerns computational methods for the bootstrap that are more efficient than the straightforward Monte Carlo methods usually used. The bootstrap is considered in its simplest form: in a one-sample nonparametric problem, where the goal is to estimate the bias or variance of some statistic by bootstrap sampling, or to set approximate confidence intervals for a parameter of interest in terms of various percentiles of the bootstrap distribution. The methods of this article can, in favorable situations, reduce the necessary number of bootstrap replications manyfold. Moreover, simple diagnostics are available to see whether or not any particular case is accessible to these methods."], ["Effects of Censoring on the Validity of Confidence Intervals", null], ["The Remedian: A Robust Averaging Method for Large Data Sets", null], ["Approaches for Empirical Bayes Confidence Intervals", "Parametric empirical Bayes (EB) methods of point estimation date to the landmark paper by James and Stein (1961). Interval estimation through parametric empirical Bayes techniques has a somewhat shorter history, which was summarized by Laird and Louis (1987). In the exchangeable case, one obtains a \u201cnaive\u201d EB confidence interval by simply taking appropriate percentiles of the estimated posterior distribution of the parameter, where the estimation of the prior parameters (\u201chyperparameters\u201d) is accomplished through the marginal distribution of the data. Unfortunately, these \u201cnaive\u201d intervals tend to be too short, since they fail to account for the variability in the estimation of the hyperparameters. That is, they do not attain the desired coverage probability in the EB sense defined by Morris (1983a, b). They also provide no statement of conditional calibration (Rubin 1984). In this article we propose a conditional bias correction method for developing EM intervals that corrects these deficiencies in the naive intervals. As an alternative, several authors have suggested use of the marginal posterior in this regard. We attempt to clarify its role in achieving EB coverage. Results of extensive simulation of coverage probability and interval length for these approaches are presented in the context of several illustrative examples."], ["Computing Least Median of Squares Regression Lines and Guided Topological Sweep", null], [null, null], ["Estimating the Survival Curve When New is Better than Used of a Specified Age", null], ["Testing for No Effect When Estimating a Smooth Function by Nonparametric Regression: A Randomization Approach", null], ["The Analysis of Nonadditivity in Two-Way Analysis of Variance", null], ["Exact Properties of Some Exact Test Statistics for Comparing Two Binomial Proportions", null], ["Statistical Inference with Data-Dependent Treatment Allocation Rules", "In comparing two treatments with dichotomous responses, the randomized play-the-winner rule (Wei and Durham 1978) tends to assign more study subjects to the better treatment. For ethical reasons, this property is desirable for studies on human subjects. The randomized play-the-winner rule, which is a modification of Zelen's play-the-winner rule (Zelen 1969), is not deterministic and is less vulnerable to experimental bias than other adaptive designs. Recently, this design has been used in a trial to evaluate extracorporeal membrane oxygenation (ECMO) for treating newborns with respiratory failures at the University of Michigan. In this article, exact conditional, exact unconditional, and approximate confidence intervals for the treatment difference are studied from a frequentist point of view with the randomized play-the-winner rule. For small and moderate-sized trials, the exact unconditional procedures perform much better than the conditional ones because of the adaptive nature of the designs. Furthermore, we find that the design used for the trial should not be ignored in the analysis. The large-sample unconditional confidence intervals based on likelihood ratio statistics are not very sensitive to the design and perform well for moderate-sized trials. On the other hand, the intervals derived from the maximum likelihood estimates behave poorly under the adaptive design. All of the procedures are illustrated with the Michigan ECMO data."], ["The Estimation of the Mean Squared Error of Small-Area Estimators", "Small-area estimation has received considerable attention in recent years because of a growing demand for reliable small-area statistics. The direct-survey estimators, based only on the data from a given small area (or small domain), are likely to yield unacceptably large standard errors because of small sample size in the domain. Therefore, alternative estimators that borrow strength from other related small areas have been proposed in the literature to improve the efficiency. These estimators use models, either implicitly or explicitly, that connect the small areas through supplementary (e.g., census and administrative) data. For example, simple synthetic estimators are based on implicit modeling. In this article, three small-area models, of Battese, Harter, and Fuller (1988), Dempster, Rubin, and Tsutakawa (1981), and Fay and Herriot (1979), are investigated. These models are all special cases of a general mixed linear model involving fixed and random effects, and a small-area mean can be expressed as a linear combination of fixed effects and realized values of random effects. Using the general theory of Henderson (1975) for a mixed linear model, a two-stage estimator (or predictor) of a small-area mean under each model is obtained, by first deriving the best linear unbiased estimator (or predictor) assuming that the variance components that determine the variance-covariance matrix are known, and then replacing the variance components in the estimator with their estimators. Second-order approximation to the mean squared error (MSE) of the two-stage estimator and the estimator of MSE approximation are obtained under normality. Finally, the results of a Monte Carlo study on the efficiency of two-stage estimators and the accuracy of the proposed approximation to MSE and its estimator are summarized. The MSE approximation provides a reliable measure of uncertainty associated with the two-stage estimator. It can also provide asymptotically valid confidence intervals on a small-area mean, as the number of small areas tends to \u221e."], ["Outliers and Credence for Location Parameter Inference", null], ["A Markov Chain Model for Unskilled Workers and the Highly Mobile", "A parametric class of Markov chains that can be used to model such phenomena as job transitions for unskilled workers is presented in this article. Maximum likelihood estimation for the Markov chain model and the mover-stayer model is discussed and is shown to be easy to carry out. An empirical example, using industry-of-occupation data, is presented at the end of the article."], ["Sample Size Allocation for Simultaneous Inference in Comparison with Control Experiments", null], ["Robust Bounded-Influence Tests in Linear Models", null], ["Some New Inequalities for the Range Distribution, with Application to the Determination of Optimum Significance Levels of Multiple Range Tests", "A problem arising in multiple range tests is the monotonicity of the defining critical values, which ws first treated by Lehmann and Shaffer (1977). This article proves a general inequality for the distribution function of the range, which guarantees the monotonicity of a special set of critical values. It also discusses the relation to some other inequalities. The importance of these inequalities is pointed out in considering some concepts of admissibility and optimality of multiple range tests."], ["Pseudo-Maximum Likelihood Estimation of Mean and Covariance Structures with Missing Data", "A nonlinear mean- and covariance-structure model for one or more groups is constructed. The model subsumes the usual linear model considered in the literature. It is then shown how to estimate the parameters of the model and the asymptotic covariance matrix of the parameter estimates using pseudo-maximum likelihood (PML) estimation. The resulting estimates are strongly consistent under general regularity conditions, provided only that the model for the first two moments is correctly specified. Nevertheless, because the data are not necessarily drawn from a multivariate normal distribution, the usual likelihood ratio tests for model comparisons in mean- and covariance-structure models do not apply. Wald tests and Lagrange multiplier tests may be used to implement such comparisons. Next, the standard results on ML estimation with missing data are extended to the case of PML estimation with missing data, and the results are applied to the model. The approach to the missing-data problem adopted, which decomposes the pseudo-log-likelihood function from normal theory into a sum of individual components, cannot generally be implemented by using existing mean- and covariance-structure programs. In some important instances, however, the approach can be implemented by using one of the standard programs (e.g., LISREL). Finally, an example is used to illustrate the approach used. In particular, data from various sources are combined to circumvent an omitted-variables problem in a linear system of equations. The example is somewhat novel because there is no complete data sample from which the model could be estimated. Comments are made on other research situations where data can be combined from multiple sources in the absence of a complete data sample to estimate models that could not otherwise be considered."], ["Pseudolikelihood Estimation for Social Networks", null], ["An Iterative Approach to Two-Dimensional Laplacian Smoothing with Application to Image Restoration", null], ["Specification Tests Based on Artificial Regressions", "Many specification tests can be computed with artificial linear regressions designed to be used as calculating devices to obtain test statistics and other quantities of interest. This article discusses the general principles that underlie all artificial regressions, and the use of such regressions to compute Lagrange multiplier and other specification tests based on estimates under the null hypothesis. The generality and power of artificial regressions as a means of computing test statistics is demonstrated; how Durbin\u2013Wu\u2013Hausman, conditional moment, and other tests that are not explicitly Lagrange multiplier tests may be computed is shown; and several special cases that illustrate the general results and can be useful in practice are discussed. These include tests of parameter restrictions in nonlinear regression models and tests of binary-choice models such as the logit and probit models."], ["Regressor Levels for Bayesian Predictive Response", "The predictor vector for a response variable in a full-rank linear model is chosen so as to maximize the probability that the response exceeds a given threshold. The problem is formulated in the Bayesian framework in a way that incorporates collateral information and allows constraints on the levels of the predictors. Industrial applications are indicated."], ["Interactional Troubles in Face-to-Face Survey Interviews", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Book Reviews", null], ["Publications Received", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"]]}