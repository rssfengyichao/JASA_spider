{"2000": [["Analysis of Smoking Trends with Incomplete Longitudinal Binary Responses", "The generalized estimating equations procedure (GEE) widely applied in the analysis of correlated binary data requires that missing data depend only on remote covariates or that they be missing completely at random (MCAR); otherwise GEE regression parameter estimates are biased. A weighted generalized estimating equations (WGEE) approach that accounts for dropouts under the less stringent assumption of missing at random (MAR) through dependence on observed responses gives unbiased estimation of parameters in the model for the marginal means if the dropout mechanism is specified correctly. WGEEs are applied in the estimation of 7-year trends in cigarette smoking in the United States from a cohort of 5,078 black and white young adults. Analysis using WGEE suggests that there was a general decline in cigarette smoking only among white females, whereas the only other subgroup for which smoking declined was white males of the older birth cohort (1955\u20131962) with college degrees. The results of WGEE are compared to a likelihood-based method valid under MAR that does not require specification of a missing data model."], ["Analysis of a Randomized Trial to Prevent Vertical Transmission of HIV-1", null], ["Transmission of Pneumococcal Carriage in Families: A Latent Markov Process Model for Binary Longitudinal Data", null], ["A Bayesian Model for Fecundability and Sterility", "There is increasing evidence that exposure to environmental toxins during key stages of development can disrupt the human reproductive system. Such effects have proven difficult to study due to the many behavioral and biological factors involved in human reproduction. We analyze data from a North Carolina fertility study to assess the effect of prenatal, childhood, and current cigarette smoking exposure on fecundability and sterility. We use a mixture model that adjusts for timing and frequency of intercourse and allows both fecundability and sterility to depend on multiple covariates. We account for dependency among menstrual cycles within individual couples using a mixture density for a latent cycle viability variable. The mixture consists of a normal distribution describing heterogeneity among fecund couples with a point mass at 0 for sterile couples. The resulting distribution is more biologically plausible than the standard beta density. A Markov chain Monte Carlo scheme is used for Bayesian estimation of the model. There is some evidence that spontaneous intrauterine mortality results in decreased fecundability in subsequent cycles. Both current cigarette smoking and prenatal exposure of the woman to her mother's cigarette smoking are shown to be associated with a decrease in the probability of menstrual cycle viability."], ["Time-Varying Network Tomography: Router Link Data", "Proposed methods are applied to two simple networks at Lucent Technologies and found to perform well. Furthermore, the estimates are validated in a single-router network for which direct measurements of origin-destination counts are available through special software."], ["Spatial Poisson Regression for Health and Exposure Data Measured at Disparate Resolutions", "Ecological regression studies are widely used to examine relationships between disease rates for small geographical areas and exposure to environmental risk factors. The raw data for such studies, including disease cases, environmental pollution concentrations, and the reference population at risk, are typically measured at various levels of spatial aggregation but are accumulated to a common geographical scale to facilitate statistical analysis. In this traditional approach, heterogeneous exposure distributions within the aggregate areas may lead to biased inference, whereas individual attributes such as age, gender, and smoking habits must either be summarized to provide area-level covariate values or used to stratify the analysis. This article presents a spatial regression analysis of the effect of traffic pollution on respiratory disorders in children. The analysis features data measured at disparate, nonnested scales, including spatially varying covariates, latent spatially varying risk factors, and case-specific individual attributes. The problem of disparate discretizations is overcome by relating all spatially varying quantities to a continuous underlying random field model. Case-specific individual attributes are accommodated by treating cases as a marked point process. Inference in these hierarchical Poisson/gamma models is based on simulated samples drawn from Bayesian posterior distributions, using Markov chain Monte Carlo methods with data augmentation."], ["A Nonstationary Multisite Model for Rainfall", null], ["On the Effect of Treatment among Would-Be Treatment Compliers: An Analysis of the Multiple Risk Factor Intervention Trial", "The Multiple Risk Factor Intervention Trial (MRFIT) was a major trial that examined the efficacy of a multifactor intervention on reducing coronary heart disease mortality. After a mean follow-up of 7 years, treatment was not significantly different from control, and extensive investigation was undertaken to explain the result. This article examines how the effect of treatment varies with compliance with treatment. The basic idea is that the binary characteristic \u201cwould comply with treatment\u201d is balanced between groups by randomization, even though it is unobservable before randomization and only observed after randomization in the treatment group. Using baseline covariates, a prediction model for this observed characteristic is developed in the treatment group, and estimated probabilities of treatment compliance are calculated for all patients in both groups. The assumption that patients who would not comply with treatment must have the same mean response in both groups is avoided. Two methods for examining how the effect of treatment depends on treatment compliance are described. Under the first approach, the predicted probabilities of treatment compliance are used to specify a treatment by covariate interaction. Under the second approach, a regression model is specified where the covariate \u201cwould comply with treatment\u201d has an interaction with treatment. This produces a standard regression model in the treatment group and a mixture model with mixing over the unobserved binary covariate in the control group. The two methods are applied to MRFIT and the results compared."], ["Bayesian Analysis and Mapping of Mortality Rates for Chronic Obstructive Pulmonary Disease", "This article summarizes our research on estimation of age-specific and age-adjusted mortality rates for chronic obstructive pulmonary disease (COPD) for white males. Our objectives are more precise and informative displays (than previously available) of geographic variation of the age-specific mortality rates for COPD, and investigation of the relationships between the geographic variation in mortality rates and the corresponding variation in selected covariates. For a given age class, our estimates are displayed in a choropleth map of mean rates. We develop a variation map that identifies the geographical areas where inferences are reliable. Here, the variation is measured by considering a set of maps produced using samples from the posterior distribution of the population mortality rates. Finally, we describe the spatial patterns in the age-specific maps and relate these to patterns in potential explanatory covariates such as smoking rate, annual rainfall, population density, elevation, and measures of air quality."], ["Nonparametric Bayes Estimation of Contamination Levels Using Observations from the Residual Distribution", "Data on contamination concentrations for chromium from one of the EPA's toxic waste sites consist of independent and identically distributed (iid) measurements along with additional observations from the residual distribution. The residual sample is obtained by sampling from hot spots, where contamination concentrations are assumed to be above a given threshold value. The data are modeled using a nonparametric Bayes estimator of the distribution function. The Dirichlet process is used to formulate prior information about the chromium contamination, and we compare the Bayes estimator of the mean concentration level to other estimators currently considered by the EPA and other sources. The Bayes estimator of the mean generally outperforms competing estimators under various cost functions. The Bayes estimator of the distribution function is derived assuming the possibility of right-censored contamination measurements along with left-truncated hot spot data. For the case in which the prior becomes noninformative, the Bayes estimator of the distribution function is the nonparametric maximum likelihood estimator, which is identical to the Kaplan-Meier estimator for concentration values observed below the residual sample threshold. Robustness of the Bayes estimator is examined with respect to misspecification of the prior and its sensitivity to the censoring distribution."], [null, null], [null, null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Rejoinder", null], ["Bias Reduction of Autoregressive Estimates in Time Series Regression Model through Restricted Maximum Likelihood", null], ["The Level-Based Stratified Sampling Plan", "If the probability distribution of the input variables to a system described by a computer code is known, the distribution function of the output variable can be obtained by computer simulations. With a complicated program, each simulation can take very long time. It thus is necessary to choose the combinations of the input parameters carefully to get as much information as possible with a small number of computer runs. Different methods (true random sampling, stratified sampling, Latin hypercube sampling, and updated Latin hypercube sampling) for choosing the input parameters are described in the literature. As a criteria for a good sampling plan, unbiased estimates and low mean squared error are used. By computer experiments, it has been shown that the sampling plans based on Latin hypercubes often have the smallest variance of the estimated output variable. In this article it is shown that a stratified sampling plan with strata defined by the surfaces in the sampling space where the output variable is constant has the lowest variance among all unbiased sampling plans. Unfortunately, it is possible to construct the sampling plan only if the solution is already available. However, an approximate sampling plan can be constructed if an approximate solution is known. In an application, it is shown how the results from a finite element method (FEM) model with a coarse element structure can be used to construct a sampling plan to be used with a FEM model with a finer element structure."], ["Estimating the Mean of an Increasing Stochastic Process at a Censored Stopping Time", null], ["Cox Regression with Accurate Covariates Unascertainable: A Nonparametric-Correction Approach", "Many survival studies involve covariates that are not accurately ascertainable; CD4 lymphocyte count in HIV/AIDS research is a typical example for which the gold standard of the measurement is not available. This article proposes a consistent estimation procedure for Cox regression under the additive measurement error model. Distinct from existing methods, the proposed estimation for regression coefficients does not require any additional assumptions. We establish the normalized partial-score function as a functional of empirical processes, which facilitates the construction of an estimating function with the same limit using replicated mismeasured covariates. The resulting regression coefficient estimators are shown to be consistent and asymptotically normal; a consistent sandwich variance estimate is presented. We also suggest estimators for the baseline cumulative hazard function. Numerical studies demonstrate that the procedure performs well under practical sample sizes. Application to an AIDS clinical trial is provided. Finally, we suggest a unified approach to estimating function construction and large-sample study with partial covariate information, in light of the functional representation of the partial-score function."], ["Inference for Exponential Order Statistic Models Based on an Integrated Likelihood Function", null], ["Whittle Pseudo-Maximum Likelihood Estimation for Nonstationary Time Series", null], ["Inference for Deterministic Simulation Models: The Bayesian Melding Approach", null], ["A Robust Analysis of Crossover Designs using Multisample Generalized L-Statistics", "In a crossover study, some or all subjects receive more than one treatment sequentially. Using a clinical example as motivation, we develop multisample generalized L-statistics (GL-statistics) to estimate and test for treatment effects in crossovers when the distribution of the response data deviates from normality. The basic idea is to adapt simple L-statistics, such as the trimmed mean and median, to data with dependencies. GL-statistics may be applied to crossovers with more than two periods and/or sequences. These designs are useful for experiments with two treatments in which carryover and treatment effects might be aliased in the commonly used two-period, two-sequence design, as well as for experiments with more than two treatments. For data analysis with large samples, the asymptotic properties of the GL-statistics suggest that the generalized trimmed mean and generalized median often should be strongly consistent and normal. A simulation study of a four-sequence, two-period crossover design found little loss in efficiency relative to a least squares approach when the trimmed mean or median is used with normal data, and substantial gains when the data are nonnormal, particularly for large sample sizes."], ["A Simpler, Affine-Invariant, Multivariate, Distribution-Free Sign Test", null], ["Vignettes for the Year 2000: Theory and Methods Foreword", null], ["Bayesian Analysis: A Look at Today and Thoughts of Tomorrow", null], ["An Essay on Statistical Decision Theory", null], ["Markov Chain Monte Carlo: 10 Years and Still Running!", null], ["Empirical Bayes: Past, Present and Future", null], ["Linear and Log-Linear Models", null], ["The Bootstrap and Modern Statistics", null], ["Prospects of Nonparametric Modeling", null], ["Gibbs Sampling", null], ["The Variable Selection Problem", "The problem of variable selection is one of the most pervasive model selection problems in statistical applications. Often referred to as the problem of subset selection, it arises when one wants to model the relationship between a variable of interest and a subset of potential explanatory variables or predictors, but there is uncertainty about which subset to use. This vignette reviews some of the key developments that have led to the wide variety of approaches for this problem."], ["Robust Nonparametric Methods", null], ["Hierarchical Models: A Current Computational Perspective", null], [null, null], ["Generalized Linear Models", null], ["Missing Data: Dial M for ???", null], ["A Robust Journey in the New Millennium", null], ["Likelihood", null], ["Conditioning, Likelihood, and Coherence: A Review of Some Foundational Concepts", null], ["The End of Time Series", null], ["Principal Information Theoretic Approaches", null], ["Measurement Error Models", null], ["Higher-Order Asymptotic Approximation: Laplace, Saddlepoint, and Related Methods", null], ["Minimaxity", null], ["Afterword", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["2000 Editorial Collaborators", "2000 Editorial Collaborators"], ["Editorial Board Page", "This article has no abstract"], ["A Bayesian Time-Course Model for Functional Magnetic Resonance Imaging Data", "Functional magnetic resonance imaging (fMRI) is a new technique for studying the workings of the active human brain. During an fMRI experiment, a sequence of magnetic resonance images is acquired while the subject performs specific behavioral tasks. Changes in the measured signal can be used to identify and characterize the brain activity resulting from task performance. The data obtained from an fMRI experiment are a realization of a complex spatiotemporal process with many sources of variation, both biological and technological. This article describes a nonlinear Bayesian hierarchical model for fMRI data and presents inferential methods that enable investigators to directly target their scientific questions of interest, many of which are inaccessible to current methods. The article describes optimization and posterior sampling techniques to fit the model, both of which must be applied many thousands of times for a single dataset. The model is used to analyze data from a psychological experiment and to test a specific prediction of a cognitive theory."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Optimal Disclosure Limitation Strategy in Statistical Databases: Deterring Tracker Attacks through Additive Noise", "Disclosure limitation methods transform statistical databases to protect confidentiality, a practical concern of statistical agencies. A statistical database responds to queries with aggregate statistics. The database administrator should maximize legitimate data access while keeping the risk of disclosure below an acceptable level. Legitimate users seek statistical information, generally in aggregate form; malicious users\u2014the data snoopers\u2014attempt to infer confidential information about an individual data subject. Tracker attacks are of special concern for databases accessed online. This article derives optimal disclosure limitation strategies under tracker attacks for the important case of data masking through additive noise. Operational measures of the utility of data access and of disclosure risk are developed. The utility of data access is expressed so that trade-offs can be made between the quantity and the quality of data to be released. Application is made to Ohio data from the 1990 census. The article derives conditions under which an attack by a data snooper is better thwarted by a combination of query restriction and data masking than by either disclosure limitation method separately. Data masking by independent noise addition and data perturbation are considered as extreme cases in the continuum of data masking using positively correlated additive noise. Optimal strategies are established for the data snooper. Circumstances are determined under which adding autocorrelated noise is preferable to using existing methods of either independent noise addition or data perturbation. Both moving average and autoregressive noise addition are considered."], ["Subsampling Callbacks to Improve Survey Efficiency", null], ["Bounding Parameter Estimates with Nonclassical Measurement Error", "The bias introduced by errors in the measurement of independent variables has increasingly been a topic of interest among researchers estimating economic parameters. However, studies typically use the assumption of classical measurement error; that is, the variable of interest and its measurement error are uncorrelated, and the expected value of the mismeasured variable is equal to the expected value of the true measure. These assumptions often arise from convenience rather than conviction. When a variable is bounded, it is likely that the measurement error and the true value of the variable are negatively correlated. We consider the case of a noisily measured variable with a negative covariance between the measurement error and the true value of the variable. We show that, asymptotically, the parameter in a univariate regression is bounded between the ordinary least squares (OLS) estimator and an instrumental variables (IV) estimator. Further, we demonstrate that the OLS bound can be improved in the case where there are two noisy reports on the variable of interest. In the case of continuous variables, this lower-bound estimate is a consistent estimate of the parameter. In the case of binary or discrete noisily measured variables, we also identify point estimates using a method-of-moments framework. We then extend our bounding results to simple multivariate models with measurement error. We provide empirical applications of our analytical results using employer and employee reports on health insurance coverage and wage growth, and reports of identical twins on the level of schooling and wages. Using OLS, health insurance coverage is associated with a reduction in wage growth of 6.5\u20137.4%, whereas IV estimates suggest a 11.2\u201311.8% reduction associated with health insurance coverage. We are able to improve the lower bound estimate to 8.2% using our bounding strategy and obtain a point estimate of 8.8% using the method-of-moments framework. The estimates using the data for identical twins, though not correcting for problems such as endogenous determination of the level of schooling, do illustrate the potential usefulness of correcting for measurement error as a complement to other approaches. Using the multiple reports on the level of schooling and the our proposed estimators, we are able to tighten the spread between the upper- and lower-bound estimates of the returns to schooling from 7\u201310 percentage points to approximately 4 percentage points."], ["Estimating and Using Propensity Scores with Partially Missing Data", "Investigators in observational studies have no control over treatment assignment. As a result, large differences can exist between the treatment and control groups on observed covariates, which can lead to badly biased estimates of treatment effects. Propensity score methods are an increasingly popular method for balancing the distribution of the covariates in the two groups to reduce this bias; for example, using matching or subclassification, sometimes in combination with model-based adjustment. To estimate propensity scores, which are the conditional probabilities of being treated given a vector of observed covariates, we must model the distribution of the treatment indicator given these observed covariates. Much work has been done in the case where covariates are fully observed. We address the problem of calculating propensity scores when covariates can have missing values. In such cases, which commonly arise in practice, the pattern of missing covariates can be prognostically important, and then propensity scores should condition both on observed values of covariates and on the observed missing-data indicators. Using the resulting generalized propensity scores to adjust for the observed background differences between treatment and control groups leads, in expectation, to balanced distributions of observed covariates in the treatment and control groups, as well as balanced distributions of patterns of missing data. The methods are illustrated using the generalized propensity scores to create matched samples in a study of the effects of postterm pregnancy."], ["On the Probability of Observing Misleading Statistical Evidence", "The law of likelihood explains how to interpret statistical data as evidence. Specifically, it gives to the discipline of statistics a precise and objective measure of the strength of statistical evidence supporting one probability distribution vis-\u00e0-vis another. That measure is the likelihood ratio. But evidence, even when properly interpreted, can be misleading\u2014observations can truly constitute strong evidence supporting one distribution when the other is true. What makes statistical evidence valuable to science is that this cannot occur very often. Here we examine two bounds on the probability of observing strong misleading evidence. One is a universal bound, applicable to every pair of probability distributions. The other bound, much smaller, applies to all pairs of distributions within fixed-dimensional parametric models in large samples. The second bound comes from examining how the probability of strong misleading evidence varies as a function of the alternative value of the parameter. We show that in large samples one curve describes how this probability first rises and then falls as the alternative moves away from the true parameter value for a very wide class of models. We also show that this large-sample curve, and the bound that its maximum value represents, applies to profile likelihood ratios for one-dimensional parameters in fixed-dimensional parametric models, but does not apply to the estimated likelihood ratios that result from replacing the nuisance parameters by their global maximum likelihood estimates."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Identifying Regression Outliers and Mixtures Graphically", null], ["Bayesian Regression Modeling with Interactions and Smooth Effects", "There have been many recent suggestions as to how to build and estimate flexible Bayesian regression models, using constructs such as trees, neural networks, and Gaussian processes. Although there is much to commend these methods, their implementation and interpretation can be daunting for practitioners. This article presents a spline-based methodology for flexible Bayesian regression that is quite simple in terms of computation and interpretation. Smooth bivariate interactions are modeled in an economical and apparently novel way, and prior distributions that penalize complexity are used. Predictions can be based on either model selection or model averaging. Taking computation, interpretation, and predictive performance into account, the method is seen to perform well when applied to simulated and real data."], ["Integer-Valued, Minimax Robust Designs for Estimation and Extrapolation in Heteroscedastic, Approximately Linear Models", "We present our findings on a new approach to robust regression design. This approach differs from previous investigations into this area in three respects: The use of a finite design space, the use of simulated annealing to carry out the numerical minimization problems, and in our search for integer-valued, rather than continuous, designs. We present designs for the situation in which the response is thought to be approximately polynomial. We also discuss the cases of approximate first- and second-order multiple regression. In each case we allow for possible heteroscedasticity and also obtain minimax regression weights. The results are extended to cover extrapolation of the regression response to regions outside of the design space. A case study involving dose-response experimentation is undertaken. The optimal robust designs, which protect against bias as well as variance, can be roughly described as being obtained from the classical variance-minimizing designs by replacing replicates with clusters of observations at nearby but distinct sites."], ["Efficient Bayesian Inference for Dynamic Mixture Models", "A Bayesian approach is presented for estimating a mixture of linear Gaussian state-space models. Such models are used to model interventions in time series and nonparametric regression. Markov chain Monte Carlo sampling is usually necessary to obtain the posterior distributions of such mixture models, because it is difficult to obtain them analytically. The methodological contribution of the article is to derive a set of recursions for dynamic mixture models that efficiently implement a Markov chain Monte Carlo sampling scheme that converges rapidly to the posterior distribution. The methodology is illustrated by fitting an autoregressive model subject to interventions to zinc concentration in sludge."], ["An Order Selection Criterion for Testing Goodness of Fit", "The classical problem of assessing the goodness of fit of a postulated parametric distribution is investigated using techniques from nonparametric density estimation. A new test is proposed based on the data-selected order of a Fourier series density estimator. This test has the novel feature of providing an associated nonparametric estimator that can be used to estimate the unknown density when the null hypothesis is rejected. The limiting null distribution of the proposed test statistic is derived, and the test is shown to be consistent against essentially any fixed alternative. Results are reported from a simulation experiment that compared empirical power properties of the new test with those of Cram\u00e9r-von Mises and data-driven Neyman smooth-type tests."], ["Geometric Understanding of Likelihood Ratio Statistics", "It is well known that twice a log-likelihood ratio statistic follows asymptotically a chi-square distribution. The result is usually understood and proved via Taylor's expansions of likelihood functions and by assuming asymptotic normality of maximum likelihood estimators (MLEs). We obtain more general results by using a different approach: The Wilks type of results hold as long as likelihood contour sets are fan-shaped. The classical Wilks theorem corresponds to the situations in which the likelihood contour sets are ellipsoidal. This provides a geometric understanding and a useful extension of the likelihood ratio theory. As a result, even if the MLEs are not asymptotically normal, the likelihood ratio statistics can still be asymptotically chi-square distributed. Our technical arguments are simple and easily understood."], ["Linear Regression with Current Status Data", "In survival analysis, a linear model often provides an adequate approximation after a suitable transformation of the survival times and possibly of the covariates. This article proposes a semiparametric regression method for estimating the regression parameter in the linear model without specifying the distribution of the random error, where the response variable is subject to so-called case 1 interval censoring. The method uses a constructed random-sieve likelihood and constraints, combining the benefits of semiparametric likelihood with estimating equations. The estimation procedure is implemented, and the asymptotic distributions for the estimated regression parameter and for the profile likelihood ratio statistic are obtained. In addition, some model diagnostics aspects are described. Finally, the small-sample operating characteristics of the proposed method is examined via simulations, and its usefulness is illustrated on datasets from an animal tumorigenicity study and from a HIV study."], ["The Least Trimmed Differences Regression Estimator and Alternatives", "This article proposes and studies the performance in theory and practice of the least trimmed differences (LTD) linear regression estimator. The estimator minimizes the sum of the smallest quartile of the squares of the differences in each pair of residuals. We obtain the breakdown point, maxbias curve, and large-sample properties of a class of estimators including the LTD as special case. The LTD estimator has a 50% breakdown point and Gaussian efficiency of 66%\u2014substantially higher than other common high-breakdown estimators such as least median of squares and least trimmed squares. The LTD estimator is difficult to compute, but can be performed using a \u201cfeasible solution\u201d algorithm. Half-sample jackknifing is effective in producing standard errors. In simulations we find the LTD to be more stable than other high-breakdown estimators. In an example, the LTD still shows instability like other high-breakdown estimators when there are small changes in the data."], ["Nonparametric Density Estimation from Biased Data with Unknown Biasing Function", "We present a kernel estimator for the density of a variable when sampling probabilities depend on that variable. Both the density and sampling bias weight functions are unknown and are estimated nonparametrically. To achieve this, the method requires that two independent samples be taken from a fixed finite population. An estimator of population size follows simply from our density estimator. Asymptotic bias and standard errors for these estimators are provided, and the methodology is illustrated both on simulation data and on a dual-list dataset of aboriginal people in the Vancouver-Richmond area of Canada."], ["Fully Model-Based Approaches for Spatially Misaligned Data", "We consider inference using multivariate data that are spatially misaligned; that is, involving variables (typically counts or rates) that are aggregated over differing sets of regional boundaries. Geographic information systems enable the simultaneous display of such datasets, but their current capabilities are essentially only descriptive, not inferential. We describe a hierarchical modeling approach that provides a natural solution to this problem through its ability to sensibly combine information from several sources of data and available prior information. Illustrating in the context of counts, allocation under nonnested regional grids is handled using conditionally independent Poisson-mullinomial models. Explanatory covariales and multilevel responses are also easily accommodated, with spatial correlation modeled using a conditionally autoregressive prior structure. Methods for dealing with missing values in spatial \u201cedge zones\u201d are also discussed. Like many recent hierarchical Bayesian applications, computing is implemented via a carefully tailored Metropolis-Hastings algorithm. We illustrate our method with a complex dataset involving inhalation exposure to radon emanating from a depleted uranium fuel processing plant in southwestern Ohio. Structure counts (obtained from U.S. Geological Survey topographical maps) are used to realign sex- and age group-specific U.S. census block group population counts onto a 160-cell circular \u201cwindrose\u201d centered at the plant."], ["Efficient Estimation and Inferences for Varying-Coefficient Models", "This article deals with statistical inferences based on the varying-coefficient models proposed by Hastie and Tibshirani. Local polynomial regression techniques are used to estimate coefficient functions, and the asymptotic normality of the resulting estimators is established. The standard error formulas for estimated coefficients are derived and are empirically tested. A goodness-of-fit test technique, based on a nonparametric maximum likelihood ratio type of test, is also proposed to detect whether certain coefficient functions in a varying-coefficient model are constant or whether any covariates are statistically significant in the model. The null distribution of the test is estimated by a conditional bootstrap method. Our estimation techniques involve solving hundreds of local likelihood equations. To reduce the computational burden, a one-step Newton-Raphson estimator is proposed and implemented. The resulting one-step procedure is shown to save computational cost on an order of tens with no deterioration in performance, both asymptotically and empirically. Both simulated and real data examples are used to illustrate our proposed methodology."], ["Jackknife Variance Estimation under Imputation for Estimators Using Poststratification Information", "Poststratified estimators are commonly used in sample surveys to improve the efficiency of estimators and to ensure calibration to known poststrata counts. Similarly, generalized regression estimators are used to handle two or more poststratifiers with known marginal counts. In addition, weighting adjustment within weighting classes is used to handle unit nonresponse, and imputation within imputation classes is used to handle item nonresponse. For the full response case, asymptotic consistency of the jackknife variance estimator under stratified multistage sampling is established using mild regularity conditions on \u201cresiduals\u201d similar to those of Scott and Wu for ratio and regression estimation under simple random sampling. A jackknife linearization variance estimator, obtained by linearizing the jackknife variance estimator, is also given. For unit nonresponse, the general case of poststrata cutting across weighting classes is considered, and a jackknife variance estimator and the corresponding jackknife linearization variance estimator are obtained. For item nonresponse, weighted mean imputation and weighted hot deck stochastic imputation within imputation classes are studied. Jackknife variance estimators, based on \u201cadjusted\u201d imputed values, are proposed, and the corresponding jackknife linearization variance estimators are obtained. Asymptotic consistency of the jackknife variance estimator is established for both the unit and item nonresponse cases under mild conditions on \u201cresiduals,\u201d assuming uniform response within classes. Simulation results for the poststratified estimator under weighted mean imputation and weighted hot deck stochastic imputation are presented."], ["Models and Algorithms for Optimizing Cell Suppression in Tabular Data with Linear Constraints", null], ["Estimation for Polynomial Structural Equation Models", "Structural equation analysis is one of the most widely used statistical methods in social and behavioral science research and has become a popular tool in marketing. Subject matter needs for considering nonlinear structural models have been well documented. But current fitting procedures are available only for a limited class of models. In this article a systematic statistical approach is developed for the general polynomial structural equation model. The new procedure applies a method of moments procedure similar to the one used in errors-in-variables regression to the factor score estimates from the measurement model fit. The asymptotic properties of the estimator are derived, and a modified estimator with better small-sample properties is introduced. Simulation studies are reported to show the usefulness of the procedure and to compare its performance to other methods. An example from a substance abuse prevention study is also discussed."], ["Functional-Coefficient Regression Models for Nonlinear Time Series", "The local linear regression technique is applied to estimation of functional-coefficient regression models for time series data. The models include threshold autoregressive models and functional-coefficient autoregressive models as special cases but with the added advantages such as depicting finer structure of the underlying dynamics and better postsample forecasting performance. Also proposed are a new bootstrap test for the goodness of fit of models and a bandwidth selector based on newly defined cross-validatory estimation for the expected forecasting errors. The proposed methodology is data-analytic and of sufficient flexibility to analyze complex and multivariate nonlinear structures without suffering from the \u201ccurse of dimensionality.\u201d The asymptotic properties of the proposed estimators are investigated under the \u03b1-mixing condition. Both simulated and real data examples are used for illustration."], ["Computational and Inferential Difficulties with Mixture Posterior Distributions", null], ["Statistics in the Physical Sciences and Engineering", null], ["Challenges in Understanding the Atmosphere", null], ["Seismology\u2014A Statistical Vignette", null], ["Internet Traffic Data", null], ["Coding and Compression: A Happy Union of Theory and Practice", null], ["Statistics in Reliability", null], ["The State of Statistical Process Control as We Proceed into the 21st Century", null], ["Statistics in Preclinical Pharmaceutical Research and Development", null], ["Statistics in Advanced Manufacturing", null], ["Book Reviews", null], ["Telegraphic Review", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["A Bayesian Approach to Combining Information from a Census, a Coverage Measurement Survey, and Demographic Analysis", "Demographic analysis of data on births, deaths, and migration and coverage measurement surveys that use capture-recapture methods have both been used to assess U.S. Census counts. These approaches have established that unadjusted Census counts are seriously flawed for groups such as young and middle-aged African-American men. There is considerable interest in methods that combine information from the Census, coverage measurement surveys, and demographic information to improve Census estimates of the population. This article describes a number of models that have been proposed to accomplish this synthesis when the demographic information is in the form of sex ratios stratified by age and race. A key difficulty is that methods for combining information require modeling assumptions that are difficult to assess based on fit to the data. We propose some general principles for aiding the choice among alternative models. We then pick a particular model based on these principles and imbed it within a more comprehensive Bayesian model for counts in poststrata of the population. Our Bayesian approach provides a principled solution to the existence of negative estimated counts in some subpopulations; provides for smoothing of estimates across poststrata, reducing the problem of isolated outlying adjustments; allows a test of whether negative cell counts are due to sampling variability or more egregious problems such as bias in Census or coverage measurement survey counts; and can be easily extended to provide estimates of precision that incorporate uncertainty in the estimates from demographic analysis and other sources. The model is applied to data for African-American age 30\u201349 from the 1990 Census, and results are compared with those from existing methods."], ["Fitting Population Dynamics Models to Count and Cull Data Using Sequential Importance Sampling", "For prudent wildlife management based on population dynamics models, it is important to incorporate parameter uncertainty into the management advice. Much parameter uncertainty originates when it is not possible to parameterize the population management model for a population of interest using data from that population alone. Instead, information about parameter values obtained from other populations of the same species, or even from similar species, must be used. In addition, the age structure of wildlife populations is generally unknown. We show how sequential importance sampling can be used for combining information on demographic processes, obtained from closely studied populations, with aggregated count and cull information from the population to be managed. We resample parameter sets using kernel smoothing, which has the effect of perturbing parameter values. We show how the fitted model can be used to explore alternative culling strategies for red deer in Scotland."], ["Risk Assessment via a Robust Probit Model, with Application to Toxicology", "Various frameworks have been suggested for assessing the risk associated with continuous toxicity outcomes. The first formulates the effect of exposure on the adverse effect via a simple normal model and then computes the risk function using tail probabilities from the standard normal distribution. Because this risk function depends heavily on the assumed model, it may be sensitive to model misspecification. Recently, a semiparametric approach that utilizes an alternative definition of excess risk has been studied. Unfortunately, it is not yet clear how the two approaches relate to one another. In this article, we investigate a semiparametric normal model in which an unknown transformation of the adverse response satisfies the linear model. We demonstrate that this formulation unifies the two existing approaches and allows for a coherent risk analysis of dose-response data. In addition, estimation and inference procedures for the unknown transformation in the semiparametric model for the continuous response are developed. These are incorporated in novel model-checking procedures, including a formal sup-norm test of the simple normal model. A well-known toxicological study of aconiazide, a drug under investigation for treatment of tuberculosis, serves as a case study for the risk assessment methodology."], ["Power Calculations for Data Missing by Design: Applications to a Follow-Up Study of Lead Exposure and Attention", "Longitudinal designs often change at critical times based on available funding, staffing, scientific opportunities, and subjects. This article presents three levels of investigation into missingness by design in a partially completed longitudinal study: Missingness that is completely at random (MCAR), at random (MAR), and nonignorable (MN). We first derive new expressions for the asymptotic variance and power based on multivariate normal data that are either MCAR or missing by design (MAR). These formulas allow for any and all patterns of missing data. The special case of a two-stage longitudinal design is described in detail. We then present a general design and analytical strategy for protecting against MN data midway into a longitudinal study. The new design involves stratified sampling for follow-up based on the pattern of missing data already obtained, and the corresponding estimator is based on an approximate likelihood. The methodology for MCAR, MAR, and MN are in turn applied to the design of a follow-up study to examine the effect of lifetime lead exposure on neuropsychological measures of attention. Our conclusion in this example is that a design exists that has sufficiently high Statistical power to address the main scientific questions and also provides protection against a broad class of nonignorably missing data."], ["Multidimensional Longitudinal Data: Estimating a Treatment Effect from Continuous, Discrete, or Time-to-Event Response Variables", "Multidimensional data arise when a number of different response variables are required to measure the outcome of interest. Examples of such outcomes include quality of life, cognitive ability, and health status. The goal of this article is to develop a methodology to estimate a treatment effect from multidimensional data that have been collected longitudinally using continuous, discrete, or time-to-event responses or a mixture of these types of responses. A transformation of the time scale that does not depend on the units of the response variables is used to capture the effect of treatment. This allows information about the treatment effect to be combined across response variables of different types. The model is specified using a pair of regression models for the first two moments, and generalized estimating equations are used for parameter estimation. The methodology is applied to quality-of-life data from an AIDS clinical trial and health status data from an Alzheimer's disease study."], ["Causal Inference without Counterfactuals", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["On Profile Likelihood", "We show that semiparametric profile likelihoods, where the nuisance parameter has been profiled out, behave like ordinary likelihoods in that they have a quadratic expansion. In this expansion the score function and the Fisher information are replaced by the efficient score function and efficient Fisher information. The expansion may be used, among others, to prove the asymptotic normality of the maximum likelihood estimator, to derive the asymptotic chi-squared distribution of the log-likelihood ratio statistic, and to prove the consistency of the observed information as an estimator of the inverse of the asymptotic variance."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Monte Carlo Evaluation of Resampling-Based Hypothesis Tests", "Monte Carlo estimation of the power of tests that require resampling can be very computationally intensive. It is possible to reduce the size of the inner resampling loop as long as the resulting estimator of power can be corrected for bias. A simple linear extrapolation method is shown to perform well in correcting for bias and thus reduces computation time in Monte Carlo power studies."], ["Phylogenetic Tree Construction Using Markov Chain Monte Carlo", null], ["Conditional Minimum Volume Predictive Regions for Stochastic Processes", "Motivated by interval/region prediction in nonlinear time series, we propose a minimum volume (MV) predictor for a strictly stationary process. The MV predictor varies with respect to the current position in the State space and has the minimum Lebesgue measure among all regions with the nominal coverage probability. We have established consistency, convergence rates, and asymptotic normality for both coverage probability and Lebesgue measure of the estimated MV predictor under the assumption that the observations are from a strong mixing process. Applications with both real and simulated datasets illustrate the proposed methods."], ["Nonparametric Function Estimation for Clustered Data When the Predictor is Measured without/with Error", "We consider local polynomial kernel regression with a single covariate for clustered data using estimating equations. We assume that at most m < \u221e observations are available on each cluster. In the case of random regressors, with no measurement error in the predictor, we show that it is generally the best strategy to ignore entirely the correlation structure within each cluster and instead pretend that all observations are independent. In the further special case of longitudinal data on individuals with fixed common observation times, we show that equivalent to the pooled data approach is the strategy of fitting separate nonparametric regressions at each observation time and constructing an optimal weighted average. We also consider what happens when the predictor is measured with error. Using the SIMEX approach to correct for measurement error, we construct an asymptotic theory for both the pooled and the weighted average estimators. Surprisingly, for the same amount of smoothing, the weighted average estimators typically have smaller variances than the pooling strategy. We apply the proposed methods to analysis of the AIDS Costs and Services Utilization Survey."], ["Methods for Density Estimation in Thick-Slice Versions of Wicksell's Problem", null], ["Interactive Tree-Structured Regression via Principal Hessian Directions", "An interactive approach to tree-structured regression is introduced. Unlike other procedures driven by cost optimization, this approach focuses on the exploration of geometric information in the data. The procedure begins with finding a direction along which the regression surface bends the most. This direction is used for splitting the data into two regions. Within each region, another direction is found, and the partition is made in the same manner. The process continues recursively until the entire regressor domain is decomposed into regions wherein the surface no longer bends significantly and linear regression fit becomes appropriate. For implementing the direction search, the method of principal Hessian directions is applied. Several simulation and empirical results are reported. Comparison with three methods\u2014CART, SUPPORT, and MARS\u2014is made. The benefit of using geometric information is highlighted."], ["Using Principal Component Analysis and Correspondence Analysis for Estimation in Latent Variable Models", null], ["Combining Propensity Score Matching with Additional Adjustments for Prognostic Covariates", "Propensity score matching refers to a class of multivariate methods used in comparative studies to construct treated and matched control samples that have similar distributions on many covariates. This matching is the observational study analog of randomization in ideal experiments, but is far less complete as it can only balance the distribution of observed covariates, whereas randomization balances the distribution of all covariates, both observed and unobserved. An important feature of propensity score matching is that it can be easily combined with model-based regression adjustments or with matching on a subset of special prognostic covariates or combinations of prognostic covariates that have been identified as being especially predictive of the outcome variables. We extend earlier results by developing approximations for the distributions of covariates in matched samples created with linear propensity score methods for the practically important situation where matching uses both the estimated linear propensity scores and a set of special prognostic covariates. Such matching on a subset of special prognostic covariates is an observational study analog of blocking in a randomized experiment. An example combining propensity score matching with Mahalanobis metric matching and regression adjustment is presented that demonstrates the flexibility of these methods for designing an observational study that effectively reduces both bias due to many observed covariates and bias and variability due to a more limited subset of covariates. Of particular importance, the general approach, which includes propensity score matching, was distinctly superior to methods that focus only on a subset of the prognostically most important covariates, even if those covariates account for most of the variation in the outcome variables. Also of importance, analyses based on matched samples were superior to those based on the full unmatched samples, even when regression adjustment was included."], ["A Decision Theoretic Approach to Imputation in Finite Population Sampling", "Consider the situation where observations are missing at random from a simple random sample drawn from a finite population. In certain cases it is of interest to create a full set of sample values such that inferences based on the full set will have the stated frequentist properties even though the statistician making those inferences is unaware that some of the observations were missing in the original sample. This article gives a Bayesian decision theoretic solution to this problem when one is primarily interested in making inferences about the population mean."], ["Asymptotic Behavior of Linear Permutation Tests under General Alternatives, with Application to Test Selection and Study Design", "Tests based on the permutation of observations are a common and attractive method of comparing two groups of outcomes, in part because they retain proper test size with minimal assumptions and can have high efficiency toward specific alternatives of interest. In addition, permutation tests may be used with discrete or categorical outcomes, for which linear rank tests are not designed. Permutation tests are now increasingly used to analyze discrete or continuous responses that themselves are functions of several statistics. Examples of such summary statistics include the area under the curve generated by repeated measures of a laboratory marker or an overall composite score from a quality of life study. Here even simple structures for the joint distribution of the component statistics can lead to complex differences between the distributions of summary statistics of the comparison groups. Despite their attractive features, surprisingly little is known about the behavior of linear permutation tests when the two groups differ even in simple ways. This lack of knowledge limits an assessment of the relative efficiency of different tests or the planning of the size of a study based on a permutation test. To address these issues, we derive the asymptotic distribution of permutation tests under a general contiguous alternative, and then investigate the implications for test selection and study design for several diverse areas of application. For discrete outcomes, areas of application include permutation tests for ordinal responses and for count data. For continuous outcomes, we explore several applications, including general results for location-scale families, a comparison of different data transformations, and a comparison to linear rank tests."], ["Analysis of Accelerated Hazards Models", "The proportional hazards model for survival time data usually assumes that the covariates of interest take constant effects proportionally on an unspecified baseline hazard function. However, it may not be applicable when the assumption of constant proportionality is violated. In a two-arm randomized clinical trial, for example, the treatment is often expected to be fully effective only after a certain lag period. Some alternatives, such as the accelerated failure time model, have been developed in statistical literature. This article introduces an accelerated hazards model when there is a scale change relationship between the hazard functions. An estimating equation is proposed to estimate the parameter semiparametrically. The methodology is demonstrated within a two-sample framework. Several extensions of the model are also considered. Real clinical trial data are used to investigate the model's practical use."], ["Two-Sample Multistate Accelerated Sojourn Times Model", "Many medical studies involve observations of patients experiencing multiple consecutive states during their follow-up. Since treatments may have differential effects on these states, comparison is of interest with respect to individual sojourn times. In this article I generalize the univariate accelerated failure time model for multistate processes and model the treatment effects as state-specific time scale changes. I propose a two-sample inference procedure that accommodates incomplete follow-up data. This semiparametric procedure, based on estimating equations, yields a class of estimators that are consistent and asymptotically normal. Sample-based consistent variance estimates are derived. Numerical studies demonstrate that the procedure performs well for practical sample sizes. Application to a cancer clinical trial is presented for illustration."], ["Statistics in Business and Social Science", null], ["Finance: A Selective Survey", null], ["Statistics and Marketing", null], ["Time Series and Forecasting: Brief History and Future Research", null], ["Contingency Tables and Log-Linear Models: Basic Results and New Developments", null], ["Causal Inference in the Social Sciences", null], ["Political Methodology: A Welcoming Discipline", null], ["Statistics in Sociology, 1950\u20132000", null], ["Psychometrics", null], ["Empirical Methods and the Law", null], ["Demography: Past, Present, and Future", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Correction", null], ["Editorial Board Page", "This article has no abstract"], ["Statisticians' Significance", "Statisticians are often not recognized at a level appropriate to their contributions. Moreover, statistical methods of analysis may be receiving more recognition than statisticians themselves. This article considers several measures of the recognition of our profession and explores why recognition is important not only to the individual statistician, but also to the health and prosperity of our profession."], ["Functional Components of Variation in Handwriting", "Functional data analysis techniques are used to analyze a sample of handwriting in Chinese. The goals are (a) to identify a differential equation that satisfactorily models the data's dynamics, and (b) to use the model to classify handwriting samples taken from differential individuals. After preliminary smoothing and registration steps, a second-order linear differential equation, for which the forcing function is small, is found to provide a good reconstruction of the original script records. The equation is also able to capture a substantial amount of the variation in the scripts across replication. The cross-validated classification process is 100% effective for the samples analyzed."], ["Transitional Regression Models, with Application to Environmental Time Series", "Environmental epidemiologists often encounter time series data in the form of discrete or other nonnormal outcomes; for example, in modeling the relationship between air pollution and hospital admissions or mortality rates. We present a case study examining the association between pollen counts and meteorologic covariates. Although such time series data are inadequately described by standard methods for Gaussian time series, they are often autocorrelated, and warrant an analysis beyond those provided by ordinary generalized linear models (GLMs). Transitional regression models (TRMs), signifying nonlinear regression models expressed in terms of conditional means and variances given past observations, provide a unifying framework for two mainstream approaches to extending the GLM for autocorrelated data. The first approach models current outcomes with a GLM that incorporates past outcomes as covariates, whereas the second models individual outcomes with marginal GLMs and then couples the error terms with an autoregressive covariance matrix. Although the two approaches coincide for the Gaussian GLM, which serves as a helpful introductory example, in general they yield fundamentally different models. We analyze the pollen study using TRM's of both types and present parameter estimates together with asymptotic and bootstrap standard errors. In several cases we find evidence of residual autocorrelation; however, when we relax the TRM to allow for a nonparametric smooth trend, the autocorrelation disappears. This kind of trade-off between autocorrelation and flexibility is to be expected, and has a natural interpretation in terms of the covariance function for a nonparametric smoother. We provide an algorithm for fitting these flexible TRM's that is relatively easy to program with the generalized additive model software in S-PLUS."], ["Genetic Susceptibility and Survival: Application to Breast Cancer", "Inherited mutations of the BRCA1 and BRCA2 genes are known to confer an elevated risk of both breast and ovarian cancers. The effect of carrying such a mutation on survival after developing breast or ovarian cancer is less well understood. We investigate the relationship between BRCA1 and BRCA2 carrier status and survival after breast cancer. We obtained data from the Cancer and Steroid Hormone Study, a large population-based study including more than 4,000 breast cancer cases. Patient data include extensive information about breast and ovarian cancer in relatives. We obtained follow-up information about patients via record linkage with the Surveillance, Epidemiology, and End Results registry, with maximum follow-up of 15 years. In the absence of genetic testing for each individual, presence or absence of mutation at a breast cancer susceptibility gene is captured by a pair of binary latent variables whose marginal probability depends on the patient's family history of breast and ovarian cancer. We estimate the effect of genotype on survival using a Cox proportional hazards model, treating genetic status as a latent variable and controlling for stage at diagnosis, histology, whether radiation treatment was administered, the individual's smoking history, body mass, race, and age at diagnosis. Inference is accomplished using a Markov chain Monte Carlo algorithm to draw a sample from the posterior distribution of model parameters accounting for sampling error, uncertainty in the genotype of study participants, and uncertainty in estimates of genetic parameters. An analysis that does not discriminate between BRCA1 and BRCA2 estimates the genetic effect on survival after breast cancer for women carrying a mutation at either site. We find evidence that survival of nonirradiated mutation carriers is better than that of noncarriers; we estimate the probability of improved survival to be .990. As a byproduct of this analysis, we estimate that a study based on 901 women with known mutation status would yield estimates of genetic effect on survival with comparable uncertainty to that obtained in the combined-gene analysis. An analysis assessing the separate effects of the two genes indicates that carriers of either mutation may have an improved prognosis. We estimate the probability of improved survival among nonirradiated BRCA1 carriers to be .844, and that among nonirradiated BRCA2 carriers to be .924. However, substantial uncertainty remains about the magnitude of the BRCA2 effect."], ["Analyzing a Randomized Cancer Prevention Trial with a Missing Binary Outcome, an Auxiliary Variable, and All-or-None Compliance", "The Prostate Cancer Prevention Trial is a randomized chemoprevention trial designed to compare the effect of daily finasteride versus placebo on prostate cancer determined by biopsy. Investigators have scheduled a biopsy at the end of the trial in 7 years or following a positive prostate-specific antigen (PSA) on annual screening. The analysis will need to adjust for two likely complications. First, some subjects will not receive a biopsy, depending in part on whether or not they had a positive PSA. The indicator of positive PSA is called an auxiliary variable, which is a variable observed after randomization and prior to outcome. Second, starting soon after randomization, some subjects randomized to finasteride will stop taking their tablets, and some subjects randomized to placebo will obtain finasteride outside of the trial. This type of noncompliance is called all-or-none. To adjust for these complications, we formulate the appropriate likelihoods and obtain closed-form maximum likelihood estimates and variances. Without these adjustments, estimates may be biased, two-sided type I errors above nominal levels, and coverage of confidence intervals below nominal levels."], ["Estimation and Inference for Logistic Regression with Covariate Misclassification and Measurement Error in Main Study/Validation Study Designs", "In epidemiological studies, continuous covariates often are measured with error and categorical covariates often are misclassified. Using the logistic regression model to represent the relationship between the binary outcome and the perfectly measured and classified covariates, the model for the observed main study data is derived. This derivation relies on the assumption that the error in the continuous covariates is multivariate normally distributed and uses a chain of logistic regression models to describe the misclassification processes. These model assumptions are empirically verified in the validation study, where the misclassified and mismeasured covariates are validated using perfectly measured and classified data. The full data likelihood, including contributions from both the main study and the validation study, is maximized to obtain the maximum likelihood estimates for the parameters of the underlying logistic regression model and of the measurement error model and reclassification models simultaneously. Standard asymptotic theory is applied. An example of this methodology is presented from the Nurses' Health Study investigating the relationship between cumulative incidence of breast cancer and saturated fat, total energy, and alcohol intake. A detailed simulation study was conducted to investigate the small-sample properties of these likelihood-based estimates and inferential quantities. No single estimation/inference option performed satisfactorily when the main study/validation study size was representative of that typically encountered in practice. When the validation size was twice or larger than from the usual one, features of asymptotic optimality were more apparent. By example and through simulation, the procedures appeared to be robust to misspecification of the order of the chain of conditional measurement error/reclassification models."], ["Model Selection and Semiparametric Inference for Bivariate Failure-Time Data", "We propose model selection procedures for bivariate survival models for censored data generated by the Archimedean copula family. In route to constructing the selection methodology, we develop estimates of some time-dependent association measures, including estimates of the local and global Kendall's tau, local odds ratio, and other measures defined throughout the literature. We propose a goodness-of-fit-based model selection methodology as well as a graphical approach. We show that the proposed methods have desirable asymptotic properties and perform well in finite samples."], ["Comment", null], ["Rejoinder", null], ["Nonparametric Analysis of Randomized Experiments with Missing Covariate and Outcome Data", "Analysis of randomized experiments with missing covariate and outcome data is problematic, because the population parameters of interest are not identified unless one makes untestable assumptions about the distribution of the missing data. This article shows how population parameters can be bounded without making untestable distributional assumptions. Bounds are also derived under the assumption that covariate data are missing completely at random. In each case the bounds are sharp; they exhaust all of the information available given the data and the maintained assumptions. The bounds are illustrated with applications to data obtained from a clinical trial and data relating family structure to the probability that a youth graduates from high school."], ["Comment", null], ["Rejoinder", null], ["A Nonparametric \u201cTrim and Fill\u201d Method of Accounting for Publication Bias in Meta-Analysis", "Meta-analysis collects and synthesizes results from individual studies to estimate an overall effect size. If published studies are chosen, say through a literature review, then an inherent selection bias may arise, because, for example, studies may tend to be published more readily if they are statistically significant, or deemed to be more \u201cinteresting\u201d in terms of the impact of their outcomes. We develop a simple rank-based data augmentation technique, formalizing the use of funnel plots, to estimate and adjust for the numbers and outcomes of missing studies. Several nonparametric estimators are proposed for the number of missing studies, and their properties are developed analytically and through simulations. We apply the method to simulated and epidemiological datasets and show that it is both effective and consistent with other criteria in the literature."], ["Efficient Monte Carlo Methods for Conditional Logistic Regression", "Exact inference for the logistic regression model is based on generating the permutation distribution of the sufficient statistics for the regression parameters of interest conditional on the sufficient statistics for the remaining (nuisance) parameters. Despite the availability of fast numerical algorithms for the exact computations, there are numerous instances where a data set is too large to be analyzed by the exact methods, yet too sparse or unbalanced for the maximum likelihood approach to be reliable. What is needed is a Monte Carlo alternative to the exact conditional approach which can bridge the gap between the exact and asymptotic methods of inference. The problem is technically hard because conventional Monte Carlo methods lead to massive rejection of samples that do not satisfy the linear integer constraints of the conditional distribution. We propose a network sampling approach to the Monte Carlo problem that eliminates rejection entirely. Its advantages over alternative saddlepoint and Markov Chain Monte Carlo approaches are also discussed."], ["Estimation of Discrete Distributions with a Class of Simplex Constraints", "Simplex constraints, such as monotonicity and convexity or concavity on the probabilities of a set of discrete distributions, are useful for modeling and analyzing discrete data. This article considers both maximum likelihood estimation and Bayesian estimation of discrete distribution with a class of simplex constraints using the Expectation-Maximization (EM) algorithm and the data augmentation (DA) algorithm. The formulation and implementation of EM and DA for binomial, Poisson, hierarchical Poisson-binomial, multinomial, and hierarchical multinomial distributions are considered in detail and illustrated with examples."], ["The Multiple-Try Method and Local Optimization in Metropolis Sampling", null], ["Safe and Effective Importance Sampling", "We present two improvements on the technique of importance sampling. First, we show that importance sampling from a mixture of densities, using those densities as control variates, results in a useful upper bound on the asymptotic variance. That bound is a small multiple of the asymptotic variance of importance sampling from the best single component density. This allows one to benefit from the great variance reductions obtainable by importance sampling, while protecting against the equally great variance increases that might take the practitioner by surprise. The second improvement is to show how importance sampling from two or more densities can be used to approach a zero sampling variance even for integrands that take both positive and negative values."], ["Inference with Imputed Conditional Means", "In this article we present analytic techniques for inference from a dataset in which missing values have been replaced by predictive means derived from an imputation model. The derivations are based on asymptotic expansions of point estimators and their associated variance estimators, and the resulting formulas can be thought of as first-order approximations to standard multiple-imputation procedures with an infinite number of imputations for the missing values. Our method, where applicable, may require substantially less computational effort than creating and managing a multiply imputed database; moreover, the resulting inferences can be more precise than those derived from multiple imputation, because they do not rely on simulation. Our techniques use components of the standard complete-data analysis, along with two summary measures from the fitted imputation model. If the imputation and analysis phases are carried out by the same person or organization, then the method provides a quick assessment of the variability due to missing data. If a data producer is supplying the imputed data set to outside analysts, then the necessary summary measures could be supplied to the analysts, enabling them to apply the method themselves. We emphasize situations with iid samples, univariate missing data, and complete-data point estimators that are smooth functions of means, but also discuss extensions to more complicated situations. We illustrate properties of our methods in several examples, including an application to a large dataset on fatal accidents maintained by the National Highway Traffic Safety Administration."], ["React Scatterplot Smoothers: Superefficiency through Basis Economy", "REACT estimators for the mean of a linear model involve three steps: transforming the model to a canonical form that provides an economical representation of the unknown mean vector, estimating the risks of a class of candidate linear shrinkage estimators, and adaptively selecting the candidate estimator that minimizes estimated risk. Applied to one- or higher-way layouts, the REACT method generates automatic scatterplot smoothers that compete well on standard datasets with the best fits obtained by alternative techniques. Historical precursors to REACT include nested model selection, ridge regression, and nested principal component selection for the linear model. However, REACT's insistence on working with an economical basis greatly increases its superefficiency relative to the least squares fit. This reduction in risk and the possible economy of the discrete cosine basis, of the orthogonal polynomial basis, or of a smooth basis that generalizes the discrete cosine basis are illustrated by fitting scatterplots drawn from the literature. Flexible monotone shrinkage of components rather than nested 1\u20130 shrinkage achieves a secondary decrease in risk that is visible in these examples. Pinsker bounds on asymptotic minimax risk for the estimation problem express the remarkable role of basis economy in reducing risk."], ["Extending the Scope of Wavelet Regression Methods by Coefficient-Dependent Thresholding", "Various aspects of the wavelet approach to nonparametric regression are considered, with the overall aim of extending the scope of wavelet techniques to irregularly spaced data, to regularly spaced datasets of arbitrary size, to heteroscedastic and correlated data, and to data that contain outliers. The core of the methodology is an algorithm for finding ail of the variances and within-level covariances in the wavelet table of a sequence with given covariance structure. If the original covariance matrix is band-limited, then the algorithm is linear in the length of the sequence. The variance calculation algorithm allows data on any set of independent variable values to be treated, by first interpolating to a fine regular grid of suitable length, and then constructing a wavelet expansion of the gridded data. Various thresholding methods are discussed and investigated. Exact risk formulas for the mean square error of the methodology for given design are derived. Good performance is obtained by noise-proportional thresholding, with thresholds somewhat smaller than the classical universal threshold. Outliers in the data can be removed or downweighted, and aspects of such robust techniques are developed and demonstrated in an example. Another natural application is to correlated data, where the covariance of the wavelet coefficients is not due to an initial grid transform but rather is an intrinsic feature. The use of the method in these circumstances is demonstrated by an application to data synthesized in the study of ion channel gating. Our basic approach has many other potential applications, some of which are discussed briefly."], ["Statistical Properties and Uses of the Wavelet Variance Estimator for the Scale Analysis of Time Series", null], ["Window Subsampling of Estimating Functions with Application to Regression Models", null], ["Asymptotics for Analysis of Variance When the Number of Levels is Large", null], ["Reference Bayesian Methods for Generalized Linear Mixed Models", "Bayesian methods furnish an attractive approach to inference in generalized linear mixed models. In the absence of subjective prior information for the random-effect variance components, these analyses are typically conducted using either the standard invariant prior for normal responses or diffuse conjugate priors. Previous work has pointed out serious difficulties with both strategies, and we show here that as in normal mixed models, the standard invariant prior leads to an improper posterior distribution for generalized linear mixed models. This article proposes and investigates two alternative reference (i.e., \u201cobjective\u201d or \u201cnoninformative\u201d) priors: an approximate uniform shrinkage prior and an approximate Jeffreys's prior. We give conditions for the existence of the posterior distribution under any prior for the variance components in conjunction with a uniform prior for the fixed effects. The approximate uniform shrinkage prior is shown to satisfy these conditions for several families of distributions, in some cases under mild constraints on the data. Simulation studies conducted using a logit-normal model reveal that the approximate uniform shrinkage prior improves substantially on a plug-in empirical Bayes rule and fully Bayesian methods using diffuse conjugate specifications. The methodology is illustrated on a seizure dataset."], ["Additive Hazards Regression with Covariate Measurement Error", "The additive hazards model specifies that the hazard function conditional on a set of covariates is the sum of an arbitrary baseline hazard function and a regression function of covariates. This article deals with the analysis of this semiparametric regression model with censored failure time data when covariates are subject to measurement error. We assume that the true covariate is measured on a randomly chosen validation set, whereas a Surrogate covariate (i.e., an error-prone version of the true covariate) is measured on all study subjects. The Surrogate covariate is modeled as a linear function of the true covariate plus a random error. Only moment conditions are imposed on the measurement error distribution. We develop a class of estimating functions for the regression parameters that involve weighted combinations of the contributions from the validation and nonvalidation sets. The optimal weight can be selected by an adaptive procedure. The resulting estimators are consistent and asymptotically normal with easily estimated variances. Simulation results demonstrate that the asymptotic approximations are adequate for practical use. Illustration with a real medical study is provided."], ["A Class of Weighted Log-Rank Tests for Survival Data When the Event is Rare", null], ["A Minimum Description Length-Based Image Segmentation Procedure, and its Comparison with a Cross-Validation-Based Segmentation Procedure", "Image segmentation is a very important problem in image analysis, as quite often it is a key component of a good practical solution to a real-life imaging problem. It aims to partition a digital image into a set of nonoverlapping homogeneous regions. One approach to segmenting an image is to fit a piecewise constant function to the image and define the segmentation by the discontinuity points of the fitted function. The article's first contribution is to present a new and automatic segmentation procedure which follows this piecewise constant function fitting approach. This procedure is based on Rissanen's minimum description length (MDL) principle and consists of two components: (a) an MDL-based criterion in which the \u201cbest\u201d segmentation (i.e., the \u201cbest\u201d fitted piecewise constant function) is defined as its minimizer and (b) a fast-merging algorithm that attempts to locate this minimizer. As a second contribution, the new MDL-based procedure is compared with a cross-validation based segmentation procedure. Empirical results from a simulation study suggest the new MDL-based procedure is superior. Some possible extensions of the MDL-based procedure are also described."], ["Inference from Dual Frame Surveys", "In a dual frame survey, samples are drawn independently from two overlapping frames that together cover the population of interest. Several estimators for population totals in dual frame surveys are discussed and compared under a unified setup. We propose jackknife variance estimators, establish their design consistency, and explore their finite-sample performance in a simulation study."], ["Statistics in the Year 2000: Vignettes", null], ["Statistics in the Life and Medical Sciences", null], ["Survival Analysis", null], ["Causal Analysis in the Health Sciences", null], ["Environmental Statistics", null], ["Capture-Recapture Models", null], ["Statistics in Animal Breeding", null], ["Some Issues in Assessing Human Fertility", null], ["Statistical Issues in Toxicology", null], ["Receiver Operating Characteristic Methodology", null], ["The Randomized Clinical Trial", null], ["Some Contributions of Statistics to Environmental Epidemiology", null], ["Challenges Facing Statistical Genetics", null], ["Computational Molecular Biology", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Editorial Board Page", "This article has no abstract"]]}