{"2009": [["A Bayesian Model for Cross-Study Differential Gene Expression", " In this article we define a hierarchical Bayesian model for microarray expression data collected from several studies and use it to identify genes that show differential expression between two conditions. Key features include shrinkage across both genes and studies, and flexible modeling that allows for interactions between platforms and the estimated effect, as well as concordant and discordant differential expression across studies. We evaluate the performance of our model in a comprehensive fashion, using both artificial data, and a \u201csplit-study\u201d validation approach that provides an agnostic assessment of the model\u2019s behavior under both the null hypothesis and a realistic alternative. The simulation results from the artificial data demonstrate the advantages of the Bayesian model. Furthermore, the simulations provide guidelines for when the Bayesian model is most likely to be useful. Most notably, in small studies the Bayesian model generally outperforms other methods when evaluated based on several performance measures across a range of simulation parameters, with the differences diminishing for larger sample sizes in the individual studies. The split-study validation illustrates appropriate shrinkage of the Bayesian model in the absence of platform, sample, and annotation differences that otherwise complicate experimental data analyses. Finally, we fit our model to four breast cancer studies using different technologies (cDNA and Affymetrix) to estimate differential expression in estrogen receptor\u2013 positive tumors versus estrogen receptor\u2013negative tumors. Software and data for reproducing our analysis are available publicly. "], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Analysis of Multifactor Affine Yield Curve Models", null], ["Bayesian Calibration of Microsimulation Models", " Microsimulation models that describe disease processes synthesize information from multiple sources and can be used to estimate the effects of screening and treatment on cancer incidence and mortality at a population level. These models are characterized by simulation of individual event histories for an idealized population of interest. Microsimulation models are complex and invariably include parameters that are not well informed by existing data. Therefore, a key component of model development is the choice of parameter values. Microsimulation model parameter values are selected to reproduce expected or known results though the process of model calibration. Calibration may be done by perturbing model parameters one at a time or by using a search algorithm. As an alternative, we propose a Bayesian method to calibrate microsimulation models that uses Markov chain Monte Carlo. We show that this approach converges to the target distribution and use a simulation study to demonstrate its finite-sample performance. Although computationally intensive, this approach has several advantages over previously proposed methods, including the use of statistical criteria to select parameter values, simultaneous calibration of multiple parameters to multiple data sources, incorporation of information via prior distributions, description of parameter identifiability, and the ability to obtain interval estimates of model parameters. We develop a microsimulation model for colorectal cancer and use our proposed method to calibrate model parameters. The microsimulation model provides a good fit to the calibration data. We find evidence that some parameters are identified primarily through prior distributions. Our results underscore the need to incorporate multiple sources of variability (i.e., due to calibration data, unknown parameters, and estimated parameters and predicted values) when calibrating and applying microsimulation models. "], ["Option Pricing With Model-Guided Nonparametric Methods", " Parametric option pricing models are widely used in finance. These models capture several features of asset price dynamics; however, their pricing performance can be significantly enhanced when they are combined with nonparametric learning approaches that learn and correct empirically the pricing errors. In this article we propose a new nonparametric method for pricing derivatives assets. Our method relies on the state price distribution instead of the state price density, because the former is easier to estimate nonparametrically than the latter. A parametric model is used as an initial estimate of the state price distribution. Then the pricing errors induced by the parametric model are fitted nonparametrically. This model-guided method, called automatic correction of errors (ACE), estimates the state price distribution nonparametrically. The method is easy to implement and can be combined with any model-based pricing formula to correct the systematic biases of pricing errors. We also develop a nonparametric test based on the generalized likelihood ratio to document the efficacy of the ACE method. Empirical studies based on S&P 500 index options show that our method outperforms several competing pricing models in terms of predictive and hedging abilities. "], ["Semiparametric Efficient Estimation for Incomplete Longitudinal Binary Data, With Application to Smoking Trends", " Incomplete longitudinal data often are analyzed with estimating equations for inference on a parameter from a marginal mean regression model. Generalized estimating equations, although commonly used for incomplete longitudinal data, are invalid for data that are not missing completely at random. There exists a class of inverse probability weighted estimating equations that are valid under dropouts missing at random, including an easy-to-implement but inefficient member. A relatively computationally complex semiparametric efficient estimator in this class has been applied to continuous data. A specific form of this estimator is developed for binary data and used as a benchmark for assessing the efficiency of the simpler estimator in a simulation study. Both are applied in the estimation of 15-year cigarette smoking trends in the United States from a cohort of 5077 young adults. The results suggest that declines in smoking from previous reports have been exaggerated. "], ["Modeling and Inference for Measured Crystal Orientations and a Tractable Class of Symmetric Distributions for Rotations in Three Dimensions", " Electron backscatter diffraction (EBSD) is a technique used in materials science to study the microtexture of metals, producing data that measure the orientations of crystals in a specimen. We examine the precision of such data based on a useful class of distributions on orientations in three dimensions (as represented by 3\u00d73 orthogonal matrices with positive determinants). Although such modeling has received attention in the statistical literature, the approach taken typically has been based on general \u201cspecial manifold\u201d considerations, and the resulting methodology may not be easily accessible to nonspecialists. We take a more direct modeling approach, beginning from a simple, intuitively appealing mechanism for generating random orientations specifically in three-dimensional space. The resulting class of distributions has many desirable properties, including directly interpretable parameters and relatively simple theory. We investigate the basic properties of the entire class and one-sample quasi-likelihood\u2013based inference for one member of the model class, producing a new statistical methodology that is practically useful in the analysis of EBSD data. This article has supplementary material online. "], ["Amplification of Sensitivity Analysis in Matched Observational Studies", " A sensitivity analysis displays the increase in uncertainty that attends an inference when a key assumption is relaxed. In matched observational studies of treatment effects, a key assumption in some analyses is that subjects matched for observed covariates are comparable, and this assumption is relaxed by positing a relevant covariate that was not observed and not controlled by matching. What properties would such an unobserved covariate need to have to materially alter the inference about treatment effects? For ease of calculation and reporting, it is convenient that the sensitivity analysis be of low dimension, perhaps indexed by a scalar sensitivity parameter, but for interpretation in specific contexts, a higher dimensional analysis may be of greater relevance. An amplification of a sensitivity analysis is defined as a map from each point in a low-dimensional sensitivity analysis to a set of points, perhaps a \u201ccurve,\u201d in a higher dimensional sensitivity analysis such that the possible inferences are the same for all points in the set. Possessing an amplification, an investigator may calculate and report the low-dimensional analysis, yet have available the interpretations of the higher dimensional analysis. "], ["A Factor Model Approach to Multiple Testing Under Dependence", " The impact of dependence between individual test statistics is currently among the most discussed topics in the multiple testing of high-dimensional data literature, especially since Benjamini and Hochberg (1995) introduced the false discovery rate (FDR). Many papers have first focused on the impact of dependence on the control of the FDR. Some more recent works have investigated approaches that account for common information shared by all the variables to stabilize the distribution of the error rates. Similarly, we propose to model this sharing of information by a factor analysis structure for the conditional variance of the test statistics. It is shown that the variance of the number of false discoveries increases along with the fraction of common variance. Test statistics for general linear contrasts are deduced, taking advantage of the common factor structure to reduce the variance of the error rates. A conditional FDR estimate is proposed and the overall performance of multiple testing procedure is shown to be markedly improved, regarding the nondiscovery rate, with respect to classical procedures. The present methodology is also assessed by comparison with leading multiple testing methods. "], ["Local Polynomial Quantile Regression With Parametric Features", null], ["Poisson Autoregression", " In this article we consider geometric ergodicity and likelihood-based inference for linear and nonlinear Poisson autoregression. In the linear case, the conditional mean is linked linearly to its past values, as well as to the observed values of the Poisson process. This also applies to the conditional variance, making possible interpretation as an integer-valued generalized autoregressive conditional heteroscedasticity process. In a nonlinear conditional Poisson model, the conditional mean is a nonlinear function of its past values and past observations. As a particular example, we consider an exponential autoregressive Poisson model for time series. Under geometric ergodicity, the maximum likelihood estimators are shown to be asymptotically Gaussian in the linear model. In addition, we provide a consistent estimator of their asymptotic covariance matrix. Our approach to verifying geometric ergodicity proceeds via Markov theory and irreducibility. Finding transparent conditions for proving ergodicity turns out to be a delicate problem in the original model formulation. This problem is circumvented by allowing a perturbation of the model. We show that as the perturbations can be chosen to be arbitrarily small, the differences between the perturbed and nonperturbed versions vanish as far as the asymptotic distribution of the parameter estimates is concerned. This article has supplementary material online. "], ["Competing Risks Quantile Regression", " Quantile regression has emerged as a significant extension of traditional linear models and its potential in survival applications has recently been recognized. In this paper we study quantile regression with competing risks data, formulating the model based on conditional quantiles defined using the cumulative incidence function, which includes as a special case an analog to the usual accelerated failure time model. The proposed competing risks quantile regression model provides meaningful physical interpretations of covariate effects and, moreover, relaxes the constancy constraint on regression coefficients, thereby providing a useful, perhaps more flexible, alternative to the popular subdistribution proportional hazards model. We derive an unbiased monotone estimating equation for regression parameters in the quantile model. The uniform consistency and weak convergence of the resulting estimators are established across a quantile continuum. We develop inferences, including covariance estimation, second-stage exploration, and model diagnostics, which can be stably implemented using standard statistical software without involving smoothing or resampling. Our proposals are illustrated via simulation studies and an application to a breast cancer clinical trial. "], ["Learn From Thy Neighbor: Parallel-Chain and Regional Adaptive MCMC", " Starting with the seminal paper of Haario, Saksman, and Tamminen (Haario, Saksman, and Tamminen 2001), a substantial amount of work has been done to validate adaptive Markov chain Monte Carlo algorithms. In this paper we focus on two practical aspects of adaptive Metropolis samplers. First, we draw attention to the deficient performance of standard adaptation when the target distribution is multimodal. We propose a parallel chain adaptation strategy that incorporates multiple Markov chains which are run in parallel. Second, we note that the current adaptive MCMC paradigm implicitly assumes that the adaptation is uniformly efficient on all regions of the state space. However, in many practical instances, different \u201coptimal\u201d kernels are needed in different regions of the state space. We propose here a regional adaptation algorithm in which we account for possible errors made in defining the adaptation regions. This corresponds to the more realistic case in which one does not know exactly the optimal regions for adaptation. The methods focus on the random walk Metropolis sampling algorithm but their scope is much wider. We provide theoretical justification for the two adaptive approaches using the existent theory build for adaptive Markov chain Monte Carlo. We illustrate the performance of the methods using simulations and analyze a mixture model for real data using an algorithm that combines the two approaches. "], ["Simultaneous Testing of Grouped Hypotheses: Finding Needles in Multiple Haystacks", " In large-scale multiple testing problems, data are often collected from heterogeneous sources and hypotheses form into groups that exhibit different characteristics. Conventional approaches, including the pooled and separate analyses, fail to efficiently utilize the external grouping information. We develop a compound decision theoretic framework for testing grouped hypotheses and introduce an oracle procedure that minimizes the false nondiscovery rate subject to a constraint on the false discovery rate. It is shown that both the pooled and separate analyses can be uniformly improved by the oracle procedure. We then propose a data-driven procedure that is shown to be asymptotically optimal. Simulation studies show that our procedures enjoy superior performance and yield the most accurate results in comparison with both the pooled and separate procedures. A real-data example with grouped hypotheses is studied in detail using different methods. Both theoretical and numerical results demonstrate that exploiting external information of the sample can greatly improve the efficiency of a multiple testing procedure. The results also provide insights on how the grouping information is incorporated for optimal simultaneous inference. "], ["On Nonparametric Variance Estimation for Second-Order Statistics of Inhomogeneous Spatial Point Processes With a Known Parametric Intensity Form", null], ["Empirical Likelihood in Missing Data Problems", " Missing data is a ubiquitous problem in medical and social sciences. It is well known that inferences based only on the complete data may not only lose efficiency, but may also lead to biased results if the data is not missing completely at random (MCAR). The inverse-probability weighting method proposed by Horvitz and Thompson (1952) is a popular alternative when the data is not MCAR. The Horvitz\u2013Thompson method, however, is sensitive to the inverse weights and may suffer from loss of efficiency. In this paper, we propose a unified empirical likelihood approach to missing data problems and explore the use of empirical likelihood to effectively combine unbiased estimating equations when the number of estimating equations is greater than the number of unknown parameters. One important feature of this approach is the separation of the complete data unbiased estimating equations from the incomplete data unbiased estimating equations. The proposed method can achieve semiparametric efficiency if the probability of missingness is correctly specified. Simulation results show that the proposed method has better finite sample performance than its competitors. Supplemental materials for this paper, including proofs of the main theoretical results and the R code used for the NHANES example, are available online on the journal website. "], ["Sequential Implementation of Monte Carlo Tests With Uniformly Bounded Resampling Risk", null], ["Forward Regression for Ultra-High Dimensional Variable Screening", " Motivated by the seminal theory of Sure Independence Screening (Fan and Lv 2008, SIS), we investigate here another popular and classical variable screening method, namely, forward regression (FR). Our theoretical analysis reveals that FR can identify all relevant predictors consistently, even if the predictor dimension is substantially larger than the sample size. In particular, if the dimension of the true model is finite, FR can discover all relevant predictors within a finite number of steps. To practically select the \u201cbest\u201d candidate from the models generated by FR, the recently proposed BIC criterion of Chen and Chen (2008) can be used. The resulting model can then serve as an excellent starting point, from where many existing variable selection methods (e.g., SCAD and Adaptive LASSO) can be applied directly. FR\u2019s outstanding finite sample performances are confirmed by extensive numerical studies. "], ["On Multivariate Runs Tests for Randomness", null], ["Maximum Likelihood Estimation of the Multivariate Normal Mixture Model", " The Hessian of the multivariate normal mixture model is derived, and estimators of the information matrix are obtained, thus enabling consistent estimation of all parameters and their precisions. The usefulness of the new theory is illustrated with two examples and some simulation experiments. The newly proposed estimators appear to be superior to the existing ones. "], ["Generalized Multilevel Functional Regression", " We introduce Generalized Multilevel Functional Linear Models (GMFLMs), a novel statistical framework for regression models where exposure has a multilevel functional structure. We show that GMFLMs are, in fact, generalized multilevel mixed models. Thus, GMFLMs can be analyzed using the mixed effects inferential machinery and can be generalized within a well-researched statistical framework. We propose and compare two methods for inference: (1) a two-stage frequentist approach; and (2) a joint Bayesian analysis. Our methods are motivated by and applied to the Sleep Heart Health Study, the largest community cohort study of sleep. However, our methods are general and easy to apply to a wide spectrum of emerging biological and medical datasets. Supplemental materials for this article are available online. "], [null, null], ["Logistic Regression With Brownian-Like Predictors", " This article introduces a new type of logistic regression model involving functional predictors of binary responses, and provides an extension of this approach to generalized linear models. The predictors are trajectories that have certain sample path properties in common with Brownian motion. Time points are treated as parameters of interest, and confidence intervals are developed under prospective and retrospective (case-control) sampling designs. In an application to functional magnetic resonance imaging data, signals from individual subjects are used to find the portion of the time course that is most predictive of the response. This allows the identification of sensitive time points specific to a brain region and associated with a certain task, which can be used to distinguish between responses. A second application concerns gene expression data in a case-control study involving breast cancer, where the aim is to identify genetic loci along a chromosome that best discriminate between cases and controls. "], ["Multivariate Statistical Process Control Using LASSO", null], ["Median-Based Classifiers for High-Dimensional Data", null], ["The Analysis of Two-Way Functional Data Using Two-Way Regularized Singular Value Decompositions", " Two-way functional data consist of a data matrix whose row and column domains are both structured, for example, temporally or spatially, as when the data are time series collected at different locations in space. We extend one-way functional principal component analysis (PCA) to two-way functional data by introducing regularization of both left and right singular vectors in the singular value decomposition (SVD) of the data matrix. We focus on a penalization approach and solve the nontrivial problem of constructing proper two-way penalties from one-way regression penalties. We introduce conditional cross-validated smoothing parameter selection whereby left-singular vectors are cross-validated conditional on right-singular vectors, and vice versa. The concept can be realized as part of an alternating optimization algorithm. In addition to the penalization approach, we briefly consider two-way regularization with basis expansion. The proposed methods are illustrated with one simulated and two real data examples. Supplemental materials available online show that several \u201cnatural\u201d approaches to penalized SVDs are flawed and explain why so. "], ["Empirical Likelihood Methods Based on Characteristic Functions With Applications to L\u00e9vy Processes", " L\u00e9vy processes have been receiving increasing attention in financial modeling. One distinctive feature of such models is that their characteristic functions are readily available. Inference based on characteristic functions is very useful for studying L\u00e9vy processes. By incorporating the recent advances in nonparametric approaches, empirical likelihood methods based on characteristic functions are developed in this paper for parameter estimation, testing a particular parametric class including the presence of a jump component in the L\u00e9vy process and testing for symmetry of a distribution. Simulation and case studies confirm the effectiveness of the proposed method. "], ["Local Rank Inference for Varying Coefficient Models", null], ["Nonparametric Bayes Conditional Distribution Modeling With Variable Selection", " This article considers a methodology for flexibly characterizing the relationship between a response and multiple predictors. Goals are (1) to estimate the conditional response distribution addressing the distributional changes across the predictor space, and (2) to identify important predictors for the response distribution change both within local regions and globally. We first introduce the probit stick-breaking process (PSBP) as a prior for an uncountable collection of predictor-dependent random distributions and propose a PSBP mixture (PSBPM) of normal regressions for modeling the conditional distributions. A global variable selection structure is incorporated to discard unimportant predictors, while allowing estimation of posterior inclusion probabilities. Local variable selection is conducted relying on the conditional distribution estimates at different predictor points. An efficient stochastic search sampling algorithm is proposed for posterior computation. The methods are illustrated through simulation and applied to an epidemiologic study. "], ["Testing for Efficacy in Primary and Secondary Endpoints by Partitioning Decision Paths", " Partition testing is an alternative to closed testing. It provides insight into confidence sets for stepwise tests, and can be more powerful than closed testing. It also can simplify problem formulation when decision making follows specific paths. For the primary\u2013secondary endpoints problem, we show that partition testing has advantages. When used to implement what we call the \u201cdecision path principle,\u201d partition testing not only drastically reduces the number of hypotheses to be tested, but also guides decision making along predefined paths. With our way of setting critical values, it has higher probabilities than gatekeeping methods of correctly inferring efficacious primary endpoints as being efficacious, while maintaining the same level of strong FWER control. These advantages are illustrated with a real data example and by simulation. "], [null, null], ["What Are the Limits of Posterior Distributions Arising From Nonidentified Models, and Why Should We Care?", " In health research and other fields, the observational data available to researchers often fall short of the data that ideally would be available, due to the inherent limitations of study design and data acquisition. Were they available, these ideal data might be readily analyzed via straightforward statistical models with such desirable properties as parameter identifiability. Conversely, realistic models for the available data that incorporate uncertainty about the link between ideal and available data may be nonidentified. While there is no conceptual difficulty in implementing Bayesian analysis with nonidentified models and proper prior distributions, it is important to know to what extent data can be informative about parameters of interest. Determining the large-sample limit of the posterior distribution is one way to characterize the informativeness of data. In some nonidentified models, it is relatively straightforward to determine the limit via a particular reparameterization of the model; however, in other nonidentified models there is no such obvious approach. Thus we have developed an algorithm for determining the limiting posterior distribution for at least some such more difficult models. The work is motivated by two specific nonidentified models that arise quite naturally, and the algorithm is applied to reveal how informative the data are for these models. This article has supplementary material online. "], ["Conditional Quantile Estimation for Generalized Autoregressive Conditional Heteroscedasticity Models", " Conditional quantile estimation is an essential ingredient in modern risk management. Although generalized autoregressive conditional heteroscedasticity (GARCH) processes have proven highly successful in modeling financial data, it is generally recognized that it would be useful to consider a broader class of processes capable of representing more flexibly both asymmetry and tail behavior of conditional returns distributions. In this article we study estimation of conditional quantiles for GARCH models using quantile regression. Quantile regression estimation of GARCH models is highly nonlinear; we propose a simple and effective two-step approach of quantile regression estimation for linear GARCH time series. In the first step, we use a quantile autoregression sieve approximation for the GARCH model by combining information over different quantiles. Then second-stage estimation for the GARCH model is carried out based on the first-stage minimum distance estimation of the scale process of the time series. Asymptotic properties of the sieve approximation, the minimum distance estimators, and the final quantile regression estimators using generated regressors are studied. These results are of independent interest and have applications in other quantile regression settings. Monte Carlo and empirical application results indicate that the proposed estimation methods outperform some existing conditional quantile estimation methods. "], ["Book Reviews", null], ["Correction", null], ["2009 Editorial Collaborators", null], ["Index to Volume 104 (2009)", null], ["Attributing Effects to a Cluster-Randomized Get-Out-the-Vote Campaign", " Early in the twentieth century, Fisher and Neyman demonstrated how to infer effects of agricultural interventions using only the very weakest of assumptions, by randomly varying which plots were to be manipulated. Although the methods permitted uncontrolled variation between experimental units, they required strict control over assignment of interventions; this hindered their application to field studies with human subjects, who ordinarily could not be compelled to comply with experimenters\u2019 instructions. In 1996, however, Angrist, Imbens, and Rubin showed that inferences from randomized studies could accommodate noncompliance without significant strengthening of assumptions. Political scientists A. Gerber and D. Green responded quickly, fielding a randomized study of voter turnout campaigns in the November 1998 general election. Noncontacts and refusals were frequent, but Gerber and Green analyzed their data in the style of Angrist et al., avoiding the need to model nonresponse. They did use models for other purposes: to address complexities of the randomization scheme; to permit heterogeneity among voters and campaigners; to account for deviations from experimental protocol; and to take advantage of highly informative covariates. Although the added assumptions seemed straightforward and unassailable, a later analysis by Imai found them to be at odds with Gerber and Green\u2019s data. Using a different model, he reaches the very opposite of Gerber and Green\u2019s central conclusion about getting out the vote. This article shows that neither of the models are necessary, addressing all of the complications of Gerber and Green\u2019s study using methods in the tradition of Fisher and Neyman. To do this, it merges recent developments in randomization-based inference for comparative studies with somewhat older developments in design-based analysis of sample surveys. The method involves regression, but large-sample analysis and simulations demonstrate its lack of dependence on regression assumptions. Its substantive results have consequences both for the design of campaigns to increase voter participation and for theories of political behavior more generally. "], ["Weighted Normal Spatial Scan Statistic for Heterogeneous Population Data", null], ["Modeling Spatiotemporal Forest Health Monitoring Data", null], ["Joint Modeling of Self-Rated Health and Changes in Physical Functioning", " Self-rated health is an important indicator of future morbidity and mortality. Research has indicated that self-rated health is related to both levels of and changes in physical functioning. But no previous study has jointly modeled longitudinal functional status and self-rated health trajectories. We propose a joint model for self-rated health and physical functioning that describes the relationship between perceptions of health and the rate of change of physical functioning or disability. Our joint model uses a nonhomogeneous Markov process for discrete physical functioning states and connects this to a logistic regression model for \u201chealthy\u201d versus \u201cunhealthy\u201d self-rated health through parameters of the physical functioning model. We use simulation studies to establish finite-sample properties of our estimators and show that this model is robust to misspecification of the functional form of the relationship between self-rated health and rate of change of physical functioning. We also show that our joint model performs better than an empirical model based on observed changes in functional status. We apply our joint model to data from the Cardiovascular Health Study (CHS), a large multicenter longitudinal study of older adults. Our analysis indicates that self-rated health is associated both with level of functioning, as indicated by difficulty with activities of daily living (ADL) and instrumental activities of daily living (IADL), and with the risk of increasing difficulty with ADLs and IADLs. "], ["Predicting Vehicle Crashworthiness: Validation of Computer Models for Functional and Hierarchical Data", " The CRASH computer model simulates the effect of a vehicle colliding against different barrier types. If it accurately represents real vehicle crashworthiness, the computer model can be of great value in various aspects of vehicle design, such as the setting of timing of air bag releases. The goal of this study is to address the problem of validating the computer model for such design goals, based on utilizing computer model runs and experimental data from real crashes. This task is complicated by the fact that (i) the output of this model consists of smooth functional data, and (ii) certain types of collision have very limited data. We address problem (i) by extending existing Gaussian process-based methodology developed for models that produce real-valued output, and resort to Bayesian hierarchical modeling to attack problem (ii). Additionally, we show how to formally test if the computer model reproduces reality. Supplemental materials for the article are available online. "], ["Singular Value Decomposition\u2013Based Alternative Splicing Detection", " Altered alternative splicing has been identified as an important factor in tumorigenesis. The Affymetrix exon tiling array is designed for detecting alternative splicing events in a transcriptome-wide fashion; however, there are currently few analysis tools that have been well studied for effective detection of alternative splicing events. We propose a new screening procedure based on singular value decomposition (SVD) of the residual matrix from a robust additive model fit to probe selection region (PSR) data. With this approach, we analyze the exon tiling array data from a brain cancer study conducted at the M. D. Anderson Cancer Center, and show that the proposed SVD-based approach is able to better accommodate outlying measures and capitalize on the multidimensional group-by-PSR gene expression profiles for more effective detection of group-specific alternative splicing events, as well as the PSRs most likely associated with the alternative splicing. Lab validation confirmed some of our findings, but the list of candidates detected with our proposed method may provide a better signpost to guide further investigations. "], ["Bayesian Model Averaging Continual Reassessment Method in Phase I Clinical Trials", " The continual reassessment method (CRM) is a popular dose-finding design for phase I clinical trials. This method requires that practitioners prespecify the toxicity probability at each dose. Such prespecification can be arbitrary, and different specifications of toxicity probabilities may lead to very different design properties. To overcome the arbitrariness and further enhance the robustness of the design, we propose using multiple parallel CRM models, each with a different set of prespecified toxicity probabilities. In the Bayesian paradigm, we assign a discrete probability mass to each CRM model as the prior model probability. The posterior probabilities of toxicity can be estimated by the Bayesian model averaging (BMA) approach. Dose escalation or deescalation is determined by comparing the target toxicity rate and the BMA estimates of the dose toxicity probabilities. We examine the properties of the BMA-CRM approach through extensive simulation studies, and also compare this new method and its variants with the original CRM. The results demonstrate that our BMA-CRM is competitive and robust, and eliminates the arbitrariness of the prespecification of toxicity probabilities. "], ["Active Learning Through Sequential Design, With Applications to Detection of Money Laundering", null], ["Thresholding Events of Extreme in Simultaneous Monitoring of Multiple Risks", " This article develops a threshold system for monitoring airline performance. This threshold system divides the sample space into regions with increasing levels of risk and allows instant assessments of risk level of any observed airline performance. Of particular concern is the performance with extreme risk. In this article, a multivariate extreme value theory approach is used to establish thresholds for signaling varying levels of extremeness in the context of simultaneous monitoring of multiple risk measures. The threshold system is justified in terms of multivariate extreme quantiles, and its sample estimator is shown to be consistent. This threshold system applies to general extreme risk management. Finally, a simulation and comparison study demonstrates the good performance of the proposed multivariate extreme quantile estimator. Supplemental materials providing technical details are available online. "], ["Nonparametric Prediction in Measurement Error Models", " Acknowledgments: Carroll's research was supported by grants from the National Cancer Institute (CA57030, CA104620). Delaigle's work was partially supported by a fellowship from the Maurice Belz foundation. Hall's work was partially supported by the Australian Reserach Council and by a grant from the National Science Foundation (DMS 0604698). "], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Empirical Bayes Estimates for Large-Scale Prediction Problems", null], ["The Multiset Sampler", null], ["Nonparametric Bayes Modeling of Multivariate Categorical Data", " Modeling of multivariate unordered categorical (nominal) data is a challenging problem, particularly in high dimensions and cases in which one wishes to avoid strong assumptions about the dependence structure. Commonly used approaches rely on the incorporation of latent Gaussian random variables or parametric latent class models. The goal of this article is to develop a nonparametric Bayes approach, which defines a prior with full support on the space of distributions for multiple unordered categorical variables. This support condition ensures that we are not restricting the dependence structure a priori. We show this can be accomplished through a Dirichlet process mixture of product multinomial distributions, which is also a convenient form for posterior computation. Methods for nonparametric testing of violations of independence are proposed, and the methods are applied to model positional dependence within transcription factor binding motifs. "], ["Testing for the Supremacy of a Multinomial Cell Probability", " Tests for the supremacy of a multinomial cell probability are developed. The tested null hypothesis states that a particular cell of interest is not more probable than all others. Rejection of this null leads to the conclusion that the cell of interest has a strictly greater probability than all other cells. The null hypothesis constrains the multinomial probability vector to a nonconvex region that is a union of closed convex cones. The likelihood ratio test for this problem is derived and shown to be equivalent to an intersection\u2013union test. The least favorable configuration of the multinomial probability vector in the null parameter space is derived, and the limiting null distribution of the test statistic that is stochastically greatest is shown to be a mixture of point mass at zero and a chi-square distribution with a single degree of freedom. Asymptotic and valid finite-sample testing procedures are proposed and examined via a simulation study and the analysis of two datasets. The proposed procedures are extended to test whether the cell with the largest observed frequency is uniquely most probable. An equivalence between a likelihood ratio test for this problem and a union\u2013intersection test is demonstrated. "], [null, null], ["Confidence Regions for the Multinomial Parameter With Small Sample Size", null], ["Constructing Confidence Regions of Optimal Expected Size", " This article presents a Monte Carlo method for approximating the minimax expected size (MES) confidence set for a parameter known to lie in a compact set. The algorithm is motivated by problems in the physical sciences in which parameters are unknown physical constants related to the distribution of observable phenomena through complex numerical models. The method repeatedly draws parameters at random from the parameter space and simulates data as if each of those values were the true value of the parameter. Each set of simulated data is compared to the observed data using a likelihood ratio test. Inverting the likelihood ratio test minimizes the probability of including false values in the confidence region, which in turn minimizes the expected size of the confidence region. We prove that as the size of the simulations grows, this Monte Carlo confidence set estimator converges to the \u0393-minimax procedure, where \u0393 is a polytope of priors. Fortran-90 implementations of the algorithm for both serial and parallel computers are available. We apply the method to an inference problem in cosmology. "], ["Split Samples and Design Sensitivity in Observational Studies", " An observational or nonrandomized study of treatment effects may be biased by failure to control for some relevant covariate that was not measured. The design of an observational study is known to strongly affect its sensitivity to biases from covariates that were not observed. For instance, the choice of an outcome to study, or the decision to combine several outcomes in a test for coherence, can materially affect the sensitivity to unobserved biases. Decisions that shape the design are, therefore, critically important, but they are also difficult decisions to make in the absence of data. We consider the possibility of randomly splitting the data from an observational study into a smaller planning sample and a larger analysis sample, where the planning sample is used to guide decisions about design. After reviewing the concept of design sensitivity, we evaluate sample splitting in theory, by numerical computation, and by simulation, comparing it to several methods that use all of the data. Sample splitting is remarkably effective, much more so in observational studies than in randomized experiments: splitting 1,000 matched pairs into 100 planning pairs and 900 analysis pairs often materially improves the design sensitivity. An example from genetic toxicology is used to illustrate the method. "], ["Nonparametric Transition-Based Tests for Jump Diffusions", " We develop a specification test for the transition density of a discretely sampled continuous-time jump-diffusion process, based on a comparison of a nonparametric estimate of the transition density or distribution function with their corresponding parametric counterparts assumed by the null hypothesis. As a special case, our method applies to pure diffusions. We provide a direct comparison of the two densities for an arbitrary specification of the null parametric model using three different discrepancy measures between the null and alternative transition density and distribution functions. We establish the asymptotic null distributions of proposed test statistics and compute their power functions. We investigate the finite-sample properties through simulations and compare them with those of other tests. This article has supplementary material online. "], ["Locally Weighted Censored Quantile Regression", " Censored quantile regression offers a valuable supplement to Cox proportional hazards model for survival analysis. Existing work in the literature often requires stringent assumptions, such as unconditional independence of the survival time and the censoring variable or global linearity at all quantile levels. Moreover, some of the work uses recursive algorithms, making it challenging to derive asymptotic normality. To overcome these drawbacks, we propose a new locally weighted censored quantile regression approach that adopts the redistribution-of-mass idea and employs a local reweighting scheme. Its validity only requires conditional independence of the survival time and the censoring variable given the covariates, and linearity at the particular quantile level of interest. Our method leads to a simple algorithm that can be conveniently implemented with R software. Applying recent theory of M-estimation with infinite dimensional parameters, we establish the consistency and asymptotic normality of the proposed estimator. The proposed method is studied via simulations and is illustrated with the analysis of an acute myocardial infarction dataset. "], ["Quantile Regression With Measurement Error", " Regression quantiles can be substantially biased when the covariates are measured with error. In this paper we propose a new method that produces consistent linear quantile estimation in the presence of covariate measurement error. The method corrects the measurement error induced bias by constructing joint estimating equations that simultaneously hold for all the quantile levels. An iterative EM-type estimation algorithm to obtain the solutions to such joint estimation equations is provided. The finite sample performance of the proposed method is investigated in a simulation study, and compared to the standard regression calibration approach. Finally, we apply our methodology to part of the National Collaborative Perinatal Project growth data, a longitudinal study with an unusual measurement error structure. "], ["Cox Models With Smooth Functional Effect of Covariates Measured With Error", " We propose, develop, and implement a fully Bayesian inferential approach for the Cox model when the log hazard function contains unknown smooth functions of the variables measured with error. Our approach is to model nonparametrically both the log-baseline hazard and the smooth components of the log-hazard functions using low-rank penalized splines. Careful implementation of the Bayesian inferential machinery is shown to produce remarkably better results than the naive approach. Our methodology was motivated by and applied to the study of progression time to chronic kidney disease as a function of baseline kidney function and applied to the Atherosclerosis Risk in Communities study, a large epidemiological cohort study. This article has supplementary material online. "], ["Reweighting Estimators for Cox Regression With Missing Covariates", " Missingness in covariates is a common problem in survival data. In this article we propose a reweighting method for estimating the regression parameters in the Cox model with missing covariates. We also consider the augmented reweighting method by subtracting the projection term onto the nuisance tangent space. The proposed method provides consistent and asymptotically normally distributed estimators when the missing-data mechanism depends on the outcome variables, as well as on the observed covariates with either monotone or arbitrary nonmonotone missingness patterns. Simulation results indicate that in most situations, the proposed reweighting estimators are more efficient than the inverse probability weighting estimators for the regression coefficients of the missing covariates and are as efficient as or more efficient than the inverse probability weighting estimators for the regression coefficients of the completely observed covariates. The simple reweighting estimators can be easily implemented in standard statistical packages. "], ["A Semiparametric Regression Cure Model for Interval-Censored Data", " Motivated by medical studies in which patients could be cured of disease but the disease event time may be subject to interval censoring, we present a semiparametric nonmixture cure model for the regression analysis of interval-censored time-to-event data. We develop semiparametric maximum likelihood estimation for the model using the expectation-maximization method for interval-censored data. The maximization step for the baseline function is nonparametric and numerically challenging. We develop an efficient and numerically stable algorithm via modern convex optimization techniques, yielding a self-consistency algorithm for the maximization step. We prove the strong consistency of the maximum likelihood estimators under the Hellinger distance, which is an appropriate metric for the asymptotic property of the estimators for interval-censored data. We assess the performance of the estimators in a simulation study with small to moderate sample sizes. To illustrate the method, we also analyze a real dataset from a medical study for the biochemical recurrence of prostate cancer among patients who have undergone radical prostatectomy. Supplemental materials for the computational algorithm are available online. "], ["Statistical Estimation in Generalized Multiparameter Likelihood Models", null], ["Analyzing Length-Biased Data With Semiparametric Transformation and Accelerated Failure Time Models", " Right-censored time-to-event data are often observed from a cohort of prevalent cases that are subject to length-biased sampling. Informative right censoring of data from the prevalent cohort within the population often makes it difficult to model risk factors on the unbiased failure times for the general population, because the observed failure times are length biased. In this paper, we consider two classes of flexible semiparametric models: the transformation models and the accelerated failure time models, to assess covariate effects on the population failure times by modeling the length-biased times. We develop unbiased estimating equation approaches to obtain the consistent estimators of the regression coefficients. Large sample properties for the estimators are derived. The methods are confirmed through simulations and illustrated by application to data from a study of a prevalent cohort of dementia patients. "], ["Intrinsic Regression Models for Positive-Definite Matrices With Applications to Diffusion Tensor Imaging", " The aim of this paper is to develop an intrinsic regression model for the analysis of positive-definite matrices as responses in a Riemannian manifold and their association with a set of covariates, such as age and gender, in a Euclidean space. The primary motivation and application of the proposed methodology is in medical imaging. Because the set of positive-definite matrices do not form a vector space, directly applying classical multivariate regression may be inadequate in establishing the relationship between positive-definite matrices and covariates of interest, such as age and gender, in real applications. Our intrinsic regression model, which is a semiparametric model, uses a link function to map from the Euclidean space of covariates to the Riemannian manifold of positive-definite matrices. We develop an estimation procedure to calculate parameter estimates and establish their limiting distributions. We develop score statistics to test linear hypotheses on unknown parameters and develop a test procedure based on a resampling method to simultaneously assess the statistical significance of linear hypotheses across a large region of interest. Simulation studies are used to demonstrate the methodology and examine the finite sample performance of the test procedure for controlling the family-wise error rate. We apply our methods to the detection of statistical significance of diagnostic effects on the integrity of white matter in a diffusion tensor study of human immunodeficiency virus. Supplemental materials for this article are available online. "], ["On Large Margin Hierarchical Classification With Multiple Paths", null], ["Jackknife Empirical Likelihood", null], ["Tail Index Regression", " In extreme value statistics, the tail index is an important measure to gauge the heavy-tailed behavior of a distribution. Under Pareto-type distributions, we employ the logarithmic function to link the tail index to the linear predictor induced by covariates, which constitutes the tail index regression model. We then propose an approximate log-likelihood function to obtain regression parameter estimators, and subsequently show the asymptotic normality of those estimators. Numerical studies are presented to illustrate theoretical findings. "], ["A Class of Semiparametric Mixture Cure Survival Models With Dependent Censoring", " Modern cancer treatments have substantially improved cure rates and have generated a great interest in and need for proper statistical tools to analyze survival data with nonnegligible cure fractions. Data with cure fractions often are complicated by dependent censoring, and the analysis of this type of data typically involves untestable parametric assumptions on the dependence of the censoring mechanism and the true survival times. Motivated by the analysis of prostate cancer survival trends, we propose a class of semiparametric transformation cure models that allows for dependent censoring without making parametric assumptions on the dependence relationship. The proposed class of models encompasses a number of common models for the latency survival function, including the proportional hazards model and the proportional odds model, and also allows for time-dependent covariates. An inverse censoring probability reweighting scheme is used to derive unbiased estimating equations. Small-sample properties with simulations are derived, and the method is demonstrated with a data application. "], ["An Incomplete-Data Quasi-Likelihood Approach to Haplotype-Based Genetic Association Studies on\u00a0Related Individuals", null], ["Assessing Robustness of Intrinsic Tests of Independence in Two-Way Contingency Tables", null], ["Book Reviews", null], ["Corrections", null], ["Note: Modeling Future Record Performances in Athletics", null], ["Joint Models for the Association of Longitudinal Binary and Continuous Processes With Application to a Smoking Cessation Trial", "Joint models for the association of a longitudinal binary and a longitudinal continuous process are proposed for situations in which their association is of direct interest. The models are parameterized such that the dependence between the two processes is characterized by unconstrained regression coefficients. Bayesian variable selection techniques are used to parsimoniously model these coefficients. A Markov chain Monte Carlo (MCMC) sampling algorithm is developed for sampling from the posterior distribution, using data augmentation steps to handle missing data. Several technical issues are addressed to implement the MCMC algorithm efficiently. The models are motivated by, and are used for, the analysis of a smoking cessation clinical trial in which an important question of interest was the effect of the (exercise) treatment on the relationship between smoking cessation and weight gain."], ["Bayesian Analysis of Cancer Rates From SEER Program Using Parametric and Semiparametric Joinpoint Regression Models", "Cancer is the second leading cause of death in the United States. Cancer incidence and mortality rates measure the progress against cancer; these rates are obtained from the Surveillance, Epidemiology, and End Results (SEER) Program of the National Cancer Institute (NCI). Lung cancer has the highest mortality rate among all cancers, whereas prostate cancer has the highest number of new cases among males. In this article, we analyze the incidence rates of these two cancers, as well as colon and rectal cancer. The NCI reports trends in cancer age-adjusted mortality and incidence rates in its annual report to the nation and analyzes them using the Joinpoint software. The location of the joinpoints signifies changes in cancer trends, whereas changes in the regression slope measure the degree of change. The Joinpoint software uses a numerical search to detect the joinpoints, fits regression within two consecutive joinpoints by least squares, and finally selects the number of joinpoints by either a series of permutation tests or the Bayesian information criterion. We propose Bayesian joinpoint models and provide statistical estimates of the joinpoints and the regression slopes. While the Joinpoint software and other work in this area assumes that the joinpoints occur on the discrete time grid, we allow a continuous prior for the joinpoints induced by the Dirichlet distribution on the spacings in between. This prior further allows the user to impose prespecified minimum gaps in between two consecutive joinpoints. We develop parametric as well as semiparametric Bayesian joinpoint models; the semiparametric framework relaxes parametric distributional assumptions by modeling the distribution of regression slopes and error variances using Dirichlet process mixtures. These Bayesian models provide statistical inference with finite sample validity. Through a simulation study, we demonstrate the performance of the proposed parametric and semiparametric joinpoint models and compare the results with the ones from the Joinpoint software. We analyze age-adjusted cancer incidence rates from the SEER Program using these Bayesian models with different numbers of joinpoints by employing the deviance information criterion and the cross-validated predictive criterion. In addition, we model the lung cancer incidence rates and the smoking rates jointly and explore the relation between the two longitudinal processes."], ["Nonparametric Priors for Ordinal Bayesian Social Science Models: Specification and Estimation", "A generalized linear mixed model, ordered probit, is used to estimate levels of stress in presidential political appointees as a means of understanding their surprisingly short tenures. A Bayesian approach is developed, where the random effects are modeled with a Dirichlet process mixture prior, allowing for useful incorporation of prior information, but retaining some vagueness in the form of the prior. Applications of Bayesian models in the social sciences are typically done with \u201cuninformative\u201d priors, although some use of informed versions exists. There has been disagreement over this, and our approach may be a step in the direction of satisfying both camps. We give a detailed description of the data, show how to implement the model, and describe some interesting conclusions. The model utilizing a nonparametric prior fits better and reveals more information in the data than standard approaches."], ["A Statistical Framework to Infer Functional Gene Relationships From Biologically Interrelated Microarray Experiments", "A major task in understanding biological processes is to elucidate the relationships between genes involved in the underlying biological pathways. Microarray data from an increasing number of biologically interrelated experiments now allows for more complete portrayals of functional gene relationships in the pathways. In current studies of gene relationships, the presence of expression dependencies attributable to the biologically interrelated experiments, however, has been widely ignored. When unaccounted for, these (experiment) dependencies can result in inaccurate inferences of functional gene relationships, and hence incorrect biological conclusions. This article contributes a framework consisting of a model and an estimation procedure to infer gene relationships when there are two-way dependencies in the gene expression matrix (the gene-wise and experiment-wise dependencies). The main aspect of the framework is the use of a Kronecker product covariance matrix to model the gene-experiment interactions. The resulting novel gene coexpression measure, named Knorm correlation, can be understood as a natural extension of the widely used Pearson coefficient when the experiment correlation matrix is known. Compared with the Pearson coefficient, the Knorm correlation has a smaller estimation variance. The Knorm is also asymptotically consistent with the Pearson coefficient. When the experiment correlation matrix is unknown, the Knorm correlation is computed based on the estimated experiment correlation matrix by an iterative estimation procedure. We demonstrate the advantages of the Knorm correlation in both simulation studies and real datasets. The Knorm correlation estimation procedure is implemented in an R package (Knorm) that is freely available from the Bioconductor website."], ["Assessing Sexual Attitudes and Behaviors of Young Women: A Joint Model with Nonlinear Time Effects, Time Varying Covariates, and Dropouts", "Understanding human sexual behaviors is essential for the effective prevention of sexually transmitted infections (STI). Analysis of longitudinally measured sexual behavioral data, however, is often complicated by zero-inflation of event counts, nonlinear time trend, time-varying covariates, and informative dropouts. Ignoring these complicating factors could undermine the validity of the study findings. In this article, we put forth a unified joint modeling structure that accommodates these features of the data. Specifically, we propose a pair of simultaneous models for the zero-inflated event counts: Each of these models contains an auto-regressive structure for the accommodation of the effect of recent event history, and a nonparametric component for the modeling of nonlinear time effect. Informative dropout and time varying covariates are modeled explicitly in the process. Model fitting and parameter estimation are carried out in a Bayesian paradigm by the use of a Markov chain Monte Carlo (MCMC) method. Analytical results showed that adolescent sexual behaviors tended to evolve nonlinearly over time, and they were strongly influenced by the day-to-day variations in mood and sexual interests. These findings suggest that adolescent sex is, to a large extent, driven by intrinsic factors rather than being compelled by circumstances, thus highlighting the need of education on self-protective measures against infection risks."], ["Inferring Optimal Peer Assignment From Experimental Data", "This article studies the problem of optimally dividing individuals into peer groups to maximize social gains from heterogeneous peer effects. The specific setting analyzed here concerns efficient ways of allocating roommates in college dormitories. Using confidential data on a sample of Dartmouth College freshmen who were randomly assigned to be roommates, the article derives efficient roommate pairing rules, based on demographic and academic background, which maximize different aggregate outcomes. Segregation by precollege academic standing and by race are seen to minimize mean enrolment into sororities and maximize mean enrolment into fraternities. Segregation has no effect on mean and median freshman year grade point average (GPA) but increases the higher and decreases the lower percentiles of the GPA distribution for both men and women. Efficiency loss due to legal constraints on allocations (e.g., race-blindness) is shown to be significant and also larger for women for whom peer effects are more nonlinear. The article develops large-sample inference methods for these optimal solutions and the resulting optimized values by using and extending insights from the mathematical programming literature. Applicability of these techniques extends beyond linear maximands such as the mean to other important policy objectives such as outcome quantiles, which, though nonlinear, are shown to be quasi-convex in the allocation probabilities."], ["Sensitivity Analysis for Equivalence and Difference in an Observational Study of Neonatal Intensive Care Units", "In randomized experiments, it is sometimes important to demonstrate that two treatments do not differ greatly in their effects, or to demonstrate that the treatments have very different effects on one outcome but similar effects on another outcome. These \u201cdemonstrations\u201d may take the form of rejecting a null hypothesis of inequivalence in an equivalence test, or rejecting a null hypothesis of equal and inequivalent effects on two outcomes. The procedures often express a complex hypothesis in terms of component hypotheses, and combine the component significance levels to test the complex hypothesis. If used in a randomized trial, randomization provides valid significance levels for each component test, and hence also for the combined procedure. In an observational study\u2014that is, in a study of treatments that were not randomly assigned\u2014there is typically concern that significance levels for testing hypotheses about treatment effects may be distorted by failure to control for some unobserved pretreatment covariate. This concern is raised in the evaluation of virtually all observational studies. The possible impact of such an unobserved covariate is clarified and displayed by a sensitivity analysis that, for various possible magnitudes of potential bias, yields a corresponding interval of possible significance levels. Some observational studies are sensitive to very small unobserved biases, whereas others are insensitive to large biases. Here, existing sensitivity analyses for component hypotheses are used to generate sensitivity analyses for complex hypotheses tested by combining component significance levels. We apply the procedure to our study of the timing of discharges of premature babies from neonatal intensive care units, focusing on the possible impact of delayed discharge on use of unplanned medical care after discharge."], ["Random Effects Models in a Meta-Analysis of the Accuracy of Two Diagnostic Tests Without a Gold Standard", "In studies of the accuracy of diagnostic tests, it is common that both the diagnostic test itself and the reference test are imperfect. This is the case for the microsatellite instability test, which is routinely used as a prescreening procedure to identify individuals with Lynch syndrome, the most common hereditary colorectal cancer syndrome. The microsatellite instability test is known to have imperfect sensitivity and specificity. Meanwhile, the reference test, mutation analysis, is also imperfect. We evaluate this test via a random effects meta-analysis of 17 studies. Study-specific random effects account for between-study heterogeneity in mutation prevalence, test sensitivities and specificities under a nonlinear mixed effects model and a Bayesian hierarchical model. Using model selection techniques, we explore a range of random effects models to identify a best-fitting model. We also evaluate sensitivity to the conditional independence assumption between the microsatellite instability test and the mutation analysis by allowing for correlation between them. Finally, we use simulations to illustrate the importance of including appropriate random effects and the impact of overfitting, underfitting, and misfitting on model performance. Our approach can be used to estimate the accuracy of two imperfect diagnostic tests from a meta-analysis of multiple studies or a multicenter study when the prevalence of disease, test sensitivities and/or specificities may be heterogeneous among studies or centers."], ["Repeated Measurements on Distinct Scales With Censoring\u2014A Bayesian Approach Applied to Microarray Analysis of Maize", null], ["Nonparametric Signal Extraction and Measurement Error in the Analysis of Electroencephalographic Activity During Sleep", "We introduce methods for signal and associated variability estimation based on hierarchical nonparametric smoothing with application to the Sleep Heart Health Study (SHHS). SHHS is the largest electroencephalographic (EEG) collection of sleep-related data, which contains, at each visit, two quasi-continuous EEG signals for each subject. The signal features extracted from EEG data are then used in second level analyses to investigate the relation between health, behavioral, or biometric outcomes and sleep. Using subject specific signals estimated with known variability in a second level regression becomes a nonstandard measurement error problem. We propose and implement methods that take into account cross-sectional and longitudinal measurement error. The research presented here forms the basis for EEG signal processing for the SHHS."], ["Nonparametric Residue Analysis of Dynamic PET Data With Application to Cerebral FDG Studies in Normals", "Kinetic analysis is used to extract metabolic information from dynamic positron emission tomography (PET) uptake data. The theory of indicator dilutions, developed in the seminal work of Meier and Zierler (1954), provides a probabilistic framework for representation of PET tracer uptake data in terms of a convolution between an arterial input function and a tissue residue. The residue is a scaled survival function associated with tracer residence in the tissue. Nonparametric inference for the residue, a deconvolution problem, provides a novel approach to kinetic analysis\u2014critically one that is not reliant on specific compartmental modeling assumptions. A practical computational technique based on regularized cubic B-spline approximation of the residence time distribution is proposed. Nonparametric residue analysis allows formal statistical evaluation of specific parametric models to be considered. This analysis needs to properly account for the increased flexibility of the nonparametric estimator. The methodology is illustrated using data from a series of cerebral studies with PET and fluorodeoxyglucose (FDG) in normal subjects. Comparisons are made between key functionals of the residue, tracer flux, flow, etc., resulting from a parametric (the standard two-compartment of Phelps et al. 1979) and a nonparametric analysis. Strong statistical evidence against the compartment model is found. Primarily these differences relate to the representation of the early temporal structure of the tracer residence\u2014largely a function of the vascular supply network. There are convincing physiological arguments against the representations implied by the compartmental approach but this is the first time that a rigorous statistical confirmation using PET data has been reported. The compartmental analysis produces suspect values for flow but, notably, the impact on the metabolic flux, though statistically significant, is limited to deviations on the order of 3%\u20134%. The general advantage of the nonparametric residue analysis is the ability to provide a valid kinetic quantitation in the context of studies where there may be heterogeneity or other uncertainty about the accuracy of a compartmental model approximation of the tissue residue."], ["Modeling Hazard Rates as Functional Data for the Analysis of Cohort Lifetables and Mortality Forecasting", "As world populations age, the analysis of demographic mortality data and demographic predictions of future mortality have met with increasing interest. The study of mortality patterns and the forecasting of future mortality with its associated impacts on social welfare, health care, and societal planning has become a more pressing issue. An ideal set of data to study patterns of change in long-term mortality is the well-known historical Swedish cohort mortality data, because of its high quality and long span of more than two centuries. We explore the use of functional data analysis to model these data and to derive mortality forecasts. Specifically, we address the challenge of flexibly modeling these data while including the effect of the birth year by regarding log-hazard functions, derived from observed cohort lifetables, as random functions. A functional model for the analysis of these cohort log-hazard functions, extending functional principal component approaches by introducing time-varying eigenfunctions, is found to adequately address these challenges. The associated analysis of the dependency structure of the cohort log-hazard functions leads to the concept of time-varying principal components of mortality. We then extend this analysis to mortality forecasting, by combining prediction of incompletely observed log-hazard functions with functional local extrapolation, and demonstrate these functional approaches for the Swedish cohort mortality data."], ["Density Estimation for Protein Conformation Angles Using a Bivariate von Mises Distribution and Bayesian Nonparametrics", "Interest in predicting protein backbone conformational angles has prompted the development of modeling and inference procedures for bivariate angular distributions. We present a Bayesian approach to density estimation for bivariate angular data that uses a Dirichlet process mixture model and a bivariate von Mises distribution. We derive the necessary full conditional distributions to fit the model, as well as the details for sampling from the posterior predictive distribution. We show how our density estimation method makes it possible to improve current approaches for protein structure prediction by comparing the performance of the so-called \u201cwhole\u201d and \u201chalf\u201d position distributions. Current methods in the field are based on whole position distributions, as density estimation for the half positions requires techniques, such as ours, that can provide good estimates for small datasets. With our method we are able to demonstrate that half position data provides a better approximation for the distribution of conformational angles at a given sequence position, therefore providing increased efficiency and accuracy in structure prediction."], ["Log-Linear Models for Gene Association", "We describe a class of log-linear models for the detection of interactions in high-dimensional genomic data. This class of models leads to a Bayesian model selection algorithm that can be applied to data that have been reduced to contingency tables using ranks of observations within subjects, and discretization of these ranks within gene/network components. Many normalization issues associated with the analysis of genomic data are thereby avoided. A prior density based on Ewens\u2019 sampling distribution is used to restrict the number of interacting components assigned high posterior probability, and the calculation of posterior model probabilities is expedited by approximations based on the likelihood ratio statistic. Simulation studies are used to evaluate the efficiency of the resulting algorithm for known interaction structures. Finally, the algorithm is validated in a microarray study for which it was possible to obtain biological confirmation of detected interactions."], ["Mapping Ancient Forests: Bayesian Inference for Spatio-Temporal Trends in Forest Composition Using the Fossil Pollen Proxy Record", "Ecologists use the relative abundance of fossil pollen in sediments to estimate how tree species abundances change over space and time. To predict historical forest composition and quantify the available information, we build a Bayesian hierarchical model of forest composition in central New England, USA, based on pollen in a network of ponds. The critical relationships between abundances of taxa in the pollen record and abundances as actual vegetation are estimated for the modern and colonial periods, for which both pollen and direct vegetation data are available, based on a latent multivariate spatial process representing forest composition. For time periods in the past with only pollen data, we use the estimated model parameters to constrain predictions about the latent spatio-temporal process conditional on the pollen data. We develop an innovative graphical assessment of feature significance to help to infer which spatial patterns are reliably estimated. The model allows us to estimate the spatial distribution and relative abundances of tree species over the last 2,500 years, with an assessment of uncertainty, and to draw inference about how these patterns have changed over time. Cross-validation suggests that our feature significance approach can reliably indicate certain large-scale spatial features for many taxa, but that features on scales smaller than 50 km are difficult to distinguish, as are large-scale features for some taxa. We also use the model to quantitatively investigate ecological hypotheses, including covariate effects on taxa abundances and questions about pollen dispersal characteristics. The critical advantages of our modeling approach over current ecological analyses are the explicit spatio-temporal representation, quantification of abundance on the scale of trees rather than pollen, and uncertainty characterization."], ["Regression Models for Identifying Noise Sources in Magnetic Resonance Images", "Stochastic noise, susceptibility artifacts, magnetic field and radiofrequency inhomogeneities, and other noise components in magnetic resonance images (MRIs) can introduce serious bias into any measurements made with those images. We formally introduce three regression models including a Rician regression model and two associated normal models to characterize stochastic noise in various magnetic resonance imaging modalities, including diffusion-weighted imaging (DWI) and functional MRI (fMRI). Estimation algorithms are introduced to maximize the likelihood function of the three regression models. We also develop a diagnostic procedure for systematically exploring MR images to identify noise components other than simple stochastic noise, and to detect discrepancies between the fitted regression models and MRI data. The diagnostic procedure includes goodness-of-fit statistics, measures of influence, and tools for graphical display. The goodness-of-fit statistics can assess the key assumptions of the three regression models, whereas measures of influence can isolate outliers caused by certain noise components, including motion artifacts. The tools for graphical display permit graphical visualization of the values for the goodness-of-fit statistic and influence measures. Finally, we conduct simulation studies to evaluate performance of these methods, and we analyze a real dataset to illustrate how our diagnostic procedure localizes subtle image artifacts by detecting intravoxel variability that is not captured by the regression models."], ["Intrinsically Autoregressive Spatiotemporal Models With Application to Aggregated Birth Outcomes", null], ["Sequential Design for Microarray Experiments", "A critical aspect in the design of microarray studies is the determination of the sample size necessary to declare genes differentially expressed across different experimental conditions. In this article, we propose a sequential approach where the decision to stop the experiment depends on the accumulated microarray data. The study could stop whenever sufficient data have been accumulated to identify gene expression changes across several experimental conditions. The gene expression response is modeled by a robust linear regression model. We then construct a sequential confidence interval for the intercept of this model, which represents the median gene expression at a given experimental condition. We derive the stopping rule of the experiment for both continuous and discrete sequential approaches and give the asymptotic properties of the stopping variable. We demonstrate the desirable properties of our sequential approach, both theoretically and numerically. In our application to a study of hormone responsive breast cancer cell lines, we estimated the stopping variable for the sample size determination to be smaller than the actual sample size available to conduct the experiment. This means that we can obtain an accurate assessment of differential gene expression without compromising the cost and size of the study. Altogether, we anticipate that this approach could have an important contribution to microarray studies by improving the usual experimental designs and methods of analysis."], ["Doubly Robust Internal Benchmarking and False Discovery Rates for Detecting Racial Bias in Police Stops", "Allegations of racially biased policing are a contentious issue in many communities. Processes that flag potential problem officers have become a key component of risk management systems at major police departments. We present a statistical method to flag potential problem officers by blending three methodologies that are the focus of active research efforts: propensity score weighting, doubly robust estimation, and false discovery rates. Compared with other systems currently in use, the proposed method reduces the risk of flagging a substantial number of false positives by more rigorously adjusting for potential confounders and by using the false discovery rate as a measure to flag officers. We apply the methodology to data on 500,000 pedestrian stops in New York City in 2006. Of the nearly 3,000 New York City Police Department officers regularly involved in pedestrian stops, we flag 15 officers who stopped a substantially greater fraction of black and Hispanic suspects than our statistical benchmark predicts."], ["Intervention and Causality: Forecasting Traffic Flows Using a Dynamic Bayesian Network", "Real-time traffic flow data across entire networks can be used in a traffic management system to monitor current traffic flows so that traffic can be directed and managed efficiently. Reliable short-term forecasting models of traffic flows are crucial for the success of any traffic management system. The model proposed in this article for forecasting traffic flows is a multivariate Bayesian dynamic model called the multiregression dynamic model (MDM). This model is an example of a dynamic Bayesian network and is designed to preserve the conditional independences and causal drive exhibited by the traffic flow series. Sudden changes can occur in traffic flow series in response to such events as traffic accidents or roadworks. A traffic management system is particularly useful at such times of change. To ensure that the associated forecasting model continues to produce reliable forecasts, despite the change, the MDM uses the technique of external intervention. This article will demonstrate how intervention works in the MDM and how it can improve forecast performance at times of change. External intervention has also been used in the context of Bayesian networks to identify causal relationships between variables, and in dynamic Bayesian networks to identify lagged causal relationships between time series. This article goes beyond the identification of lagged causal relationships previously addressed using intervention in dynamic Bayesian networks, to show how intervention in the MDM can be used to identify contemporaneous causal relationships between time series."], ["On Consistency and Sparsity for Principal Components Analysis in High Dimensions", null], ["Discussion", null], ["Discussion", null], ["Comments", null], ["Rejoinder", null], ["Estimating Derivatives for Samples of Sparsely Observed Functions, With Application to Online Auction Dynamics", "It is often of interest to recover derivatives of a sample of random functions from sparse and noise-contaminated measurements, especially when the dynamics of underlying processes is of interest. We propose a novel approach based on estimating derivatives of eigenfunctions and expansions of random functions into their eigenfunctions to obtain a representation for derivatives. In combination with estimates for functional principal component scores for sparse data, this leads to a viable solution of the challenging problem to recover derivatives for sparsely observed functions. We establish consistency results and demonstrate in simulations that the method is superior to alternative approaches (derivative estimation with random effects models based on B-spline bases, kernel smoothing, smoothing splines, or P-splines). Our study is motivated by an analysis of bidding histories for eBay auctions, for which bids are typically very sparse in the middle and somewhat more frequent near the beginning and end of an auction. We demonstrate the estimation of derivatives of price curves for individual auctions from the sparsely observed bidding histories and also derive a model-free first-order differential equation that applies in the case of Gaussian processes. This provides a data-driven dynamic model that we use to elucidate auction dynamics."], ["On the Concept of Depth for Functional Data", "The statistical analysis of functional data is a growing need in many research areas. In particular, a robust methodology is important to study curves, which are the output of many experiments in applied statistics. As a starting point for this robust analysis, we propose, analyze, and apply a new definition of depth for functional observations based on the graphic representation of the curves. Given a collection of functions, it establishes the \u201ccentrality\u201d of an observation and provides a natural center-outward ordering of the sample curves. Robust statistics, such as the median function or a trimmed mean function, can be defined from this depth definition. Its finite-dimensional version provides a new depth for multivariate data that is computationally feasible and useful for studying high-dimensional observations. Thus, this new depth is also suitable for complex observations such as microarray data, images, and those arising in some recent marketing and financial studies. Natural properties of these new concepts are established and the uniform consistency of the sample depth is proved. Simulation results show that the corresponding depth based trimmed mean presents better performance than other possible location estimators proposed in the literature for some contaminated models. Data depth can be also used to screen for outliers. The ability of the new notions of depth to detect \u201cshape\u201d outliers is presented. Several real datasets are considered to illustrate this new concept of depth, including applications to microarray observations, weather data, and growth curves. Finally, through this depth, we generalize to functions the Wilcoxon rank sum test. It allows testing whether two groups of curves come from the same population. This functional rank test when applied to children growth curves shows different growth patterns for boys and girls."], ["Partial Correlation Estimation by Joint Sparse Regression Models", null], ["Shrinkage Estimation of the Varying Coefficient Model", "The varying coefficient model is a useful extension of the linear regression model. Nevertheless, how to conduct variable selection for the varying coefficient model in a computationally efficient manner is poorly understood. To solve the problem, we propose here a novel method, which combines the ideas of the local polynomial smoothing and the Least Absolute Shrinkage and Selection Operator (LASSO). The new method can do nonparametric estimation and variable selection simultaneously. With a local constant estimator and the adaptive LASSO penalty, the new method can identify the true model consistently, and that the resulting estimator can be as efficient as the oracle estimator. Numerical studies clearly confirm our theories. Extension to other shrinkage methods (e.g., the SCAD, i.e., the Smoothly Clipped Absolute Deviation.) and other smoothing methods is straightforward."], ["Bayesian Mixture Labeling by Highest Posterior Density", "A fundamental problem for Bayesian mixture model analysis is label switching, which occurs as a result of the nonidentifiability of the mixture components under symmetric priors. We propose two labeling methods to solve this problem. The first method, denoted by PM(ALG), is based on the posterior modes and an ascending algorithm generically denoted ALG. We use each Markov chain Monte Carlo sample as the starting point in an ascending algorithm, and label the sample based on the mode of the posterior to which it converges. Our natural assumption here is that the samples converged to the same mode should have the same labels. The PM(ALG) labeling method has some computational advantages over other popular labeling methods. Additionally, it automatically matches the \u201cideal\u201d labels in the highest posterior density credible regions. The second method does labeling by maximizing the normal likelihood of the labeled Gibbs samples. Using a Monte Carlo simulation study and a real dataset, we demonstrate the success of our new methods in dealing with the label switching problem."], ["Prior Distributions From Pseudo-Likelihoods in the Presence of Nuisance Parameters", null], ["Confidence Intervals for Population Ranks in the Presence of Ties and Near Ties", null], ["Computationally Efficient Nonparametric Importance Sampling", "The variance reduction established by importance sampling strongly depends on the choice of the importance sampling distribution. A good choice is often hard to achieve especially for high-dimensional integration problems. Nonparametric estimation of the optimal importance sampling distribution (known as \u201cnonparametric importance sampling\u201d) is a reasonable alternative to parametric approaches. In this article, nonparametric variants of both the self-normalized and the unnormalized importance sampling estimator are proposed and investigated. A common critique of nonparametric importance sampling is the increased computational burden compared with parametric methods. We solve this problem to a large degree by utilizing the linear blend frequency polygon estimator instead of a kernel estimator. Mean square error convergence properties are investigated, leading to recommendations for the efficient application of nonparametric importance sampling. Particularly, we show that nonparametric importance sampling asymptotically attains optimal importance sampling variance. The efficiency of nonparametric importance sampling algorithms relies heavily on the computational efficiency of the nonparametric estimator used. The linear blend frequency polygon outperforms kernel estimators in terms of certain criteria such as efficient sampling and evaluation. Furthermore, it is compatible with the inversion method for sample generation. This allows one to combine nonparametric importance sampling with other variance reduction techniques such as stratified sampling. Empirical evidence for the usefulness of the suggested algorithms is obtained by means of three benchmark integration problems. We show empirically that these methods may work in higher dimensions, at least up to dimension eight. As an application, we estimate the distribution of the queue length of a spam filter queuing system based on real data."], ["A Class of Transformed Mean Residual Life Models With Censored Survival Data", null], ["A Multivariate Extension of the Dynamic Logit Model for Longitudinal Data Based on a Latent Markov Heterogeneity Structure", "For the analysis of multivariate categorical longitudinal data, we propose an extension of the dynamic logit model. The resulting model is based on a marginal parameterization of the conditional distribution of each vector of response variables given the covariates, the lagged response variables, and a set of subject-specific parameters for the unobserved heterogeneity. The latter ones are assumed to follow a first-order Markov chain. For the maximum likelihood estimation of the model parameters, we outline an EM algorithm. The data analysis approach based on the proposed model is illustrated by a simulation study and an application to a dataset, which derives from the Panel Study on Income Dynamics and concerns fertility and female participation to the labor market."], ["Hunting for Significance With the False Discovery Rate", "When testing a single hypothesis, it is common knowledge that increasing the sample size after nonsignificant results and repeating the hypothesis test several times at unadjusted critical levels inflates the overall Type I error rate severely. In contrast, if a large number of hypotheses are tested controlling the False Discovery Rate, such \u201chunting for significance\u201d has asymptotically no impact on the error rate. More specifically, if the sample size is increased for all hypotheses simultaneously and only the test at the final interim analysis determines which hypotheses are rejected, a data dependent increase of sample size does not affect the False Discovery Rate. This holds asymptotically (for an increasing number of hypotheses) for all scenarios but the global null hypothesis where all hypotheses are true. To control the False Discovery Rate also under the global null hypothesis, we consider stopping rules where stopping before a predefined maximum sample size is reached is possible only if sufficiently many null hypotheses can be rejected. The procedure is illustrated with several datasets from microarray experiments."], ["Adversarial Risk Analysis", null], ["Book Reviews", null], ["Corrections", null], ["Communicating Statistics and Developing Professionals: The 2008 ASA Presidential Address", null], ["A Spatio-Temporal Model for Mean, Anomaly, and Trend Fields of North Atlantic Sea Surface Temperature", "We consider the problem of fitting a statistical model to 30 years of sea surface temperature records collected over a large portion of the Northern Atlantic. The observations were collected sparsely in space and time with different levels of accuracy. The purpose of the model is to produce an atlas of oceanic properties, including climatological mean fields, estimates of historical trends, and a spatio-temporal reconstruction of the anomalies, i.e., the transient deviations from the climatological mean. These products are of interest to climate change and climate variability research, numerical modeling, and remote sensing analyses. Our model improves upon the current tools used by oceanographers in that it constructs instantaneous temperature fields before averaging them into the climatology, thus giving equal weight to all years in the time frame, regardless of the temporal distribution of data. It also accounts for nonisotropic and nonstationary space and time dependencies, owing to its use of discrete process convolutions. Particular attention is given to the handling of massive datasets such as the one under study. This is achieved by considering compact support kernels that allow an efficient parallelization of the Markov chain Monte Carlo method used in the estimation of the model parameters. Resulting monthly climatologies are compared with those of the World Ocean Atlas 2001, version 2. Different water masses appear better separated in our climatology, and a close link emerges between the kernels' shape and the dominating patterns of ocean currents. The subpolar and the temperate North Atlantic display opposite trends, with the former mainly cooling over the years and the latter mainly warming, especially in the Gulf Stream region. Long-term changes in annual cycles are also detected. As in any hierarchical Bayesian model, parameter estimates come with credibility intervals, which are useful to compare results with other approaches and detect areas where sampling campaigns are needed the most."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Bayesian Semiparametric Joint Models for Functional Predictors", "Motivated by the need to understand and predict early pregnancy loss using hormonal indicators of pregnancy health, this article proposes a semiparametric Bayesian approach for assessing the relationship between functional predictors and a response. A multivariate adaptive spline model is used to describe the functional predictors, and a generalized linear model with a random intercept describes the response. Through specifying the random intercept to follow a Dirichlet process jointly with the random spline coefficients, we obtain a procedure that clusters trajectories according to shape and according to the parameters of the response model for each cluster. This very flexible method allows for the incorporation of covariates in the models for both the response and the trajectory. We apply the method to postovulatory progesterone data from the Early Pregnancy Study and find that the model successfully predicts early pregnancy loss."], ["A Case Study in Exploratory Functional Data Analysis: Geometrical Features of the Internal Carotid Artery", "This pilot study is a product of the AneuRisk Project, a scientific program that aims at evaluating the role of vascular geometry and hemodynamics in the pathogenesis of cerebral aneurysms. By means of functional data analyses, we explore the AneuRisk dataset to highlight the relations between the geometric features of the internal carotid artery, expressed by its radius profile and centerline curvature, and the aneurysm location. After introducing a new similarity index for functional data, we eliminate ancillary variability of vessel radius and curvature profiles through an iterative registration procedure. We then reduce data dimension by means of functional principal components analysis. Last, a quadratic discriminant analysis of functional principal components scores allows us to discriminate patients with aneurysms in different districts."], ["Estimating the Effect of a Time-Dependent Treatment by Levels of an Internal Time-Dependent Covariate: Application to the Contrast Between Liver Wait-List and Posttransplant Mortality", "Few health policy issues in the U.S. are scrutinized as intensely as the distribution of organs for transplantation, with much effort currently underway to examine the efficacy of existing organ allocation algorithms. The design of efficient organ allocation policies depends strongly upon accurate estimates of the survival benefit of transplantation. Many organ allocation policies order patients on the wait-list based on time-dependent health status measures (internal time-dependent covariates), such that priority is given to candidates at the greatest risk for wait-list mortality. It is of great interest to measure the transplant benefit by levels of such health status measures to identify the status levels that represent either futile or unnecessary transplants. In the presence of observational data, the survival benefit of transplantation has, to date, been quantified through the parameter corresponding to a binary time-dependent transplant indicator variable. Parameters from a standard time-dependent analysis using existing methods (i.e., separate transplant indicator for each status level) are difficult to interpret because they apply while the patient is at a particular level; i.e., they account for the patient's current condition, but do not account for the possibility that the patient's condition may worsen. We propose a novel method for estimating the effect of a time-dependent treatment by levels of an internal time-dependent covariate. The method yields parameter estimates which, rather than applying to the patient's current health status, average over future potential changes in health status. The proposed method is applied to end-stage liver disease data obtained from a national organ failure registry."], ["Robust Estimation of Mean Functions and Treatment Effects for Recurrent Events Under Event-Dependent Censoring and Termination: Application to Skeletal Complications in Cancer Metastatic to Bone", "In clinical trials featuring recurrent clinical events, the definition and estimation of treatment effects involves a number of interesting issues, especially when loss to follow-up may be event-related and when terminal events such as death preclude the occurrence of further events. This paper discusses a clinical trial of breast cancer patients with bone metastases where the recurrent events are skeletal complications, and where patients may die during the trial. We argue that treatment effects should be based on marginal rate and mean functions. When recurrent event data are subject to event-dependent censoring, however, ordinary marginal methods may yield inconsistent estimates. Incorporating correctly specified inverse probability of censoring weights into analyses can protect against dependent censoring and yield consistent estimates of marginal features. An alternative approach is to obtain estimates of rate and mean functions from models that involve some conditioning to render censoring conditionally independent. We consider three methods of estimating mean functions of recurrent event processes and examine the bias and efficiency of unweighted and inverse probability weighted versions of the methods with and without a terminating event. We compare the methods via simulation and use them to analyse the data from the breast cancer trial."], ["Bayesian Emulation and Calibration of a Stochastic Computer Model of Mitochondrial DNA Deletions in Substantia Nigra Neurons", "This article considers the problem of parameter estimation for a stochastic biological model of mitochondrial DNA population dynamics using experimental data on deletion mutation accumulation. The stochastic model is an attempt to describe the hypothesized link between deletion accumulation and neuronal loss in the substantia nigra region of the human brain. Inference for the parameters of the model is complicated by the fact that the model is both analytically intractable and slow to sample from. We show how the stochastic model can be approximated using a simple parametric statistical model with smoothly varying parameters. These parameters are treated as unknown functions and modeled using Gaussian process priors. Several simplifications of our Bayesian model are implemented to ease the computational burden. Throughout the article, we validate our models using predictive simulations. We demonstrate the validity of our fitted model on an independent dataset of substantia nigra neuron survival."], ["Estimating the Time-Varying Rate of Transmission of SARS in Singapore and Hong Kong Under Two Environments", "In modeling disease transmission there is much emphasis on how many people are infected but less attention is paid to when the infections occur, and the unrealistic assumption of constant infectiousness is often made. We propose a missing data formulation that enables us to apply the ECME algorithm to estimate a discretized intensity function of an inhomogeneous Poisson process. This approach requires interval-censored data only, but known infection times can be incorporated as well. We apply the proposed method to transmission data on severe respiratory syndrome (SARS) collected in Singapore and Hong Kong. The resulting estimates show that the rate of infection as a function of time may have more than one peak. By fitting a two-environment proportional intensity model to the Singapore data, we estimate that the rate of infection in an (unisolated) hospital environment is almost ten times greater than occurs in a nonhospital environment. This lends support to the theory that the SARS epidemic in Singapore was mainly driven by hospital-acquired infections. Estimates of individual infectivity reveal that three persons commonly regarded as \u201csuperspreaders\u201d actually do not have unusually high individual infectiousness. The observed superspreading events seem to have been caused by environmental rather than biological factors."], ["Bayesian Modeling of Uncertainty in Ensembles of Climate Models", "Projections of future climate change caused by increasing greenhouse gases depend critically on numerical climate models coupling the ocean and atmosphere (global climate models [GCMs]). However, different models differ substantially in their projections, which raises the question of how the different models can best be combined into a probability distribution of future climate change. For this analysis, we have collected both current and future projected mean temperatures produced by nine climate models for 22 regions of the earth. We also have estimates of current mean temperatures from actual observations, together with standard errors, that can be used to calibrate the climate models. We propose a Bayesian analysis that allows us to combine the different climate models into a posterior distribution of future temperature increase, for each of the 22 regions, while allowing for the different climate models to have different variances. Two versions of the analysis are proposed: a univariate analysis in which each region is analyzed separately, and a multivariate analysis in which the 22 regions are combined into an overall statistical model. A cross-validation approach is proposed to confirm the reasonableness of our Bayesian predictive distributions. The results of this analysis allow for a quantification of the uncertainty of climate model projections as a Bayesian posterior distribution, substantially extending previous approaches to uncertainty in climate models."], ["On the Determination of General Scientific Models With Application to Asset Pricing", "We consider a consumption-based asset pricing model that uses habit persistence to overcome the known statistical inadequacies of the classical consumption-based asset pricing model. We find that the habit model fits reasonably well and agrees with results reported in the literature if conditional heteroskedasticity is suppressed but that it does not fit nor do results agree if conditional heteroskedasticity, well known to be present in financial market data, is allowed to manifest itself. We also find that it is the preference parameters of the model that are most affected by the presence or absence of conditional heteroskedasticity, especially the risk aversion parameter. The habit model exhibits four characteristics that are often present in models developed from scientific considerations: (1) a likelihood is not available; (2) prior information is available; (3) a portion of the prior information is expressed in terms of functionals of the model that cannot be converted into an analytic prior on model parameters; (4) the model can be simulated. The underpinning of our approach is that, in addition, (5) a parametric statistical model for the data, determined without reference to the scientific model, is known. In general one can expect to be able to determine a model that satisfies (5) because very richly parameterized statistical models are easily accommodated. We develop a computationally intensive, generally applicable, Bayesian strategy for estimation and inference for scientific models that meet this description together with methods for assessing model adequacy. An important adjunct to the method is that a map from the parameters of the scientific model to functionals of the scientific and statistical models becomes available. This map is a powerful tool for understanding the properties of the scientific model."], ["Bayesian Analysis of Isochores", "Statistical identification of isochore structure, the variation in large-scale GC composition (proportion of DNA bases that are G or C as opposed to A or T), of mammalian genomes is a necessary requirement for understanding both the evolution of base composition and the many genomic features such as mutation and recombination rates, which covary with base composition. We have developed a Bayesian method for isochore analysis that we demonstrate to be more accurate than the commonly used binary segmentation approach implemented within the program IsoFinder. The method accounts for both fine-scale and large-scale structure. We adapt direct simulation methods to allow for iid samples from the posterior distribution of our model, and provide an accurate approximation to this that can analyze data from a chromosome in a matter of seconds. We apply our method to human chromosome 1. The resulting estimate of how GC content varies across this region is shown to be a better predictor of local recombination rates than IsoFinder, and we are able to detect regions consistent with the classic definition of isochores that cover 85% of the chromosome. We also show a measure of relative GC content to be particularly predictive of local recombination rates."], ["A Bayesian Hierarchical Model for Analysis of Single-Nucleotide Polymorphisms Diversity in Multilocus, Multipopulation Samples", null], ["Estimating Response-Maximized Decision Rules With Applications to Breastfeeding", " Please see the online supplements for a correction to this article. "], ["Likelihood-Based Analysis of Causal Effects of Job-Training Programs Using Principal Stratification", "Government-sponsored job-training programs must be subject to evaluation to assess whether their effectiveness justifies their cost to the public. The evaluation usually focuses on employment and total earnings, although the effect on wages is also of interest, because this effect reflects the increase in human capital due to the training program, whereas the effect on total earnings may be simply reflecting the increased likelihood of employment without any effect on wage rates. Estimating the effects of training programs on wages is complicated by the fact that, even in a randomized experiment, wages are \u201ctruncated\u201d (or less accurately \u201ccensored\u201d) by nonemployment, that is, they are only observed and well-defined for individuals who are employed. In this article, we develop a likelihood-based approach to estimate the wage effect of the US federally-funded Job Corps training program using \u201cPrincipal Stratification\u201d. Our estimands are formulated in terms of: (1) the effect of the training program on wages for those who would be employed whether they were trained or not, also called the survivor average causal effect (SACE), and the proportion of people in this category; (2) the wages when trained for those who would be employed only when trained, and the proportion of people in this category; (3) the wages when not trained for those who would be employed only when not trained, and the proportion of people in this category; (4) the proportion of people who would be not employed whether trained or not. We conduct likelihood-based analysis using the EM algorithm, and investigate the plausibility of important submodels with scaled log-likelihood ratio statistics. We also conduct a sensitivity analysis with respect to specific parametric assumptions. Our results suggest that all four types of people [(1)\u2013(4) previously] exist, which is impossible under the usual monotonicity assumptions made in traditional econometric evaluation methods."], ["Generalized Thresholding of Large Covariance Matrices", null], ["Order Selection in Finite Mixture Models With a Nonsmooth Penalty", "Order selection is a fundamental and challenging problem in the application of finite mixture models. In this article, we develop a new penalized likelihood approach. The new method, modified smoothly clipped absolute deviation (MSCAD), deviates from information-based methods such as Akaike information criterion (AIC) and Bayesian information criterion (BIC) by introducing two penalty functions that depend on the mixing proportions and the component parameters. It is consistent at estimating both the order of the mixture model and the mixing distribution. Simulations show that MSCAD has much better performance than a number of existing methods. Two real-data examples are examined to illustrate the performance of MSCAD."], ["Likelihood-Based Sufficient Dimension Reduction", "We obtain the maximum likelihood estimator of the central subspace under conditional normality of the predictors given the response. Analytically and in simulations we found that our new estimator can preform much better than sliced inverse regression, sliced average variance estimation and directional regression, and that it seems quite robust to deviations from normality."], ["Local Multidimensional Scaling for Nonlinear Dimension Reduction, Graph Drawing, and Proximity Analysis", null], ["Shrinkage Estimators for Robust and Efficient Inference in Haplotype-Based Case-Control Studies", "Case-control association studies often aim to investigate the role of genes and gene-environment interactions in terms of the underlying haplotypes (i.e., the combinations of alleles at multiple genetic loci along chromosomal regions). The goal of this article is to develop robust but efficient approaches to the estimation of disease odds-ratio parameters associated with haplotypes and haplotype-environment interactions. We consider \u201cshrinkage\u201d estimation techniques that can adaptively relax the model assumptions of Hardy-Weinberg-Equilibrium and gene-environment independence required by recently proposed efficient \u201cretrospective\u201d methods. Our proposal involves first development of a novel retrospective approach to the analysis of case-control data, one that is robust to the nature of the gene-environment distribution in the underlying population. Next, it involves shrinkage of the robust retrospective estimator toward a more precise, but model-dependent, retrospective estimator using novel empirical Bayes and penalized regression techniques. Methods for variance estimation are proposed based on asymptotic theories. Simulations and two data examples illustrate both the robustness and efficiency of the proposed methods."], ["Variable Selection for Partially Linear Models With Measurement Errors", "This article focuses on variable selection for partially linear models when the covariates are measured with additive errors. We propose two classes of variable selection procedures, penalized least squares and penalized quantile regression, using the nonconvex penalized principle. The first procedure corrects the bias in the loss function caused by the measurement error by applying the so-called correction-for-attenuation approach, whereas the second procedure corrects the bias by using orthogonal regression. The sampling properties for the two procedures are investigated. The rate of convergence and the asymptotic normality of the resulting estimates are established. We further demonstrate that, with proper choices of the penalty functions and the regularization parameter, the resulting estimates perform asymptotically as well as an oracle procedure as proposed by Fan and Li. Choice of smoothing parameters is also discussed. Finite sample performance of the proposed variable selection procedures is assessed by Monte Carlo simulation studies. We further illustrate the proposed procedures by an application."], ["Local Spectral Analysis via a Bayesian Mixture of Smoothing Splines", "In many practical problems, time series are realizations of nonstationary random processes. These processes can often be modeled as processes with slowly changing dynamics or as piecewise stationary processes. In these cases, various approaches to estimating the time-varying spectral density have been proposed. Our approach in this article is to estimate the log of the Dahlhaus local spectrum using a Bayesian mixture of splines. The basic idea of our approach is to first partition the data into small sections. We then assume that the log spectral density of the evolutionary process in any given partition is a mixture of individual log spectra. We use a mixture of smoothing splines model with time varying mixing weights to estimate the evolutionary log spectrum. The mixture model is fit using Markov chain Monte Carlo techniques that yield estimates of the log spectra of the individual subsections. In addition to an estimate of the local log spectral density, the method yields pointwise credible intervals. We use a reversible jump step to automatically determine the number of different spectral components."], ["A Bayesian Reassessment of Nearest-Neighbor Classification", null], ["A Bayesian Hierarchical Nonoverlapping Random Disc Growth Model", "A methodology is proposed to efficiently model a random set via a multistage hierarchical Bayesian model. We define a NonOverlapping Random Disk Model (NORDM), which is similar in spirit to the well-known Poisson\u2013Boolean model. This model is formulated in a conditional setting that facilitates Bayesian sampling of important parameters in the model. This framework can accommodate any object, not just those with disk shapes, although the model can be easily extended to include any known compact convex set instead of the disc (e.g., polygons or ellipses). We further propose a growth model that is conceptually simple and allows straightforward estimation of parameters, without the need for tedious calculations of hitting or inclusion probabilities. The model is applied to severe storm cell development as obtained from weather radar."], ["Time Series Modelling With Semiparametric Factor Dynamics", "High-dimensional regression problems, which reveal dynamic behavior, are typically analyzed by time propagation of a few number of factors. The inference on the whole system is then based on the low-dimensional time series analysis. Such high-dimensional problems occur frequently in many different fields of science. In this article we address the problem of inference when the factors and factor loadings are estimated by semiparametric methods. This more flexible modeling approach poses an important question: Is it justified, from an inferential point of view, to base statistical inference on the estimated times series factors? We show that the difference of the inference based on the estimated time series and \u201ctrue\u201d unobserved time series is asymptotically negligible. Our results justify fitting vector autoregressive processes to the estimated factors, which allows one to study the dynamics of the whole high-dimensional system with a low-dimensional representation. We illustrate the theory with a simulation study. Also, we apply the method to a study of the dynamic behavior of implied volatilities and to the analysis of functional magnetic resonance imaging (fMRI) data."], ["Consistent Classification of Nonstationary Time Series Using Stochastic Wavelet Representations", "Consider the situation when we have training data containing many time series having known group membership and testing data with unknown group membership. The goals are to find timescale features (using training data) that can best separate the groups, and to use these highly discriminant features to classify test data. We propose a method for classification using a bias-corrected nondecimated wavelet transform. Wavelets are ideal for identifying highly discriminant local time and scale features. The observed signals will be treated as realizations of locally stationary wavelet processes, under which we define and rigorously estimate the evolutionary wavelet spectrum (timescale decomposition of variance). The evolutionary wavelet spectrum, which contains the second-moment information on the signals, is used as the classification signature. For each test time series, we compute the empirical wavelet spectrum and its divergence from the wavelet spectrum of each group. The test time series is then assigned to the group to which it is the least dissimilar. Under the locally stationary wavelet framework, we rigorously demonstrate that the classification procedure is consistent (i.e., misclassification probability goes to zero at the rate that is inversely proportional to divergence between the evolutionary wavelet spectra). The method is illustrated using seismic signals (earthquake vs. explosion events) and is demonstrated to work very well in simulation studies."], ["Testing the Nullity of GARCH Coefficients: Correction of the Standard Tests and Relative Efficiency Comparisons", "This article is concerned with testing the nullity of coefficients in generalized autoregressive conditionally heteroscedastic (GARCH) models. The problem is nonstandard because the quasi-maximum likelihood estimator is subject to positivity constraints. This article establishes the asymptotic null and local alternative distributions of Wald, score, and quasi-likelihood ratio tests. Efficiency comparisons under fixed alternatives are considered. Two cases of special interest are tests of the null hypothesis of one coefficient equal to zero and tests of the null hypothesis of no conditional heteroscedasticity. Finally, the proposed approach is used in the analysis of financial data and suggests reconsidering the preeminence of GARCH(1,1) among GARCH models."], ["Testing Dependence Among Serially Correlated Multicategory Variables", "The contingency table literature on tests for dependence among discrete multicategory variables is extensive. Standard tests assume, however, that draws are independent and only limited results exist on the effect of serial dependency\u2014a problem that is important in areas such as economics, finance, medical trials, and meteorology. This article proposes new tests of independence based on canonical correlations from dynamically augmented reduced rank regressions. The tests allow for an arbitrary number of categories as well as multiway tables of arbitrary dimension and are robust in the presence of serial dependencies that take the form of finite-order Markov processes. For three-way or higher order tables we propose new tests of joint and marginal independence. Monte Carlo experiments show that the proposed tests have good finite sample properties. An empirical application to microeconomic survey data on firms' forecasts of changes to their production and prices demonstrates the importance of correcting for serial dependencies in predictability tests."], ["Estimation of Parameters Subject to Order Restrictions on a Circle With Application to Estimation of Phase Angles of Cell Cycle Genes", "Motivated by a problem encountered in the analysis of cell cycle gene expression data, this article deals with the estimation of parameters subject to order restrictions on a unit circle. A normal eukaryotic cell cycle has four major phases during cell division, and a cell cycle gene has its peak expression (phase angle) during the phase that may correspond to its biological function. Because the phases are ordered along a circle, the phase angles of cell cycle genes are ordered unknown parameters on a unit circle. The problem of interest is to estimate the phase angles using the information regarding the order among them. We address this problem by developing a circular version of the well-known isotonic regression for Euclidean data. Because of the underlying geometry, the standard pool adjacent violator algorithm (PAVA) cannot be used for deriving the circular isotonic regression estimator (CIRE). However, PAVA can be modified to obtain a computationally efficient algorithm for deriving the CIRE. We illustrate the CIRE by estimating the phase angles of some of well-known cell cycle genes using the unrestricted estimators obtained in the literature."], ["A Design-Adaptive Local Polynomial Estimator for the Errors-in-Variables Problem", "Local polynomial estimators are popular techniques for nonparametric regression estimation and have received great attention in the literature. Their simplest version, the local constant estimator, can be easily extended to the errors-in-variables context by exploiting its similarity with the deconvolution kernel density estimator. The generalization of the higher order versions of the estimator, however, is not straightforward and has remained an open problem for the last 15 years. We propose an innovative local polynomial estimator of any order in the errors-in-variables context, derive its design-adaptive asymptotic properties and study its finite sample performance on simulated examples. We provide not only a solution to a long-standing open problem, but also provide methodological contributions to error-in-variable regression, including local polynomial estimation of derivative functions."], ["Robust Response Transformations Based on Optimal Prediction", "Nonlinear regression problems can often be reduced to linearity by transforming the response variable (e.g., using the Box-Cox family of transformations). The classic estimates of the parameter defining the transformation as well as of the regression coefficients are based on the maximum likelihood criterion, assuming homoscedastic normal errors for the transformed response. These estimates are nonrobust in the presence of outliers and can be inconsistent when the errors are nonnormal or heteroscedastic. This article proposes new robust estimates that are consistent and asymptotically normal for any unimodal and homoscedastic error distribution. For this purpose, a robust version of conditional expectation is introduced for which the prediction mean squared error is replaced with an M scale. This concept is then used to develop a nonparametric criterion to estimate the transformation parameter as well as the regression coefficients. A finite sample estimate of this criterion based on a robust version of smearing is also proposed. Monte Carlo experiments show that the new estimates compare favorably with respect to the available competitors."], ["Nonparametric Quantile Estimations for Dynamic Smooth Coefficient Models", null], ["Rank-Based Estimation and Associated Inferences for Linear Models With Cluster Correlated Errors", "R estimators based on the joint ranks (JR) of all the residuals have been developed over the last 20 years for fitting linear models with independently distributed errors. In this article, we extend these estimators to estimating the fixed effects in a linear model with cluster correlated continuous error distributions for general score functions. We discuss the asymptotic theory of the estimators and standard errors of the estimators. For the related mixed model with a single random effect, we discuss robust estimators of the variance components. These are used to obtain Studentized residuals for the JR fit. A real example is discussed, which illustrates the efficiency of the JR analysis over the traditional analysis and the efficiency of a prudent choice of a score function. Simulation studies over situations similar to the example confirm the validity and efficiency of the analysis."], ["Screening Experiments for Developing Dynamic Treatment Regimes", "Dynamic treatment regimes are time-varying treatments that individualize sequences of treatments to the patient. The construction of dynamic treatment regimes is challenging because a patient will be eligible for some treatment components only if he has not responded (or has responded) to other treatment components. In addition, there are usually a number of potentially useful treatment components and combinations thereof. In this article, we propose new methodology for identifying promising components and screening out negligible ones. First, we define causal factorial effects for treatment components that may be applied sequentially to a patient. Second, we propose experimental designs that can be used to study the treatment components. Surprisingly, modifications can be made to (fractional) factorial designs\u2014more commonly found in the engineering statistics literature\u2014for screening in this setting. Furthermore, we provide an analysis model that can be used to screen the factorial effects. We demonstrate the proposed methodology using examples motivated in the literature and also via a simulation study."], ["Book Reviews", null], ["Correction", " For a new result obtained by VanderWeele, please see the supplementary material attached to this correction. "]]}