{"2010": [["Building a Stronger Instrument in an Observational Study of Perinatal Care for Premature Infants", " An instrument is a random nudge toward acceptance of a treatment that affects outcomes only to the extent that it affects acceptance of the treatment. Nonetheless, in settings in which treatment assignment is mostly deliberate and not random, there may exist some essentially random nudges to accept treatment, so that use of an instrument might extract bits of random treatment assignment from a setting that is otherwise quite biased in its treatment assignments. An instrument is weak if the random nudges barely influence treatment assignment or strong if the nudges are often decisive in influencing treatment assignment. Although ideally an ostensibly random instrument is perfectly random and not biased, it is not possible to be certain of this; thus a typical concern is that even the instrument might be biased to some degree. It is known from theoretical arguments that weak instruments are invariably sensitive to extremely small biases; for this reason, strong instruments are preferred. The strength of an instrument is often taken as a given. It is not. In an evaluation of effects of perinatal care on the mortality of premature infants, we show that it is possible to build a stronger instrument, we show how to do it, and we show that success in this task is critically important. We also develop methods of permutation inference for effect ratios, a key component in an instrumental variable analysis. "], ["Self-Selectivity in Firm\u2019s Decision to Withdraw IPO: Bayesian Inference for Hazard Models of Bankruptcy With Feedback", " Examination on firm performance subsequent to a chosen event is widely used in finance studies to analyze the motivation behind managerial decisions. However, results are often subject to bias when the self-selectivity behind managerial decisions is ignored and unspecified. This study investigates a unique corporate event of initial public offering (IPO) withdrawal, where a firm\u2019s subsequent likelihood of bankruptcy is specified in a system of switching hazard models, and the expected difference in post-IPO and postwithdrawal survival probabilities serves as a \u201cfeedback\u201d on a firm\u2019s decision to cancel its offering. Our Bayesian inference procedure generates strong evidence that incidence of withdrawal unfavorably affects the subsequent performance of a firm, and that the \u201cfeedback\u201d is an important determinant in managerial decisions. The econometric and statistical model specification and the accompanying estimation procedure we used can be widely applicable to study self-selective corporate transactions. "], ["Modeling Competing Infectious Pathogens From a Bayesian Perspective: Application to Influenza Studies With Incomplete Laboratory Results", " In seasonal influenza epidemics, pathogens such as respiratory syncytial virus (RSV) often cocirculate with influenza and cause influenza-like illness (ILI) in human hosts. However, it is often impractical to test for each potential pathogen or to collect specimens for each observed ILI episode, making inference about influenza transmission difficult. In the setting of infectious diseases, missing outcomes impose a particular challenge because of the dependence among individuals. We propose a Bayesian competing-risk model for multiple cocirculating pathogens for inference on transmissibility and intervention efficacies under the assumption that missingness in the biological confirmation of the pathogen is ignorable. Simulation studies indicate a reasonable performance of the proposed model even if the number of potential pathogens is misspecified. They also show that a moderate amount of missing laboratory test results has only a small impact on inference about key parameters in the setting of close contact groups. Using the proposed model, we found that a nonpharmaceutical intervention is marginally protective against transmission of influenza A in a study conducted in elementary schools. "], ["Using a Short Screening Scale for Small-Area Estimation of Mental Illness Prevalence for Schools", " We use data collected in the National Comorbidity Survey-Adolescent (NCS-A) to develop a methodology to estimate the small-area prevalence of serious emotional distress (SED) in schools in the United States, exploiting the clustering of the main NCS-A sample by school. The NCS-A instrument includes both a short screening scale, the K6, and extensive diagnostic assessments of the individual disorders and associated impairment that determine the diagnosis of SED. We fitted a Bayesian bivariate multilevel regression model with correlated effects for the probability of SED and a modified K6 score at the individual and school levels. Our results provide evidence for the existence of variation in the prevalence of SED across schools and geographical regions. Although the concordance between the modified K6 scale and SED is only modest for individuals, the school-level random effects for the two measures are strongly correlated. Under this model we obtain a prediction equation for the rate of SED based on the mean K6 score and covariates. This finding supports the feasibility of using short screening scales like the K6 as an alternative to more comprehensive lay assessments in estimating school-level rates of SED. These methods may be applicable to other studies aiming at small-area estimation for geographical units. "], ["A Bayesian Shrinkage Model for Incomplete Longitudinal Binary Data With Application to the Breast Cancer Prevention Trial", " We consider inference in randomized longitudinal studies with missing data that is generated by skipped clinic visits and loss to followup. In this setting, it is well known that full data estimands are not identified unless unverified assumptions are imposed. We assume a non-future dependence model for the drop-out mechanism and partial ignorability for the intermittent missingness. We posit an exponential tilt model that links nonidentifiable distributions and distributions identified under partial ignorability. This exponential tilt model is indexed by nonidentified parameters, which are assumed to have an informative prior distribution, elicited from subject-matter experts. Under this model, full data estimands are shown to be expressed as functionals of the distribution of the observed data. To avoid the curse of dimensionality, we model the distribution of the observed data using a Bayesian shrinkage model. In a simulation study, we compare our approach to a fully parametric and a fully saturated model for the distribution of the observed data. Our methodology is motivated by, and applied to, data from the Breast Cancer Prevention Trial. "], ["Sampling With Synthesis: A New Approach for Releasing Public Use Census Microdata", " Many statistical agencies disseminate samples of census microdata, that is, data on individual records, to the public. Before releasing the microdata, agencies typically alter identifying or sensitive values to protect data subjects\u2019 confidentiality, for example by coarsening, perturbing, or swapping data. These standard disclosure limitation techniques distort relationships and distributional features in the original data, especially when applied with high intensity. Furthermore, it can be difficult for analysts of the masked public use data to adjust inferences for the effects of the disclosure limitation. Motivated by these shortcomings, we propose an approach to census microdata dissemination called sampling with synthesis. The basic idea is to replace the identifying or sensitive values in the census with multiple imputations, and release samples from these multiply-imputed populations. We demonstrate that sampling with synthesis can improve the quality of public use data relative to sampling followed by standard statistical disclosure limitation; simulation results showing this are available online as supplemental material. We derive methods for analyzing the multiple datasets generated by sampling with synthesis. We present algorithms for selecting which census values to synthesize based on considerations of disclosure risk and data utility. We illustrate sampling with synthesis on a population constructed with data from the U.S. Current Population Survey. "], ["Bayesian Random Segmentation Models to Identify Shared Copy Number Aberrations for Array CGH Data", " Array-based comparative genomic hybridization (aCGH) is a high-resolution, high-throughput technique for studying the genetic basis of cancer. The resulting data consist of log fluorescence ratios as a function of the genomic DNA location and provide a cytogenetic representation of the relative DNA copy number variation. Analysis of such data typically involves estimating the underlying copy number state at each location and segmenting regions of DNA with similar copy number states. Most current methods proceed by modeling a single sample\u2009/\u2009array at a time, and thus fail to borrow strength across multiple samples to infer shared regions of copy number aberrations. We propose a hierarchical Bayesian random segmentation approach for modeling aCGH data that uses information across arrays from a common population to yield segments of shared copy number changes. These changes characterize the underlying population and allow us to compare different population aCGH profiles to assess which regions of the genome have differential alterations. Our method, which we term Bayesian detection of shared aberrations in aCGH (BDSAScgh), is based on a unified Bayesian hierarchical model that allows us to obtain probabilities of alteration states as well as probabilities of differential alterations that correspond to local false discovery rates for both single and multiple groups. We evaluate the operating characteristics of our method via simulations and an application using a lung cancer aCGH data set. This article has supplementary material online. "], ["Localized Realized Volatility Modeling", " With the recent availability of high-frequency financial data the long-range dependence of volatility regained researchers\u2019 interest and has led to the consideration of long-memory models for volatility. The long-range diagnosis of volatility, however, is usually stated for long sample periods, while for small sample sizes, such as one year, the volatility dynamics appears to be better described by short-memory processes. The ensemble of these seemingly contradictory phenomena point towards short-memory models of volatility with nonstationarities, such as structural breaks or regime switches, that spuriously generate a long memory pattern. In this paper we adopt this view on the dependence structure of volatility and propose a localized procedure for modeling realized volatility. That is at each point in time we determine a past interval over which volatility is approximated by a local linear process. A simulation study shows that long memory processes as well as short memory processes with structural breaks can be well approximated by this local approach. Furthermore, using S&P500 data we find that our local modeling approach outperforms long-memory type models and models with structural breaks in terms of predictability. "], ["Estimating Individual-Level Risk in Spatial Epidemiology Using Spatially Aggregated Information on the Population at Risk", " We propose a novel alternative to case-control sampling for the estimation of individual-level risk in spatial epidemiology. Our approach uses weighted estimating equations to estimate regression parameters in the intensity function of an inhomogeneous spatial point process, when information on risk-factors is available at the individual level for cases, but only at a spatially aggregated level for the population at risk. We develop data-driven methods to select the weights used in the estimating equations and show through simulation that the choice of weights can have a major impact on efficiency of estimation. We develop a formal test to detect non-Poisson behavior in the underlying point process and assess the performance of the test using simulations of Poisson and Poisson cluster point processes. We apply our methods to data on the spatial distribution of childhood meningococcal disease cases in Merseyside, U.K. between 1981 and 2007. "], ["Autoregressive Mixture Models for Dynamic Spatial Poisson Processes: Application to Tracking Intensity of Violent Crime", " This article develops a set of tools for smoothing and prediction with dependent point event patterns. The methodology is motivated by the problem of tracking weekly maps of violent crime events, but is designed to be straightforward to adapt to a wide variety of alternative settings. In particular, a Bayesian semiparametric framework is introduced for modeling correlated time series of marked spatial Poisson processes. The likelihood is factored into two independent components: the set of total integrated intensities and a series of process densities. For the former it is assumed that Poisson intensities are realizations from a dynamic linear model. In the latter case, a novel class of dependent stick-breaking mixture models are proposed to allow nonparametric density estimates to evolve in discrete time. This, a simple and flexible new model for dependent random distributions, is based on autoregressive time series of marginally beta random variables applied as correlated stick-breaking proportions. The approach allows for marginal Dirichlet process priors at each time and adds only a single new correlation term to the static model specification. Sequential Monte Carlo algorithms are described for online inference with each model component, and marginal likelihood calculations form the basis for inference about parameters governing temporal dynamics. Simulated examples are provided to illustrate the methodology, and we close with results for the motivating application of tracking violent crime in Cincinnati. "], ["Correcting for Survey Nonresponse Using Variable Response Propensity", " All surveys with less than full response potentially suffer from nonresponse bias. Poststratification weights can only correct for selection into the sample based on observables whose distribution is known in the population. Variables such as gender, race, income, and region satisfy this requirement because they are available from the U.S. Census Bureau, but poststratification based on these variables may not eliminate nonresponse bias. I develop an approach for correcting for nonignorable nonresponse bias. Survey respondents can be classified by their \u201cresponse propensity.\u201d Proxies for response propensity include the number of attempted phone calls, indicators of temporary refusal, and interviewer-coded measures of cooperativeness. We can then learn about the population of nonrespondents by extrapolating from the low-propensity respondents. I apply this new estimator to correct for unit nonresponse bias in the American National Election Study and in a CBS\u2009/\u2009New York Times preelection poll. I find that nonresponse bias can be a serious problem, particularly for items that relate to political participation. I find that my method is successful in substantially reducing nonresponse bias. "], ["Chronic Disease Prevention Research Methods and Their Reliability, With Illustrations From the Women\u2019s Health Initiative", " This article reviews the status of statistical methods for chronic disease prevention research, with emphasis on the reliability of findings and on future methodological needs and opportunities. Observational studies, especially cohort studies, play a major role in disease prevention research, but depend on adequate confounding control methods for a useful interpretation. Stratification and regression methods that are commonly used to control confounding are described, and comparative findings from the Women\u2019s Health Initiative (WHI) randomized controlled trial and companion cohort study of the benefits and risk of postmenopausal hormone therapy are used to illustrate the success of these methods. Measurement error in exposure assessment may be a potentially dominating source of bias in such important prevention research areas as nutrition and physical activity epidemiology. Statistical methods to correct for measurement error are briefly reviewed, and the need for methods to accommodate systematic bias in exposure assessment is described. Recent analysis using nutrient exposure biomarkers in WHI cohorts is used to illustrate the impact of such methods. Randomized, controlled intervention trials have the potential to obviate these biases and to reliably assess intervention benefits and risks. However, trials in healthy persons with chronic disease outcomes are typically large, long-term, and expensive. Statistical methods for randomized, controlled prevention trials are briefly reviewed, and the roles of trials in the overall chronic disease prevention research enterprise are examined. Given the logistical and cost challenges, a full-scale disease prevention trial needs to be preceded by careful hypothesis development and initial testing. The potential role of biomarkers, especially high-dimensional biomarkers, in disease prevention hypothesis development, is described and illustrated. The presentation concludes with comments on the methodological research and research infrastructure developments needed to invigorate the chronic disease prevention research agenda, with emphasis on the important role for statisticians in enhancing prevention research methods and applications. "], ["A Hidden Markov Model Approach to Testing Multiple Hypotheses on a Tree-Transformed Gene Ontology Graph", " Gene category testing problems involve testing hundreds of null hypotheses that correspond to nodes in a directed acyclic graph. The logical relationships among the nodes in the graph imply that only some configurations of true and false null hypotheses are possible and that a test for a given node should depend on data from neighboring nodes. We developed a method based on a hidden Markov model that takes the whole graph into account and provides coherent decisions in this structured multiple hypothesis testing problem. The method is illustrated by testing Gene Ontology terms for evidence of differential expression. "], ["Dimension Reduction in Regressions Through Cumulative Slicing Estimation", null], ["Modeling Longitudinal Data Using a Pair-Copula Decomposition of Serial Dependence", " Copulas have proven to be very successful tools for the flexible modeling of cross-sectional dependence. In this paper we express the dependence structure of continuous-valued time series data using a sequence of bivariate copulas. This corresponds to a type of decomposition recently called a \u201cvine\u201d in the graphical models literature, where each copula is entitled a \u201cpair-copula.\u201d We propose a Bayesian approach for the estimation of this dependence structure for longitudinal data. Bayesian selection ideas are used to identify any independence pair-copulas, with the end result being a parsimonious representation of a time-inhomogeneous Markov process of varying order. Estimates are Bayesian model averages over the distribution of the lag structure of the Markov process. Using a simulation study we show that the selection approach is reliable and can improve the estimates of both conditional and unconditional pairwise dependencies substantially. We also show that a vine with selection outperforms a Gaussian copula with a flexible correlation matrix. The advantage of the pair-copula formulation is further demonstrated using a longitudinal model of intraday electricity load. Using Gaussian, Gumbel, and Clayton pair-copulas we identify parsimonious decompositions of intraday serial dependence, which improve the accuracy of intraday load forecasts. We also propose a new diagnostic for measuring the goodness of fit of high-dimensional multivariate copulas. Overall, the pair-copula model is very general and the Bayesian method generalizes many previous approaches for the analysis of longitudinal data. Supplemental materials for the article are also available online. "], ["Multiple Change-Point Estimation With a Total Variation Penalty", null], ["Pseudo\u2013Empirical Likelihood Inference for Multiple Frame Surveys", null], ["High-Frequency Covariance Estimates With Noisy and Asynchronous Financial Data", " This article proposes a consistent and efficient estimator of the high-frequency covariance (quadratic covariation) of two arbitrary assets, observed asynchronously with market microstructure noise. This estimator is built on the marriage of the quasi\u2013maximum likelihood estimator of the quadratic variation and the proposed generalized synchronization scheme and thus is not influenced by the Epps effect. Moreover, the estimation procedure is free of tuning parameters or bandwidths and is readily implementable. Monte Carlo simulations show the advantage of this estimator by comparing it with a variety of estimators with specific synchronization methods. The empirical studies of six foreign exchange future contracts illustrate the time-varying correlations of the currencies during the 2008 global financial crisis, demonstrating the similarities and differences in their roles as key currencies in the global market. "], ["Consistent Model Selection for Marginal Generalized Additive Model for Correlated Data", " Supplemental materials including technical details are available online. "], ["Composite Likelihood Bayesian Information Criteria for Model Selection in High-Dimensional Data", " For high-dimensional data sets with complicated dependency structures, the full likelihood approach often leads to intractable computational complexity. This imposes difficulty on model selection, given that most traditionally used information criteria require evaluation of the full likelihood. We propose a composite likelihood version of the Bayes information criterion (BIC) and establish its consistency property for the selection of the true underlying marginal model. Our proposed BIC is shown to be selection-consistent under some mild regularity conditions, where the number of potential model parameters is allowed to increase to infinity at a certain rate of the sample size. Simulation studies demonstrate the empirical performance of this new BIC, especially for the scenario where the number of parameters increases with sample size. Technical proofs of our theoretical results are provided in the online supplemental materials. "], ["Variable Selection Using Adaptive Nonlinear Interaction Structures in High Dimensions", null], ["Combining Nonparametric and Optimal Linear Time Series Predictions", " We introduce a semiparametric procedure for more efficient prediction of a strictly stationary process admitting an ARMA representation. The procedure is based on the estimation of the ARMA representation, followed by a nonparametric regression where the ARMA residuals are used as explanatory variables. Compared to standard nonparametric regression methods, the number of explanatory variables can be reduced because our approach exploits the linear dependence of the process. We establish consistency and asymptotic normality results for our estimator. Numerical experiments show that significant gains can be achieved with our approach. All the supplemental materials used by this article are available online. "], ["Weighted Optimality in Designed Experimentation", " An optimality framework is developed for designing experiments in which not all treatments are of equal interest, such as those including an established control. Differential interest in treatments is formalized by assignment of weights, incorporated into optimality measures through a weighted version of the information matrix. All conventional measures of design efficacy are shown to have weighted analogs. The properties of weighted measures are explored, some general theory is developed, and weighted optimal designs are determined for unblocked experimentation. This new approach includes \u201ctest treatments versus control\u201d experiments as a special case. Supplementary materials for the article are available online. "], ["Optimal and Efficient Crossover Designs for Test-Control Study When Subject Effects Are Random", " We study crossover designs based on the criteria of A-optimality and MV-optimality under the model with random subject effects, for the purpose of comparing several test treatments with a standard control treatment. Optimal and efficient designs are proposed, and their efficiencies are also evaluated. A family of totally balanced test-control incomplete crossover designs based on a function of the ratio of the subject effect variance to the error variance are shown to be highly efficient and robust. The results have interesting connections with those in Hedayat and Yang (2005) and Hedayat, Stufken, and Yang (2006). The omitted proofs in the article are included in a supplemental material online. "], ["Constrained Factor Models", null], ["Bootstrapping Robust Estimates for Clustered Data", " In mixed models, the use of robust estimates raises several interesting inferential challenges. One of these challenges arises from the realization that the effect of contamination is to increase the variability in the data, but robust estimates of variance components are usually smaller than their nonrobust counterparts. The robust estimates reflect the variability of the bulk of the data, which is not the same as the variability in the data-generating process. This means that the naive implementation of bootstrap procedures might not work. In this article we consider several bootstrap procedures, including random effect, transformation, and weighted bootstraps. We give conditions for the asymptotic validity of the bootstraps and assess their performance via a small simulation study. Both the transformation and generalized cluster bootstrap perform well and are asymptotically valid under reasonable conditions. "], ["Estimability and Likelihood Inference for Generalized Linear Mixed Models Using Data Cloning", " Maximum likelihood estimation for Generalized Linear Mixed Models (GLMM), an important class of statistical models with substantial applications in epidemiology, medical statistics, and many other fields, poses significant computational difficulties. In this article, we use data cloning, a simple computational method that exploits advances in Bayesian computation, in particular the Markov Chain Monte Carlo method, to obtain maximum likelihood estimators of the parameters in these models. This method also leads to a simple estimator of the asymptotic variance of the maximum likelihood estimators. Determining estimability of the parameters in a mixed model is, in general, a very difficult problem. Data cloning provides a simple graphical test to not only check if the full set of parameters is estimable but also, and perhaps more importantly, if a specified function of the parameters is estimable. One of the goals of mixed models is to predict random effects. We suggest a frequentist method to obtain prediction intervals for random effects. We illustrate data cloning in the GLMM context by analyzing the Logistic\u2013Normal model for over-dispersed binary data, and the Poisson\u2013Normal model for repeated and spatial counts data. We consider Normal\u2013Normal and Binary\u2013Normal mixture models to show how data cloning can be used to study estimability of various parameters. We contend that whenever hierarchical models are used, estimability of the parameters should be checked before drawing scientific inferences or making management decisions. Data cloning facilitates such a check on hierarchical models. "], ["Book Reviews", null], ["Letters to the Editor", null], ["Correction", null], ["Correction", null], ["2010 Editorial Collaborators", null], ["Index to Volume 105 (2010)", null], ["The Value of Multiproxy Reconstruction of Past Climate", " Understanding the dynamics of climate change in its full richness requires the knowledge of long temperature time series. Although long-term, widely distributed temperature observations are not available, there are other forms of data, known as climate proxies, that can have a statistical relationship with temperatures and have been used to infer temperatures in the past before direct measurements. We propose a Bayesian hierarchical model to reconstruct past temperatures that integrates information from different sources, such as proxies with different temporal resolution and forcings acting as the external drivers of large scale temperature evolution. Additionally, this method allows us to quantify the uncertainty of the reconstruction in a rigorous manner. The reconstruction method is assessed, using a global climate model as the true climate system and with synthetic proxy data derived from the simulation. The target is to reconstruct Northern Hemisphere temperature from proxies that mimic the sampling and errors from tree ring measurements, pollen indices, and borehole temperatures. The forcing series used as covariates are solar irradiance, volcanic aerosols, and greenhouse gas concentrations. The Bayesian model was successful in integrating these different sources of information in creating a coherent reconstruction. Within the context of this numerical testbed, a statistical process model that includes the external forcings can improve the quality of a hemispheric reconstruction when long time scale proxy information is not available. This article has supplementary material online. "], ["Comment: Hierarchical Statistical Modeling for Paleoclimate Reconstruction", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Causal Effects of Treatments for Informative Missing Data due to Progression/Death", " In longitudinal clinical trials, when outcome variables at later time points are only defined for patients who survive to those times, the evaluation of the causal effect of treatment is complicated. In this paper, we describe an approach that can be used to obtain the causal effect of three treatment arms with ordinal outcomes in the presence of death using a principal stratification approach. We introduce a set of flexible assumptions to identify the causal effect and implement a sensitivity analysis for nonidentifiable assumptions which we parameterize parsimoniously. Methods are illustrated on quality of life data from a recent colorectal cancer clinical trial. This article has supplementary material online. "], ["Prediction of Functional Status for the Elderly Based on a New Ordinal Regression Model", " The functional mobility of the elderly is a very important factor in aging research, and prognostic information is valuable in making clinical and health care policy decisions. We develop a predictive model for the functional status of the elderly based on data from the Second Longitudinal Study of Aging (LSOA II). The functional status is an ordinal response variable. The ordered probit model has been moderately successful in analyzing such data; however, its reliance on the normal distribution for its latent variable hinders its accuracy and potential. In this paper, we focus on the prediction of conditional quantiles of the functional status based on a more general transformation model. The proposed estimation procedure does not rely on any parametric specification of the conditional distribution functions, aiming to reduce model misspecification errors in the prediction. Cross-validation within the LSOA II data shows that our prediction intervals are more informative than those from the ordered probit model. Monte Carlo simulations also demonstrate the merits of our approach in the analysis of ordinal response variables. "], ["Informative Retesting", " In situations where individuals are screened for an infectious disease or other binary characteristic and where resources for testing are limited, group testing can offer substantial benefits. Group testing, where subjects are tested in groups (pools) initially, has been successfully applied to problems in blood bank screening, public health, drug discovery, genetics, and many other areas. In these applications, often the goal is to identify each individual as positive or negative using initial group tests and subsequent retests of individuals within positive groups. Many group testing identification procedures have been proposed; however, the vast majority of them fail to incorporate heterogeneity among the individuals being screened. In this paper, we present a new approach to identify positive individuals when covariate information is available on each. This covariate information is used to structure how retesting is implemented within positive groups; therefore, we call this new approach \u201cinformative retesting.\u201d We derive closed-form expressions and implementation algorithms for the probability mass functions for the number of tests needed to decode positive groups. These informative retesting procedures are illustrated through a number of examples and are applied to chlamydia and gonorrhea testing in Nebraska for the Infertility Prevention Project. Overall, our work shows compelling evidence that informative retesting can dramatically decrease the number of tests while providing accuracy similar to established noninformative retesting procedures. This article has supplementary material online. "], [null, null], ["Optimal Partitioning for Linear Mixed Effects Models: Applications to Identifying Placebo Responders", " A longstanding problem in clinical research is distinguishing drug-treated subjects that respond due to specific effects of the drug from those that respond to nonspecific (or placebo) effects of the treatment. Linear mixed effect models are commonly used to model longitudinal clinical trial data. In this paper we present a solution to the problem of identifying placebo responders using an optimal partitioning methodology for linear mixed effects models. Since individual outcomes in a longitudinal study correspond to curves, the optimal partitioning methodology produces a set of prototypical outcome profiles. The optimal partitioning methodology can accommodate both continuous and discrete covariates. The proposed partitioning strategy is compared and contrasted with the growth mixture modeling approach. The methodology is applied to a two-phase depression clinical trial where subjects in a first phase were treated openly for 12 weeks with fluoxetine followed by a double blind discontinuation phase where responders to treatment in the first phase were randomized to either stay on fluoxetine or switched to a placebo. The optimal partitioning methodology is applied to the first phase to identify prototypical outcome profiles. Using time to relapse in the second phase of the study, a survival analysis is performed on the partitioned data. The optimal partitioning results identify prototypical profiles that distinguish whether subjects relapse depending on whether or not they stay on the drug or are randomized to a placebo. "], ["An Ensemble Kalman Filter and Smoother for Satellite Data Assimilation", " This paper proposes a methodology for combining satellite images with advection-diffusion models for interpolation and prediction of environmental processes. We propose a dynamic state-space model and an ensemble Kalman filter and smoothing algorithm for on-line and retrospective state estimation. Our approach addresses the high dimensionality, measurement bias, and nonlinearities inherent in satellite data. We apply the method to a sequence of SeaWiFS satellite images in Lake Michigan from March 1998, when a large sediment plume was observed in the images following a major storm event. Using our approach, we combine the images with a sediment transport model to produce maps of sediment concentrations and uncertainties over space and time. We show that our approach improves out-of-sample RMSE by 20%\u201330% relative to standard approaches. This article has supplementary material online. "], [null, null], ["Sensitivity Analysis for the Cross-Match Test, With Applications in Genomics", null], ["Exploiting Regional Treatment Intensity for the Evaluation of Labor Market Policies", " We estimate the effects of active labor market policies (ALMPs) on subsequent employment and earnings by nonparametric instrumental variable estimators. Very informative administrative Swiss data with detailed regional information are combined with exogenous regional variation in program participation probabilities to generate an instrument within well-defined local labor markets. We find that implementation of an ALMP increased individual employment probabilities by about 15% for unemployed that might be considered \u201cmarginal\u201d participants. "], ["A Statistical Approach to Thermal Management of Data Centers Under Steady State and System Perturbations", " Temperature control for a large data center is both important and expensive. On the one hand, many of the components produce a great deal of heat, and on the other hand, many of the components require temperatures below a fairly low threshold for reliable operation. A statistical framework is proposed within which the behavior of a large cooling system can be modeled and forecast under both steady state and perturbations. This framework is based upon an extension of multivariate Gaussian autoregressive hidden Markov models (HMMs). The estimated parameters of the fitted model provide useful summaries of the overall behavior of and relationships within the cooling system. Predictions under system perturbations are useful for assessing potential changes and improvements to be made to the system. Many data centers have far more cooling capacity than necessary under sensible circumstances, thus resulting in energy inefficiencies. Using this model, predictions for system behavior after a particular component of the cooling system is shut down or reduced in cooling power can be generated. Steady-state predictions are also useful for facility monitors. System traces outside control boundaries flag a change in behavior to examine. The proposed model is fit to data from a group of air conditioners within an enterprise data center from the IT industry. The fitted model is examined, and a particular unit is found to be underutilized. Predictions generated for the system under the removal of that unit appear very reasonable. Steady-state system behavior also is predicted well. "], [null, null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Robust Data-Driven Inference for Density-Weighted Average Derivatives", " This paper presents a novel data-driven bandwidth selector compatible with the small bandwidth asymptotics developed in Cattaneo, Crump, and Jansson (2009) for density-weighted average derivatives. The new bandwidth selector is of the plug-in variety, and is obtained based on a mean squared error expansion of the estimator of interest. An extensive Monte Carlo experiment shows a remarkable improvement in performance when the bandwidth-dependent robust inference procedures proposed by Cattaneo, Crump, and Jansson (2009) are coupled with this new data-driven bandwidth selector. The resulting robust data-driven confidence intervals compare favorably to the alternative procedures available in the literature. The online supplemental material to this paper contains further results from the simulation study. "], ["Testing the Order of a Finite Mixture", null], ["A New Approach to Optimal Design for Linear Models With Correlated Observations", " We also compare the asymptotic optimal design concepts of Sacks and Ylvisaker (1966, 1968) and Bickel and Herzberg (1979) and point out some inconsistencies of the latter. Finally, we combine the best features of both concepts to develop a new approach for the design of experiments for correlated observations, and it is demonstrated that the resulting design problems are related to the (logarithmic) potential theory. "], ["Least Absolute Relative Error Estimation", " Multiplicative regression model or accelerated failure time model, which becomes linear regression model after logarithmic transformation, is useful in analyzing data with positive responses, such as stock prices or life times, that are particularly common in economic/financial or biomedical studies. Least squares or least absolute deviation are among the most widely used criterions in statistical estimation for linear regression model. However, in many practical applications, especially in treating, for example, stock price data, the size of relative error, rather than that of error itself, is the central concern of the practitioners. This paper offers an alternative to the traditional estimation methods by considering minimizing the least absolute relative errors for multiplicative regression models. We prove consistency and asymptotic normality and provide an inference approach via random weighting. We also specify the error distribution, with which the proposed least absolute relative errors estimation is efficient. Supportive evidence is shown in simulation studies. Application is illustrated in an analysis of stock returns in Hong Kong Stock Exchange. "], ["Tests for Error Correlation in the Functional Linear Model", null], ["Goodness of Fit for Generalized Linear Latent Variables Models", null], ["Nonparametric Regression With Missing Outcomes Using Weighted Kernel Estimating Equations", " We consider nonparametric regression of a scalar outcome on a covariate when the outcome is missing at random (MAR) given the covariate and other observed auxiliary variables. We propose a class of augmented inverse probability weighted (AIPW) kernel estimating equations for nonparametric regression under MAR. We show that AIPW kernel estimators are consistent when the probability that the outcome is observed, that is, the selection probability, is either known by design or estimated under a correctly specified model. In addition, we show that a specific AIPW kernel estimator in our class that employs the fitted values from a model for the conditional mean of the outcome given covariates and auxiliaries is double-robust, that is, it remains consistent if this model is correctly specified even if the selection probabilities are modeled or specified incorrectly. Furthermore, when both models happen to be right, this double-robust estimator attains the smallest possible asymptotic variance of all AIPW kernel estimators and maximally extracts the information in the auxiliary variables. We also describe a simple correction to the AIPW kernel estimating equations that while preserving double-robustness it ensures efficiency improvement over nonaugmented IPW estimation when the selection model is correctly specified regardless of the validity of the second model used in the augmentation term. We perform simulations to evaluate the finite sample performance of the proposed estimators, and apply the methods to the analysis of the AIDS Costs and Services Utilization Survey data. Technical proofs are available online. "], ["Nested Lattice Sampling: A New Sampling Scheme Derived by Randomizing Nested Orthogonal Arrays", null], ["Optimal Sparse Segment Identification With Application in Copy Number Variation Analysis", " Motivated by DNA copy number variation (CNV) analysis based on high-density single nucleotide polymorphism (SNP) data, we consider the problem of detecting and identifying sparse short segments in a long one-dimensional sequence of data with additive Gaussian white noise, where the number, length, and location of the segments are unknown. We present a statistical characterization of the identifiable region of a segment where it is possible to reliably separate the segment from noise. An efficient likelihood ratio selection (LRS) procedure for identifying the segments is developed, and the asymptotic optimality of this method is presented in the sense that the LRS can separate the signal segments from the noise as long as the signal segments are in the identifiable regions. The proposed method is demonstrated with simulations and analysis of a real dataset on identification of copy number variants based on high-density SNP data. The results show that the LRS procedure can yield greater gain in power for detecting the true segments than some standard signal identification methods. "], ["Mat\u00e9rn Cross-Covariance Functions for Multivariate Random Fields", " We introduce a flexible parametric family of matrix-valued covariance functions for multivariate spatial random fields, where each constituent component is a Mat\u00e9rn process. The model parameters are interpretable in terms of process variance, smoothness, correlation length, and colocated correlation coefficients, which can be positive or negative. Both the marginal and the cross-covariance functions are of the Mat\u00e9rn type. In a data example on error fields for numerical predictions of surface pressure and temperature over the North American Pacific Northwest, we compare the bivariate Mat\u00e9rn model to the traditional linear model of coregionalization. "], ["Approximate Bayesian Computation: A Nonparametric Perspective", null], ["Groupwise Dimension Reduction", " In many regression applications, the predictors fall naturally into a number of groups or domains, and it is often desirable to establish a domain-specific relation between the predictors and the response. In this article, we consider dimension reduction that incorporates such domain knowledge. The proposed method is based on the derivative of the conditional mean, where the differential operator is constrained to the form of a direct sum. This formulation also accommodates the situations where dimension reduction is focused only on part of the predictors; as such it extends Partial Dimension Reduction to cases where the blocked predictors are continuous. Through simulation and real data analyses, we show that the proposed method achieves greater accuracy and interpretability than the dimension reduction methods that ignore group information. Furthermore, the new method does not require the stringent conditions on the predictor distribution that are required by existing methods. "], ["Bayesian Variable Selection in Structured High-Dimensional Covariate Spaces With Applications in Genomics", null], ["False Discovery Rate Control With Groups", null], ["Testing for Change Points in Time Series", null], ["Spectral Connectivity Analysis", " Spectral kernel methods are techniques for mapping data into a coordinate system that efficiently reveals the geometric structure\u2014in particular, the \u201cconnectivity\u201d\u2014of the data. These methods depend on tuning parameters. We analyze the dependence of the method on these tuning parameters. We focus on one particular technique\u2014diffusion maps\u2014but our analysis can be used for other spectral methods as well. We identify the key population quantities, we define an appropriate risk function for analyzing the estimators, and we explain how these methods relate to classical kernel smoothing. We also show that, in some cases, fast rates of convergence are possible even in high dimensions. The Appendix of the article is available online as supplementary materials. "], ["Functional Varying Coefficient Models for Longitudinal Data", " The proposed functional varying coefficient model provides a versatile and flexible analysis tool for relating longitudinal responses to longitudinal predictors. Specifically, this approach provides a novel representation of varying coefficient functions through suitable auto and cross-covariances of the underlying stochastic processes, which is particularly advantageous for sparse and irregular designs, as often encountered in longitudinal studies. Existing methodology for varying coefficient models is not adapted to such data. The proposed approach extends the customary varying coefficient models to a more general setting, in which not only current but also recent past values of the predictor time course may have an impact on the current value of the response time course. The influence of past predictor values is modeled by a smooth history index function, while the effects on the response are described by smooth varying coefficient functions. The resulting estimators for varying coefficient and history index functions are shown to be asymptotically consistent for sparse designs. In addition, prediction of unobserved response trajectories from sparse measurements on a predictor trajectory is obtained, along with asymptotic pointwise confidence bands. The proposed methods perform well in simulations, especially when compared with commonly used local polynomial smoothing methods for varying coefficient models, and are illustrated with longitudinal primary biliary liver cirrhosis data. The data application and detailed assumptions and proofs can be found in online Supplemental Material. "], ["Using Calibration Weighting to Adjust for Nonignorable Unit Nonresponse", " When calibration weighting is be used to adjust for unit nonresponse in a sample survey, the response/nonresponse mechanism is often assumed to be a function of a set of covariates, which we call \u201cmodel variables.\u201d These model variables usually also serve as the benchmark variables in the calibration equation. In principle, however, the model variables do not have to coincide with the benchmark variables. Since the model-variable values need only be known for the respondents, this allows the treatment of what is usually considered nonignorable nonresponse in the prediction approach to survey sampling. One can invoke either a quasi-randomization or prediction approach to justify calibration weighting as a means for adjusting for nonresponse. Both frameworks rely on unverifiable model assumptions, and both require large samples to produce nearly unbiased estimators even when those assumptions hold. We will explore these issues theoretically using a joint framework and with an empirical study. "], ["Book Reviews", null], ["Identifying Intraparty Voting Blocs in the U.K. House of Commons", " Legislative voting records are an important source of information about legislator preferences, intraparty cohesiveness, and the divisiveness of various policy issues. Standard methods of analyzing a legislative voting record tend to have serious drawbacks when applied to legislatures, such as the United Kingdom House of Commons, that feature highly disciplined parties, strategic voting, and large amounts of missing data. We present a method (based on a Dirichlet process mixture model) for analyzing such voting records that does not suffer from these same problems. Our method is model-based and thus allows one to make probability statements about quantities of interest. It allows one to estimate the number of voting blocs within a party or any other group of members of parliament (MPs). Finally, it can be used as both a predictive model and an exploratory model. We illustrate these points through an application of the method to the voting records of Labour Party MPs in the 1997\u20132001 session of the U.K. House of Commons. "], ["Dynamic Nonparametric Bayesian Models for Analysis of Music", " The dynamic hierarchical Dirichlet process (dHDP) is developed to model complex sequential data, with a focus on audio signals from music. The music is represented in terms of a sequence of discrete observations, and the sequence is modeled using a hidden Markov model (HMM) with time-evolving parameters. The dHDP imposes the belief that observations that are temporally proximate are more likely to be drawn from HMMs with similar parameters, while also allowing for \u201cinnovation\u201d associated with abrupt changes in the music texture. The sharing mechanisms of the time-evolving model are derived, and for inference a relatively simple Markov chain Monte Carlo sampler is developed. Segmentation of a given musical piece is constituted via the model inference. Detailed examples are presented on several pieces, with comparisons to other models. The dHDP results are also compared with a conventional music-theoretic analysis. All the supplemental materials used by this paper are available online. "], ["An Association Test for Multiple Traits Based on the Generalized Kendall\u2019s Tau", " In many genetics studies, especially in the investigation of mental illness and behavioral disorders, it is common for researchers to collect multiple phenotypes to characterize the complex disease of interest. It may be advantageous to analyze those phenotypic measurements simultaneously if they share a similar genetic mechanism. In this study, we present a nonparametric approach to studying multiple traits together rather than examining each trait separately. Through simulation we compared the nominal Type I error and power of our proposed test to an existing test, that is, a generalized family-based association test. The empirical results suggest that our proposed approach is superior to the existing test in the analysis of ordinal traits. The advantage is demonstrated on a dataset concerning alcohol dependence. In this application, the use of our methods enhanced the signal of the association test. "], ["A Bayesian Vector Multidimensional Scaling Procedure for the Analysis of Ordered Preference Data", " Multidimensional scaling (MDS) comprises a family of geometric models for the multidimensional representation of data and a corresponding set of methods for fitting such models to actual data. In this paper, we develop a new Bayesian vector MDS model to analyze ordered successive categories preference/dominance data commonly collected in many social science and business studies. A joint spatial representation of the row and column elements of the input data matrix is provided in a reduced dimensionality such that the geometric relationship of the row and column elements renders insight into the utility structure underlying the data. Unlike classical deterministic MDS procedures, the Bayesian method includes a probability based criterion to determine the number of dimensions of the derived joint space map and provides posterior interval as well as point estimates for parameters of interest. Also, our procedure models the raw integer successive categories data which ameliorates the need of any data preprocessing as required for many metric MDS procedures. Furthermore, the proposed Bayesian procedure allows external information in the form of an intractable posterior distribution derived from a related dataset to be incorporated as a prior in deriving the spatial representation of the preference data. An actual commercial application dealing with consumers\u2019 intentions to buy new luxury sport utility vehicles are presented to illustrate the proposed methodology. Favorable comparisons are made with more traditional MDS approaches. "], ["Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California\u2019s Tobacco Control Program", " Building on an idea in Abadie and Gardeazabal (2003), this article investigates the application of synthetic control methods to comparative case studies. We discuss the advantages of these methods and apply them to study the effects of Proposition 99, a large-scale tobacco control program that California implemented in 1988. We demonstrate that, following Proposition 99, tobacco consumption fell markedly in California relative to a comparable synthetic control region. We estimate that by the year 2000 annual per-capita cigarette sales in California were about 26 packs lower than what they would have been in the absence of Proposition 99. Using new inferential methods proposed in this article, we demonstrate the significance of our estimates. Given that many policy interventions and events of interest in social sciences take place at an aggregate level (countries, regions, cities, etc.) and affect a small number of aggregate units, the potential applicability of synthetic control methods to comparative case studies is very large, especially in situations where traditional regression methods are not appropriate. "], ["Hierarchical Spatial Process Models for Multiple Traits in Large Genetic Trials", null], ["Probabilistic Weather Forecasting for Winter Road Maintenance", " We evaluate both methods by comparing their probabilistic forecasts with observations of ice formation for Interstate Highway 90 in Washington State for the 2003\u20132004 and 2004\u20132005 winter seasons. The use of the probabilistic forecasts reduces costs by about 50% when compared to deterministic forecasts. The spatial method improves the reliability of the forecasts, but does not result in further cost reduction when compared to the first method. "], ["Spatio-Temporal Analysis of Total Nitrate Concentrations Using Dynamic Statistical Models", null], ["Using DNA Fingerprints to Infer Familial Relationships Within NHANES III Households", null], ["Bayesian Multiscale Multiple Imputation With Implications for Data Confidentiality", null], ["A Rank-Based Test for Comparison of Multidimensional Outcomes", " For comparison of multiple outcomes commonly encountered in biomedical research, Huang et al. (2005) improved O\u2019Brien\u2019s (1984) rank-sum tests through the replacement of the ad hoc variance by the asymptotic variance of the test statistics. The improved tests control the type I error rate at the desired level and gain power when the differences between the two comparison groups in each outcome variable lie in the same direction; however, they may lose power when the differences are in different directions (e.g., some are positive and some are negative). These tests and the popular Bonferroni correction failed to show important significant differences when applied to compare heart rates from a clinical trial to evaluate the effect of a procedure to remove the cardioprotective solution HTK. We propose an alternative test statistic, taking the maximum of the individual rank-sum statistics, which controls the type I error rate and maintains satisfactory power regardless of the direction of the differences. Simulation studies show the proposed test to be of higher power than other tests in a certain alternative parameter space of interest. Furthermore, when used to analyze the heart rate data, the proposed test yields more satisfactory results. "], ["Group Comparison of Eigenvalues and Eigenvectors of Diffusion Tensors", " Diffusion tensor imaging (DTI) data differ from most medical images in that values at each voxel are not scalars, but 3\u00d73 symmetric positive definite matrices called diffusion tensors (DTs). The anatomic characteristics of the tissue at each voxel are reflected by the DT eigenvalues and eigenvectors. In this article we consider the problem of testing whether the means of two groups of DT images are equal at each voxel in terms of the DT\u2019s eigenvalues, eigenvectors, or both. Because eigendecompositions are highly nonlinear, existing likelihood ratio statistics (LRTs) for testing differences in eigenvalues or eigenvectors of means of Gaussian symmetric matrices assume an orthogonally invariant covariance structure between the matrix entries. While retaining the form of the LRTs, we derive new approximations to their true distributions when the covariance between the DT entries is arbitrary and possibly different between the two groups. The approximate distributions are those of similar LRT statistics computed on the tangent space to the parameter manifold at the true value of the parameter, but plugging in an estimate for the point of application of the tangent space. The resulting distributions, which are weighted sums of chi-squared distributions, are further approximated by scaled chi-squared distributions by matching the first two moments. For validity of the Gaussian model, the positive definite constraints on the DT are removed via a matrix log transformation, although this is not crucial asymptotically. Voxelwise application of the test statistics leads to a multiple-testing problem, which is solved by false discovery rate inference. The foregoing methods are illustrated in a DTI group comparison of boys versus girls. "], ["Bayesian Inference With Incomplete Multinomial Data: A Problem in Pathogen Diversity", " With recent advances in genetic analysis, it has become feasible to classify a pathogen into genetically distinct variants even though they apparently cause an infected subject similar symptoms. The availability of such data opens up the interesting problem of studying the spatiotemporal variation in the diversity of variants of a pathogen. Data on pathogen variants often suffer the problems of (i) low cell counts, (ii) incomplete classification due to laboratory problems (e.g., contamination), and (iii) unseen variants. Shannon\u2019s entropy may be used as a measure of variant diversity. A Bayesian approach can be used to deal with the problems of low cell counts and unseen variants. Bayesian analysis of incomplete multinomial data may be carried out by Markov chain Monte Carlo techniques. However, for pathogen-variant data, there often is only one source of missingness\u2014namely, some subjects are known to be infected by some unidentified pathogen variant. We point out that for incomplete data with disjoint sources of missingness, Bayesian analysis can be done more efficiently using an iid sampling scheme from the posterior distribution. We illustrate the method by analyzing a data set on the prevalence of bartonella infection among individual colonies of prairie dogs at the study site in Colorado between 2003 and 2006. We compare the result from the proposed Monte Carlo method with the results from other methods, including a model that entertains within-variant spatial correlation but no between-variant spatial correlation. This article has supplementary material online. "], ["Test of Association Between Two Ordinal Variables While Adjusting for Covariates", null], ["Generalized Functional Linear Models With Semiparametric Single-Index Interactions", " We introduce a new class of functional generalized linear models, where the response is a scalar and some of the covariates are functional. We assume that the response depends on multiple covariates, a finite number of latent features in the functional predictor, and interaction between the two. To achieve parsimony, the interaction between the multiple covariates and the functional predictor is modeled semiparametrically with a single-index structure. We propose a two-step estimation procedure based on local estimating equations, and investigate two situations: (a) when the basis functions are predetermined, e.g., Fourier or wavelet basis functions and the functional features of interest are known; and (b) when the basis functions are data driven, such as with functional principal components. Asymptotic properties are developed. Notably, we show that when the functional features are data driven, the parameter estimates have an increased asymptotic variance due to the estimation error of the basis functions. Our methods are illustrated with a simulation study and applied to an empirical dataset where a previously unknown interaction is detected. Technical proofs of our theoretical results are provided in the online supplemental materials. "], ["Tree-Structured Wavelet Estimation in a Mixed Effects Model for Spectra of Replicated Time Series", " This article develops a method for estimating the spectrum of a stationary process using time series traces recorded from experimental designs. Our procedure estimates the \u201ccommon\u201d log-spectrum and the variability over the traces (or subjects) using a mixed effects model. We combine spatially adaptive smoothing methods with recursive dyadic partitioning to construct a model for predicting subject-specific effects. The method is easy to implement and can handle large datasets because it uses the discrete wavelet transform which is computationally efficient. Numerical studies confirm that the proposed method performs very well despite its simplicity. The method is also applied to a multisubject electroencephalogram dataset. "], ["Latent Stick-Breaking Processes", " We develop a model for stochastic processes with random marginal distributions. Our model relies on a stick-breaking construction for the marginal distribution of the process, and introduces dependence across locations by using a latent Gaussian copula model as the mechanism for selecting the atoms. The resulting latent stick-breaking process (LaSBP) induces a random partition of the index space, with points closer in space having a higher probability of being in the same cluster. We develop an efficient and straightforward Markov chain Monte Carlo (MCMC) algorithm for computation and discuss applications in financial econometrics and ecology. This article has supplementary material online. "], ["Shortcuts for Locally Consonant Closed Test Procedures", null], ["Second-Order Comparison of Gaussian Random Functions and the Geometry of DNA Minicircles", " Given two samples of continuous zero-mean iid Gaussian processes on [0,1], we consider the problem of testing whether they share the same covariance structure. Our study is motivated by the problem of determining whether the mechanical properties of short strands of DNA are significantly affected by their base-pair sequence; though expected to be true, had so far not been observed in three-dimensional electron microscopy data. The testing problem is seen to involve aspects of ill-posed inverse problems and a test based on a Karhunen\u2013Lo\u00e8ve approximation of the Hilbert\u2013Schmidt distance of the empirical covariance operators is proposed and investigated. When applied to a dataset of DNA minicircles obtained through the electron microscope, our test seems to suggest potential sequence effects on DNA shape. Supplemental material available online. "], ["On Estimation of Partially Linear Transformation Models", " We study a general class of partially linear transformation models, which extend linear transformation models by incorporating nonlinear covariate effects in survival data analysis. A new martingale-based estimating equation approach, consisting of both global and kernel-weighted local estimation equations, is developed for estimating the parametric and nonparametric covariate effects in a unified manner. We show that with a proper choice of the kernel bandwidth parameter, one can obtain the consistent and asymptotically normal parameter estimates for the linear effects. Asymptotic properties of the estimated nonlinear effects are established as well. We further suggest a simple resampling method to estimate the asymptotic variance of the linear estimates and show its effectiveness. To facilitate the implementation of the new procedure, an iterative algorithm is developed. Numerical examples are given to illustrate the finite-sample performance of the procedure. Supplementary materials are available online. "], ["Design Sensitivity and Efficiency in Observational Studies", " An observational study attempts to draw inferences about the effects caused by a treatment when subjects are not randomly assigned to treatment or control as they would be in a randomized trial. After adjustments have been made for imbalances in measured covariates, the key source of uncertainty in an observational study is the possibility that subjects were not comparable prior to treatment in terms of some unmeasured covariate, so that differing outcomes in treated and control groups are not effects caused by the treatment. A sensitivity analysis asks about the magnitude of the departure from random assignment needed to alter the qualitative conclusions of the study, and the power of a sensitivity analysis and the design sensitivity anticipate the outcome of a sensitivity analysis under an assumed model for treatment effect. Lacking theoretical guidance, we tend to select statistical methods for use in observational studies based on their efficiency in randomized experiments. This turns out to be a mistake. A highly efficient method for detecting small treatment effects in randomized experiments need not, and often does not, have the highest power in a sensitivity analysis or the largest design sensitivity. That is, the best procedure assuming the observational study is a randomized experiment need not be the best procedure under more realistic assumptions. Small effects are sensitive to small biases, and methods targeted at detecting small effects in the absence of bias may not give due emphasis to evidence that the effect is stable and not small, and hence not easily attributed to small or moderate biases. The issue is illustrated in a practical example, is explored informally using a graphical heuristic, is studied asymptotically by determining the design sensitivity for a signed rank statistic with general scores, and is evaluated in a simulation. Among robust procedures with similar Pitman efficiencies for several distributions, there are large differences in design sensitivity and hence substantial differences in the power of a sensitivity analysis. "], ["Infinitesimal Robustness for Diffusions", null], ["A Framework for Feature Selection in Clustering", null], ["Grouping Pursuit Through a Regularization Solution Surface", " Extracting grouping structure or identifying homogenous subgroups of predictors in regression is crucial for high-dimensional data analysis. A low-dimensional structure in particular\u2014grouping, when captured in a regression model\u2014enables to enhance predictive performance and to facilitate a model\u2019s interpretability. Grouping pursuit extracts homogenous subgroups of predictors most responsible for outcomes of a response. This is the case in gene network analysis, where grouping reveals gene functionalities with regard to progression of a disease. To address challenges in grouping pursuit, we introduce a novel homotopy method for computing an entire solution surface through regularization involving a piecewise linear penalty. This nonconvex and overcomplete penalty permits adaptive grouping and nearly unbiased estimation, which is treated with a novel concept of grouped subdifferentials and difference convex programming for efficient computation. Finally, the proposed method not only achieves high performance as suggested by numerical analysis, but also has the desired optimality with regard to grouping pursuit and prediction as showed by our theoretical results. "], ["Likelihood Ratio Tests With Three-Way Tables", null], ["Global Partial Likelihood for Nonparametric Proportional Hazards Models", null], ["Dimension Reduction and Adaptation in Conditional Density Estimation", " An orthogonal series estimator of the conditional density of a response given a vector of continuous and ordinal/nominal categorical predictors is suggested. The estimator is based on writing a conditional density as a sum of orthogonal projections on all possible subspaces of reduced dimensionality and then estimating each projection via a shrinkage procedure. The shrinkage procedure uses a universal thresholding and a dyadic-blockwise shrinkage for low and high frequencies, respectively. The estimator is data-driven, is adaptive to underlying smoothness of a conditional density, and attains a minimax rate of the mean integrated squared error convergence. Furthermore, if a conditional density depends only on a subgroup of predictors, then the estimator seizes the opportunity and attains a corresponding minimax rate of convergence. The latter property relaxes the notorious \u201ccurse of dimensionality.\u201d Moreover, the estimator is fast, because neither projections nor shrinkages are computation-intensive. A numerical study for finite samples and a real example are presented. Our results indicate that the proposed estimation procedure is practical and has a rigorous theoretical justification. "], ["Posterior Simulation in Countable Mixture Models for Large Datasets", " Mixture models, or convex combinations of a countable number of probability distributions, offer an elegant framework for inference when the population of interest can be subdivided into latent clusters having random characteristics that are heterogeneous between, but homogeneous within, the clusters. Traditionally, the different kinds of mixture models have been motivated and analyzed from very different perspectives, and their common characteristics have not been fully appreciated. The inferential techniques developed for these models usually necessitate heavy computational burdens that make them difficult, if not impossible, to apply to the massive data sets increasingly encountered in real world studies. This paper introduces a flexible class of models called generalized P\u00f3lya urn (GPU) processes. Many common mixture models, including finite mixtures, hidden Markov models, and Dirichlet processes, are obtained as special cases of GPU processes. Other important special cases include finite-dimensional Dirichlet priors, infinite hidden Markov models, analysis of densities models, nested Chinese restaurant processes, hierarchical DP models, nonparametric density models, spatial Dirichlet processes, weighted mixtures of DP priors, and nested Dirichlet processes. An investigation of the theoretical properties of GPU processes offers new insight into asymptotics that form the basis of cost-effective Markov chain Monte Carlo (MCMC) strategies for large datasets. These MCMC techniques have the advantage of providing inferences from the posterior of interest, rather than an approximation, and are applicable to different mixture models. The versatility and impressive gains of the methodology are demonstrated by simulation studies and by a semiparametric Bayesian analysis of high-resolution comparative genomic hybridization data on lung cancer. The appendixes are available online as supplemental material. "], ["Inference in Semiparametric Regression Models Under Partial Questionnaire Design and Nonmonotone Missing Data", null], ["Using Evidence of Mixed Populations to Select Variables for Clustering Very High-Dimensional Data", " In this paper we develop a nonparametric approach to clustering very high-dimensional data, designed particularly for problems where the mixture nature of a population is expressed through multimodality of its density. Therefore, a technique based implicitly on mode testing can be particularly effective. In principle, several alternative approaches could be used to assess the extent of multimodality, but in the present problem the excess mass method has important advantages. We show that the resulting methodology for determining clusters is particularly effective in cases where the data are relatively heavy tailed or show a moderate to high degree of correlation, or when the number of important components is relatively small. Conversely, in the case of light-tailed, almost-independent components when there are many clusters, clustering in terms of modality can be less reliable than more conventional approaches. This article has supplementary material online. "], ["Tests for High-Dimensional Covariance Matrices", null], ["On Generating Monte Carlo Samples of Continuous\u00a0Diffusion Bridges", " Diffusion processes are widely used in engineering, finance, physics, and other fields. Usually continuous-time diffusion processes can be observed only at discrete time points. For many applications, it is often useful to impute continuous-time bridge samples that follow the diffusion dynamics and connect each pair of the consecutive observations. The sequential Monte Carlo (SMC) method is a useful tool for generating the intermediate paths of the bridge. The paths often are generated forward from the starting observation and forced in some ways to connect with the end observation. In this article we propose a constrained SMC algorithm with an effective resampling scheme guided by backward pilots carrying the information of the end observation. This resampling scheme can be easily combined with any forward SMC sampler. Two synthetic examples are used to demonstrate the effectiveness of the resampling scheme. "], ["Validating Stationarity Assumptions in Time Series Analysis by Rolling Local Periodograms", null], ["Simultaneous Confidence Bands for Penalized\u00a0Spline\u00a0Estimators", null], ["Nonparametric Analysis of Clustered Multivariate Data", null], ["Book Reviews", null], ["Letters to the Editor", null], ["Correction", null], ["Statistics: From Evidence to Policy", " The discipline of statistics, we as individual statisticians, and the American Statistical Association (ASA) have been called to action. We can and must play a vital part in the gathering, analysis, interpretation, and communication of evidence for informed policy decisions. I ask you to answer \u201cyes\u201d and to take on the role of an honest broker of policy alternatives. I ask you to support a proactive engagement by the ASA in informing policy. While interaction in policy forums by the ASA is an opportunity to integrate the science of statistics into policy, it also represents a responsibility, because the association serves not only its members, but also the public. Statistics can have a major impact in the journey from evidence to policy. A current illustration in the evidence-based medicine context is comparative effectiveness research, which received focus and funding from the American Recovery and Reinvestment Act of 2009. The time is now for us to take action as a discipline, as individuals, and as an association to ensure that policy decisions are driven by evidence. "], ["A Moving Average Approach for Spatial Statistical Models of Stream Networks", " In this article we use moving averages to develop new classes of models in a flexible modeling framework for stream networks. Streams and rivers are among our most important resources, yet models with autocorrelated errors for spatially continuous stream networks have been described only recently. We develop models based on stream distance rather than on Euclidean distance. Spatial autocovariance models developed for Euclidean distance may not be valid when using stream distance. We begin by describing a stream topology. We then use moving averages to build several classes of valid models for streams. Various models are derived depending on whether the moving average has a \u201ctail-up\u201d stream, a \u201ctail-down\u201d stream, or a \u201ctwo-tail\u201d construction. These models also can account for the volume and direction of flowing water. The data for this article come from the Ecosystem Health Monitoring Program in Southeast Queensland, Australia, an important national program aimed at monitoring water quality. We model two water chemistry variables, pH and conductivity, for sample sizes close to 100. We estimate fixed effects and make spatial predictions. One interesting aspect of stream networks is the possible dichotomy of autocorrelation between flow-connected and flow-unconnected locations. For this reason, it is important to have a flexible modeling framework, which we achieve on the example data using a variance component approach. "], ["Comment: Statistical Dependence in Stream Networks", " This note is based on an invited discussion of the article, \u201cA Moving Average Approach for Spatial Statistical Models on Stream Networks\u201d by Jay M. Ver Hoef and Erin E. Peterson. Ver Hoef and Peterson (hereafter VHP) have extended the idea of flow-related statistical dependence in streams to one where dependence may not respect flow, such as might happen when modeling data on fish in connected streams. We congratulate VHP for their innovative paper on using moving average models in stream networks. "], ["Comment", null], ["Rejoinder", null], ["Probabilistic Wind Speed Forecasting Using Ensembles and Bayesian Model Averaging", " The current weather forecasting paradigm is deterministic, based on numerical models. Multiple estimates of the current state of the atmosphere are used to generate an ensemble of deterministic predictions. Ensemble forecasts, while providing information on forecast uncertainty, are often uncalibrated. Bayesian model averaging (BMA) is a statistical ensemble postprocessing method that creates calibrated predictive probability density functions (PDFs). Probabilistic wind forecasting offers two challenges: a skewed distribution, and observations that are coarsely discretized. We extend BMA to wind speed, taking account of these challenges. This method provides calibrated and sharp probabilistic forecasts. Comparisons are made between several formulations. "], ["Regularized Reconstruction of Wave Height and Slope Fields From Refracted Images of Water", " Refractive imaging of wave fields in an established experimental technique. We consider the associated reconstruction problem and investigate some statistically motivated refinements, including (a) bias correction of local slope estimates, (b) regularization of directional slopes, (c) spatially weighted reconstruction using the estimated variability of local slope estimates, and (d) more accurate estimates of reference light profiles from time sequence data. These refinements are based on a nonparametric observational model for refractive imaging data. Simulation studies show that the refinements can result in substantial improvements in the mean squared error of reconstruction. A computationally efficient algorithm that exploits sparsity is used to evaluate the regularized estimator. Our approach is illustrated by an application to real image data. "], ["Bayesian and Frequentist Methods for Provider Profiling Using Risk-Adjusted Assessments of Medical Outcomes", " \u201cProvider profiling\u201d is the evaluation of the performance of hospitals, doctors, and other medical practitioners to enhance the quality of medical care. We propose a new method and compare conventional and Bayesian methodologies that are used or proposed for use for such \u201creport cards.\u201d Conventional statistical approaches to these provider assessments use likelihood-based frequentist methodologies, and the new Bayesian method is patterned after these. For each of three models, we compare the frequentist and Bayesian approaches using data used by the New York State Department of Health for its annually released reports that profile hospitals permitted to perform coronary artery bypass graft surgery. We use additional, constructed data sets to sharpen our conclusions. Comparisons across methods associated with different models are important because of current proposals to use random-effects (exchangeable) models for provider profiling. We also summarize and discuss important issues in the conduct of provider profiling, such as inclusion of provider characteristics in the model and choice of criteria for determining unsatisfactory performance. "], ["How Many People Do You Know?: Efficiently Estimating Personal Network Size", " In this article we develop a method to estimate both individual social network size (i.e., degree) and the distribution of network sizes in a population by asking respondents how many people they know in specific subpopulations (e.g., people named Michael). Building on the scale-up method of Killworth et al. (1998b) and other previous attempts to estimate individual network size, we propose a latent non-random mixing model which resolves three known problems with previous approaches. As a byproduct, our method also provides estimates of the rate of social mixing between population groups. We demonstrate the model using a sample of 1,370 adults originally collected by McCarty et al. (2001). Based on insights developed during the statistical modeling, we conclude by offering practical guidelines for the design of future surveys to estimate social network size. Most importantly, we show that if the first names asked about are chosen properly, the estimates from the simple scale-up model enjoy the same bias-reduction as the estimates from our more complex latent nonrandom mixing model. "], ["Scaling the Critics: Uncovering the Latent Dimensions of Movie Criticism With an Item Response Approach", " We study the critical opinions of expert movie reviewers as an item response problem. Building on earlier \u201cunfolding\u201d models, we develop a framework that models an individual\u2019s decision to approve or disapprove of an item. Using this approach, we are able to recover the locations of movies and ideal points of critics in the same multidimensional space. We demonstrate that a three-dimensional model captures much of the variation in critical opinions. The first dimension signifies movie \u201cquality\u201d while the other two connote the nature and subject matter of the films. We then demonstrate that the dimensions uncovered from our \u201cutility threshold model\u201d are statistically significant predictors of a movie\u2019s success, and are particularly useful in predicting the success of independent films. "], ["Resolving Contested Elections: The Limited Power of Post-Vote Vote-Choice Data", " In close elections, the losing side has an incentive to obtain evidence that the election result is incorrect. Sometimes this evidence comes in the form of court testimony from a sample of invalid voters, and this testimony is used to adjust vote totals (Belcher v. Mayor of Ann Arbor 1978; Borders v. King County 2005). However, while courts may be reluctant to make explicit findings about out-of-sample data (e.g., invalid voters that do not testify), when samples are used to adjust vote totals, the court is making such findings implicitly. In this paper, we show that the practice of adjusting vote totals on the basis of potentially unrepresentative samples can lead to incorrectly voided election results. More generally, we demonstrate that even when frame error and measurement error are minimal, random samples of post-vote vote-choice data can have limited power to detect incorrect election results without high response rates, precinct level polarization, or the acceptance of large Type I error rates. Therefore, in U.S. election disputes, even high-quality post-vote vote-choice data may be insufficient to resolve contested elections without the use of modeling assumptions (whether or not these assumptions are acknowledged). "], ["Powering Up With Space-Time Wind Forecasting", null], ["Local Post-Stratification in Dual System Accuracy and Coverage Evaluation for the U.S. Census", " We consider a local post-stratification approach to analyze the capture\u2013recapture dual system Accuracy and Coverage Evaluation (A.C.E.) data associated with the 2000 U.S. Census. The local post-stratification is carried out via a nonparametric regression estimation of the census enumeration and the correct enumeration functions. We propose a nonparametric population size estimator that is designed to accommodate some key aspects of the A.C.E.: missing values, erroneous enumerations, and extra covariates affecting the missingness and correct enumeration. The resulting estimates are compared with estimates from a conventional post-stratification and a logistic regression approach in an analysis on the 2000 Census A.C.E. data. "], ["A Bayesian Analysis of Body Mass Index Data From Small Domains Under Nonignorable Nonresponse and Selection", null], [null, " Technical details for the proofs in this article are available as supplementary material online. "], ["Multivariate Outlier Detection With High-Breakdown Estimators", " In this paper we develop multivariate outlier tests based on the high-breakdown Minimum Covariance Determinant estimator. The rules that we propose have good performance under the null hypothesis of no outliers in the data and also appreciable power properties for the purpose of individual outlier detection. This achievement is made possible by two orders of improvement over the currently available methodology. First, we suggest an approximation to the exact distribution of robust distances from which cut-off values can be obtained even in small samples. Our thresholds are accurate, simple to implement and result in more powerful outlier identification rules than those obtained by calibrating the asymptotic distribution of distances. The second power improvement comes from the addition of a new iteration step after one-step reweighting of the estimator. The proposed methodology is motivated by asymptotic distributional results. Its finite sample performance is evaluated through simulations and compared to that of available multivariate outlier tests. "], ["Marginal and Nested Structural Models Using Instrumental Variables", null], ["Approximate Methods for State-Space Models", null], ["Semiparametric Mean\u2013Covariance Regression Analysis for Longitudinal Data", " Efficient estimation of the regression coefficients in longitudinal data analysis requires a correct specification of the covariance structure. Existing approaches usually focus on modeling the mean with specification of certain covariance structures, which may lead to inefficient or biased estimators of parameters in the mean if misspecification occurs. In this article, we propose a data-driven approach based on semiparametric regression models for the mean and the covariance simultaneously, motivated by the modified Cholesky decomposition. A regression spline-based approach using generalized estimating equations is developed to estimate the parameters in the mean and the covariance. The resulting estimators for the regression coefficients in both the mean and the covariance are shown to be consistent and asymptotically normally distributed. In addition, the nonparametric functions in these two structures are estimated at their optimal rate of convergence. Simulation studies and a real data analysis show that the proposed approach yields highly efficient estimators for the parameters in the mean, and provides parsimonious estimation for the covariance structure. Supplemental materials for the article are available online. "], ["Highly Efficient Aggregate Unbiased Estimating Functions Approach for Correlated Data With Missing at Random", " We develop a consistent and highly efficient marginal model for missing at random data using an estimating function approach. Our approach differs from inverse weighted estimating equations (Robins, Rotnitzky, and Zhao 1995) and the imputation method (Paik 1997) in that our approach does not require estimating the probability of missing or imputing the missing response based on assumed models. The proposed method is based on an aggregate unbiased estimating function approach, which does not require the likelihood function; however, it is equivalent to the score equation if the likelihood is known. The aggregate-unbiased approach is based on the best linear approximation of efficient scores from the full dataset. We provide comparisons of the three approaches using simulated data and also a human immunodeficiency virus (HIV) data example. "], ["High-Dimensional Variable Selection for Survival Data", null], ["The Dependent Wild Bootstrap", " We propose a new resampling procedure, the dependent wild bootstrap, for stationary time series. As a natural extension of the traditional wild bootstrap to time series setting, the dependent wild bootstrap offers a viable alternative to the existing block-based bootstrap methods, whose properties have been extensively studied over the last two decades. Unlike all of the block-based bootstrap methods, the dependent wild bootstrap can be easily extended to irregularly spaced time series with no implementational difficulty. Furthermore, it preserves the favorable bias and mean squared error property of the tapered block bootstrap, which is the state-of-the-art block-based method in terms of asymptotic accuracy of variance estimation and distribution approximation. The consistency of the dependent wild bootstrap in distribution approximation is established under the framework of the smooth function model. In addition, we obtain the bias and variance expansions of the dependent wild bootstrap variance estimator for irregularly spaced time series on a lattice. For irregularly spaced nonlattice time series, we prove the consistency of the dependent wild bootstrap for variance estimation and distribution approximation in the mean case. Simulation studies and an empirical data analysis illustrate the finite-sample performance of the dependent wild bootstrap. Some technical details and tables are included in the online supplemental material. "], ["Statistical Agent-Based Models for Discrete Spatio-Temporal Systems", " Agent-based models have been used to mimic natural processes in a variety of fields, from biology to social science. By specifying mechanistic models that describe how small-scale processes function and then scaling them up, agent-based approaches can result in very complicated large-scale behavior while often relying on only a small set of initial conditions and intuitive rules. Although many agent-based models are used strictly in a simulation context, statistical implementations are less common. To characterize complex dynamic processes, such as the spread of epidemics, we present a hierarchical Bayesian framework for formal statistical agent-based modeling using spatiotemporal binary data. Our approach is based on an intuitive parameterization of the system dynamics and can explicitly accommodate directionally varying dispersal, long distance dispersal, and spatial heterogeneity. "], ["A Family of Distributions on the Circle With Links to, and Applications Arising From, M\u00f6bius Transformation", " We propose a family of four-parameter distributions on the circle that contains the von Mises and wrapped Cauchy distributions as special cases. The family is derived by transforming the von Mises distribution via a M\u00f6bius transformation, which maps the unit circle onto itself. The densities in the family have a symmetric or asymmetric, unimodal or bimodal shape, depending on the values of the parameters. Conditions for unimodality are explored. Further properties of the proposed model are obtained, many by applying the theory of M\u00f6bius transformation. Properties of a three-parameter symmetric submodel are investigated as well; these include maximum likelihood estimation, its asymptotics, and a reparameterization that proves useful quite generally. A three-parameter asymmetric subfamily, which often proves to be an adequate model, is also discussed, with emphasis on its mean direction and circular skewness. The proposed family and subfamilies are used to model an asymmetrically distributed data set and are then adopted as the angular error distribution of a circular\u2013circular regression model. Two applications of the latter are given. It is in this regression context that the M\u00f6bius transformation especially comes into its own. Comparisons with other families of circular distributions are made. Supplemental materials for this article are available online. "], ["Likelihood-Based Inference for Max-Stable Processes", " The last decade has seen max-stable processes emerge as a common tool for the statistical modeling of spatial extremes. However, their application is complicated due to the unavailability of the multivariate density function, and so likelihood-based methods remain far from providing a complete and flexible framework for inference. In this article we develop inferentially practical, likelihood-based methods for fitting max-stable processes derived from a composite-likelihood approach. The procedure is sufficiently reliable and versatile to permit the simultaneous modeling of marginal and dependence parameters in the spatial context at a moderate computational cost. The utility of this methodology is examined via simulation, and illustrated by the analysis of United States precipitation extremes. "], ["Dimension Reduction and Semiparametric Estimation of Survival Models", " In this paper, we propose a new dimension reduction method by introducing a nominal regression model with the hazard function as the conditional mean, which naturally retrieves information from complete data and censored data as well. Moreover, without requiring the linearity condition, the new method can estimate the entire central subspace consistently and exhaustively. The method also provides an alternative approach for the analysis of censored data assuming neither the link function nor the distribution. Hence, it exhibits superior robustness properties. Numerical studies show that the method can indeed be readily used to efficiently estimate survival models, explore the data structures and identify important variables. "], ["Alternative Goodness-of-Fit Tests for Linear Models", " Fan and Huang (2001) presented a goodness-of-fit test for linear models based on Fourier transformations of the residuals of the fitted model. We present two more theoretically appealing tests in which the Fourier transforms are incorporated into a fitted model. We show that when suitably normalized, the new test statistics have the same asymptotic distribution as Fan and Huang\u2019s test. We propose modifications to the asymptotic normalization constants to improve the small sample sizes of our tests while retaining their asymptotic distributions. Small sample sizes and powers are examined via simulations. An illustration is given. "], ["Semiparametric Efficient Estimation for a Class of Generalized Proportional Odds Cure Models", " We present a mixture cure model with the survival time of the \u201cuncured\u201d group coming from a class of linear transformation models, which is an extension of the proportional odds model. This class of model, first proposed by Dabrowska and Doksum (1988), which we term \u201cgeneralized proportional odds model,\u201d is well suited for the mixture cure model setting due to a clear separation between long-term and short-term effects. A standard expectation\u2013maximization algorithm can be employed to locate the nonparametric maximum likelihood estimators, which are shown to be consistent and semiparametric efficient. However, there are difficulties in the M-step due to the nonparametric component. We overcome these difficulties by proposing two different algorithms. The first is to employ an majorize-minimize (MM) algorithm in the M-step instead of the usual Newton\u2013Raphson method, and the other is based on an alternative form to express the model as a proportional hazards frailty model. The two new algorithms are compared in a simulation study with an existing estimating equation approach by Lu and Ying (2004). The MM algorithm provides both computational stability and efficiency. A case study of leukemia data is conducted to illustrate the proposed procedures. "], ["Regularization Parameter Selections via Generalized Information Criterion", " We apply the nonconcave penalized likelihood approach to obtain variable selections as well as shrinkage estimators. This approach relies heavily on the choice of regularization parameter, which controls the model complexity. In this paper, we propose employing the generalized information criterion, encompassing the commonly used Akaike information criterion (AIC) and Bayesian information criterion (BIC), for selecting the regularization parameter. Our proposal makes a connection between the classical variable selection criteria and the regularization parameter selections for the nonconcave penalized likelihood approaches. We show that the BIC-type selector enables identification of the true model consistently, and the resulting estimator possesses the oracle property in the terminology of Fan and Li (2001). In contrast, however, the AIC-type selector tends to overfit with positive probability. We further show that the AIC-type selector is asymptotically loss efficient, while the BIC-type selector is not. Our simulation results confirm these theoretical findings, and an empirical example is presented. Some technical proofs are given in the online supplementary material. "], ["Variational Inference for Large-Scale Models of Discrete Choice", " Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate inference possible, but the computational cost is prohibitive on the large datasets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. One version of the procedures relies on a new approximation to the variational objective function, based on the multivariate delta method. Extensive simulations, along with an analysis of real-world data, demonstrate that variational methods achieve accuracy competitive with Markov chain Monte Carlo at a small fraction of the computational cost. Thus, variational methods permit inference on datasets that otherwise cannot be analyzed without possibly adverse simplifications of the underlying discrete choice model. Appendices C through F are available as online supplemental materials. "], ["Weighted Generalized Estimating Functions for Longitudinal Response and Covariate Data That Are Missing at Random", " Longitudinal studies often feature incomplete response and covariate data. It is well known that biases can arise from naive analyses of available data, but the precise impact of incomplete data depends on the frequency of missing data and the strength of the association between the response variables and covariates and the missing-data indicators. Various factors may influence the availability of response and covariate data at scheduled assessment times, and at any given assessment time the response may be missing, covariate data may be missing, or both response and covariate data may be missing. Here we show that it is important to take the association between the missing data indicators for these two processes into account through joint models. Inverse probability-weighted generalized estimating equations offer an appealing approach for doing this. Here we develop these equations for a particular model generating intermittently missing-at-random data. Empirical studies demonstrate that the consistent estimators arising from the proposed methods have very small empirical biases in moderate samples. Supplemental materials are available online. "], ["Variable Selection With the Strong Heredity Constraint and Its Oracle Property", null], ["Linear Mixed-Effects Modeling by Parameter Cascading", " A linear mixed-effects model (LME) is a familiar example of a multilevel parameter structure involving nuisance and structural parameters, as well as parameters that essentially control the model\u2019s complexity. Marginalization over nuisance parameters, such as the restricted maximization likelihood method, has been the usual estimation strategy, but it can involve onerous and complex algorithms to achieve the integrations involved. Parameter cascading is described as a multicriterion optimization algorithm that is relatively simple to program and leads to fast and stable computation. The method is applied to LME, where well-developed marginalization methods are already available. Our results suggest that parameter cascading is at least as good as, if not better than, the available methods. We also extend the LME model to multicurve data smoothing by introducing a basis partitioning scheme and defining roughness penalty terms for both functional fixed effect and random effects. The results are substantially better than those obtained by using the previous LME methods. A supplemental document is available online. "], ["A Statistical Framework for Differential Privacy", null], ["Reduced Rank Mixed Effects Models for Spatially Correlated Hierarchical Functional Data", " Hierarchical functional data are widely seen in complex studies where subunits are nested within units, which in turn are nested within treatment groups. We propose a general framework of functional mixed effects model for such data: within-unit and within-subunit variations are modeled through two separate sets of principal components; the subunit level functions are allowed to be correlated. Penalized splines are used to model both the mean functions and the principal components functions, where roughness penalties are used to regularize the spline fit. An expectation\u2013maximization (EM) algorithm is developed to fit the model, while the specific covariance structure of the model is utilized for computational efficiency to avoid storage and inversion of large matrices. Our dimension reduction with principal components provides an effective solution to the difficult tasks of modeling the covariance kernel of a random function and modeling the correlation between functions. The proposed methodology is illustrated using simulations and an empirical dataset from a colon carcinogenesis study. Supplemental materials are available online. "], ["Weighted Distance Weighted Discrimination and Its Asymptotic Properties", null], ["Indirect Cross-Validation for Density Estimation", null], ["Robust Model-Free Multiclass Probability Estimation", " Classical statistical approaches for multiclass probability estimation are typically based on regression techniques such as multiple logistic regression, or density estimation approaches such as linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA). These methods often make certain assumptions on the form of probability functions or on the underlying distributions of subclasses. In this article, we develop a model-free procedure to estimate multiclass probabilities based on large-margin classifiers. In particular, the new estimation scheme is employed by solving a series of weighted large-margin classifiers and then systematically extracting the probability information from these multiple classification rules. A main advantage of the proposed probability estimation technique is that it does not impose any strong parametric assumption on the underlying distribution and can be applied for a wide range of large-margin classification methods. A general computational algorithm is developed for class probability estimation. Furthermore, we establish asymptotic consistency of the probability estimates. Both simulated and real data examples are presented to illustrate competitive performance of the new approach and compare it with several other existing methods. "], ["Book Reviews", null]]}