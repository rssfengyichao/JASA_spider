{"1996": [["Separate Estimation of Primary and Secondary Cancer Preventive Impact: Analysis of a Case-Control Study of Skin Self-Examination and Melanoma", "Case-control studies increasingly have been used to evaluate the impact of cancer screening strategies. In this context the intent of the screening test has been to reduce cancer mortality by early detection of cancers, permitting curative therapy in some patients who would die of the disease if diagnosis were delayed until the disease was detected clinically. This phenomenon is known as secondary prevention. In an analysis of a case-control study of skin self-examination (SSE) in reducing mortality from melanoma, it was recognized that the exposure (SSE) may encourage the removal of precancerous nevi (moles), thereby reducing cancer incidence (primary prevention). This article describes an analytic strategy for obtaining separate estimates of the primary and secondary preventive impact of the screening practice. The method is focused primarily on resolving the problem of lead-time bias, caused by the artifactual advancement of the time of diagnosis in cases detected by screening."], ["Parametric Event Sequence Analysis: An Application to an Analysis of Gender and Racial/Ethnic Differences in Patterns of Drug-Use Progression", "This article introduces novel statistical models for the sequence analysis of events. The models are formulated to analyze occurrence, association, and sequencing among events as an extension of log-linear models. A set of parameters characterizes marginal odds and odds ratios of frequencies summed across sequence patterns for each combination of the occurrence/nonoccurrence of events. These parameters are used for the analysis of the occurrence and association of events. Another set of parameters characterizes conditional odds and odds ratios among sequence patterns within each combination of the occurrence/nonoccurrence of events. These parameters are used for the analysis of sequencing of events. The models permit a decomposition of the likelihood function into a marginal likelihood component that includes only parameters for occurrence and association among events and a conditional likelihood component that includes only parameters for sequencing among events. The models are then extended further for regressions with covariates. An application analyzes gender and racial/ethnic differences in patterns of drug use progression. Sequential patterns of initiations and association among initiations are analyzed for three groups of drugs: alcoholic beverages, cigarettes, and marijuana. Findings that cross-validate previous findings based on different datasets and findings that are novel are reported."], ["Physiological Pharmacokinetic Analysis Using Population Modeling and Informative Prior Distributions", "We describe a general approach using Bayesian analysis for the estimation of parameters in physiological pharmacokinetic models. The chief statistical difficulty in estimation with these models is that any physiological model that is even approximately realistic will have a large number of parameters, often comparable to the number of observations in a typical pharmacokinetic experiment (e.g., 28 measurements and 15 parameters for each subject). In addition, the parameters are generally poorly identified, akin to the well-known ill-conditioned problem of estimating a mixture of declining exponentials. Our modeling includes (a) hierarchical population modeling, which allows partial pooling of information among different experimental subjects; (b) a pharmacokinetic model including compartments for well-perfused tissues, poorly perfused tissues, fat, and the liver; and (c) informative prior distributions for population parameters, which is possible because the parameters represent real physiological variables. We discuss how to estimate the models using Bayesian posterior simulation, a method that automatically includes the uncertainty inherent in estimating such a large number of parameters. We also discuss how to check model fit and sensitivity to the prior distribution using posterior predictive simulation. We illustrate the application to the toxicokinetics of tetrachloroethylene (perchloroethylene [PERC]), the problem that motivated this work."], ["A Random-Effects Model for Cycle Viability in Fertility Studies", "Models for fertility that take into account the timing of intercourse relative to ovulation are needed to estimate the influence of both endogenous and exogenous factors on human fertility. The classical model assumes that some menstrual cycles are \u201cviable\u201d and some are not, where \u201cviability\u201d is determined by whether hormonal, uterine, and gamete-related factors are favorable to gestation. Within each viable cycle, the various days with intercourse are assumed to act independently; within each nonviable cycle, the days with intercourse can have no effect. Cycle viability for individual cycles is latent in that it is not ascertainable when conception does not occur. The classical model neglects the statistical dependency of outcomes among menstrual cycles within individual couples. Current marginal approaches cannot determine the degree to which heterogeneity in fecundability is biologically based versus the degree to which it is secondary to variation in intercourse behavior from couple to couple. We describe a random-effects model based on assuming that the cycle viability probability varies from couple to couple according to a beta distribution, and we use an EM algorithm to fit the model. The proposed estimating procedure is fully expandable to allow covariate effects on the beta variate. Our method can be applied more generally whenever dependency among Bernoulli trials is induced by a susceptibility state and the outcomes can be observed only in the aggregate. Based on data from a cohort of couples with no known fertility problems who were attempting pregnancy, cycle viability is found to be heterogeneous among couples. Stratification on the presence or absence of prenatal exposure of the woman to her mother's cigarette smoking revealed a statistically significant difference in the two cycle viability distributions. We discuss differences in the interpretation of the beta model compared to the marginal approach based on generalized estimating equations."], ["Estimation of Median Income of Four-Person Families: A Bayesian Time Series Approach", "This article develops a general methodology for small domain estimation based on data from repeated surveys. The results are directly applied to the estimation of median income of four-person families for the 50 states and the District of Columbia. These estimates are needed by the U.S. Department of Health and Human Services (HHS) to formulate its energy assistance program for low income families. The U.S. Bureau of the Census, by an informal agreement, has provided such estimates to HHS through a linear regression methodology since the latter part of the 1970s. The current method is an empirical Bayes method (EB) that uses the Current Population Survey (CPS) estimates as well as the most recent decennial census estimates updated by the per capita income estimates of the Bureau of Economic Analysis. However, with the existing methodology, standard errors associated with these estimates are not easy to obtain. The EB estimates, when used naively, can lead to underestimation of standard errors. Moreover, because the sample estimates are collected through the CPS every year, there is a very natural time series aspect of the data that is currently ignored. We have performed a full Bayesian analysis using a hierarchical Bayes (HB) time series model. In addition to providing the median income estimates as the posterior means, we have provided also the posterior standard deviations. Included in our model is the information on the median incomes of three- and five-person families as well. In this way a multivariate HB procedure is used. The Bayesian analysis requires evaluation of high-dimensional integrals. We have overcome this problem by using the Gibbs sampling technique, which has turned out to be a very convenient tool for Monte Carlo integration. Also, we have validated our results by comparing them against the 1989 four-person median income figures obtained from the 1990 census. We used four different criteria for such comparisons. It turns out that the estimates obtained by using a bivariate time-series model are the best overall. We use a criterion based on deviances for model selection and also provide a sensitivity analysis of the proposed hierarchical model."], ["Time-Dependent Hazard Ratio: Modeling and Hypothesis Testing with Application in Lupus Nephritis", "We investigate the association between duration of untreated disease and survival in lupus nephritis, a rare rheumatologic disease. In this case, as in many other studies of survival, a priori considerations suggest that the effect of the predictor on hazard may change with increasing follow-up time. To accommodate such situations, we use regression splines to model the hazard ratio as a flexible function of time. We propose model-based tests of the hypotheses of hazards proportionality and of no association. We evaluate the accuracy of estimation and inference in simulations and also present analysis of a larger medical data set."], ["A Semiparametric Transformation Approach to Estimating Usual Daily Intake Distributions", "The distribution of usual intakes of dietary components is important to individuals formulating food policy and to persons designing nutrition education programs. The usual intake of a dietary component for a person is the long-run average of daily intakes of that component for that person. Because it is impossible to directly observe usual intake for an individual, it is necessary to develop an estimator of the distribution of usual intakes based on a sample of individuals with a small number of daily observations on a subsample of the individuals. Daily intake data for individuals are nonnegative and often very skewed. Also, there is large day-to-day variation relative to the individual-to-individual variation, and the within-individual variance is correlated with the individual means. We suggest a methodology for estimating usual intake distributions that allows for varying degrees of departure from normality and recognizes the measurement error associated with one-day dietary intakes. The estimation method contains four steps. First, the original data are standardized by adjusting for nuisance effects, such as day-of-week and interview sequence. Second, the daily intake data are transformed to normality using a combination of power and grafted polynomial transformations. Third, using a normal components-of-variance model, the distribution of usual intakes is constructed for the transformed data. Finally, a transformation of the normal usual intake distribution to the original scale is defined. The approach is applied to data from the 1985 Continuing Survey of Food Intakes by Individuals and works well for a set of dietary components that are consumed nearly daily and exhibit varying distributional shapes."], ["A New Perspective on Priors for Generalized Linear Models", "This article deals with specifications of informative prior distributions for generalized linear models. Our emphasis is on specifying distributions for selected points on the regression surface; the prior distribution on regression coefficients is induced from this specification. We believe that it is inherently easier to think about conditional means of observables given the regression variables than it is to think about model-dependent regression coefficients. Previous use of conditional means priors seems to be restricted to logistic regression with one predictor variable and to normal theory regression. We expand on the idea of conditional means priors and extend these to arbitrary generalized linear models. We also consider data augmentation priors where the prior is of the same form as the likelihood. We show that data augmentation priors are special cases of conditional means priors. With current Monte Carlo methodology, such as importance sampling and Gibbs sampling, our priors result in tractable posteriors."], ["The Effect of Improper Priors on Gibbs Sampling in Hierarchical Linear Mixed Models", "Often, either from a lack of prior information or simply for convenience, variance components are modeled with improper priors in hierarchical linear mixed models. Although the posterior distributions for these models are rarely available in closed form, the usual conjugate structure of the prior specification allows for painless calculation of the Gibbs conditionals. Thus the Gibbs sampler may be used to explore the posterior distribution without ever having established propriety of the posterior. An example is given showing that the output from a Gibbs chain corresponding to an improper posterior may appear perfectly reasonable. Thus one cannot expect the Gibbs output to provide a \u201cred flag,\u201d informing the user that the posterior is improper. The user must demonstrate propriety before a Markov chain Monte Carlo technique is used. A theorem is given that classifies improper priors according to the propriety of the resulting posteriors. Applications concerning Bayesian analysis of animal breeding data and the location of maxima of unwieldy (restricted) likelihood functions are discussed. Gibbs sampling with improper posteriors is then considered in more generality. The concept of functional compatibility of conditional densities is introduced and is used to construct an invariant measure for a class of Markov chains. These results are used to show that Gibbs chains corresponding to improper posteriors are, in theory, quite ill-behaved."], ["The Notion of \u201cComposite Reliability\u201d and its Hierarchical Bayes Estimation", "In this article we introduce the notion of \u201ccomposite reliability\u201d as a measure of the overall reliability of a collection of heterogeneous but similar items. We then propose a hierarchical Bayes model for estimating the composite reliability of systems whose lifelengths are expressed as binary random variables. Subsequently, we develop an alternative approach for the simultaneous inference about many small but related binomial parameters. We propose a two-stage prior for addressing problems of this type and use the Gibbs sampler algorithm for addressing problems involving small proportions. Our topic, though suggested by an issue pertaining to the safety of nuclear power plants, arises in other commonly occurring situations pertaining to consumerism, public policy government regulation. Our development generalizes for situations involving lifelengths that need not be binary."], ["Predictive Oscillation Patterns: A Synthesis of Methods for Spatial-Temporal Decomposition of Random Fields", "Spatial-temporal decompositions of climatologic fields have been obtained using a range of techniques, including principal component analysis (PCA) and principal oscillation patterns (POPS). PCA decompositions are forced to be correlated to the original field, but they may not capture interesting aspects of temporal variation. On the other hand, POPS decompositions focus on temporal variation but are not forced to correlate to the field. Here we introduce a hybrid of these methods that attempts to retain desirable aspects of both PCA and POPS. The approach attempts to project the field onto a lower dimensional subspace with the property that the average error associated with forecasting a future state of the field on the basis of the history contained in the projection is minimized. A recursive algorithm for estimating a spatial-temporal decomposition based on this idea is developed. The methodology is applied to a 47-year climatological record of the 5-day average 500-millibar-height anomaly field, sampled on a 445 grid over the Northern Hemisphere extra-tropics. Some asymptotic properties of the estimation method for the new technique are examined in a simple situation. Although the estimation method requires a consistent estimator of a certain spectral density matrix, the target parameters are estimated at a parametric rate. Interestingly, the details of the nonparametric estimation of the spectral density, such as the choice of the smoothing kernel, do not appear to affect the asymptotic variance of the target parameters."], ["Nonhomogeneity Analysis Using Borrowed Strength", "This article develops a \u201cborrowed strength\u201d methodology for estimating local probability densities in a random field. Under a piecewise stationarity condition similarities between the densities in different regions of the field can be exploited by estimating a global mixture density and imposing the parameter-space support of this borrowed strength estimate on the local estimation problems. The local estimates are then used in an analysis of the homogeneity of the field, which is shown to benefit from the method of borrowing strength."], ["Modeling Flat Stretches, Bursts Outliers in Time Series Using Mixture Transition Distribution Models", null], ["Bootstrap Confidence Regions for the Intensity of a Poisson Point Process", null], ["On Locally Adaptive Density Estimation", null], ["A Frequency Domain Bootstrap-Based Method for Checking the Fit of a Transfer Function Model", "Transfer function models have been used for modeling the linear dependence between stationary time series and for improving the accuracy of forecasts. This article proposes a method to evaluate the fit of such a model by comparing certain frequency domain functionals of the model with those of the data using a model-based bootstrap algorithm. The method works by checking the ability of the fitted model to reproduce the dynamic relationship and the linear dependence structure of the data. Furthermore, the method addresses the problem of the possible sources of model inadequacy. The asymptotic validity of the bootstrap procedure used to evaluate the distribution of the statistics considered is proved some examples illustrating the ability of the proposed method to check the overall fit and to detect sources of model inadequacy are given."], ["On Periodic Structures and Testing for Seasonal Unit Roots", "The standard testing procedures for seasonal unit roots developed so far have been based mainly on time-invariant autoregressive integrated moving average (ARIMA) processes with AR polynomials involving seasonal differencing. One attractive alternative is to use periodic ARMA models. Convenient procedures are presented for testing for the presence of unit roots at the zero and seasonal frequencies in periodic time series. The limiting distributions are derived and tabulated simulation evidence illustrates the advantages of allowing for periodicity. The tests are illustrated via applications to macroeconomic and ozone level data."], ["Quantile Estimation from Repeated Measurements", "Quantile estimators for a nonparametric components of variance situation are proposed consistency and asymptotic normality are proved. Situations with different numbers of measurements for different subjects are considered. Measurements on separate subjects are assumed to be independent, whereas measurements on the same subject have a fixed dependence. The estimators are obtained by inverting weighted empirical distribution functions. An \u201coptimal\u201d estimator and a simple estimator based on within-subject averages are studied. Small-sample properties are studied by simulation as an illustration the estimators are applied to give normal limits for differential light sensitivity of the human eye."], ["Density Estimation with Bivariate Censored Data", "In this article we construct a kernel estimate of the probability density function from bivariate data that have been randomly censored. We study the large-sample properties of the proposed estimator using a strong approximation result. We establish consistency and asymptotic normality and give a convenient representation of the kernel density estimator. Simulation studies show that the proposed procedure gives a good estimate of the true density function even when the sample size is moderate. We discuss various issues about implementation of the estimator, including bandwidth selection and boundary effects. The procedure can be generalized to higher dimensional variables in a straightforward manner."], ["A Generalization of the Weibull Distribution with Application to the Analysis of Survival Data", "The Weibull distribution, which is frequently used for modeling survival data, is embedded in a larger family obtained by introducing an additional shape parameter. This generalized family not only contains distributions with unimodal and bathtub hazard shapes, but also allows for a broader class of monotone hazard rates. Furthermore, the distributions in this family are analytically tractable and computationally manageable. The modeling and analysis of survival data using this family is discussed and illustrated in terms of a lifetime dataset and the results of a two-arm clinical trial."], ["Smoothing Hazard Functions and Time-Varying Effects in Discrete Duration and Competing Risks Models", "State-space or dynamic approaches to discrete or grouped duration data with competing risks or multiple terminating events allow simultaneous modeling and smooth estimation of hazard functions and time-varying effects in a flexible way. Full Bayesian or posterior mean estimation, using numerical integration techniques or Monte Carlo methods, can become computationally rather demanding or even infeasible for higher dimensions and larger datasets. Therefore, based on previous work on filtering and smoothing for multicategorical time series and longitudinal data, our approach uses posterior mode estimation. Thus we have to maximize posterior densities or, equivalently, a penalized likelihood, which enforces smoothness of hazard functions and time-varying effects by a roughness penalty. Dropping the Bayesian smoothness prior and adopting a nonparametric viewpoint, one might also start directly from maximizing this penalized likelihood. We show how Fisher scoring smoothing iterations can be carried out efficiently by iteratively applying linear Kalman filtering and smoothing to a working model. This algorithm can be combined with an EM-type procedure to estimate unknown smoothing parameters or hyperparameters. The methods are applied to a larger set of unemployment duration data with one terminating event and, in a further analysis, multiple terminating events from the German socioeconomic panel GSOEP."], ["On Runs and Longest Run Tests: A Method of Finite Markov Chain Imbedding", "In this article, under very general assumptions on Bernoulli trials, a simple numerical method based on the finite Markov chain imbedding technique is systematically developed to determine (a) the joint distribution of the number of success runs and the number of successes; (b) the conditional distribution of the number of success runs given the number of successes, that is, the exact distribution of the success runs test statistic; (c) the joint distribution of the length of the longest success run and the number of successes; and (d) the conditional distribution of the length of the longest success run given the number of successes, that is, the exact distribution of the longest success run test statistic. The critical regions and powers of these two tests (success runs and longest success run) are derived under the null hypothesis of independence and identical distribution (iid) as well as under the alternative hypothesis of Markov dependence. The application of the success runs test is illustrated using real datasets from a multicenter risk factor study on asthmatics."], ["Tests of Independence, Treatment Heterogeneity Dose-Related Trend with Exchangeable Binary Data", null], ["Interdirection Tests for Simple Repeated-Measures Designs", null], ["On Rank Transformation Techniques for Balanced Incomplete Repeated-Measures Designs", null], ["Testing for Polynomial Regression Using Nonparametric Regression Techniques", null], ["Direct Semiparametric Estimation of Single-Index Models with Discrete Covariates", null], ["Combining Estimates in Regression and Classification", "We consider the problem of how to combine a collection of general regression fit vectors to obtain a better predictive model. The individual fits may be from subset linear regression, ridge regression, or something more complex like a neural network. We develop a general framework for this problem and examine a cross-validation\u2014based proposal called \u201cmodel mix\u201d or \u201cstacking\u201d in this context. We also derive combination methods based on the bootstrap and analytic methods and compare them in examples. Finally, we apply these ideas to classification problems where the estimated combination weights can yield insight into the structure of the problem."], ["Measures of the Sensitivity of Regression Estimates to the Choice of Estimator", null], ["Tail Index Estimation, Pareto Quantile Plots Regression Diagnostics", "Successful application of extreme value statistics for estimating the Pareto tail index relies heavily on the choice of the number of extreme values taken into account. It is shown that these tail index estimators can be considered estimates of the slope at the right upper tail of a Pareto quantile plot, obtained using a weighted least squares algorithm. From this viewpoint, based on classical ideas on regression diagnostics, algorithms can be constructed searching for that order statistic to the right of which one obtains an optimal linear fit of the quantile plot."], ["Normal Scores, Normal Plots Tests for Normality", "In this article we develop new plotting positions for normal plots. The use of the plots usually centers on detecting irregular tail behavior or outliers. Along with the normal plot, we develop tests for various departures from normality, especially for skewness and heavy tails. The tests can be considered as components of a Shapiro-Wilk type test that has been decomposed into different sources of nonnormality. Convergence to the limiting distributions is slow, so finite sample corrections are included to make the tests useful for small sample sizes."], ["A Likelihood Ratio Test against Stochastic Ordering in Several Populations", "The likelihood ratio test is often used to test hypotheses involving a stochastic ordering. Distribution theory for the likelihood ratio test has been developed only for two stochastically ordered distributions. For testing equality of distributions against a stochastic ordering in several populations, this paper derives the null asymptotic distribution of the likelihood ratio test statistic, which is characterized by minimization problems and has no closed form. A Monte Carlo simulation is conducted to study the limiting distribution. Because the limiting distribution depends on the specific values of the unknown distributions under the null hypothesis, asymptotic and bootstrap approaches are proposed to overcome practical difficulties and implement tests based on the likelihood principle. Asymptotic validities for these tests are established and simulations are carried out to check their performances for finite sample sizes. The tests are applied to an example involving data for survival time for carcinoma of the oropharynx."], ["Estimation of Survival Functions under Uniform Stochastic Ordering", null], ["A Test in the Presence of Nuisance Parameters", null], ["Control Charts for Dependent and Independent Measurements Based on Bootstrap Methods", "Shewhart charts are widely accepted as standard tools for monitoring manufacturing processes of univariate, independent \u201cnearly\u201d normal measurements. They are not as well developed beyond these types of data. We generalize the idea of Shewhart charts to cover other types of data commonly encountered in practice. More specifically, we develop some valid control charts for dependent data and for independent data that are not necessarily \u201cnearly\u201d normal. We derive the proposed charts from the moving blocks bootstrap and the standard bootstrap methods. Their constructions are completely nonparametric no distributional assumptions are required. Some simulated as well as real data examples are included they are very supportive of the proposed methods."], ["A Markov Chain Model for the Multivariate Exponentially Weighted Moving Averages Control Chart", "A Markov chain approximation is used to determine the run length performance of a multivariate statistical process control chart. The Markov chain approach is widely used in the analysis of univariate control charts we extend the advantages of this type of analysis to a multivariate exponentially weighted moving averages control chart. The analysis can be applied whenever the multivariate control statistic can be modeled as a Markov chain and the run length performance depends on the off-target mean only through the noncentrality parameter."], ["Combining Independent Studies in a Calibration Problem", null], ["Minimum Hellinger Distance Estimation for Finite Mixture Models", "Minimum Hellinger distance estimates are considered for finite mixture models when the exact forms of the component densities are unknown in detail but are thought to be close to members of some parametric family. Minimum Hellinger distance estimates are asymptotically efficient if the data come from a member of the parametric family and are robust to certain departures from the parametric family. A new algorithm is introduced that is similar to the EM algorithm a specialized adaptive density estimate is also introduced. Standard measures of robustness are discussed some difficulties are noted. The robustness and asymptotic efficiency of the estimators are illustrated using simulations."], ["Optimal Estimation for Response-Dependent Retrospective Sampling", null], [null, null], ["Regularized Gaussian Discriminant Analysis through Eigenvalue Decomposition", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["A General Representation of Equally Correlated Variates", null], ["Addendum and Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["The Bayesian Modeling of Covariates for Population Pharmacokinetic Models", null], ["Causal Inference in a Placebo-Controlled Clinical Trial with Binary Outcome and Ordered Compliance", "We propose a likelihood-based method to analyze the causal effect of partial compliance (i.e., unplanned partial exposure to treatment or placebo) in the LRC-CPPT data, a prevention trial with long term follow-up previously analyzed by Efron and Feldman. Initially, we construct ordered compliance categories and dichotomize response. Assuming increased exposure to cholestyramine does not increase cholesterol, we estimate exposure\u2014response curves in different compliance subsets. Subjects in different arms with similar levels of compliance to the assignment may have a different placebo prognosis (i.e., success probability under a possible zero exposure level). The sole assumption that the placebo group reflects response to zero exposure for the treatment group as a whole allows estimation of a causal parameter in a special case only. When a single parameter represents the association between responses to possible treatment exposures and treatment compliance, simple estimates are derived for a set of causal parameters. The example is analyzed in detail, and more general applicability and extensions of the method are discussed."], ["Estimating the HIV Vertical Transmission Rate and the Pediatric AIDS Incubation Period from Prospective Data", "This article considers the estimation of the rate of vertical human immunodeficiency virus (HIV) transmission and the pediatric acquired immune deficiency syndrome (AIDS) incubation period using data from birth cohort studies. Standard methods of analysis, which ignore children whose infection status is not established, may lead to biased estimates. Methods based on modeling the disappearance of HIV antibody or appearance of virus are inefficient, as they essentially rely on a single variable. We describe an alternative model that takes into account clinical, immunological, and virological data. Maximum likelihood estimates of rate of vertical transmission, sensitivity of virus tests, pediatric AIDS incubation period, and age distribution at antibody loss are readily obtained by an EM algorithm. The method was applied to data from the European Collaborative Study and revealed evidence of temporal changes in the transmission rate and AIDS incubation period. This new approach, with possible modification, should allow efficient analysis of data from randomized controlled trials of interventions to reduce vertical transmission. It may also be applicable to other vertically transmissible infections."], ["An Image Analysis Problem in Electron Microscopy", "This article considers the problem of identifying the irregular boundary of a magnetic domain in a thin multilayer film, using data in the form of an electron micrograph. Two approaches are illustrated, both of which are based on the concept of two smooth intensity surfaces, one corresponding to the domain and one corresponding to the background, so that the objective is to determine the boundary between the parts of the image where the two surfaces are present. In the first, a hierarchical Bayesian approach, priors are assumed for the set of domain/background states and for the two intensity surfaces, and restoration is carried out using a version of Besag's ICM procedure. It is shown that it is important to initialize the method efficiently. The second approach uses a template-like model for the boundary and also relies on a Bayesian approach. The results from the second approach can be used as an end in themselves or as a way of initializing the more flexible first approach. Several illustrations are provided."], ["Bayesian Inference in Mixtures-of-Experts and Hierarchical Mixtures-of-Experts Models with an Application to Speech Recognition", "Machine classification of acoustic waveforms as speech events is often difficult due to context dependencies. Here a vowel recognition task with multiple speakers is studied via the use of a class of modular and hierarchical systems referred to as mixtures-of-experts and hierarchical mixtures-of-experts models. The statistical model underlying the systems is a mixture model in which both the mixture coefficients and the mixture components are generalized linear models. A full Bayesian approach is used as a basis of inference and prediction. Computations are performed using Markov chain Monte Carlo methods. A key benefit of this approach is the ability to obtain a sample from the posterior distribution of any functional of the parameters of the given model. In this way, more information is obtained than can be provided by a point estimate. Also avoided is the need to rely on a normal approximation to the posterior as the basis of inference. This is particularly important in cases where the posterior is skewed or multimodal. Comparisons between a hierarchical mixtures-of-experts model and other pattern classification systems on the vowel recognition task are reported. The results indicate that this model showed good classification performance and also gave the additional benefit of providing for the opportunity to assess the degree of certainty of the model in its classification predictions."], ["On Procedures for Evaluating the Effectiveness of Reinterview Survey Methods: Application to Labor Force Data", "Several types of reinterview surveys have been designed to measure the different kinds of response errors that occur in survey data. This article is based on the reinterview procedures conducted by the U.S. Census Bureau in its monthly Current Population Survey. We present estimates of the misclassification error rates associated with the original survey data collection procedures and the reinterview methods. A new procedure enables us to estimate the dependence between the error rates of the original survey and the initial nonreconciled reinterview when the reconciled reinterview method is used. Our approach extends to the survey setting methods previously developed for estimating error rates of diagnostic tests. This new approach indicates that the response-variance reinterview methods are effective in replicating the original survey's error rates; however, the reconciled reinterview method creates a substantial dependence between the original and the initial nonreconciled reinterview error rates."], ["The \u201cWindow Problem\u201d in Studies of Children's Attainments: A Methodological Exploration", "We conclude that single-year and limited duration window variables serve as weak proxies for information describing the entire childhood experience, and often lead to inferences of effects that may be misleading; we draw the implications of this finding for future data collection and research."], ["Graphics for Regressions with a Binary Response", "Central dimension-reduction subspaces, which characterize the dependence of a response variable on one or more predictors, are developed and then used to guide the construction and interpretation of graphics for regression problems with a binary response variable. Graphical methods requiring neither a link function nor residuals are suggested for both development and criticism of model components implied by the central dimension-reduction subspace."], ["The Signed Root Deviance Profile and Confidence Intervals in Maximum Likelihood Analysis", null], ["Instrumental Variable Estimation in Generalized Linear Measurement Error Models", "Instrumental variable estimation in generalized linear measurement error models are studied. For models with canonical link functions, unbiased estimating equations are derived. The maximum likelihood estimator for the normal theory, structural linear instrumental variable model is shown to be a solution to the estimating equations derived herein. Logistic regression is studied in detail. An example is given and a simulation study described for the logistic model based on the Framingham Heart Study data."], ["Bias Correction in Generalized Linear Mixed Models with Multiple Components of Dispersion", "General formulas are derived for the asymptotic bias in regression coefficients and variance components estimated by penalized quasi-likelihood (PQL) in generalized linear mixed models with canonical link function and multiple sets of independent random effects. Easily computed correction matrices result in variance component estimates that have satisfactory asymptotic behavior for small values of the variance components and significantly reduce bias for larger values. Both first-order and second-order correction procedures are developed for regression coefficients estimated by PQL. The methods are illustrated through an analysis of an experiment on salamander matings involving crossed male and female random effects, and their properties are evaluated in a simulation study."], ["On the Partitioning of Goodness-of-Fit Statistics for Multivariate Categorical Response Models", "Numerical and asymptotic stochastic partitioning of goodness-of-fit statistics are considered for a broad class of simultaneous multivariate categorical response models. These simultaneous models impose constraints on the joint and marginal distributions of categorical response variables. Under certain conditions, the tenability of the corresponding simultaneous hypothesis can be assessed by separately testing the two subhypotheses: one regarding the joint distributions and the other regarding the marginal distributions. Specifically, easily verifiable sufficient conditions are introduced that allow us to partition the overall goodness-of-fit statistic into two interesting goodness-of-fit statistics: one for testing whether the joint distribution model holds and the other for testing whether the marginal distribution model holds. Moreover, it is proven that when the sufficient conditions hold and the simultaneous hypothesis is true, the two component goodness-of-fit statistics are asymptotically independent. These results are illustrated through several examples."], ["Marginal Regression Models for Clustered Ordinal Measurements", "This article constructs statistical models for clustered ordinal measurements. We specify two regression models: one for the marginal means and one for the marginal pairwise global odds ratios. Of particular interest are problems in which the odds ratio regression is a focus. Simple assumptions about higher-order conditional moments give a quadratic exponential likelihood function with second-order estimating equations (GEE2) as score equations. But computational difficulty can arise for large clusters when both the mean response and the association between measures is of interest. First, we present GEE1 as an alternative estimation strategy. Second, we extend to repeated ordinal measurements the method developed by Carey et al. for binary observations that is based on alternating logistic regressions (ALR) for the marginal mean parameters and the pairwise log-odds ratio parameters. We study the efficiency of GEE1 and ALR relative to full maximum likelihood. We demonstrate the utility of our regression methods for ordinal data by applying the methods to a surgical follow-up study."], ["Permutation Distributions via Generating Functions with Applications to Sensitivity Analysis of Discrete Data", "Generating functions provide a simple and elegant way to describe probability or frequency distributions of discrete statistics and, in particular, permutation distributions. They are also a computational tool. Many efficient algorithms, including those described as fast Fourier transform methods, network methods, and multiple shift methods, are different implementations of the recursions needed to evaluate generating functions efficiently. Our goals here are twofold. First, we make the relationship between these efficient methods and generating functions explicit; this establishes a language for looking at other questions in randomization/exact inference and may help in finding more efficient implementations. Second, we propose methods to examine the sensitivity of results of exact analysis of discrete data to small perturbations in the data. Specifically, we consider two settings: how the analysis would change if one outcome changed, and how the analysis would change if one observation was added to the data set. Many of the computations needed to do a single exact analysis can be reused to study sensitivity; looking at this problem as one of computing generating functions makes the relationship explicit."], ["Identification of Outliers in Multivariate Data", "New insights are given into why the problem of detecting multivariate outliers can be difficult and why the difficulty increases with the dimension of the data. Significant improvements in methods for detecting outliers are described, and extensive simulation experiments demonstrate that a hybrid method extends the practical boundaries of outlier detection capabilities. Based on simulation results and examples from the literature, the question of what levels of contamination can be detected by this algorithm as a function of dimension, computation time, sample size, contamination fraction, and distance of the contamination from the main body of data is investigated. Software to implement the methods is available from the authors and STATLIB."], ["Improved Pivotal Methods for Constructing Confidence Regions with Directional Data", "The importance of pivoting is well established in the context of nonparametric confidence regions. It ensures enhanced coverage accuracy. However, pivoting for directional data cannot be achieved simply by rescaling. A somewhat cumbersome pivotal method, which involves passing first into a space of higher dimension, has been developed by Fisher and Hall for samples of unit vectors. Although that method has some advantages over nonpivotal techniques, it does suffer from certain drawbacks\u2014in particular, the operation of passing to a higher dimension. Here we suggest alternative pivotal approaches, the implementation of which does not require us to increase the intrinsic dimension of the data and which in practice seem to achieve greater coverage accuracy. These methods are of two types: new pivotal bootstrap techniques and techniques that exploit the \u201cimplicit pivotalness\u201d of the empirical likelihood algorithm. Unlike the method proposed by Fisher and Hall, these methods are also applicable to axial data and lead to the first pivotal, small-sample nonparametric confidence methods for mean principal or polar axes."], ["Confidence Intervals from Monte Carlo Tests", "Monte Carlo tests have exact level when the distribution of the test statistic is free of nuisance parameters. Confidence sets obtained by inverting such tests are also exact but may have a complicated structure. The sets reduce to intervals if the sampling can be organized in a special way. A sufficient condition is that the test statistic is monotone in the parameter of interest when all random drawings are kept fixed. Examples given include models from the one-parameter exponential class. A simple theory quantifying the impact of Monte Carlo uncertainty is also developed."], ["A Wavelet Shrinkage Approach to Tomographic Image Reconstruction", "A method is proposed for reconstructing images from tomographic data with respect to a two-dimensional wavelet basis. The Wavelet-vaguelette decomposition (WVD) is used as a framework within which expressions for the necessary wavelet coefficients may be derived. These coefficients are calculated using a version of the filtered back-projection algorithm as a computational tool, in a multiresolution fashion. The necessary filters are defined in terms of the underlying wavelets. Denoising is accomplished through an adaptation of the wavelet shrinkage (WS) approach of Donoho et al. and amounts to a form of regularization. Combining these two steps yields the proposed WVD/WS reconstruction algorithm, which is compared to the traditional filtered backprojection method in a small study."], ["The Directional Neighborhoods Approach to Contextual Classification of Images from Noisy Data", "The directional neighborhoods approach (DNA) to classifying pixels and reconstructing images from remotely sensed noisy data is a newly proposed computer-intensive procedure that is partly Bayesian and partly data analytic. It uses the observational data to select an optimal, generally asymmetric, but relatively homogeneous neighborhood for contextually classifying pixels. A criterion for \u201chomogeneity of neighborhood\u201d is developed. DNA involves two stages: a zero-neighbor preclassification stage, followed by selection of the most homogeneous neighborhood, and then a final classification. We provide Monte Carlo simulations for a two-population image and compare DNA results with those from a reference Bayesian contextual classification. We show that DNA improves substantially on the reference classification procedure."], ["Bayesian Inference of Survival Probabilities, under Stochastic Ordering Constraints", "In the statistical analysis of survival data arising from two populations, it often happens that the analyst knows, a priori, that the life lengths in one population are stochastically shorter than those in the other. Nevertheless, survival probability estimates, if determined separately from the corresponding samples, may not be consistent with this prior assumption, because of inherent statistical variability in the observations. This problem has been considered in a number of papers during the past decade, by adopting a (generalized) maximum likelihood approach. Our approach is Bayesian and, in essence, nonparametric. The a priori assumption regarding stochastic ordering is formulated naturally in terms of a joint prior distribution defined for pairs of survival functions. Nonparametric specification of the model, based on hazard rates and using a few hyperparameters, allows for sufficient flexibility in practical applications. The numerical computations are based on a coupled version of the Metropolis\u2014Hastings algorithm. The results from a statistical analysis are summarized nicely by a pair of predictive survival functions that are consistent with the assumed stochastic ordering."], ["On Estimation for Monotone Dose\u2014Response Curves", "Estimating monotone dose\u2014response curves with independent, normal errors has been studied in detail in the literature. A simple procedure using a generalization of the studentized maximum modulus technique is proposed for constructing confidence bands for monotone regression functions. This procedure is shown to compare well with its predecessors. The center of this smooth and efficient confidence band is then used to estimate a smooth monotone regression function. Its pointwise mean squared errors compare favorably with the maximum likelihood estimate, a weighted average of the isotonic regression and the least squares line, and the rank-transformed regression. A numerical example of a binding inhibition assay is included to illustrate the proposed interval and point estimation procedures."], ["Efficient Estimation of Ordered Probit Models", null], ["Bootstrap Selection of the Smoothing Parameter in Nonparametric Hazard Rate Estimation", "An asymptotic representation of the mean weighted integrated squared error for the kernel-based estimator of the hazard rate in the presence of right-censored samples is obtained for different bootstrap resampling methods. As a consequence, a new bandwidth selector based on the bootstrap is introduced. Very satisfactory simulations results are obtained in comparison to the cross-validation selector for different models, using WARPed (i.e., binned) versions of the estimators."], ["A Smooth Nonparametric Estimate of a Mixing Distribution Using Mixtures of Gaussians", null], ["Bivariate Estimation with Right-Truncated Data", null], ["Nonparametric Estimation and Regression Analysis with Left-Truncated and Right-Censored Data", null], ["The Product-Moment Correlation Coefficient and Linear Regression for Truncated Data", "The random truncation model has been considered extensively in the literature. Tsai has noted that many previous results hold under the weaker assumption of quasi-independence between the failure time and the truncation time in the observable region of truncated data. We generalize the Pearson product-moment correlation coefficient to measure the association between both time variables in the observable region. We show that if the failure time and the truncation time follow a truncated bivariate normal distribution, then a zero value of the generalized correlation coefficient is equivalent to the quasi-independence. We propose a corresponding sample correlation coefficient and consider its asymptotic behavior. We also study an application of quasi-independence to truncated linear regression with its asymptotic results. The proposed estimator, stemming directly from the least-squares approach, is computationally much simpler and has a natural extension to multiple linear regression. A simulation study shows that the proposed estimator for regression slope competes well with available nonparametric estimators."], ["Equivalence and Interval Testing for Lehmann's Alternative", "Equivalence and interval tests for Lehmann's alternative that extend the well-known Savage test for one-sided hypotheses are proposed. The proposed tests are shown to be unbiased with a strictly unimodal power function, provided the sample sizes in both treatment groups are equal. By means of a numerical investigation of the bias in the case of unequal sample sizes that are not too far apart, the suggested tests still turn out to provide practicable solutions. Because the computational effort to perform the suggested tests is considerable, tables containing the critical values are displayed to perform these tests easily. A numerical analysis of the power function of the interval test establishes this procedure as a powerful tool for detection of a significantly relevant difference in the small-sample case. In contrast to the case of interval testing, the fact arises that the performance of a powerful equivalence study under Lehmann's alternative requires an extensive amount of data. Because the proposed tests are based on the locally optimal scores under Lehmann's alternative, we cannot improve the suggested equivalence test essentially. Therefore, we also provide the asymptotic version of this test and display tables containing the required numerical values."], ["Prediction via Orthogonalized Model Mixing", "We introduce an approach and algorithms for model mixing in large prediction problems with correlated predictors. We focus on the choice of predictors in linear models, and mix over possible subsets of candidate predictors. Our approach is based on expressing the space of models in terms of an orthogonalization of the design matrix. Advantages are both statistical and computational. Statistically, orthogonalization often leads to a reduction in the number of competing models by eliminating correlations. Computationally, large model spaces cannot be enumerated; recent approaches are based on sampling models with high posterior probability via Markov chains. Based on orthogonalization of the space of candidate predictors, we can approximate the posterior probabilities of models by products of predictor-specific terms. This leads to an importance sampling function for sampling directly from the joint distribution over the model space, without resorting to Markov chains. Compared to the latter, orthogonalized model mixing by importance sampling is faster in sampling models and is also more efficient in finding models that contribute significantly to the prediction. Further advantages are in the speed of convergence and the availability of more reliable convergence diagnostic tools. We illustrate these in practice, using a data set on prediction of crime rates. The model space is small enough so that enumeration of all models is available for comparison and convergence checks. Also, we demonstrate the feasibility of orthogonalized model mixing in a large-size problem, which is very difficult to attack by other methods. The data set is from a designed experiment dealing with predicting protein activity under different storage conditions. The model space is large (the rank of the design matrix is 88) and very difficult to explore if expressed in terms of the original variables. We obtain prediction intervals and a probability distribution of the setting that produces the highest response."], ["Bayesian Analysis of Time Evolution of Earthquakes", "We adopt a Bayesian approach to analyze the occurrence times of seismic events and their magnitudes. We choose an epidemic model for the process of occurrence times conditional on the observed magnitude values. The locations of, and dependencies between, the model parameters are determined on the basis of historical and physical information. The overall prior variability is deliberately made diffuse. We generate samples from the joint posterior distribution of the model parameters by using a variant of the Metropolis\u2014Hastings algorithm. We use the results in a variety of ways, including the construction of pointwise posterior confidence bands for the conditional intensity of the point process as a function of time."], ["Bayesian Robust Multivariate Linear Regression with Incomplete Data", null], ["Reasoning to a Foregone Conclusion", null], ["The Equivalence of Constrained and Weighted Designs in Multiple Objective Design Problems", "Several competing objectives may be relevant in the design of an experiment. The competing objectives may not be easy to characterize in a single optimality criterion. One approach to these design problems has been to weight each criterion and find the design that optimizes the weighted average of the criteria. An alternative approach has been to optimize one criterion subject to constraints on the other criteria. An equivalence theorem is presented for the Bayesian constrained design problem. Equivalence theorems are essential in verifying optimality of proposed designs, especially when (as in most nonlinear design problems) numerical optimization is required. This theorem is used to show that the results of Cook and Wong on the equivalence of the weighted and constrained problems apply much more generally. The results are applied to Bayesian nonlinear design problems with several objectives."], ["Nonparametric Importance Sampling", "Importance sampling is a widely used variance reduction simulation technique for the evaluation of high-dimensional integrals. A key step in the implementation of importance sampling is to choose a proper distribution function from which pseudorandom numbers are generated. Parametric sampling distributions, if available at all, are often inadequate for high-dimensional integrals over irregular regions. One possible remedy is to use a nonparametric method to estimate the unknown optimal sampling function. We show that the nonparametric approach yields integral estimates that converge faster than estimates obtained from parametric approaches. We also demonstrate that an adaptive method, which has been used successfully in parametric settings, does not yield better results than simple one-step methods in the nonparametric setting."], ["Fitting Full-Information Item Factor Models and an Empirical Investigation of Bridge Sampling", null], ["Assessing Evidence in Multiple Hypotheses", null], ["Bootstrap for Imputed Survey Data", "Most surveys use imputation to compensate for missing data. However, treating the imputed data set as the complete data set and directly applying existing methods (e.g., the linearization, the jackknife, and the bootstrap) for variance estimation and/or statistical inference does not produce valid results, because these methods do not account for the effect of missing data and/or imputation. In this article we show that correct bootstrap estimates can be obtained by imitating the process of imputing the original data set in the bootstrap resampling; that is, by imputing the bootstrap data sets in exactly the same way that the original data set is imputed. The proposed bootstrap is asymptotically valid irrespective of the sampling design, the imputation method, or the type of statistic used in inference. This enables us to use a unified method in a variety of problems, and in fact this is the only method that works without any restriction on the sampling design, the imputation method, or the type of statistic."], ["Efficient Estimators with Simple Variance in Unequal Probability Sampling", "For unequal probability sampling designs, design-based variance estimation is cumbersome because it requires second-order inclusion probabilities. For most fixed sample size probability proportional-to-size (\u03c6PS) schemes, these probabilities are difficult to compute, and the variance estimation depends on them for a tedious double-sum calculation. We show how to replace the traditional \u03c6PS scenario with simpler design/estimator alternatives that preserve the high efficiency characteristic of \u03c6PS schemes. These use the generalized regression estimator, and the variance estimation entails only the calculation of a simple weighted squared residual sum."], ["On the Asymptotic Properties of LDU-Based Tests of the Rank of a Matrix", null], ["Nonlinear Additive Models for Environmental Time Series, with Applications to Ground-Level Ozone Data Analysis", "Environmental time series usually vary systematically in response to meteorological conditions and thus often are not stationary. In this article a class of additive models are introduced for environmental time series, in which both mean levels and variances of the series are nonlinear functions of relevant meteorological variables. Backfitting algorithms in nonlinear regression are adopted to estimate the unknown functions in the model, and the maximum likelihood method is used to estimate the parameters in the noise component. Asymptotic properties of the parameter estimates, including consistency and limiting distribution, are derived under mild conditions. The model is applied to daily maxima of ground-level ozone concentrations in the Chicago area for possible long-term trend assessment. Compared to alternative models, the proposed models gave more accurate estimations for the 95th and 99th percentiles of the ozone distribution."], ["An EM Algorithm Fitting First-Order Conditional Autoregressive Models to Longitudinal Data", null], ["Testing for Serial Correlation against an ARMA(1, 1) Process", "This article is concerned with tests for serial correlation in time series and in the errors of regression models. In particular, the nonstandard problem of testing for white noise against autoregressive moving average model ARMA(1, 1) alternatives is considered. The likelihood ratio (LR), sup Lagrange multiplier (LM), and exponential average LM and LR tests are shown to be asymptotically admissible for ARMA(1, 1) alternatives. In addition, they are shown to be consistent against all (weakly stationary strong mixing) non-white noise alternatives. Simulation results compare the tests to several tests in the literature. These results show that the LR and Exp-LR\u221e tests have very good all-around power properties for nonseasonal alternatives."], ["The Selection of Prior Distributions by Formal Rules", "Subjectivism has become the dominant philosophical foundation for Bayesian inference. Yet in practice, most Bayesian analyses are performed with so-called \u201cnoninformative\u201d priors, that is, priors constructed by some formal rule. We review the plethora of techniques for constructing such priors and discuss some of the practical and philosophical issues that arise when they are used. We give special emphasis to Jeffreys's rules and discuss the evolution of his viewpoint about the interpretation of priors, away from unique representation of ignorance toward the notion that they should be chosen by convention. We conclude that the problems raised by the research on priors chosen by formal rules are serious and may not be dismissed lightly: When sample sizes are small (relative to the number of parameters being estimated), it is dangerous to put faith in any \u201cdefault\u201d solution; but when asymptotics take over, Jeffreys's rules and their variants remain reasonable choices. We also provide an annotated bibliography."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Correction", null], ["Editorial Board Page", "This article has no abstract"], ["Editors' Report for 1995", null], ["Identification of Causal Effects Using Instrumental Variables", "We outline a framework for causal inference in settings where assignment to a binary treatment is ignorable, but compliance with the assignment is not perfect so that the receipt of treatment is nonignorable. To address the problems associated with comparing subjects by the ignorable assignment\u2014an \u201cintention-to-treat analysis\u201d\u2014we make use of instrumental variables, which have long been used by economists in the context of regression models with constant treatment effects. We show that the instrumental variables (IV) estimand can be embedded within the Rubin Causal Model (RCM) and that under some simple and easily interpretable assumptions, the IV estimand is the average causal effect for a subgroup of units, the compliers. Without these assumptions, the IV estimand is simply the ratio of intention-to-treat causal estimands with no interpretation as an average causal effect. The advantages of embedding the IV approach in the RCM are that it clarifies the nature of critical assumptions needed for a causal interpretation, and moreover allows us to consider sensitivity of the results to deviations from key assumptions in a straightforward manner. We apply our analysis to estimate the effect of veteran status in the Vietnam era on mortality, using the lottery number that assigned priority for the draft as an instrument, and we use our results to investigate the sensitivity of the conclusions to critical assumptions."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Multiple Imputation after 18+ Years", "Multiple imputation was designed to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities. The objective is valid frequency inference for ultimate users who in general have access only to complete-data software and possess limited knowledge of specific reasons and models for nonresponse. For this situation and objective, I believe that multiple imputation by the data-base constructor is the method of choice. This article first provides a description of the assumed context and objectives, and second, reviews the multiple imputation framework and its standard results. These preliminary discussions are especially important because some recent commentaries on multiple imputation have reflected either misunderstandings of the practical objectives of multiple imputation or misunderstandings of fundamental theoretical results. Then, criticisms of multiple imputation are considered, and, finally, comparisons are made to alternative strategies."], ["Alternative Paradigms for the Analysis of Imputed Survey Data", null], ["On Variance Estimation with Imputed Survey Data", "Unit nonresponse and item nonresponse both occur frequently in surveys. Unit nonresponse is customarily handled by weighting adjustment, whereas item nonresponse is usually treated by some form of imputation. In particular, deterministic or stochastic imputation is often used to assign values for missing item responses. We provide an account of some recent work on jackknife variance estimation based on adjusted imputed values, using only a single imputation and hence a single completed data set. We also present linearized versions of the jackknife variance estimators. We study both stratified simple random sampling and stratified multistage sampling. Existing computer programs for jackknife and linearization variance estimation can be modified to implement the proposed variance estimators without requiring the creation and permanent retention of supplemental data sets. But for secondary analyses, the completed data set must include information on response status for each item as well as on the imputation class."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Rejoinder", null], ["Rejoinder", null], ["Detection of Coseismic Changes of Underground Water Level", "An underground water level data set, regularly observed over a 10-year period in the Tokai area, one of the most intensively observed areas in Japan, is analyzed. First, the 500,000+ point data set was smoothed using a non-Gaussian state-space model to interpolate missing observations and to correct outlying observations. Then the effects of barometric pressure, earth tide, and precipitation on the water level were estimated using state-space modeling. After subtracting these effects, the coseismic effects on 16 examined earthquakes are clear. The amount of the coseismic effect is explained as a function of magnitude and hypocentral distance of the earthquake. It is also shown that the underground water level had a tendency to increase approximately 6 cm/year. Presumably, this corresponds to the increase of strain in this area due to the subduction of the Philippine Sea tectonic plate under the Eurasian plate."], ["On the Validity of Using Census Geocode Characteristics to Proxy Individual Socioeconomic Characteristics", "Investigators of social differentials in health outcomes commonly augment incomplete microdata by appending socioeconomic characteristics of residential areas (such as median income in a zip code) to proxy for individual characteristics. But little empirical attention has been paid to how well this aggregate information serves as a proxy for the individual characteristics of interest. We build on recent work addressing the biases inherent in proxies and consider two health-related examples within a statistical framework that illuminates the nature and sources of biases. Data from the Panel Study of Income Dynamics and the National Maternal and Infant Health Survey are linked to census data. We assess the validity of using the aggregate census information as a proxy for individual information when estimating main effects and when controlling for potential confounding between socioeconomic and sociodemographic factors in measures of general health status and infant mortality. We find a general, but not universal, tendency for aggregate proxies to exaggerate the effects of micro-level variables and to do more poorly than micro-level variables at controlling for confounding. The magnitude and direction of these biases vary across samples, however. Our statistical framework and empirical findings suggest the difficulties in and limits to interpreting proxies derived from aggregate census data as if they were micro-level variables. The statistical framework that we outline for our study of health outcomes should be generally applicable to other situations where researchers have merged aggregate data with microdata samples."], ["Empirical Bayes Methods for Combining Likelihoods", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Replicate Histograms", "The \u201creplicate histogram\u201d is introduced as a simple diagnostic tool for describing the sampling distribution of a general statistic. It can be applied to virtually any statistic that has an asymptotic distribution, and the data on which the statistic is computed may be serially or spatially dependent. The method is completely sample based, requiring no theoretical analysis by the user, no knowledge of the proper standardization for the statistic, and no specification of the underlying dependence mechanism generating the data. The replicate histogram warns the user of nonnormal sampling distributions and also indicates the type of departure from normality (e.g., skewness, peakedness). In the case of spatially dependent data, the statistic may be computed on observations from irregularly shaped index sets. Large-sample validity of the replicate histogram is established via strong consistency results, which are proved under mild conditions for both the time series and random field cases. Numerical examples are presented illustrating the diagnostic power of the replicate histogram for time series and spatial data sets. The bias, variance, and mean squared error performance of the replicate histogram are analyzed via second-order theory."], ["Time Trend Estimation for a Geographic Region", "We discuss a framework for estimation of temporal trend for an evolving spatial field. The spatial field is regularly sampled in time at arbitrary monitoring locations whose position may change over time. The estimation of time trend and the quantification of estimation error derives from a probabilistic model. We illustrate with an example involving the surface temperature field in the steppe region of eastern Europe."], ["Inference for Autocorrelations under Weak Assumptions", "In this article we consider the large-sample behavior of estimates of autocorrelations and autoregressive moving average (ARMA) coefficients, as well as their distributions, under weak conditions. Specifically, the usual text book formulas for variances of these estimates are based on strong assumptions and should not be routinely applied without careful consideration. Such is the case when the time series follows an ARMA process with uncorrelated innovations that may not be assumed to be independent and identically distributed. As a specific case, it is well known that if the process is independent and identically distributed, then the sample autocorrelation estimates, scaled by the square root of the sample size, are asymptotically standard normal. This result is used extensively as a diagnostic check on the residuals of a fitted model, or as an initial test on the observed time series to determine whether further model fitting is warranted. In this article we show that this result can be quite misleading. Specifically, if the underlying process is assumed to be uncorrelated rather than independent, then the asymptotic distribution is not necessarily standard normal. Although this distinction may appear superficial, the implications for making valid inference in time series modeling are broad. Usual procedures in time series analysis model correlation structure by fitting models whose estimated errors mimic an uncorrelated sequence. Therefore, testing for the presence of zero autocorrelation using a result that assumes independence may lead to incorrect conclusions. Furthermore, there exist stationary time series that have zero autocorrelation at all lags but yet are not independent, and so it is important to have valid procedures under general dependence structures. Here we present general asymptotic theory for the estimated autocorrelation function and discuss testing for lack of correlation without the further assumption of independence. We propose appropriate resampling methods that can be used to approximate the sampling distribution of the autocorrelation estimates under weak assumptions."], ["Some Models for Discretized Series of Events", "A discretized series of events is a binary time series that indicates whether or not events of a point process in the line occur in successive intervals. Such data are common in environmental applications. We describe a class of models for them, based on an unobserved continuous-time discrete-state Markov process, which determines the rate of a doubly stochastic Poisson process, from which the binary time series is constructed by discretization. We discuss likelihood inference for these processes and their second-order properties and extend them to multiple series. An application involves modeling the times of exposures to air pollution at a number of receptors in Western Europe."], ["Bandwidth Selection for Indirect Density Estimation Based on Corrupted Histogram Data", "Motivated by a number of practical applications, we consider a class of indirect nonparametric density estimation problems in which the observed data consist of a histogram of binned empirically corrected counts. Due to variability in the process of correction, the histogram cannot be modeled in terms of simple scaled Poisson statistics. This departure necessitates the development of a new methodology for bandwidth selection. A variant of the method of unbiased risk estimation is proposed. The methodology is studied, using numerical simulations and asymptotic analysis tools, in the context of a class of idealized density deconvolution problems. The methodology is adapted for application to the practical reconstruction problem of positron emission tomography (PET). Realistic numerical simulations and physical phantom data are presented to validate the approach in this setting. Some illustrations with cerebral glucose utilization and myocardial blood flow studies from some actual patient data sets are also presented."], ["Partial Least Squares Regression on Smooth Factors", "In this article we present a modification of partial least squares regression to account for inherent nonexchangeabilities of the columns of the design matrix. In chemometrics applications it is common to write the matrix as a bilinear form of latent variables and loadings. These loadings are often interpreted as sampled values of functions; hence they should exhibit a degree of smoothness. Our method forces the partial least squares factors to be smooth, by using a roughness penalty motivated by nonparametric regression. We present a computational method to determine the loadings that guarantees a desired orthogonality at successive steps. We propose a cross-validatory choice of the smoothing parameter and the number of loadings. We illustrate the algorithm by an example and describe our experience with real data."], ["Measurement Error in the Response in the General Linear Model", null], ["Practical Small-Sample Asymptotics for Regression Problems", "Saddlepoint approximations are derived for sums of independent, but not necessarily identically distributed random variables, along with generalizations to estimating equations and multivariate problems. These results are particularly useful for accurately approximating the distribution of regression coefficients. General formulas are given for the distribution of the coefficients arising out of a generalized linear model with both canonical and noncanonical link functions. We illustrate the case of logistic regression with a real data example and show how the Gibbs sampler may be used to obtain confidence sets for each regression parameter based on the saddlepoint approximation."], ["Bootstrap Model Selection", null], ["General Saddlepoint Approximations of Marginal Densities and Tail Probabilities", "Saddlepoint approximations of marginal densities and tail probabilities of general nonlinear statistics are derived. These are based on the expansion of the statistic up to the second order. Their accuracy is shown in a variety of examples, including logit and probit models and rank estimators for regression."], ["Test of Significance Based on Wavelet Thresholding and Neyman's Truncation", null], ["Quantile Comparison Functions in Two-Sample Problems, with Application to Comparisons of Diagnostic Markers", "In this article a control percentile test, a chi-squared test, and a Kolmogorov-type test are proposed for comparing two distributions from incomplete survival data. These tests are obtained by examining a vertical shift comparison function at a single point, a finite number of points, and an entire set of points on an interval. The proposed methods also have applications in receiver operating characteristic (ROC) analysis, which has been widely used in such diverse fields as signal detection theory, psychology, epidemiology, and medicine. The results are derived under very general conditions that hold for the well-known random censorship and random truncation models. The performances of the proposed procedures are studied using Monte Carlo simulation. The methods are applied to analyze Mayo Clinic ovarian carcinoma data."], ["Rank-Based Analysis of the Heteroscedastic Linear Model", "Heteroscedasticity often causes problems in the analysis of linear models. Frequently for such cases, scale is a function of the response. Here, for such models, a methodology is presented based on well-known rank-based procedures. The procedure is iterative. Using the residuals from an initial R estimate of the regression coefficients, scale is estimated by inverting a linear rank test for scale. This in turn leads to a weighted R estimate of the regression coefficients and then to a final estimate of scale. Asymptotic linearity results for these estimates are derived, from which their asymptotic distribution is obtained. The weighted R estimate has the same asymptotic distribution as the optimal (known scale) R estimate; hence it is efficiently robust. Consistent estimates of the standard errors of the R estimates of scale and the regression coefficients are determined. Based on these results, a complete inference for the regression coefficients and the scale parameter is realized, including a test for homogeneity. The procedure is illustrated by the analysis of a dose\u2014response data set drawn from pharmaceutical science. Results of a simulation study that support the asymptotic theory are reported."], ["A Semiparametric Proportional Odds Regression Model for the Analysis of Current Status Data", "An approach to covariate analysis of current status data is described. The method is applicable when the logit of the conditional probability of survival given the covariates is some increasing function of time plus a linear combination of the covariates. The approach is based on approximating the infinite-dimensional nuisance parameter, the baseline log-odds of failure, with a step function, and carrying out a maximum likelihood procedure. The resulting finite dimensional parameter estimates for the regression parameters are shown to be asymptotically normal and semiparametric efficient. Numerical studies with small and moderate samples are discussed."], ["A Semiparametric Mixture Approach to Case-Control Studies with Errors in Covariables", null], ["How to be a Better Bayesian", null], ["Bayesian Methods for Variance Component Models", null], ["Diagnostic Measures for Model Criticism", "We discuss the problem of model criticism, with emphasis on developing summary diagnostic measures. We approach model criticism by identifying possible troublesome features of the currently entertained model, embedding the model in an elaborated model, and measuring the value of elaborating. This requires three elements: a model elaboration, a prior distribution, and a utility function. Each triplet generates a different diagnostic measure. We focus primarily on the measure given by a Kullback\u2014Leibler divergence between the marginal prior and posterior distributions on the elaboration parameter. We also develop a linearized version of this diagnostic and use it to show that our procedure is related to other tools commonly used for model diagnostics, such as Bayes factors and the score function. One attraction of this approach is that it allows model criticism to be performed jointly with parameter inference and prediction. Also, this diagnostic approach aims at maintaining an exploratory nature to the criticism process, while affording feasibility of implementation. In this article we present the general outlook and discuss general families of elaborations for use in practice; the exponential connection elaboration plays a key role. We then describe model elaborations for use in diagnosing: departures from normality, goodness of fit in generalized linear models, and variable selection in regression and outlier detection. We illustrate our approach with two applications."], ["Bayesian Computation for Nonhomogeneous Poisson Processes in Software Reliability", "A unified approach to the nonhomogeneous Poisson process in software reliability models is given. This approach models the epochs of failures according to a general order statistics model or to a record value statistics model. Their corresponding point processes can be related to the nonhomogeneous Poisson processes, for example, the Goel\u2014Okumoto, the Musa\u2014Okumoto, the Duane, and the Cox\u2014Lewis processes. Bayesian inference for the nonhomogeneous Poisson processes is studied. The Gibbs sampling approach, sometimes with data augmentation and with the Metropolis algorithm, is used to compute the Bayes estimates of credible sets, mean time between failures, and the current system reliability. Model selection based on a predictive likelihood is studied. A numerical example with a real software failure data set is given."], ["Local Sensitivity of Inferences to Prior Marginals", "The sensitivity of posterior expectations to perturbations of prior marginals is investigated, using a local method. This approach is particularly well suited to high-dimensional parameter spaces. Qualitative and quantitative results are obtained regarding the propagation of sensitivity in multiparameter settings. More specifically, a posterior expectation is treated as a functional of the prior marginal. The derivative of this functional is evaluated at the nominal prior marginal, yielding a measure of sensitivity. This is computationally more feasible than evaluating the range of a posterior quantity over a class of priors; hence sensitivity can be assessed in high-dimensional problems. The local technique is used to assess the influence of distributional assumptions at the various stages of a hierarchical model."], ["Bayesian Sequential Two-Phase Sampling", "We propose a sequential two-phase sample design to accommodate applications where conventional two-phase sampling cannot be used. First, we derive the optimal, the optimal myopic, and several approximate optimal myopic sequential decision procedures for subsampling a first-phase sample. Then we compare our procedure to that of the optimal (nonsequential) conventional two-phase sample design. In our application, the objective is inference about the age distribution of a population of fish using data on covariates (e.g., length, weight) obtained from all members of the first-phase sample and data on age obtained from the fish in the second-phase subsample."], ["On the Generalization of the Likelihood Function and the Likelihood Principle", "The parametric likelihood and likelihood principle (LP) play a central role in parametric methodology and in the foundations of statistics. The main purpose of this article is to extend the concepts of likelihood and LP to general inferential aims and models covering, for example, prediction and empirical Bayes models. The likelihood function is the joint distribution of the data and the unobserved variables of inferential interest, considered as a function of the parameters and these inferential variables. LP is based on this likelihood, and the principles of sufficiency (SP) and conditionality (CP) are modified such that the equivalence SP and CP \u21d4 LP is valid, generalizing Birnbaum's theorem."], ["The Analysis of Longitudinal Ordinal Response Data in Continuous Time", "A simple Markov model is developed for assessing the predictive effect of time-dependent covariates on an intermittently observed ordinal response in continuous time. This is accomplished by reparameterizing an ergodic intensity matrix in terms of its equilibrium distribution and a parametrically independent component that assesses the rate of movement between ordinal categories. The effect of covariates on the equilibrium distribution can then be modeled using any link appropriate for ordinal data. A robust maximum likelihood estimator based on this model that is consistent and asymptotically normal is constructed. Practical data analysis issues are discussed, and a simple diagnostic tool for assessing model adequacy is developed. The utility of these methods is demonstrated with several analyses of visual acuity data, including a comparison analysis based on generalized estimating equation (GEE) methods."], ["The Likelihood Ratio Test for Poisson versus Binomial Distributions", null], ["Exact Inference for Proportions from a Stratified Finite Population", null], ["On Estimating Incidence Rates of Diseases with Delayed Onset Using Biased Samples", "Consistent estimators are derived for the age-specific and lifetime incidence rates of late-onset diseases from censored, biased samples, using a discrete time model and a traditional assumption for size-biased sampling. The estimators are alternatives to the so-called index case estimators often used by epidemiologists under similar circumstances. The asymptotic normality of the proposed estimators is established and an adaptation of Greenwood's formula is given for estimating and comparing their asymptotic variances. We show that index case estimators can be inconsistent and find a necessary and sufficient condition for their consistency. The results are applicable in the study of familial aggregation of diseases that are not manifest at birth when the sample consists of families identified through a family member who has the disease of interest. A survey of families of nursing home patients with a narrowly defined form of Alzheimer's disease, undertaken to clarify its mode of inheritance is an example of such a study. Our work shows a possible lack of consistency of the estimators obtained in that study and in other similar studies. Besides familial aggregation, the results may also be applicable to the study of slowly working environmental causes of a disease insofar as they affect individuals as members of special groups, identified through some of their diseased members; for example, factory workers exposed to certain toxic materials."], ["A Maximum Entropy Approach to Recovering Information from Multinomial Response Data", "The classical maximum entropy (ME) approach to estimating the unknown parameters of a multinomial discrete choice problem, which is equivalent to the maximum likelihood multinomial logit (ML) estimator, is generalized. The generalized maximum entropy (GME) model includes noise terms in the multinomial information constraints. Each noise term is modeled as the mean of a finite set of a priori known points in the interval [\u22121,1] with unknown probabilities where no parametric assumptions about the error distribution are made. A GME model for the multinomial probabilities and for the distributions associated with the noise terms is derived by maximizing the joint entropy of multinomial and noise distributions, under the assumption of independence. The GME formulation reduces to the ME in the limit as the sample grows large or when no noise is included in the entropy maximization. Further, even though the GME and the logit estimators are conceptually different, the dual GME model is related to a generalized class of logit models. In finite samples, modeling the noise terms along with the multinomial probabilities results in a gain of efficiency of the GME over the ME and ML. In addition to analytical results, some extensions, sampling experiments, and an example based on real-world data are presented."], ["A Simple Multivariate Test for One-Sided Alternatives", "A simple test for multivariate normal data is proposed that has good power for alternatives where the mean vector tends to be positive. The test rejects if the quadratic form of the sample mean vector exceeds its 2\u03b1 critical value and the sum of the elements of the mean vector exceeds zero. The proposed test is shown to have type I error rate equal to \u03b1 whether or not the covariance matrix is known. Tight bounds on the power of the proposed test are provided, and the test is compared to three likelihood ratio tests."], ["On a Geometric Notion of Quantiles for Multivariate Data", null], ["The Lorenz Zonoid of a Multivariate Distribution", null], ["Markov Chain Monte Carlo Convergence Diagnostics: A Comparative Review", "A critical issue for users of Markov chain Monte Carlo (MCMC) methods in applications is how to determine when it is safe to stop sampling and use the samples to estimate characteristics of the distribution of interest. Research into methods of computing theoretical convergence bounds holds promise for the future but to date has yielded relatively little of practical use in applied work. Consequently, most MCMC users address the convergence problem by applying diagnostic tools to the output produced by running their samplers. After giving a brief overview of the area, we provide an expository review of 13 convergence diagnostics, describing the theoretical basis and practical implementation of each. We then compare their performance in two simple models and conclude that all of the methods can fail to detect the sorts of convergence failure that they were designed to identify. We thus recommend a combination of strategies aimed at evaluating and accelerating MCMC sampler convergence, including applying diagnostic procedures to a small number of parallel chains, monitoring autocorrelations and cross-correlations, and modifying parameterizations or sampling algorithms appropriately. We emphasize, however, that it is not possible to say with certainty that a finite sample from an MCMC algorithm is representative of an underlying stationary distribution."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Correction", null], ["Editorial Board Page", "This article has no abstract"], ["Statistics in Action", null], ["Statistics in Epidemiology: The Case-Control Study", "Statisticians have contributed enormously to the conceptualization, development, and success of case-control methods for the study of disease causation and prevention. This article reviews the major developments. It starts with Cornfield's demonstration of odds ratio invariance under cohort versus case-control sampling, proceeds through the still-popular Mantel\u2014Haenszel procedure and its extensions for dependent data, and highlights (conditional) likelihood methods for relative risk regression. Recent work on nested case-control, case-cohort, and two-stage case-control designs demonstrates the continuing impact of statistical thinking on epidemiology. The influence of R. A. Fisher's work on these developments is mentioned wherever possible. His objections to the drawing of causal conclusions from observational data on cigarette smoking and lung cancer are used to introduce the problems of measurement error and confounding bias. The resolution of such difficulties, whether by further development and implementation of randomized intervention trials or by causal analysis of observational data using graphical models containing latent variables, will challenge future generations of statisticians."], ["Discharge Rates of Medicare Stroke Patients to Skilled Nursing Facilities: Bayesian Logistic Regression with Unobserved Heterogeneity", "We determine factors, both hospital-specific and market area-specific, associated with hospitals' propensities for discharging Medicare stroke patients to skilled nursing facilities (SNF's) in California and Florida. Logistic regression is generalized to the case of a betabinomial, hierarchical model, in which covariate information is included in the hyperparameters of the second-stage beta distribution. It is found that the posterior mean of the proportion discharged to SNF is approximately a weighted average (i.e., shrinkage estimator) of the logistic regression estimator and the observed rate. We develop fully Bayesian inference that takes into account uncertainty about the hyperparameters, and we find that this also allows us to test for overdispersion in a natural way. The number of observed zeros (i.e., hospitals that sent no stroke patients to a SNF) is excessive compared to the number expected from a standard logistic regression model and is fit better by the hierarchical betabinomial model. The factors associated with discharge to SNF differ between California and Florida. In California the case-mix index and percent Medicaid admissions of the hospital, as well as the per capita income for the area and whether there is a rehabilitation facility in the area, are associated with discharge rates to SNF's. In Florida, whether there is a rehabilitation facility in the area is the only factor that exhibits association with discharge rates to SNF's."], ["On Bayesian Analysis of Multirater Ordinal Data: An Application to Automated Essay Grading", "A framework is proposed for the analysis of ordinal categorical data when ratings from several judges are available. I emphasize the tasks of estimating latent trait characteristics of individual items, regressing these latent traits on observed covariates, and comparing the performance of raters. The model is illustrated in the design and evaluation of an automated essay grader. This grader is based on a regression of variables, obtained from a grammar checker, on essay scores estimated from a panel of experts. The performance of the grader is evaluated relative to human graders, and implications on the reliability and repeatability of both automated and human raters is investigated."], ["Nonparametric Mixed-Effects Models for Repeated Binary Data Arising in Serial Dilution Assays: An Application to Estimating Viral Burden in AIDS", "This article develops methods for estimating treatment effects in mixed-effects models using outcome data gathered from serial dilution assays. Our application allows us to estimate the viral burden of HIV infection before and after antiviral treatment from cell dilution assays. This assay is designed to determine the infectious units per patient peripheral blood mononuclear cell (PBMC). The infectious unit is the amount of virus required to produce detectable HIV infection in PBMC's from healthy, uninfected donors. At each dilution level of the patient cells, one observes whether or not it was possible for the virus from these cells to infect donor cells. Thus the assay result for each subject consists of a series of repeated binary outcomes. We propose an analytic approach in which patient-specific titers (measures of viral burden) are modeled as random effects from an unknown distribution, and treatment effects are modeled as fixed. This approach makes use of all assay results, even if many assays fail to reach endpoint (i.e., they turn negative at the highest dilution level) and the assay design (dilution scheme) changes over time."], ["The Bayesian Analysis of Population Pharmacokinetic Models", "Pharmacokinetics is the study of the time course of a drug and its metabolites following its introduction into the body. Population pharmacokinetic studies are becoming increasingly important as an aid to drug development. The data from such studies typically consist of dose histories, drug concentrations with associated sampling times, and often covariate measurements such as the age and weight of each subject. These studies aim to provide an understanding of the pharmacokinetics of the drug in question and so lead to an informed choice of dosage regimen. Such an understanding includes determining those covariates that are important predictors of fundamental pharmacokinetic parameters, such as clearance, defined as the volume of plasma cleared of drug in a unit of time. Determining those subpopulations (e.g., the elderly) with altered kinetics has implications for the choice of an appropriate dosage regimens, because predictive concentration profiles arising from a particular regimen in different populations may be very different. In this article a general Bayesian hierarchical model is described. Pharmacokinetic models relating concentration to time are generally nonlinear, and the data are often sparse and/or noisy. The number of individuals on whom data have been collected is often large, and so the dimensionality of the parameter space is large. Consequently, estimation, from a Bayesian or a classical perspective, is not straightforward. In this article the Hastings\u2014Metropolis algorithm is used for learning about the posterior distribution. An analysis of concentration data collected after the administration of the antiarrhythmic drug quinidine is presented. The data consist of 361 measurements on a total of 136 patients. Nine covariates are also available for each individual. These covariates are a mixture of discrete and continuous measurements. Some of the covariates are constant within an individual during the course of the study, whereas others change. A covariate model is constructed, and the sensitivity of the inferences to distributional assumptions is examined. The importance of assessing the appropriateness of modeling assumptions is emphasized and extensive model checking is carried out for the quinidine data using graphical diagnostics."], ["Likelihood Inference for Permuted Data with Application to Gene Regulation", "Given that all the cells of an individual have the same genetic information stored in their DNA, how can cells be as different as those of the retina and heart? Nature solves this problem through gene regulation, which often involves the binding of regulatory proteins to regulatory sites. These sites are short subsequences of 10 to 20 DNA base pairs whose pattern may be multinomially modeled. These sites usually occur \u201cupstream\u201d of the genes they regulate in a segment of a few hundred DNA base pairs called the promoter. But the positions of regulatory sites within promoters vary and are unobservable. This uncertainty in site position misaligns the data and renders the indices of the observations uncertain. Data with uncertain indices arise commonly in experimental biology whenever uncontrolled variability alters unobservable auxiliary identifying information. Current technology breaks the analysis of such data into two steps: alignment and analyses applied to the aligned data. This article proposes a methodology that combines these two steps and thus produces inferences that directly incorporate random alignment errors. The introduction of an index permutation indicator variable, which is treated as missing data, permits the formulation of these problems as novel finite mixtures. Using a missing information approach, we separate the likelihood into components representing variable uncertainty and index uncertainty. An EM algorithm to obtain the maximum likelihood estimates of the parameters for both of these components is also presented. Inferences specific to the index permutations stemming from index uncertainty are examined. An application to regulatory sites for a bacterial regulatory protein\u2014cyclic adenosine monophosphate receptor protein (CRP)\u2014is presented."], ["Bayesian Tobit Modeling of Longitudinal Ordinal Clinical Trial Compliance Data with Nonignorable Missingness", "In the Lung Health Study (LHS), compliance with the use of inhaled medication was assessed at each follow-up visit both by self-report and by weighing the used medication canisters. One or both of these assessments were missing if the participant failed to attend the visit or to return all canisters. Approximately 30% of canister-weight data and 5% to 15% of self-report data were missing at different visits. We use Gibbs sampling with data augmentation and a multivariate Hastings update step to implement a Bayesian hierarchical model for LHS inhaler compliance. Incorporating individual-level random effects to account for correlations among repeated measures on the same participant, our model is a longitudinal extension of the Tobit models used in econometrics to deal with partially unobservable data. It enables (a) assessment of the relationships among visit attendance, canister return, self-reported compliance level, and canister weight compliance, and (b) determination of demographic, physiological, and behavioral predictors of compliance. In addition to addressing the estimation and prediction questions of substantive interest, we use sampling-based methods for covariate screening and model selection and investigate a range of informative priors on missing data."], ["Multivariate Logistic Models for Incomplete Binary Responses", null], ["The Intrinsic Bayes Factor for Model Selection and Prediction", null], ["Robust Bayesian Model Selection for Autoregressive Processes with Additive Outliers", null], ["Local Adaptive Importance Sampling for Multivariate Densities with Strong Nonlinear Relationships", "We consider adaptive importance sampling techniques that use kernel density estimates at each iteration as importance sampling functions. These can provide more nearly constant importance weights and more precise estimates of quantities of interest than the sampling importance resampling algorithm when the initial importance sampling function is diffuse relative to the target. We propose a new method that adapts to the varying local structure of the target. When the target has unusual structure, such as strong nonlinear relationships between variables, this method provides estimates with smaller mean squared error than alternative methods."], ["Bayesian Inference for Semiparametric Binary Regression", "We propose a regression model for binary response data that places no structural restrictions on the link function except monotonicity and known location and scale. Predictors enter linearly. We demonstrate Bayesian inference calculations in this model. By modifying the Dirichlet process, we obtain a natural prior measure over this semiparametric model, and we use Polya sequence theory to formulate this measure in terms of a finite number of unobserved variables. We design a Markov chain Monte Carlo algorithm for posterior simulation and apply the methodology to data on radiotherapy treatments for cancer."], ["Studying Convergence of Markov Chain Monte Carlo Algorithms Using Coupled Sample Paths", "I describe a simple procedure for investigating the convergence properties of Markov chain Monte Carlo sampling schemes. The procedure uses coupled chains from the same sampler, obtained by using the same sequence of random deviates for each run. By examining the distribution of the iteration at which all sample paths couple, convergence properties for the system can be established. The procedure also provides a simple diagnostic for detecting modes in multimodal posteriors. Several examples of the procedure are provided. In Ising models, the relation between the correlation parameter and the convergence rate of rudimentary Gibbs samplers is investigated. In another example, the effects of multiple modes on the convergence of coupled paths are explored using mixtures of bivariate normal distributions. The technique is also used to evaluate the convergence properties of a Gibbs sampling scheme applied to a model for rat growth rates."], ["Maximum Likelihood Estimation under Order Restrictions by the Prior Feedback Method", "Algorithms for deriving isotonic regression estimators in order-restricted linear models and more generally restricted maximum likelihood estimators are usually quite dependent on the particular problem considered. We propose here an optimization method based on a sequence of formal Bayes estimates whose variances converge to zero. This method, akin to simulated annealing, can be applied \u201cuniversally\u201d; that is, as long as these Bayes estimators can be derived by exact computation or Markov chain Monte Carlo sampling approximation. We then give an illustration of our method for two real-life examples."], ["Implications of Reference Priors for Prior Information and for Sample Size", "Here we use posterior densities based on relative entropy reference priors for two purposes. The first purpose is to identify data implicit in the use of informative priors. We represent an informative prior as the posterior from an experiment with a known likelihood and a reference prior. Minimizing the relative entropy distance between this posterior and the informative prior over choices of data results in a data set that can be regarded as representative of the information in the informative prior. The second implication from reference priors is obtained by replacing the informative prior with a class of densities from which one might wish to make inferences. For each density in this class, one can obtain a data set that minimizes a relative entropy. The maximum of these sample sizes as the inferential density varies over its class can be used as a guess as to how much data is required for the desired inferences. We bound this sample size above and below by other techniques that permit it to be approximated."], ["Bayesian Experimental Design for Multiple Hypothesis Testing", "The problem of designing an optimal experiment for the purpose of performing one or more hypothesis tests is considered. The Bayesian decision theoretic approach is used to arrive at several new optimality criteria for this purpose. For a single hypothesis test, the resulting optimal designs are the well-known \u03c6-optimal designs that minimize the posterior variance of the parameter being tested. For multiple tests, an experimental design must perform well under several competing criteria. Different approaches to achieving this goal are explored, including constrained optimization and an additive weighted loss. The resulting optimality criteria are sensitive not only to the posterior variances of the parameters under test but also to their prior means."], ["Goodness of Prediction Fit for Multivariate Linear Models", null], ["The Matrix-Logarithmic Covariance Model", null], [null, null], ["A Linear Mixed-Effects Model with Heterogeneity in the Random-Effects Population", "This article investigates the impact of the normality assumption for random effects on their estimates in the linear mixed-effects model. It shows that if the distribution of random effects is a finite mixture of normal distributions, then the random effects may be badly estimated if normality is assumed, and the current methods for inspecting the appropriateness of the model assumptions are not sound. Further, it is argued that a better way to detect the components of the mixture is to build this assumption in the model and then \u201ccompare\u201d the fitted model with the Gaussian model. All of this is illustrated on two practical examples."], ["Indicator and Stratification Methods for Missing Explanatory Variables in Multiple Linear Regression", "The statistical literature and folklore contain many methods for handling missing explanatory variable data in multiple linear regression. One such approach is to incorporate into the regression model an indicator variable for whether an explanatory variable is observed. Another approach is to stratify the model based on the range of values for an explanatory variable, with a separate stratum for those individuals in which the explanatory variable is missing. For a least squares regression analysis using either of these two missing-data approaches, the exact biases of the estimators for the regression coefficients and the residual variance are derived and reported. The complete-case analysis, in which individuals with any missing data are omitted, is also investigated theoretically and is found to be free of bias in many situations, though often wasteful of information. A numerical evaluation of the bias of two missing-indicator methods and the complete-case analysis is reported. The missing-indicator methods show unacceptably large biases in practical situations and are not advisable in general."], ["Bootstrap Confidence Region Estimation of the Motion of Rigid Bodies", null], ["Asymptotics for the SIMEX Estimator in Nonlinear Measurement Error Models", "Cook and Stefanski have described a computer-intensive method, the SIMEX method, for approximately consistent estimation in regression problems with additive measurement error. In this article we derive the asymptotic distribution of their estimators and show how to compute estimated standard errors. These standard error estimators can either be used alone or as prepivoting devices in a bootstrap analysis. We also give theoretical justification to some of the phenomena observed by Cook and Stefanski in their simulations."], ["Quasi-Likelihood for Median Regression Models", "This article proposes quasi-likelihood equations for median regression models. The quasi-likelihood can be used for dependent observations such as repeated measurements or time series data. To construct a quasi-likelihood equation, we need to specify the relation between the median and the dispersion and also specify the dependency of the observations. If a monotone transformation of the original observation has a Laplace distribution, then the quasi-likelihood is the exact likelihood. Under moderate assumptions, the quasi-likelihood estimates are consistent and have asymptotically normal distributions. The estimates are also shown to have minimal asymptotic variance within a certain class of consistent estimates. The proposed method is illustrated using data from a clinical trial."], ["On Local Smoothing of Nonparametric Curve Estimators", "We develop new local versions of familiar smoothing methods, such as cross-validation and smoothed cross-validation, in the contexts of density estimation and regression. These new methods are locally adaptive in the sense that they capture smooth local fluctuations in the curve by using smoothly varying bandwidths that change as the character of the curve changes. Moreover, the new methods are accurate, easy to apply, and computationally expedient."], ["Finite-Sample Variance of Local Polynomials: Analysis and Solutions", "Fitting local polynomials in nonparametric regression has a number of advantages. The attractive theoretical features are in a partial contradiction to variance properties for random design and to practical experience over a broad range of situations. No upper bound can be given for the conditional variance. The unconditional variance is infinite when using optimal weights with compact support. Properties are better for Gaussian weights. We analyze local polynomials for finite sample size, both theoretically and numerically. It turns out that difficulties arise in sparse regions in the realization of the design, when the realization has locally a small variance and/or a skew empirical distribution. Two small-sample modifications of local polynomials are presented: local increase of bandwidth in sparse regions of the design, and local polynomial ridge regression. Both modifications combine a good finite-sample behavior with the asymptotic advantages of local polynomials."], ["Spline Estimation of Paths Using Bearings-Only Tracking Data", "In many applications, bearings are measured to a moving object with the goal of estimating the object's course of movement. If movement is appropriately modeled as a smooth deterministic curve in the plane, then a cubic spline is a reasonable representation of the curve. Maximum likelihood estimators are presented for parameters of regression splines, assuming that observation errors follow a Von Mises distribution. Location estimates are obtainable even when data are sparse. Path estimation error, number and placement of knots, and outlier detection are discussed. Examples, including both simulated paths and observations from a wildlife radio-tracking study, are presented."], ["Discrimination of Time Series by Parametric Filtering", "A new approach to time series discrimination is discussed. The approach, called parametric filtering, combines a parametric filter bank with an analysis of first-order autocorrelation from the filtered time series. As a function of the filter parameter, the first-order autocorrelation is shown to uniquely characterize the correlation structure of the original stationary time series. It is also shown to produce a diagnostic that not only characterizes correlation structures, but also transforms these structures into smooth and monotone functions. Natural sample estimators of these characterization functions are shown to be uniformly consistent even for mixed-spectrum time series. Examples are provided to demonstrate the potential applications of the method in discrimination of time series with a special emphasis on time series of mixed spectra."], ["Estimation of the Generalized Prediction Error Variance of a Multiple Time Series", "For a multivariate stationary time series, we propose a nonparametric estimator for its generalized prediction error variance using a multivariate analog of the Szeg\u00f6-Kolmogorov formula, replacing the integral by a sum and replacing the unknown spectral density matrix by a consistent estimator. Asymptotic normality of this estimator is established, and its small-sample behavior is assessed through simulation and application to two real data sets. These examples show that the proposed method works reasonably well in comparison with parametric models. In contrast to the univariate case where smoothing the periodogram is optional and generally not recommended, in the multivariate case this smoothing is a necessity, because the raw periodogram is a matrix of rank one. To obtain a consistent estimator of a full-rank spectral density matrix, one must necessarily choose larger bandwidths for higher-dimensional time series. A bias-correction factor is computed using the asymptotic properties of our proposed estimator, and a simulation study indicates its important role in reducing the bias."], ["Estimation of Rate and Mean Functions from Truncated Recurrent Event Data", "This article investigates data on recurrent events that arise from sources such as warranty claims, where the observation period for a unit is unknown until it experiences at least one event. This creates a type of truncation in the data. We consider nonparametric estimation of means and rates of the event occurrences with such \u201czero-truncated\u201d data and examine the case where the population size and the distribution of observation times across units are at least approximately known. We study the behaviors of the proposed estimators by simulation. We examine some car warranty data by applying the methodology developed here and considering the underlying assumptions."], ["Comparing Groups with Umbrella Orderings", "One commonly observed response pattern in a one-factor design with ordered treatment levels is the umbrella or unimodal ordering, in which the response variable increases with an increase in the treatment level up to a point, then decreases with further increase in the treatment level. The turning point is called the peak of an umbrella ordering. In many practical settings, there are two or more different populations with umbrella orderings, and it is of interest to compare these populations. In this article, likelihood ratio tests are developed for testing whether two groups with umbrella orderings have the same peaks. Three cases are considered: variances known, variances unknown and different for each group, and variances unknown and the same for both groups. Extensions to randomized block designs and to a more general class of testing problems are discussed."], ["Dual Cones, Dual Norms, and Simultaneous Inference for Partially Ordered Means", null], ["Nonequivariant Simultaneous Confidence Intervals Less Likely to Contain Zero", null], ["Lower Confidence Bounds Using Pilot Samples with an Application to Auditing", "A pilot study is often done prior to a more thorough study, to determine whether a larger-scale effort is likely to yield useful results. In one sense, the pilot study can discourage or encourage further investigation. This is true in many situations in auditing. For example, an auditor seeks to determine that amount an insurance provider overpays. Often the provider makes few mistakes and overpays little. On the other hand, occasionally a provider overpays a lot. Because the cost of auditing is expensive and the procedure is somewhat disruptive, a pilot study can prove very useful. When overpayment activity is present, the auditor would like a more precise estimate of overpayment, and then a second sample is called for. Pilot studies generally have small or moderate sample sizes. Nevertheless, the data from these samples should be used in two ways: first, to determine whether further sampling should be done, and second, to be used along with second samples for inference purposes. In this article we consider a model in which the population is normal with unknown mean and unknown variance. A pilot sample is taken and the null hypothesis that the mean is zero is tested against the alternative that the mean is positive. If null hypothesis is rejected, then a second sample is taken. Based on the data from both samples, an exact conditional lower confidence bound is obtained for the mean. The conditioning set consists of those sample points in the first sample that lead to rejection of the null hypothesis. In addition, a bias-corrected asymptotic lower confidence bound is obtained. Generalizations to models other than the normal are indicated."], ["On Balanced Half-Sample Variance Estimation in Stratified Random Sampling", null], ["Estimation in Dual Frame Surveys with Complex Designs", "In a dual frame survey, samples are drawn independently from two overlapping frames that are assumed to cover the population of interest. This article considers the case when at least one of the samples is selected by a complex design involving, e.g., multistage sampling. A \u201cpseudo\u201d\u2014maximum likelihood estimator of a population total or a mean for such dual frame surveys is proposed. An advantage of the proposed estimator is that the same weights are used for all the variables, unlike the estimators of Hartley and Fuller and Burmeister. Asymptotic properties of the estimator are studied, including its efficiency. An alternative \u201csingle frame\u201d estimator, based on the design induced by the two separate designs, is also studied. Results of a limited simulation study indicate that our estimator is essentially as efficient as those of Hartley and Fuller and Burmeister and can lead to significant efficiency gains over the single frame estimator."], ["Optimal Stopping with Random Horizon with Application to the Full-Information Best-Choice Problem with Random Freeze", null], ["Network Tomography: Estimating Source-Destination Traffic Intensities from Link Data", "The problem of estimating the node-to-node traffic intensity from repeated measurements of traffic on the links of a network is formulated and discussed under Poisson assumptions and two types of traffic-routing regimens: deterministic (a fixed known path between each directed pair of nodes) and Markovian (a random path between each directed pair of nodes, determined according to a known Markov chain fixed for that pair). Maximum likelihood estimation and related approximations are discussed, and computational difficulties are pointed out. A detailed methodology is presented for estimates based on the method of moments. The estimates are derived algorithmically, taking advantage of the fact that the first and second moment equations give rise to a linear inverse problem with positivity restrictions that can be approached by an EM algorithm, resulting in a particularly simple solution to a hard problem. A small simulation study is carried out."], ["On Tail Categorization of Probability Laws", null], ["Fisher Information in Order Statistics", null], ["A Review of the Development and Application of Recursive Residuals in Linear Models", "Recursive residuals have been shown to be useful in a variety of applications in linear models. Unlike the more familiar ordinary least squares residuals or studentized residuals, recursive residuals are independent as well as homoscedastic under the model. Their independence is particularly appealing for use in developing test statistics. They are not uniquely defined; their values depend on the order in which they are calculated, although their properties do not. In some applications one can exploit this order dependence, coupled with the fact that they are in clear one-to-one correspondence with the observations for which they are calculated. Uses for recursive residuals have been suggested in almost all areas of regression model validation. Regression diagnostics have been constructed from recursive residuals for detecting serial correlation, heteroscedasticity, functional misspecification, and structural change. Other statistics based on recursive residuals have focused on detection of outliers or observations that are influential or have high leverage. Recent work has explored properties and possible uses of recursive residuals in models with a general covariance matrix, multivariate linear models, and nonlinear models. Computing routines are available for obtaining recursive residuals accurately and efficiently."], ["A Brief Survey of Bandwidth Selection for Density Estimation", "There has been major progress in recent years in data-based bandwidth selection for kernel density estimation. Some \u201csecond generation\u201d methods, including plug-in and smoothed bootstrap techniques, have been developed that are far superior to well-known \u201cfirst generation\u201d methods, such as rules of thumb, least squares cross-validation, and biased cross-validation. We recommend a \u201csolve-the-equation\u201d plug-in bandwidth selector as being most reliable in terms of overall performance. This article is intended to provide easy accessibility to the main ideas for nonexperts."], ["A Single General Method for the Analysis of Cross-Classified Data: Reconciliation and Synthesis of Some Methods of Pearson, Yule, and Fisher, and Also Some Methods of Correspondence Analysis and Association Analysis", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Editorial Board Page", "This article has no abstract"]]}