{"2002": [["Discovering Combinations of Genomic Aberrations Associated With Cancer", "This article introduces a model-based statistical methodology for the analysis of copy-number variations in cancer genomes measured by comparative genomic hybridization. The methodology allows one to infer combinations of genomic aberrations associated with the cancer phenotype. The stochastic model conjoins two features of cancer biology to infuse some context into an otherwise unsupervised learning problem. It asserts random genomic instability in a potential progenitor cell, followed by selection into a tumor of the descending cell lineage if the lineage experiences certain ensembles of genomic aberration. Disease heterogeneity is reflected in the possibility of a network containing multiple ensembles. The network of ensembles is an identifiable parameter. By forming the sampling model conditionally on selection, statistical dependencies (both positive and negative) can be induced between aberrations, and the model entails heterogeneity in the marginal rate of occurrence of aberrations. A double-P\u00f3lya distribution is introduced as a prior over the network of ensembles, and Markov chain Monte Carlo is developed to enable posterior computation. As an example, the methodology is used to reanalyze genomic aberrations from 116 renal cell carcinomas. It produces posterior probabilities that any given aberration is relevant to oncogenesis, posterior probabilities that pairs of aberrations reside in a common ensemble, and a point estimate of the network of ensembles. The methodology provides a model-based clustering of all measured aberrations according to these estimated ensembles and a model-based clustering of tumors according to the probable ensembles of genomic aberration that they have experienced. Although it is formulated here to analyze aberrations in cancer genomes, the instability-selection-network model may provide an approach to modeling dependence in correlated binary data on various biological systems. Limitations and possible extensions of the methodology are discussed."], ["The Clustering of Infected SIV Cells in Lymphatic Tissue", "Here we investigate the clustering of simian immunodeficiency virus (SIV)\u2013infected cells in a lymphatic tissue sample taken from a rhesus macaque to test a spatial proximity model of the spread of infection. We see that standard methods for analysis of the clustering of point processes are not entirely satisfactory for this application, so we define a novel statistic to understand the clustering in the data. This statistic examines how events spread out from certain points deemed cluster centers. Using this statistic, we can demonstrate the statistical significance of the clustering and examine over what distances this clustering is witnessed. We use Bayesian methods to fully assess the uncertainty in the estimation of this statistic by positing a model for the process. (We assume the process is a nonhomogeneous Poisson process with an intensity that is a linear combination of Gaussian densities.) We see that the distances at which clustering is present are consistent with a simple model of SIV spread within the lymph node."], ["A Joint Model for Nonlinear Mixed-Effects Models With Censoring and Covariates Measured With Error, With Application to AIDS Studies", "In recent years AIDS researchers have shown great interest in the study of HIV viral dynamics. Nonlinear mixed-effects models (NLMEs) have been proposed for modeling intrapatient and interpatient variations in viral load measurements. The interpatient variation often receives great attention and may be partially explained by time-varying covariates, such as CD4 cell counts. Statistical analyses in these studies are complicated by the following problems: (a) the viral load measurements may be subject to left censoring due to a detection limit, (b) covariates are often measured with substantial errors, and (c) covariates frequently contain missing data. In this article we address these three problems simultaneously by jointly modeling the covariate and the response processes. We adapt a Monte Carlo EM algorithm and a linearization procedure to estimate the model parameters. Our approach is preferable to naive methods and the two-step method in the sense that it produces less-biased estimates with more-reliable standard errors. We analyze a real AIDS dataset and show that the fitted model may provide good prediction for unobserved viral loads."], ["Modeling Spatial Variation in Leukemia Survival Data", "In this article we combine ideas from spatial statistics with lifetime data analysis techniques to investigate possible spatial variation in survival of adult acute myeloid leukemia patients in northwest England. Exploratory analysis suggests both clinically and statistically significant variation in survival rates across the region. A multivariate gamma frailty model incorporating spatial dependence is proposed and applied, with results confirming the dependence of hazard on location."], ["Semiparametric Estimation of Brand Choice Behavior", "In the marketing literature there does not seem to be a widely accepted answer to the question of whether, when purchasing brands in a given product category, consumers react differently to a reduction in price and an increase in the deal discount. Previous studies that have attempted to address this issue have assumed that prices and deals have a linear effect on a brand's indirect utility. In this article, we estimate the utility of price and discount nonparametrically. Instead of imposing a linear structure on this function, we require only that it be decreasing in price and increasing in deal amount. This specification allows for a general pattern of interaction effects between prices and deals to influence the systematic component of utility. Consistent with the recent literature on estimating brand choice models, we account for heterogeneity in brand preferences across consumers. We use both a semiparametric approach, in which the distribution of the stochastic components of brand utilities is specified parametrically, and a fully nonparametric approach, in which this distribution is left unspecified. We carry out our empirical analyses on household scanner panel datasets for four different product categories. Our empirical results reveal deviations from linearity in deal effects. In particular, deal effects appear to be concave for some products."], ["Modeling the Survival of Chinook Salmon Smolts Outmigrating Through the Lower Sacramento River System", "To study the factors associated with the freshwater mortality of outmigrating chinook salmon, releases of tagged juvenile salmon were made at multiple locations in the Sacramento River each spring between the years 1979 and 1995. A midwater trawl located downstream of the release sites caught salmon soon after release and, 1 to 4 years later, samples taken from the catches of marine fisheries recovered other tagged fish. An extended quasi-likelihood model was fit to both the freshwater and the marine recoveries. A ridge parameter was included to stabilize the parameter estimates and to improve predictive ability. Overdispersion was due, at least in part, to heterogeneity in the trawl's capture efficiency, as well as to the complex aggregation of marine recoveries. Different dispersion parameters were used for the river and ocean recoveries because of the additional sources of variation experienced by ocean recoveries relative to river recoveries. Interpretation of estimated coefficients was delicate, given the correlation between some of the covariates, the biases introduced by the ridge parameter, and possible confounding factors. With these caveats in mind, we found the most influential covariate to be the temperature of the water into which the fish were released, with increasing temperatures having a negative association with recoveries. Three covariates were of particular interest to the biologists and water managers: water flow, position of a water diversion gate (open or closed) separating the mainstem from the central delta, and relative fraction of water exported for irrigation and urban consumption. The effects of flow were slightly positive but were confounded by salinity levels. The effect of the water diversion gate being open was to lower apparent survival for fish released above the gate, but apparent survival increased for fish released in the central delta into which the water was diverted. There was evidence that increasing the export-to-inflow ratio lowered survival, but the effect was slight and not statistically significant."], ["Nearest-Neighbor Variance Estimation (NNVE)", null], ["Modeling Regression Error With a Mixture of Polya Trees", "We model the error distribution in the standard linear model as a mixture of absolutely continuous Polya trees constrained to have median 0. By considering a mixture, we smooth out the partitioning effects of a simple Polya tree and the predictive error density has a derivative everywhere except 0. The error distribution is centered around a standard parametric family of distributions and thus may be viewed as a generalization of standard models in which important, data-driven features, such as skewness and multimodality, are allowed. By marginalizing the Polya tree, exact inference is possible up to Markov chain Monte Carlo error."], ["A Unified Theory of Two-Stage Adaptive Designs", "Adaptation of a clinical trial occurs when there is a change from the planned course of action in design, sample size, or method of analysis. There are many proposed adaptations in the literature, but no comprehensive mathematical theory to support them. The standard Neyman\u2013Pearson theory is not suitable for the setting, and its use may result in invalid inference. This article provides a general formulation of adaptation, under which its validity can be broadly and rigorously established. The basic theory on distribution, stochastic independence, point estimation, confidence intervals, hypothesis testing, and asymptotics is developed. Various examples are given to illustrate applications of the theory. The goal is to provide a solid theoretical foundation to aid the development of adaptive methods for clinical investigations."], ["Penalized Spline Estimation for Partially Linear Single-Index Models", null], ["Hidden Markov Models and Disease Mapping", "We present new methodology to extend hidden Markov models to the spatial domain, and use this class of models to analyze spatial heterogeneity of count data on a rare phenomenon. This situation occurs commonly in many domains of application, particularly in disease mapping. We assume that the counts follow a Poisson model at the lowest level of the hierarchy, and introduce a finite-mixture model for the Poisson rates at the next level. The novelty lies in the model for allocation to the mixture components, which follows a spatially correlated process, the Potts model, and in treating the number of components of the spatial mixture as unknown. Inference is performed in a Bayesian framework using reversible jump Markov chain Monte Carlo. The model introduced can be viewed as a Bayesian semiparametric approach to specifying flexible spatial distribution in hierarchical models. Performance of the model and comparison with an alternative well-known Markov random field specification for the Poisson rates are demonstrated on synthetic datasets. We show that our allocation model avoids the problem of oversmoothing in cases where the underlying rates exhibit discontinuities, while giving equally good results in cases of smooth gradient-like or highly autocorrelated rates. The methodology is illustrated on an epidemiologic application to data on a rare cancer in France."], ["Marginal Methods for Incomplete Longitudinal Data Arising in Clusters", "Inverse probability\u2013weighted generalized estimating equations are commonly used to deal with incomplete longitudinal data arising from a missing-at-random mechanism when the marginal means are of primary interest. In many cases, however, the repeated measurements themselves may arise in clusters, which leads to both a cross-sectional and a longitudinal correlation structure. In some applications, the degree of these types of correlation may become of scientific interest. Here we develop inverse probability\u2013weighted second-order estimating equations for monotone missing-data patterns which, under specified assumptions, facilitate consistent estimation of the marginal mean parameters and association parameters. Here the missing-data model accommodates cross-sectional clustering in the missing-data indicators, and the probabilities are estimated under a multivariate Plackett model. For computational reasons, we also consider using the alternating logistic regression algorithm for estimation of the association parameters for the responses. We investigate the importance of modeling the cross-sectional clustering in the missing-data process by simulation. An extension to deal with intermittently missing data is provided, and an application to a longitudinal cluster-randomized smoking prevention trial is presented."], ["Exact Distributions of Multiple Comparisons Rank Statistics", null], ["Latent Space Approaches to Social Network Analysis", "Network models are widely used to represent relational information among interacting units. In studies of social networks, recent emphasis has been placed on random graph models where the nodes usually represent individual social actors and the edges represent the presence of a specified relation between actors. We develop a class of models where the probability of a relation between actors depends on the positions of individuals in an unobserved \u201csocial space.\u201d We make inference for the social space within maximum likelihood and Bayesian frameworks, and propose Markov chain Monte Carlo procedures for making inference on latent positions and the effects of observed covariates. We present analyses of three standard datasets from the social networks literature, and compare the method to an alternative stochastic blockmodeling approach. In addition to improving on model fit for these datasets, our method provides a visual and interpretable model-based spatial representation of social relationships and improves on existing methods by allowing the statistical uncertainty in the social space to be quantified and graphically represented."], ["Semiparametric Receiver Operating Characteristic Analysis to Evaluate Biomarkers for Disease", null], ["Coordinate Based Empirical Likelihood-Like Estimation in Ill-Conditioned Inverse Problems", "In the context of a semiparametric regression model with underlying probability distribution unspecified, an extremum estimator formulation is proposed that makes use of empirical likelihood and information theoretic estimation and inference concepts to mitigate the problem of an ill-conditioned design matrix. A squared error loss measure is used to assess estimator performance in finite samples. In large samples, the estimator can be designed to be consistent and asymptotically normal, so that limiting chi-squared distributions provide a basis for hypothesis tests and confidence intervals. Empirical risk results based on a large-scale Monte Carlo sampling experiment suggest that the estimator has, relative to traditional competitors, superior finite-sample properties under a squared error loss measure when the design matrix is ill-conditioned."], ["Nonparametric Hypothesis Testing for a Spatial Signal", "Nonparametric hypothesis testing for a spatial signal can involve a large number of hypotheses. For instance, two satellite images of the same scene, taken before and after an event, could be used to test a hypothesis that the event has no environmental impact. This is equivalent to testing that the mean difference of \u201cafter-before\u201d is zero at each of the (typically thousands of) pixels that make up the scene. In such a situation, conventional testing procedures that control the overall type I error deteriorate as the number of hypotheses increases. Powerful testing procedures are needed for this problem of testing for the presence of a spatial signal. In this article we propose a procedure called enhanced false discovery rate (EFDR), which is based on controlling the false discovery rate (FDR) and a concept known as generalized degrees of freedom. EFDR differs from the standard FDR procedure by reducing the number of hypotheses tested. This is done in two ways: first, the model is represented more parsimoniously in the wavelet domain; second, an optimal selection of hypotheses is made using a criterion based on generalized degrees of freedom. Not only does the EFDR procedure tell us whether a spatial signal is present, but, if a signal is deemed present, it can also indicate its location and magnitude. We examine EFDR's operating characteristics and in simulations show that it outperforms the standard FDR and conventional testing procedures. Finally, we apply the EFDR procedure to an air temperature dataset generated from the climate system model of the National Center for Atmospheric Research, in which air temperatures in the 1980s are compared to those in the 1990s. We conclude that temperature change has occurred between the two decades, mostly warming in the central part of the United States and in coastal regions of South America at about 20\u00b0S."], ["Parsimonious Covariance Matrix Estimation for Longitudinal Data", "This article proposes a data-driven method to identify parsimony in the covariance matrix of longitudinal data and to exploit any such parsimony to produce a statistically efficient estimator of the covariance matrix. The approach parameterizes the covariance matrix through the Cholesky decomposition of its inverse. For longitudinal data, this is a one-step-ahead predictive representation, and the Cholesky factor is likely to have off-diagonal elements that are zero or close to zero. A hierarchical Bayesian model is used to identify any such zeros in the Cholesky factor, similar to approaches that have been successful in Bayesian variable selection. The model is estimated using a Markov chain Monte Carlo sampling scheme that is computationally efficient and can be applied to covariance matrices of high dimension. It is demonstrated through simulations that the proposed method compares favorably in terms of statistical efficiency with a highly regarded competing approach. The estimator is applied to three real examples in which the dimension of the covariance matrix is large relative to the sample size. The first two examples are from biometry and electricity demand modeling and are longitudinal. The third example is from finance and highlights the potential of our method for estimating cross-sectional covariance matrices."], ["Independent and Identically Distributed Monte Carlo Algorithms for Semiparametric Linear Mixed Models", "Hybrid versions of independent and identically distributed weighted Chinese restaurant (WCR) algorithms are developed for inference in semiparametric linear mixed models under minimal assumptions for the random-effects distributions. The WCR method of working with the posterior partition structure leads to Rao\u2013Blackwell estimates for higher-order moments of random effects, such as skewness and kurtosis, and can be used to estimate densities for random effects. A key feature of our approach is the manner in which we incorporate external estimates into our algorithms. The use of such information leads to simplified computational procedures, reduces the amount of user input required for specifying models, and results in numerical stability and accuracy. The resulting procedures are automated and can be readily used in standard statistical software. Our methods are tested by simulation and illustrated by application to a longitudinal study involving chronic renal disease."], ["Forecasting Using Principal Components From a Large Number of Predictors", null], ["Rank Regression in Ranked-Set Samples", "The use of statistical methods based on ranked-set sampling can lead to a substantial improvement over analog methods associated with simple random sampling schemes. This article develops a rank-based estimator and testing procedures for linear models for ranked-set samples. The estimator is defined as the minimizer of the rank dispersion function with Wilcoxon scores. It is shown that the estimator of the regression parameter is asymptotically normal and has higher Pitman asymptotic efficiency than a simple random-sample rank regression estimator. Three testing procedures are developed to test a general linear hypothesis: dispersion, Wald, and aligned rank tests. It is shown that all of these test statistics converge to a chi-squared distribution and the aligned rank test reduces to a simple random-sample analog of Kruskal-Wallis test for one-way analysis of variance. Under the assumption of perfect judgment ranking, optimal allocation of order statistics are constructed for set sizes smaller than 7. The optimal allocation procedures quantify middle observation(s) for symmetric unimodal distributions and smallest (largest) observation for right- (left-) skewed distributions."], ["Extended RC Association Models Allowing for Order Restrictions and Marginal Modeling", null], ["Statisticians of the Centuries", null], ["The Science of Conjecture: Evidence and Probability Before Pascal", null], ["Mathematical Statistics With Mathematica", null], ["Empirical Likelihood", null], ["An Introduction to Statistical Modeling of Extreme Values", null], ["Conditional Specification of Statistical Models", null], ["Runs and Scans With Applications", null], ["Matrix Algebra for Applied Economics, and Matrix Algebra Exercises and Solutions", null], ["Analysis of Messy Data, Vol III: Analysis of Covariance", null], ["Observational Studies", null], ["Foundations of Time Series Analysis and Prediction Theory", null], ["Analysis of Time Series Structure: SSA and Related Techniques", null], ["Asymptotic Theory of Statistical Inference for Time Series", null], ["Stochastic Processes: From Physics to Finance", null], ["The Laplace Distribution and Generalizations: A Revisit With Applications to Communications, Economics, Engineering, and Finance", null], ["Ruin Probabilities", null], ["Introduction to Stochastic Networks", null], ["Probability Essentials", null], ["Practical Considerations in Computer-Based Testing", null], ["Independent Component Analysis: Principles and Practice", null], ["Telegraphic Reviews", null], ["Correction", null], ["Accounting for the Black\u2013White Wealth Gap", "Many applications involve a decomposition of the mean intergroup difference in a given variable into the portion attributable to differences in the distribution of one or more explanatory variables and that due to differences in the conditional expectation function. This article notes two interrelated reasons why the Blinder\u2013Oaxaca (B\u2013O) method\u2014the approach most commonly used in the literature\u2014may yield misleading results. We suggest a natural solution that both provides a more reliable answer to the original problem and affords a richer examination of the sources of intergroup differences in the variable of interest. The conventional application of the B\u2013O method requires a parametric assumption about the form of the conditional expectation function. Furthermore, it often uses estimates based on that functional form to extrapolate outside the range of the observed explanatory variables. We show that misspecification of the conditional expectation function is likely to result in nontrivial errors in inference regarding the portion attributable to differences in the distribution of explanatory variables, a problem compounded by the computation of conditional expectations outside the observed range of the conditioning variables. Here we propose a nonparametric alternative to the B\u2013O method that reweights the empirical distribution of the outcome variable using weights that equalize the empirical distributions of the explanatory variable. We apply this method to the role of earnings in explaining the black-white wealth difference. The problems with the B\u2013O method show up clearly in this application, because the function relating wealth to earnings is highly nonlinear (with a functional form unspecified by theory) and because the earnings distribution for blacks is shifted sharply to the left of that for whites. We argue that it is not possible to examine the hypothetical distribution of black wealth holdings conditional on the observed white earnings function. For the question that we can answer\u2014the distribution of wealth for a synthetic sample of blacks and whites with comparable earnings\u2014we find that two-thirds of the mean difference in wealth can appropriately be attributed to earnings. In addition, we fully characterize the distribution of white and black wealth conditional on earnings."], ["A Unified Approach to Conjoint Analysis Models", "We present a unified approach to conjoint analysis models using a Bayesian framework. One data source is used to form a prior distribution for the partworths, whereas full-profile evaluations under a rating scale, ranking, discrete choice, or constant-sum scale constitute the likelihood data (\u201cone model fits all\u201d). Standard existing models for conjoint analysis, considered in the literature, become particular cases of the proposed specification, and explicit formulas for the gains of using multiple sources of data are presented. We demonstrate our method on a conjoint analysis dataset containing both self-explicated evaluations and constant-sum profile data on new automobiles originally collected and described by Krieger, Green, and Umesh. Our empirical findings are \u201cmixed\u201d in that for some out-of-sample predictive measures our Bayesian approach is superior to using profile-only or self-explicated\u2013only data, and for other measures it is not. Our findings suggest that the primary determinant as to whether self-explicated data add information above and beyond the profile data is the degree of incongruity between the calibration and validation data formats. Specifically, when the same type of data are collected for both sources, self-explicated data add less, and vice versa. A further contribution of our work (and one that is easily implemented, given the general nature of our approach) is that we take our data and fit constant sum, ranking, and binary choice models to it, allowing us to infer the \u201cchange\u201d in information when taking data and transforming its scale (a common practice). A simulation study indicates the viability of this approach. A simple Gibbs sampler simulation scheme adapted to the form of the outcome measure, using data augmentation and Metropolis sampling, is considered for inference under the model."], ["A Marked Point Process Model for the Source Proximity Effect in the Indoor Environment", "In indoor air quality studies, discrepancies between personal and stationary indoor air quality monitors arise because of the source proximity effect, in which pollutant sources near the respondent cause elevated and highly variable exposures. In a set of experiments in a home, concentrations of a continuously emitting tracer gas were simultaneously monitored at different distances from the tracer gas source. Concentration time series are modeled at collinear monitoring sites as the sum of a slowly varying baseline time series and the superposition of transient elevated concentrations, or \u201cmicroplumes.\u201d Microplume arrivals appear as pulses in the time series, with pulse magnitudes and duration varying by location relative to the source. A nonparametric method is developed to estimate the time-varying parameters of the baseline time series. Parameters of superposed microplumes are estimated using the method of moments. Bias and sampling error of estimates are investigated using a simulation study. Estimates of superposition model parameters provide insight into the physical reasons behind the source proximity effect as well as a description of components of exposure at different distances from an emitting source."], ["Modeling Spatial Variation in Disease Risk", null], ["On the Relative Importance of Input Factors in Mathematical Models", "This article deals with global quantitative sensitivity analysis of the Level E model, a computer code used in safety assessment for nuclear waste disposal. The Level E code has been the subject of two international benchmarks of risk assessment codes and Monte Carlo methods and is well known in the literature. We discuss the Level E model with reference to two different settings. In the first setting, the objective is to find the input factor that drives most of the output variance. In the second setting, we strive to achieve a preestablished reduction in the variance of the model output by fixing the smallest number of factors. The emphasis of this work is on how to define the concept of importance in an unambiguous way and how to assess it in the simultaneous occurrence of correlated input factors and non-additive models."], ["Comparing the Performance of Baseball Players", "This article extends ideas from the economics literature on multiple output production and efficiency to develop methods for comparing baseball players that take into account the many dimensions to batting performance. A key part of this approach is the output aggregator. The weights in this output aggregator can be selected a priori (as is done with batting or slugging averages) or can be estimated statistically based on the performance of the best players in baseball. Once the output aggregator is obtained, an individual player can then be measured relative to the best, and a number between 0 and 1 characterizes his performance as a fraction of the best. The methods are applied to hitters using data from 1995\u20131999 on all regular players in baseball's major leagues."], ["Latent Class Analysis of Complex Sample Survey Data", "High fruit and vegetable intake is associated with decreased cancer risk. However, dietary recall data from national surveys suggest that, on any given day, intake falls below the recommended minima of three daily servings of vegetables and two daily servings of fruit. There is no single widely accepted measure of \u201cusual\u201d intake. One approach is to regard the distribution of intake as a mixture of \u201cregular\u201d (relatively frequent) and \u201cnonregular\u201d (relatively infrequent) consumers, using an indicator of whether an individual consumed the food of interest on the recall day. We use a new approach to summarizing dietary data, latent class analysis (LCA), to estimate \u201cusual\u201d intake of vegetables. The data consist of four 24-hour dietary recalls from the 1985 Continuing Survey of Intakes by Individuals collected from 1,028 women. Traditional LCA based on simple random sampling was extended to complex survey data by introducing sample weights into the latent class estimation algorithm and by accounting for the complex sample design through the use of jackknife standard errors. A two-class model showed that 18%% do not regularly consume vegetables, compared to an unweighted estimate of 33%%. Simulations showed that ignoring sample weights resulted in biased parameter estimates and that jackknife variances were slightly conservative but provided satisfactory confidence interval coverage. Using a survey-wide estimate of the design effect for variance estimation is not accurate for LCA. The methods proposed in this article are readily implemented for the analysis of complex sample survey data."], ["Adjusting for Population Heterogeneity and Misspecified Haplotype Frequencies When Testing Nonparametric Null Hypotheses in Statistical Genetics", "Population heterogeneity and misspecified population haplotype frequencies are potential sources of bias in a variety of hypothesis testing problems in statistical genetics. Several strategies have been proposed for adjusting for population heterogeneity, but the theoretical basis for the strategies has been developed in relatively narrow contexts, and the methods in general do not exploit all of the available information. Here a generally applicable approach for efficient adjustment is proposed. Practical issues related to the implementation of the approach are described, and a number of settings where the approach may be applied are outlined."], ["Identification of Shared Components in Large Ensembles of Time Series Using Dimension Reduction", "In this article we present a framework for parsimonious modeling of large ensembles of time series. The idea is to identify a small number of stochastic time series components such that each series in the ensemble is a weighted sum of series-specific realizations of the components. We present an estimation procedure that is computationally tractable for large datasets. Through simulations, we argue that the estimators perform well. We illustrate the method using two datasets: monthly U.S. unemployment rates from 2069 counties over 134 months, and a functional magnetic resonance imaging series covering 10,740 voxels over 108 time points. Our framework is conceptually different from the principal component analysis (PCA) commonly used in multivariate data analysis. We argue that applying PCA directly to an ensemble of time series captures a shared deterministic structure, rather than the series-specific stochastic components that we seek."], [null, null], ["Markov Chain Marginal Bootstrap", "Markov chain marginal bootstrap (MCMB) is a new method for constructing confidence intervals or regions for maximum likelihood estimators of certain parametric models and for a wide class of M estimators of linear regression. The MCMB method distinguishes itself from the usual bootstrap methods in two important aspects: it involves solving only one-dimensional equations for parameters of any dimension and produces a Markov chain rather than a (conditionally) independent sequence. It is designed to alleviate computational burdens often associated with bootstrap in high-dimensional problems. The validity of MCMB is established through asymptotic analyses and illustrated with empirical and simulation studies for linear regression and generalized linear models."], ["Invariance, Identifiability, and Morphometrics", "The form of an object is that characteristic that remains invariant under a group of transformations comprising translation, rotation, and possibly reflection. Group invariance thus naturally plays an important role in the statistical analysis of forms. We examine the existing methods for the statistical analysis of form from the invariance perspective. We begin with a review of the important basic ideas behind invariance and derive a maximal invariant under the group of transformations consisting of rotation, reflection, and translation and its distribution under the Gaussian and elliptically symmetric perturbation models. We first consider the single-sample case and discuss the issue of identifiability of the parameters. We show that method-of-moments estimators based on the distances between landmarks and maximum likelihood estimators (MLEs) based on the size and shape coordinates are invariant and estimate identifiable parameters. However, a number of commonly used methods do not. We compare the statistical and computational efficiencies of the method-ofmoments estimator and MLE and show that the method of moments substantially simplifies the estimation procedure computationally with only a small loss of statistical efficiency. We then extend the discussion of invariance to the comparison of two forms. We discuss the relationship between identifiability and invariance in the two-sample case and again show that many commonly used methods base inference on nonidentifiable parameters and discuss the scientific implications of basing inferences on nonidentifiable parameters using a biological example. We provide a brief summary of a method for shape analysis that is invariant."], ["Dynamically Weighted Importance Sampling in Monte Carlo Computation", "This article describes a new Monte Carlo algorithm, dynamically weighted importance sampling (DWIS), for simulation and optimization. In DWIS, the state of the Markov chain is augmented to a population. At each iteration, the population is subject to two move steps, dynamic weighting and population control. These steps ensure that DWIS can move across energy barriers like dynamic weighting, but with the weights well controlled and with a finite expectation. The estimates can converge much faster than they can with dynamic weighting. A generalized theory for importance sampling is introduced to justify the new algorithm. Numerical examples are given to show that dynamically weighted importance sampling can perform significantly better than the Metropolis\u2013Hastings algorithm and dynamic weighting in some situations."], ["An Adaptive, Rate-Optimal Test of Linearity for Median Regression Models", "This article is concerned with testing the hypothesis that a conditional median function is linear against a nonparametric alternative with unknown smoothness. We develop a test that is uniformly consistent against alternatives whose distance from the linear model converges to zero at the fastest possible rate. The test does not require knowledge of the distribution of the model's random noise component, and it permits conditional heteroscedasticity of unknown form. The numerical performance and usefulness of the test are illustrated by the results of Monte Carlo experiments and an empirical example."], ["Saddlepoint Approximation and Bootstrap Inference for the Satterthwaite Class of Ratios", null], ["Designing Follow-Up Times", " Specifically, we consider time-independent hazard rates. We derive posterior and predictive distributions in three scenarios: single follow-up time for the estimation of a single hazard rate, group-specific follow-up time for the comparison of hazard rates in two treatment groups or cohorts, and multiple follow-up times for a single hazard rate. We encounter a novel family of mixtures of gamma functions and characterize its moments, which play a critical role in the determination of optima. We then provide a solution to the optimal follow-up time in the single follow-up problem. We develop a practical and accurate approximation to the optimal solution as a function of prior hyperparameters that can be used to implement real time calculations and more complex sequential strategies. Finally, we consider the sequential choice of follow-up times. We discuss the general dynamic programming solution and illustrate it in the setting of a two-stage design."], ["Predictive Variable Selection in Generalized Linear Models", "Here we extend predictive method for model selection of Laud and Ibrahim to the generalized linear model. This prescription avoids the need to directly specify prior probabilities of models and prior densities for the parameters. Instead, a prior prediction for the response induces the required priors. We propose normal and conjugate priors for generalized linear models, each using a single prior prediction for the mean response to induce suitable priors for each variable-subset model. In this way, an informative prior is used to select a subset of variables. In addition to producing a ranking of models by size of the predictive criterion, the standard deviation of the criterion is used as a calibration number to produce a set of equally good models. A straightforward Markov chain Monte Carlo algorithm is used to accomplish the necessary computations. We illustrate this method with real and simulated datasets and compare results with the Bayes factors and the Akaike information and Bayes information model selection criteria. The simulation results confirm the efficacy of the method, because the correct model is known. An illustrative application demonstrates selection of important predictors of success in identifying the sentinel lymph node during surgical treatment of breast cancer. A forward selection procedure is described to avoid a full search over the 218 possible models in this case."], ["Three-Step Censored Quantile Regression and Extramarital Affairs", "This article suggests very simple three-step estimators for censored quantile regression models with a separation restriction on the censoring probability. The estimators are theoretically attractive (i.e., asymptotically as efficient as the celebrated Powell's censored least absolute deviation estimator). At the same time, they are conceptually simple and have trivial computational expenses. They are especially useful in samples of small size or models with many regressors, with desirable finite-sample properties and small bias. The separation restriction costs a small reduction of generality relative to the canonical censored regression quantile model, yet its main plausible features remain intact. The estimator can also be used to estimate a large class of traditional models, including the normal Amemiya\u2013Tobin model and many accelerated failure and proportional hazard models. We illustrate the approach with an extramarital affairs example and contrast our findings with those of Fair."], ["Local Polynomial Mixed-Effects Models for Longitudinal Data", null], ["Optimal Crossover Designs in a Model With Self and Mixed Carryover Effects", "We consider a variant of the usual model for crossover designs with carryover effects. Instead of assuming that the carryover effect of a treatment is the same regardless of the treatment in the next period, the model assumes that the carryover effect of a treatment on itself is different from the carryover effect on other treatments. For the traditional model, optimal designs tend to have pairs of consecutive identical treatments; for the model considered here, they tend to avoid such pairs. Practitioners have long expressed reservations about designs that exhibit such pairs and about the traditional model. The new model provides an attractive alternative that leads to appealing optimal designs."], ["Nonparametric Estimation of a Bivariate Mean Residual Life Function", "In many statistical studies involving failure data, mean residual life function is of prime importance. The bivariate mean residual life function has received relatively less attention in the literature. In this article we use a simple nonparametric estimator for a bivariate mean residual life function. The estimator is shown to be uniformly strongly consistent and, on proper normalization, converges weakly to a zero-mean bivariate Gaussian process. Numerical studies demonstrate that the estimator performs well even for moderate sample sizes. Results are applied to a real dataset related to cancer recurrence. A few supporting results in connection with weak convergence proved in Appendix C may be of independent interest."], ["Curve Ball: Baseball, Statistics, and the Role of Chance in the Game", null], ["Mathematics and Sports (Mathematical World, Vol. 3)", null], ["Time-Series Forecasting", null], ["Asymptotic Theory for Econometricians", null], ["Probability, Econometrics and Truth: The Methodology of Econometrics", null], ["Probability and Finance: It's Only a Game!", null], ["The Theory of the Design of Experiments", null], ["Experimental Quality: A Strategic Approach to Achieve and Improve Quality", null], ["Introductory Statistics and Random Phenomena: Uncertainty, Complexity and Chaotic Behavior in Engineering and Science", null], ["Probabilistic Risk Analysis: Foundations and Methods", null], ["Limit Distributions for Sums of Independent Random Vectors", null], ["Applying and Interpreting Statistics, A Comprehensive Guide", null], ["A Contingency Table Approach to Nonparametric Testing", null], ["Scan Statistics", null], ["An Invariant Approach to Statistical Analysis of Shapes", null], ["Telegraphic Reviews", null], ["Correction", null], ["Stochastic Declustering of Space-Time Earthquake Occurrences", "This article is concerned with objective estimation of the spatial intensity function of the background earthquake occurrences from an earthquake catalog that includes numerous clustered events in space and time, and also with an algorithm for producing declustered catalogs from the original catalog. A space-time branching process model (the ETAS model) is used for describing how each event generates offspring events. It is shown that the background intensity function can be evaluated if the total spatial seismicity intensity and the branching structure can be estimated. In fact, the whole space-time process is split into two subprocesses, the background events and the clustered events. The proposed algorithm combines a parametric maximum likelihood estimate for the clustering structures using the space-time ETAS model and a nonparametric estimate of the background seismicity that we call a variable weighted kernel estimate. To demonstrate the present methods, we estimate the background seismic activities in the central region of New Zealand and in the central and western regions of Japan, then use these estimates to produce catalogs of background events."], ["Hierarchical Bayesian Nonresponse Models for Binary Data From Small Areas With Uncertainty About Ignorability", "In the National Crime Survey (NCS), data on victimization can be poststratified into domains determined by urbanization, type of place, and poverty level. There is much difficulty in the analysis of binary data with substantial nonresponse. We consider three Bayesian hierarchical models for binary nonresponse data, like those from the NCS, which are clustered within a number of domains or areas. As in small area estimation, one key feature is that each model \u201cborrows strength\u201d across the areas through the selection approach to nonresponse. This is necessary to estimate the parameters with the least association to the observed data (i.e., weakly identified parameters). The first model assumes that the nonresponse mechanism is ignorable, and the second model assumes that it is nonignorable. We argue that a discrete model expansion (a probabilistic mixture) may be inappropriate for modeling uncertainty about ignorability. Therefore, we propose a third model through a continuous model expansion on an odds ratio for each area. When the odds ratio is 1, we have the ignorable model; otherwise, the model is nonignorable. One important feature is that uncertainty about ignorability is incorporated by \u201ccentering\u201d on the ignorable model. We analyze the poststratified data from the NCS to reveal latent features associated with nonresponse. The complexity of the posterior distributions of the parameters forces us to implement the methodology using Markov chain Monte Carlo methods. When the proportion of households with a characteristic (i.e., victimization in the NCS) and the response probability of a household in the population are estimated, we find that the nonignorable model and the expansion model are similar but that they differ from the ignorable model. Although considerable prior information about the nonresponse mechanism is needed, the expansion model indicates that nonresponse for most of the areas is nonignorable. An analysis shows that inference is not very sensitive to an important distribution assumption, and a simulation exercise shows that the expansion model works very well."], ["Estimation of Waning Vaccine Efficacy", "Whether the protection afforded by a vaccine wanes over time and if so, by how much, are important public health questions with implications for mass immunization programs. However, the measurement of such effects is fraught with difficulties. Generally, clinical trials are too short to provide evidence on waning effects. Thus it is necessary to rely on epidemiologic data, which are subject to ascertainment bias. Furthermore, standard epidemiologic measures, such as hazard ratios, may not accurately reflect waning effects. We develop two models of waning vaccine efficacy, the selection and deterioration models. In the selection model, waning arises from heterogeneity in the duration of protection. In the deterioration model, the hazard ratio of infection in vaccinated individuals relative to unvaccinated individuals increases with time. The two models are the with-waning analogs of the previously defined \u201call or nothing\u201d and \u201dleaky\u201d models. Likelihood functions based on these models are obtained for cohort and case-report data, as typically collected in surveillance studies of vaccine efficacy. These likelihood functions involve three groups of parameters, corresponding to the hazard of infection, vaccine efficacy (possibly including waning effects), and ascertainment of infections. Our strategy is to fit a range of models incorporating a variety of assumptions about the vaccine efficacy model, ascertainment bias, and age dependence of the hazard. If allowing for waning improves the fit of the model under a range of scenarios, then we infer that it is a genuine effect. We apply the methods to two sets of data on whooping cough vaccine efficacy in the United Kingdom."], ["A Nonparametric Test of Gene Region Heterogeneity Associated With Phenotype", null], ["Empirical Bayes and Item-Clustering Effects in a Latent Variable Hierarchical Model", "Empirical Bayes regression procedures are often used in educational and psychological testing as extensions to latent variables models. The National Assessment of Educational Progress (NAEP) is an important national survey using such procedures. The NAEP applies empirical Bayes methods to models from item response theory to calibrate student responses to questions of varying difficulty. Due partially to the limited computing technology that existed when NAEP was first conceived, NAEP analyses are carried out using a two-stage estimation procedure that ignores uncertainty about some model parameters. Furthermore, the item response theory model that NAEP uses ignores the effect of item clustering created by the design of a test form. Using Markov chain Monte Carlo, we simultaneously estimate all parameters of an expanded model that considers item clustering to investigate the impact of item clustering and ignoring uncertainty about model parameters on an important outcome measure that NAEP report. Ignoring these two effects causes substantial underestimation of standard errors and induces a modest bias in location estimates."], ["The Effect of Duration and Delay of Licensure on Risk of Crash", "The driving history records of a sample of 13,794 Michigan public school students were followed for up to 13 years from their initial time-of-license to determine the separate effects of duration of licensure and delay of licensure on risk of crash. We propose a subject-specific lognormal accelerated failure time to model the expected time-to-crash as a function of age at time of licensure, duration of licensure, and a set of control covariates. When multiple time-to-crash measures are observed for an individual, within-subject correlation can create substantial bias in the estimation of the effect of duration of licensure under an independence model. Generalized estimating equations provide consistent estimators of the variance when independence is misspecified but do not correct for this bias. Full maximum likelihood models generally require numerical integration and differentiation, and in practice, parameter estimates were unattainable for the dataset of interest. We instead adopt a Bayesian approach, imputing the unobserved failure times and slope-intercept random effects to account for right censoring and between-subject variability. We implement this approach using a Gibbs algorithm. We assess model fit via posterior predictive distributions. Our approach also allows for subject-specific risk estimates based on subject-level history. We compare the repeated sampling properties of this approach with those obtained using some frequentist approaches, and find that duration of licensure is a stronger predictor of risk of crash than age of licensure."], ["Multiple-Output Production With Undesirable Outputs", "Many production processes yield both good outputs and undesirable ones (e.g., pollutants). In this article we develop a generalization of a stochastic frontier model that is appropriate for such technologies. We discuss efficiency analysis and, in particular, define technical and environmental efficiency in the context of our model. We develop methods for carrying out Bayesian inference and apply them to a panel data set of Dutch dairy farms, where excess nitrogen production constitutes an important environmental problem."], ["Sieve Bootstrap With Variable-Length Markov Chains for Stationary Categorical Time Series", "A bootstrap for stationary categorical time series based on the method of sieves is studied here. The data-generating process is approximated by the so-called variable-length Markov chain (VLMC), a flexible class of Markov models that allows for parsimonious structure. Then the resampling is given by simulating from the fitted model. It is shown that for a whole class of stationary categorical time series that is more general than VLMC, the VLMC sieve has faster rate of convergence for variance estimation than the more general block bootstrap. Results are illustrated from a theoretical and empirical perspective. For the latter, a real data application about (in-) homogeneity classification of a DNA strand is also presented. Finally, the VLMC sieve scheme enjoys an implementational advantage of using the plug-in rule for bootstrapping a statistical procedure, which generally is not the case for the block method."], ["Corrected Score Estimation via Complex Variable Simulation Extrapolation", "A Monte Carlo method of computing unbiased estimating equations for the analysis of data measured with error is described. Asymptotic distribution results are obtained for estimators derived from the Monte Carlo estimating equations. The method is illustrated with examples, applications, and simulation studies. The Monte Carlo estimating equations are corrected scores in the sense of Nakamura, and the proposed methods are closely related to the simulation method described by Cook and Stefanski."], ["Marginal Longitudinal Nonparametric Regression", "We consider nonparametric regression in a longitudinal marginal model of generalized estimating equation (GEE) type with a time-varying covariate in the situation where the number of observations per subject is finite and the number of subjects is large. In such models, the basic shape of the regression function is affected only by the covariate values and not otherwise by the ordering of the observations. Two methods of estimating the nonparametric function can be considered: kernel methods and spline methods. Recently, surprising evidence has emerged suggesting that for kernel methods previously proposed in the literature, it is generally asymptotically preferable to ignore the correlation structure in our marginal model and instead assume that the data are independent, that is, working independence in the GEE jargon. As seen through equivalent kernel results, in univariate independent data problems splines and kernels have similar behavior; smoothing splines are equivalent to kernel regression with a specific higher-order kernel, and hence smoothing splines are local. This equivalence suggests that in our marginal model, working independence might be preferable for spline methods. Our results suggest the opposite; via theoretical and numerical calculations, we provide evidence suggesting that for our marginal model, marginal smoothing and penalized regression splines are not local in their behavior. In contrast to the kernel results, our evidence suggests that when using spline methods, it is worthwhile to account for the correlation structure. Our results also suggest that spline methods appear to be more efficient than the previously proposed kernel methods for our marginal model."], ["Locally Efficient Estimation of a Multivariate Survival Function in Longitudinal Studies", null], ["Detecting the Presence of Mixing with Multiscale Maximum Likelihood", null], ["Semiparametric Mixed-Effects Models for Clustered Failure Time Data", "The Cox proportional hazards model with a random effect has been proposed for the analysis of data which consist of a large number of small clusters of correlated failure time observations. The class of linear transformation models provides many useful alternatives to the Cox model for analyzing univariate failure time data. In this article, we generalize these models by incorporating random effects, which generate the dependence among the failure times within the cluster, to handle correlated data. Inference and prediction procedures for such random effects models are proposed. They are relatively simple compared with the methods based on the nonparametric maximum likelihood estimators for the Cox frailty model in the literature. Our proposals are illustrated with a data set from a well-known eye study. Extensive numerical studies are conducted to evaluate various robustness properties of the new procedures."], ["Estimating a Changepoint, Boundary, or Frontier in the Presence of Observation Error", null], ["Efficient Estimation of Quadratic Finite Population Functions in the Presence of Auxiliary Information", "By viewing quadratic and other second-order finite population functions as totals or means over a derived synthetic finite population, we show that the recently proposed model calibration and pseudoempirical likelihood methods for effective use of auxiliary information from survey data can be readily extended to obtain efficient estimators of quadratic and other second-order finite population functions. In particular, estimation of a finite population variance, covariance, or variance of a linear estimator can be greatly improved when auxiliary information is available. The proposed methods are model assisted in that the resulting estimators are asymptotically design unbiased irrespective of the correctness of a working model but very efficient if the working model is nearly correct. They have a number of attractive features, which include applicability to a general sampling design, incorporation of information on possibly multivariate auxiliary variables, and the ability to entertain linear or nonlinear working models, and they result in nonnegative estimates for certain strictly positive quantities such as variances. Several existing estimators are shown to be special cases of the proposed general methodology under a linear working model."], ["Sample Correlation Coefficients Based on Survey Data Under Regression Imputation", "Regression imputation is commonly used to compensate for item nonresponse when auxiliary data are available. It is common practice to compute survey estimators by treating imputed values as observed data and using the standard unbiased (or nearly unbiased) estimation formulas designed for the case of no nonresponse. Although the commonly used regression imputation method preserves unbiasedness for population marginal totals (i.e., survey estimators computed from imputed data are still nearly unbiased), it does not preserve unbiasedness for population correlation coefficients. A joint regression imputation method is proposed that preserves unbiasedness for marginal totals, second moments, and correlation coefficients. Some simulation results show that the joint regression imputation method produces not only sample correlation coefficients that are nearly unbiased, but also estimates that are more stable than those produced by marginal nonrandom regression imputation when correlation coefficients are in a certain range. Variance estimation for sample correlation coefficients under joint regression imputation is also studied, using a jackknife method that takes imputation into account."], ["An Exact Multivariate Model-Based Structural Decomposition", "We propose a simple and structured procedure for decomposing a vector of time series into trend, cycle, seasonal, and irregular components. Contrary to common practice, we do not assume these components to be orthogonal conditional on their past. However, the state\u2013space representation employed ensures that their estimates converge to values with null variances and covariances. Null variances are very important, as they ensure that the components do not change when the sample increases. This lack of \u201crevisions\u201d is the most important feature of our method, in comparison with most alternative procedures. On the other hand, null covariances provide a solid statistical foundation for the decomposition, as it ensures that a given component can be analyzed and interpreted independently of any other component(s). Other convenient properties of our method derive from the use of a state\u2013space approach. First, defining the problem in state\u2013space avoids dependence on particular model specifications, so the same procedure can be applied to a wide class of data representations, including ARIMA, VARMAX, univariate transfer functions, and structural time series models. Also, state\u2013space methods deal easily with nonstandard situations, such as samples with missing values or constraints upon the structural components. Practical application of the procedure is illustrated with both simulated and real data. A MATLAB toolbox for time series modeling and decomposition is available via the Internet."], ["Double-Semiparametric Method for Missing Covariates in Cox Regression Models", "The problem of nuisance covariate model specification is considered in Cox regression where the maximum semiparametric likelihood method is used to handle the missing covariates. A component of the covariates is modeled nonparametrically to achieve robustness against covariate model misspecification and to reduce the number of possibly intractable integrations involved in the parametric modeling of the covariates. The statistical properties of the proposed method are examined. It is found that in some important situations, the maximum semiparametric likelihood can be applied without making any additional parametric model assumptions on covariates. The proposed method can yield a more efficient estimator than the nonparametric imputation methods and does not require specification of the missingness mechanism when compared with the inverse probability weighting method. A real data example is analyzed to demonstrate use of the proposed method."], ["Estimating the Expected Total Number of Events in a Process", null], ["Nonseparable, Stationary Covariance Functions for Space\u2013Time Data", "Geostatistical approaches to spatiotemporal prediction in environmental science, climatology, meteorology, and related fields rely on appropriate covariance models. This article proposes general classes of nonseparable, stationary covariance functions for spatiotemporal random processes. The constructions are directly in the space\u2013time domain and do not depend on closed-form Fourier inversions. The model parameters can be associated with the data's spatial and temporal structures, respectively; and a covariance model with a readily interpretable space\u2013time interaction parameter is fitted to wind data from Ireland."], ["A Powerful Portmanteau Test of Lack of Fit for Time Series", null], ["Model-Based Clustering, Discriminant Analysis, and Density Estimation", "Cluster analysis is the automated search for groups of related observations in a dataset. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures, and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as how many clusters are there, which clustering method should be used, and how should outliers be handled. We review a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, minefield detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology and discuss recent developments in model-based clustering for non-Gaussian data, high-dimensional datasets, large datasets, and Bayesian estimation."], ["Combining Incompatible Spatial Data", "Global positioning systems (GPSs) and geographical information systems (GISs) have been widely used to collect and synthesize spatial data from a variety of sources. New advances in satellite imagery and remote sensing now permit scientists to access spatial data at several different resolutions. The Internet facilitates fast and easy data acquisition. In any one study, several different types of data may be collected at differing scales and resolutions, at different spatial locations, and in different dimensions. Many statistical issues are associated with combining such data for modeling and inference. This article gives an overview of these issues and the approaches for integrating such disparate data, drawing on work from geography, ecology, agriculture, geology, and statistics. Emphasis is on state-of-the-art statistical solutions to this complex and important problem."], ["The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century", null], ["Medical Biostatistics", null], ["An Introduction to Randomized Controlled Clinical Trials", null], ["Statistical Methods for the Reliability of Repairable Systems", null], ["Correlation and Dependence", null], ["Experiments: Planning, Analysis, and Parameter Design Optimization", null], ["Analysis of the Pretest\u2013Posttest Designs", null], ["Econometric Foundations", null], ["Time Series Analysis and Its Applications", null], ["Simulation-Based Inference in Econometrics", null], ["Principles of Multivariate Analysis: A User's Perspective", null], ["Statistical Curves and Parameters: Choosing an Appropriate Approach", null], ["Probability for Statisticians", null], ["A Course in Probability Theory", null], ["Stochastic Epidemic Models and Their Statistical Analysis", null], ["Telegraphic Reviews", null], ["Letters to the Editor", null], ["Corrections", null], ["Statistical Bridges", "In the information age of today, statistics is essential but statisticians are not. Yet statisticians have much to offer and must be proactive in their approaches to leaders in education, producers and users of data, the scientific community of scholars, and the public at large. Properly constructed bridges to these constituencies can convey the positive contributions of statistical thinking for all, strong academic programs in statistics, the value-added practice of statistics, and the infusion of statistics into interdisciplinary research. The conveyance must go both ways, however."], ["Bayesian Analysis of Rank Data With Application to Primate Intelligence Experiments", "A model for analyzing rank data obtained from multiple evaluators, possibly using different ranking criteria, is proposed. The model is specified hierarchically within the Bayesian paradigm and includes parameters that represent the probabilities that two items are assigned equal rankings. Also included are parameters that account for the relative precision of rankings obtained from distinct evaluation schemes. The model is illustrated through a meta-analysis of rank data collected to compare the cognitive abilities of various primate genera."], ["Prediction of Reservoir Variables Based on Seismic Data and Well Observations", "Offshore petroleum reservoirs are usually explored by acquisition of vast amounts of seismic reflection data and observations along a small number of wells drilled through the reservoirs. The seismic data are indirect measurements of reservoir characteristics and have good spatial coverage but low precision. The well observations are sparse but precise. Information from these sources of data are integrated to characterize the reservoir variables of interest\u2014reservoir porosity in the current study. Critical parameters in the acquisition procedure are unknown and are simultaneously estimated. This process, termed seismic inversion, constitutes a spatial, multivariate, ill-posed inverse problem. The problem is cast in a Bayesian framework with a focus on sampling from the posterior model. High dimensionality, nonlinear components, and unknown parameters in the likelihood model, along with a complex design of the conditioning data, complicate the problem. A fast sequential sampling algorithm containing several parts subject to analytical evaluation is developed under relatively weak, realistic assumptions. Samples of spatial reservoir porosity characteristics can be used to predict and assess uncertainty in the petroleum volume contained in the reservoir. Moreover, porosity is highly correlated with permeability, which represents the fluid flow characteristics. Porosity and permeability are the critical factors in determining and optimizing the future production from petroleum reservoirs. The study is based on data from the Troll field in the North Sea."], ["Selecting Therapeutic Strategies Based on Efficacy and Death in Multicourse Clinical Trials", "Therapy of rapidly fatal diseases often requires multiple courses of treatment. In each course, the treatment may achieve the desired clinical goal (\u201cresponse\u201d), the patient may survive without response (\u201cfailure\u201d), or the patient may die. When treatment fails in a given course, it is common medical practice to switch to a different treatment for the next course. Most statistical approaches to such settings simply ignore the multicourse structure. They characterize patient outcome as a single binary variable, combine death and failure, and identify only one treatment for each patient. Such approaches waste important information. We provide a statistical framework, including a family of generalized logistic regression models and an approximate Bayesian method, that incorporates historical data while accommodating multiple treatment courses, a trinary outcome in each course, and patient prognostic covariates. The framework serves as a basis for data analysis, treatment evaluation, and clinical trial design. In contrast with the usual approach of evaluating individual treatments, our methodology evaluates outcome-adaptive, multicourse treatment strategies that specify, within prognostic subgroups, which treatment to give in each course. We describe a general approach for constructing clinical trial designs that may be tailored to different multicourse settings. For each prognostic subgroup, based on a real-valued function of the covariate-adjusted probabilities of response and death, the design drops inferior treatment strategies during the trial and selects the best strategy at the end. The methodology is illustrated in the context of designing a randomized two-course, three-treatment acute leukemia trial with two prognostic covariates. To validate the model and develop a prior, we first fit the model to a historical dataset. We describe a simulation study of the design under several clinical scenarios. The simulations show that the method can reliably identify treatment\u2013subgroup interactions based on moderate sample sizes."], ["Analysis of Multivariate Longitudinal Outcomes With Nonignorable Dropouts and Missing Covariates", "This article analyzes changes in treatment practices in outpatient methadone treatment units from a national panel study. The analysis of this dataset is challenging due to several difficulties, including multiple longitudinal outcomes, nonignorable nonresponses, and missing covariates. Specifically, the data included several variables that measure the effectiveness of methadone treatment practices for each unit. A substantial percentage of units (33%%) did not respond during the follow-up. These dropout units tended to be units with less effective treatment practices; the dropout mechanism thus may be nonignorable. Finally, the time-varying covariates for the units that dropped out were missing at the time of dropout. A valid analysis hence needs to address these three issues simultaneously. Our approach assumes that the observed outcomes measure a latent variable (e.g., treatment practice effectiveness) with error. We model the relationship between this latent variable and covariates using a linear mixed model. To account for nonignorable dropouts, we apply a selection model in which the dropout probability depends on the latent variable. Finally, we accommodate missing time-varying covariates by modeling them using a transition model. In view of multidimensional integration in full-likelihood estimation, we develop the EM algorithm to estimate the model parameters. We apply the proposed approach to the methadone treatment practices data. Our results show that methadone treatment practices have improved in the last decade. Our results are also useful for identifying the types of methadone treatment units that need improvement."], ["Latent Class Models for Joint Analysis of Longitudinal Biomarker and Event Process Data", "A retrospective substudy of the nutritional prevention of cancer (NPC) trials investigated the utility of longitudinally measured prostate-specific antigen (PSA) as a biomarker for subsequent onset of prostate cancer (PCa). Serial PSA levels were determined retrospectively from frozen blood samples that had been collected from all patients at successive clinic visits with the timing and the number of these visits highly variable. Diagnosis dates of all incident cases of PCa were recorded. Heterogeneity in PSA trajectories was observed that could not be fully explained by the usual linear mixed-effects model and measured covariates. Latent class models that incorporate both a longitudinal biomarker process and an event process offer a way to handle additional heterogeneity, to uncover distinct subpopulations, to incorporate correlated nonnormally distributed outcomes, and to classify individuals into risk classes. Our latent class joint model can aid the prediction of PCa probability given the longitudinal biomarker information available on an individual up to any date. The proposed model easily accommodates highly unbalanced longitudinal data and recurrent events. There are two levels of structure in the latent class joint model. First, the uncertainty of latent class membership is specified through a multinomial logistic model. Second, the class-specific marker trajectory and event process are specified parametrically and semiparametrically, under the assumption of conditional independence given the latent class membership. We use a likelihood approach to obtain parameter estimates via the EM algorithm. We fit the latent class joint model to the data from the NPC trials; four distinct subpopulations are identified that differ with regard to their PSA trajectories and risk for prostate cancer. Higher PSA level is significantly associated with increased risk of PCa, but appears to be conditionally independent once the latent classes are taken into account. Among the covariates, selenium supplementation and age at entry are statistically significant for various parts of the model. Assumptions\u2014in particular the conditional independence between the longitudinal PSA biomarker and time to PCa diagnosis\u2014are assessed."], ["Clustering of Translocation Breakpoints", "Translocation, a physical movement of genetic material from one chromosome to another, can result in the aberrant linkage of two cellular genes. This type of fusion may disrupt cellular function by producing novel, biologically active fused genes, or by triggering the activation of normally quiescent growth-associated genes. Either of these mechanisms provides a putative oncogenic stimulus, and indeed, several gene fusions from translocations have been identified in leukemias, lymphomas, and sarcomas. Although the oncogenic effects of genes involved in translocations have been under intensive study, little is known regarding the formation of translocation fusions themselves. The locations of these fusions are typically independent of the resultant oncogenic protein because they usually arise within certain bounded noncoding regions of the genes. Thus the resultant proteins can be ignored in studying translocations, and we can focus exclusively on the fusions. A patterned (in particular, clustered) distribution of fusion breakpoints will potentially yield relevant information about the fusion process by identifying regions prone to recombination. Accordingly, the statistical analysis of translocation breakpoints has focused on the extent to which they cluster. Somewhat questionable methods have been used in this regard. After highlighting these shortcomings, we introduce a variety of approaches, including scan statistics, bandwidth tests, and gap statistics, that provide a comprehensive means for appraising clustering. We apply this battery to TEL\u2013AML1 translocations, the most common translocation in childhood acute lymphoblastic leukemia. The results obtained indicate generally weaker evidence for clustering than previously reported, and also highlight differences between the statistical approaches."], ["Comparison of Discrimination Methods for the Classification of Tumors Using Gene Expression Data", "A reliable and precise classification of tumors is essential for successful diagnosis and treatment of cancer. cDNA microarrays and high-density oligonucleotide chips are novel biotechnologies increasingly used in cancer research. By allowing the monitoring of expression levels in cells for thousands of genes simultaneously, microarray experiments may lead to a more complete understanding of the molecular variations among tumors and hence to a finer and more informative classification. The ability to successfully distinguish between tumor classes (already known or yet to be discovered) using gene expression data is an important aspect of this novel approach to cancer classification. This article compares the performance of different discrimination methods for the classification of tumors based on gene expression data. The methods include nearest-neighbor classifiers, linear discriminant analysis, and classification trees. Recent machine learning approaches, such as bagging and boosting, are also considered. The discrimination methods are applied to datasets from three recently published cancer gene expression studies."], ["Bayesian Models for Gene Expression With DNA Microarray Data", "Two of the critical issues that arise when examining DNA microarray data are (1) determination of which genes best discriminate among the different types of tissue, and (2) characterization of expression patterns in tumor tissues. For (1), there are many genes that characterize DNA expression, and it is of critical importance to try and identify a small set of genes that best discriminate between normal and tumor tissues. For (2), it is critical to be able to characterize the DNA expression of the normal and tumor tissue samples and develop suitable models that explain patterns of DNA expression for these types of tissues. Toward this goal, we propose a novel Bayesian model for analyzing DNA microarray data and propose a model selection methodology for identifying subsets of genes that show different expression levels between normal and cancer tissues. In addition, we propose a novel class of hierarchical priors for the parameters that allow us to borrow strength across genes for making inference. The properties of the priors are examined in detail. We introduce a Bayesian model selection criterion for assessing the various models, and develop Markov chain Monte Carlo algorithms for sampling from the posterior distributions of the parameters and for computing the criterion. We present a detailed case study in endometrial cancer to demonstrate our proposed methodology."], ["Air Pollution and Mortality", null], [null, null], ["Combining Images Across Multiple Subjects", "This article introduces a Bayesian hierarchical model for combining information across multiple images. Our work was motivated by an invasive functional brain mapping technique called direct cortical electrical interference that gives a sparse set of binary observations of an underlying \u201ctrue\u201d region at multiple sites on the brain surface. To model region shapes that may vary widely across individuals, we use mixtures of simple templates, for example, circles. These subject-specific templates are treated as random effects, governed by a set of population templates that make up a population region. The numbers of subject-specific and population templates are treated as unknown variables to be estimated from the data. Conditional on the subject-specific regions, the observed data are modeled using logistic regression. To estimate the variability among images across patients, we develop a measure based on Baddeley's error measure for binary images. Because the dimension of the parameter space changes as the numbers of subject-specific and population templates change, inference is made using reversible jump Markov chain Monte Carlo. Using a hierarchical approach, we may better estimate each individual's region by borrowing strength from other subjects' data, we can estimate a population region by pooling information across subjects, and we can use a collection of data from previous patients to predict the location of a future patient's region of interest. The approach is illustrated with DCEI data collected on 20 patients for two motor tasks: tongue and hand movements."], ["Inconsistency of Resampling Algorithms for High-Breakdown Regression Estimators and a New Algorithm", null], ["Bayesian Smoothing and Regression Splines for Measurement Error Problems", "In the presence of covariate measurement error, estimating a regression function nonparametrically is extremely difficult, the problem being related to deconvolution. Various frequentist approaches exist for this problem, but to date there has been no Bayesian treatment. In this article we describe Bayesian approaches to modeling a flexible regression function when the predictor variable is measured with error. The regression function is modeled with smoothing splines and regression P-splines. Two methods are described for exploration of the posterior. The first, called the iterative conditional modes (ICM), is only partially Bayesian. ICM uses a componentwise maximization routine to find the mode of the posterior. It also serves to create starting values for the second method, which is fully Bayesian and uses Markov chain Monte Carlo (MCMC) techniques to generate observations from the joint posterior distribution. Use of the MCMC approach has the advantage that interval estimates that directly model and adjust for the measurement error are easily calculated. We provide simulations with several nonlinear regression functions and provide an illustrative example. Our simulations indicate that the frequentist mean squared error properties of the fully Bayesian method are better than those of ICM and also of previously proposed frequentist methods, at least in the examples that we have studied."], ["Nonparametric Estimation of a Distribution Subject to a Stochastic Precedence Constraint", null], ["Attributing Effects to Treatment in Matched Observational Studies", null], ["Estimation With Survey Data Under Nonignorable Nonresponse or Informative Sampling", "Nonresponse is a very common phenomenon in survey sampling. Nonignorable nonresponse\u2014that is, a response mechanism that depends on the values of the variable having nonresponse\u2014is the most difficult type of nonresponse to handle. This article studies a likelihood-based estimation method for data with nonignorable nonresponse. The likelihood is semiparametric in the sense that it consists of a parametric component for the response mechanism (such as a logistic model) and a nonparametric component for the distribution of the variable of interest and the covariates. We show how auxiliary information can be augmented to improve the efficiency of estimators. Asymptotic distributions for the resulting parameter estimates are derived. A simulation study shows that the proposed method gives promising results. The method can also be applied to problems with two-phase sampling in which the second-phase sampling is informative."], ["Length-Biased Sampling With Right Censoring", "When survival data arise from prevalent cases ascertained through a cross-sectional study, it is well known that the survivor function corresponding to these data is length biased and different from the survivor function derived from incident cases. Length-biased data have been treated both unconditionally and conditionally in the literature. In the latter case, where length bias is viewed as being induced by random left truncation of the survival times, the truncating distribution is assumed to be unknown. Conditioning on the observed truncation times hence causes very little loss of information. In many instances, however, it can be supposed that the truncating distribution is uniform, and it has been pointed out that under these circumstances, an unconditional analysis will be more informative. There are no results in the current literature that give the asymptotic properties of the unconditional nonparametric maximum likelihood estimator (NPMLE) of the unbiased survivor function in the presence of censoring. This article fills that gap by giving this NPMLE and its accompanying asymptotic properties when the data are purely length biased. An example of survival with dementia is presented in which the conditional and unconditional estimators are compared."], ["Adaptive Model Selection", null], ["Asymptotic Normality of Semiparametric and Nonparametric Posterior Distributions", "In this article it is shown that the marginal semiparametric and nonparametric posterior distributions for a parameter of interest behave like an ordinary parametric posterior distribution. This in practice provides support of the utility of marginal semiparametric and nonparametric posterior distributions. In particular, the marginal semiparametric and nonparametric posterior distributions are asymptotically normal and centered at the corresponding maximum likelihood estimates (MLEs) or posterior means, with covariance matrix the inverse of the Fisher information. Additionally, the semiparametric and nonparametric MLEs for the parameter of interest and the marginal posterior means are asymptotically normal and centered at the true parameter, with the same covariance matrix. The results are a semiparametric version and a nonparametric version of the parametric Bayesian central limit theorem that establish a connection between the semiparametric and the nonparametric Bayesian inference and their frequentist counterparts."], ["Recursive Combination Tests", null], ["Adjusted Nelson\u2013Aalen Estimates With Retrospective Matching", "In certain situations, randomized trials are not possible, but formal comparison of survival in two or more groups is still desirable. Although no substitute for randomization, application of the matched case-control design to survival studies may minimize the potential bias. Comparable estimates for each treatment group can be obtained by matching or stratifying according to prognostic variables, and weighting each individual to make stratum distributions the same over the treatment groups. Here we develop a weighted Nelson\u2013Aalen estimator and a corresponding survival estimator. When data are closely matched, the stratum effects should be considered as random, leading to additional variance components, and standard asymptotic results do not apply. We derive a variance estimator and show that, asymptotically, the estimator converges to a Gaussian process. Simulations show that the asymptotic results are a good approximation in moderate-sized samples. It should be noted that ignoring the heterogeneity will lead to underestimation of the variance. An application is presented in a clinical study where data on patients receiving standard treatments are obtained for comparison with patients receiving a novel treatment, with retrospective matching to obtain comparable estimates."], ["Statistical Methods in Assessing Agreement", null], ["Conditional Second-Order Generalized Estimating Equations for Generalized Linear and Nonlinear Mixed-Effects Models", "Generalized linear and nonlinear mixed-effects models are used extensively in health care research, including applications in pharmacokinetics, clinical trials, and epidemiology. Because the underlying model may be nonlinear in the random effects, there will generally be no closed-form expression for the marginal likelihood or indeed for the marginal moments. Consequently, estimation is often carried out either using numerical integration techniques or by approximating the marginal likelihood and/or marginal moments using first-order expansion methods. An advantage of the first-order methods is that they do not necessarily require specification of a conditional like-lihood to estimate the regression parameters of interest. However, in many cases they may not take full advantage of the fact that the conditional variance depends on both fixed and random effects. In this article we propose using conditional second-order generalized estimating equations (CGEE2) to estimate both fixed- and random-effects parameters. Under mild regularity conditions, the CGEE2 estimator is shown to be consistent and asymptotically efficient with a rate of convergence depending on both the number of subjects and the number of observations per subject. We compare the CGEE2 estimator against alternative estimators using limited simulation and demonstrate its utility with a numerical example."], ["Bootstrap Tests for Distributional Treatment Effects in Instrumental Variable Models", null], ["Multiple Test Procedures for Identifying the Minimum Effective and Maximum Safe Doses of a Drug", null], ["Latent Variable Analysis of Multivariate Spatial Data", "Multivariate spatial or geo-referenced data arise naturally in such disciplines as ecology, agriculture, geology, and atmospheric sciences. In practice, interest often lies in modeling underlying structure and representing interrelationships in terms of a smaller number of variables. For such situations, statistical analysis using a latent variable model is proposed. We present a general model that incorporates spatial correlation and potential lagged or shifted dependencies and that can represent subject matter theory or serve as a practical exploratory model. Procedures for model fitting, parameter estimation, inferences, and latent variable prediction are developed without restrictive assumptions on distribution and covariance function forms. The properties and usefulness of the proposed approaches are assessed by asymptotic theory and an extensive simulation study. An example from precision agriculture is also presented."], ["Calibration Regression of Censored Lifetime Medical Cost", "Analysis of lifetime medical cost is challenged by its irregular distribution and by incomplete follow-up, particularly the latter with limited study duration as is typical in medical research. This article proposes the so-called calibration regression to address these issues by modeling not only lifetime medical cost but also survival time, in a semiparametric fashion. It is postulated that both outcomes, on possibly transformed scales, linearly relate to the covariates; however, the bivariate model error distribution is completely unspecified. Whereas this conceptually simple model is a natural multivariate generalization of the accelerated failure time model, this article proposes an inference procedure by extending the weighted log-rank estimating function to the marked point process framework. The resulting estimators are consistent and asymptotically normal. In addition, robustness and efficiency issues are discussed. Furthermore, a novel sample-based variance estimation procedure is proposed for estimators based on nonsmooth estimating functions in general. Finally, simulation studies show that this proposal is reliable for practical use. An illustration with application to a lung cancer clinical trial is provided."], [null, null], ["Bayesian Methods for Hidden Markov Models", "Markov chain Monte Carlo (MCMC) sampling strategies can be used to simulate hidden Markov model (HMM) parameters from their posterior distribution given observed data. Some MCMC methods used in practice (for computing likelihood, conditional probabilities of hidden states, and the most likely sequence of states) can be improved by incorporating established recursive algorithms. The most important of these is a set of forward-backward recursions calculating conditional distributions of the hidden states given observed data and model parameters. I show how to use the recursive algorithms in an MCMC context and demonstrate mathematical and empirical results showing a Gibbs sampler using the forward-backward recursions mixes more rapidly than another sampler often used for HMMs. Iintroduce an augmented variables technique for obtaining unique state labels in HMMs and finite mixture models. I show how recursive computing allows the statistically efficient use of MCMC output when estimating the hidden states. I directly calculate the posterior distribution of the hidden chain's state-space size by MCMC, circumventing asymptotic arguments underlying the Bayesian information criterion, which is shown to be inappropriate for a frequently analyzed dataset in the HMM literature. The use of log-likelihood for assessing MCMC convergence is illustrated, and posterior predictive checks are used to investigate application specific questions of model adequacy."], ["S Programming", null], ["Modeling Survival Data: Extending the Cox Model", null], ["Applied Stochastic Modelling", null], ["Bootstrap Methods: A Practitioner's Guide", null], ["Simulation: A Modeler's Approach", null], ["Asymptotics in Statistics\u2013Some Basic Concepts", null], ["Statistical Methods for Clinical Trials", null], ["Topics in Survey Sampling, Finite Population Sampling and Inference: A Prediction Approach", null], ["The Psychology of Survey Response", null], ["Modelling Extremal Events for Insurance and Finance", null], ["Classical and Spatial Stochastic Processes", null], ["Lundberg Approximations for Compound Distributions With Insurance Applications", null], ["Econometric Analysis of Count Data", null], ["Econometric Analysis of Count Data", null], ["Wavelet Methods for Time Series Analysis", null], ["Practical Time Series", null], ["Measuring Business Cycles in Economic Time Series", null], ["Continuous Stochastic Calculus With Applications to Finance", null], ["Probability Theory: An Analytic View", null], ["Robust Diagnostic Regression Analysis", null], ["Interpolating Cubic Splines", null], [null, null], ["Genetic Algorithms", null], ["Correction", null]]}