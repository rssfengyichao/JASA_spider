{"2014": [["Sparse Semiparametric Nonlinear Model With Application to Chromatographic Fingerprints", "Traditional Chinese herbal medications (TCHMs) are composed of a multitude of compounds and the identification of their active composition is an important area of research. Chromatography provides a visual representation of a TCHM sample\u2019s composition by outputting a curve characterized by spikes corresponding to compounds in the sample. Across different experimental conditions, the location of the spikes can be shifted, preventing direct comparison of curves and forcing compound identification to be possible only within each experiment. In this article, we propose a sparse semiparametric nonlinear modeling framework for the establishment of a standardized chromatographic fingerprint. Data-driven basis expansion is used to model the common shape of the curves, while a parametric time warping function registers across individual curves. Penalized weighted least-squares with the adaptive lasso penalty provides a unified criterion for registration, model selection, and estimation. Furthermore, the adaptive lasso estimators possess attractive sampling properties. A back-fitting algorithm is proposed for estimation. Performance is assessed through simulation and we apply the model to chromatographic data of rhubarb collected from different experimental conditions and establish a standardized fingerprint as a first step in TCHM research."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Biomarker Detection in Association Studies: Modeling SNPs Simultaneously via Logistic ANOVA", null], ["Bayesian Modeling and Forecasting of 24-Hour High-Frequency Volatility", "This article estimates models of high-frequency index futures returns using \u201caround-the-clock\u201d 5-min returns that incorporate the following key features: multiple persistent stochastic volatility factors, jumps in prices and volatilities, seasonal components capturing time of the day patterns, correlations between return and volatility shocks, and announcement effects. We develop an integrated MCMC approach to estimate interday and intraday parameters and states using high-frequency data without resorting to various aggregation measures like realized volatility. We provide a case study using financial crisis data from 2007 to 2009, and use particle filters to construct likelihood functions for model comparison and out-of-sample forecasting from 2009 to 2012. We show that our approach improves realized volatility forecasts by up to 50% over existing benchmarks and is also useful for risk management and trading applications. Supplementary materials for this article are available online."], ["Combining Dynamic Predictions From Joint Models for Longitudinal and Time-to-Event Data Using Bayesian Model Averaging", "The joint modeling of longitudinal and time-to-event data is an active area of statistics research that has received a lot of attention in recent years. More recently, a new and attractive application of this type of model has been to obtain individualized predictions of survival probabilities and/or of future longitudinal responses. The advantageous feature of these predictions is that they are dynamically updated as extra longitudinal responses are collected for the subjects of interest, providing real time risk assessment using all recorded information. The aim of this article is two-fold. First, to highlight the importance of modeling the association structure between the longitudinal and event time responses that can greatly influence the derived predictions, and second, to illustrate how we can improve the accuracy of the derived predictions by suitably combining joint models with different association structures. The second goal is achieved using Bayesian model averaging, which, in this setting, has the very intriguing feature that the model weights are not fixed but they are rather subject- and time-dependent, implying that at different follow-up times predictions for the same subject may be based on different models. Supplementary materials for this article are available online."], ["Bayesian Emulation and Calibration of a Dynamic Epidemic Model for A/H1N1 Influenza", null], ["Joint Modeling and Clustering Paired Generalized Longitudinal Trajectories With Application to Cocaine Abuse Treatment Data", null], ["Variable-Domain Functional Regression for Modeling ICU Data", "We introduce a class of scalar-on-function regression models with subject-specific functional predictor domains. The fundamental idea is to consider a bivariate functional parameter that depends both on the functional argument and on the width of the functional predictor domain. Both parametric and nonparametric models are introduced to fit the functional coefficient. The nonparametric model is theoretically and practically invariant to functional support transformation, or support registration. Methods were motivated by and applied to a study of association between daily measures of the Intensive Care Unit (ICU) sequential organ failure assessment (SOFA) score and two outcomes: in-hospital mortality, and physical impairment at hospital discharge among survivors. Methods are generally applicable to a large number of new studies that record a continuous variables over unequal domains. Supplementary materials for this article are available online."], ["Quantifying Causal Effects of Road Network Capacity Expansions on Traffic Volume and Density via a Mixed Model Propensity Score Estimator", "Road network capacity expansions are frequently proposed as solutions to urban traffic congestion but are controversial because it is thought that they can directly \u201cinduce\u201d growth in traffic volumes. This article quantifies causal effects of road network capacity expansions on aggregate urban traffic volume and density in U.S. cities using a mixed model propensity score (PS) estimator. The motivation for this approach is that we seek to estimate a dose-response relationship between capacity and volume but suspect confounding from both observed and unobserved characteristics. Analytical results and simulations show that a longitudinal mixed model PS approach can be used to adjust effectively for time-invariant unobserved confounding via random effects (RE). Our empirical results indicate that network capacity expansions can cause substantial increases in aggregate urban traffic volumes such that even major capacity increases can actually lead to little or no reduction in network traffic densities. This result has important implications for optimal urban transportation strategies. Supplementary materials for this article are available online."], ["Exact Meta-Analysis Approach for Discrete Data and its Application to 2 \u00d7 2 Tables With Rare Events", null], ["Generalized Species Sampling Priors With Latent Beta Reinforcements", "Many popular Bayesian nonparametric priors can be characterized in terms of exchangeable species sampling sequences. However, in some applications, exchangeability may not be appropriate. We introduce a novel and probabilistically coherent family of nonexchangeable species sampling sequences characterized by a tractable predictive probability function with weights driven by a sequence of independent Beta random variables. We compare their theoretical clustering properties with those of the Dirichlet process and the two parameters Poisson\u2013Dirichlet process. The proposed construction provides a complete characterization of the joint process, differently from existing work. We then propose the use of such process as prior distribution in a hierarchical Bayes\u2019 modeling framework, and we describe a Markov chain Monte Carlo sampler for posterior inference. We evaluate the performance of the prior and the robustness of the resulting inference in a simulation study, providing a comparison with popular Dirichlet process mixtures and hidden Markov models. Finally, we develop an application to the detection of chromosomal aberrations in breast cancer by leveraging array comparative genomic hybridization (CGH) data. Supplementary materials for this article are available online."], ["Bayesian Multiscale Modeling of Closed Curves in Point Clouds", "Modeling object boundaries based on image or point cloud data is frequently necessary in medical and scientific applications ranging from detecting tumor contours for targeted radiation therapy, to the classification of organisms based on their structural information. In low-contrast images or sparse and noisy point clouds, there is often insufficient data to recover local segments of the boundary in isolation. Thus, it becomes critical to model the entire boundary in the form of a closed curve. To achieve this, we develop a Bayesian hierarchical model that expresses highly diverse 2D objects in the form of closed curves. The model is based on a novel multiscale deformation process. By relating multiple objects through a hierarchical formulation, we can successfully recover missing boundaries by borrowing structural information from similar objects at the appropriate scale. Furthermore, the model\u2019s latent parameters help interpret the population, indicating dimensions of significant structural variability and also specifying a \u201ccentral curve\u201d that summarizes the collection. Theoretical properties of our prior are studied in specific cases and efficient Markov chain Monte Carlo methods are developed, evaluated through simulation examples and applied to panorex teeth images for modeling teeth contours and also to a brain tumor contour detection problem."], ["Monte Carlo Simulation for Lasso-Type Problems by Estimator Augmentation", null], ["A Simple Method for Estimating Interactions Between a Treatment and a Large Number of Covariates", "We consider a setting in which we have a treatment and a potentially large number of covariates for a set of observations, and wish to model their relationship with an outcome of interest. We propose a simple method for modeling interactions between the treatment and covariates. The idea is to modify the covariate in a simple way, and then fit a standard model using the modified covariates and no main effects. We show that coupled with an efficiency augmentation procedure, this method produces clinically meaningful estimators in a variety of settings. It can be useful for practicing personalized medicine: determining from a large set of biomarkers, the subset of patients that can potentially benefit from a treatment. We apply the method to both simulated datasets and real trial data. The modified covariates idea can be used for other purposes, for example, large scale hypothesis testing for determining which of a set of covariates interact with a treatment variable. Supplementary materials for this article are available online."], ["A Likelihood-Based Framework for Association Analysis of Allele-Specific Copy Numbers", "Copy number variants (CNVs) and single nucleotide polymorphisms (SNPs) coexist throughout the human genome and jointly contribute to phenotypic variations. Thus, it is desirable to consider both types of variants, as characterized by allele-specific copy numbers (ASCNs), in association studies of complex human diseases. Current SNP genotyping technologies capture the CNV and SNP information simultaneously via fluorescent intensity measurements. The common practice of calling ASCNs from the intensity measurements and then using the ASCN calls in downstream association analysis has important limitations. First, the association tests are prone to false-positive findings when differential measurement errors between cases and controls arise from differences in DNA quality or handling. Second, the uncertainties in the ASCN calls are ignored. We present a general framework for the integrated analysis of CNVs and SNPs, including the analysis of total copy numbers as a special case. Our approach combines the ASCN calling and the association analysis into a single step while allowing for differential measurement errors. We construct likelihood functions that properly account for case-control sampling and measurement errors. We establish the asymptotic properties of the maximum likelihood estimators and develop EM algorithms to implement the corresponding inference procedures. The advantages of the proposed methods over the existing ones are demonstrated through realistic simulation studies and an application to a genome-wide association study of schizophrenia. Extensions to next-generation sequencing data are discussed."], ["Nonparametric Estimation of Probability Density Functions for Irregularly Observed Spatial Data", null], ["Modeling Space and Space-Time Directional Data Using Projected Gaussian Processes", "Directional data naturally arise in many scientific fields, such as oceanography (wave direction), meteorology (wind direction), and biology (animal movement direction). Our contribution is to develop a fully model-based approach to capture structured spatial dependence for modeling directional data at different spatial locations. We build a projected Gaussian spatial process, induced from an inline bivariate Gaussian spatial process. We discuss the properties of the projected Gaussian process and show how to fit this process as a model for data, using suitable latent variables, with Markov chain Monte Carlo methods. We also show how to implement spatial interpolation and conduct model comparison in this setting. Simulated examples are provided as proof of concept. A data application arises for modeling wave direction data in the Adriatic sea, off the coast of Italy. In fact, this directional data is available across time, requiring a spatio-temporal model for its analysis. We discuss and illustrate this extension."], ["Fast Prediction of Deterministic Functions Using Sparse Grid Experimental Designs", "Random field models have been widely employed to develop a predictor of an expensive function based on observations from an experiment. The traditional framework for developing a predictor with random field models can fail due to the computational burden it requires. This problem is often seen in cases where the input of the expensive function is high dimensional. While many previous works have focused on developing an approximative predictor to resolve these issues, this article investigates a different solution mechanism. We demonstrate that when a general set of designs is employed, the resulting predictor is quick to compute and has reasonable accuracy. The fast computation of the predictor is made possible through an algorithm proposed by this work. This article also demonstrates methods to quickly evaluate the likelihood of the observations and describes some fast maximum likelihood estimates for unknown parameters of the random field. The computational savings can be several orders of magnitude when the input is located in a high-dimensional space. Beyond the fast computation of the predictor, existing research has demonstrated that a subset of these designs generate predictors that are asymptotically efficient. This work details some empirical comparisons to the more common space-filling designs that verify the designs are competitive in terms of resulting prediction accuracy."], ["Optimal Supersaturated Designs", null], ["Inference for Misspecified Models With Fixed Regressors", "Following the work by Eicker, Huber, and White it is common in empirical work to report standard errors that are robust against general misspecification. In a regression setting, these standard errors are valid for the parameter that minimizes the squared difference between the conditional expectation and a linear approximation, averaged over the population distribution of the covariates. Here, we discuss an alternative parameter that corresponds to the approximation to the conditional expectation based on minimization of the squared difference averaged over the sample, rather than the population, distribution of the covariates. We argue that in some cases this may be a more interesting parameter. We derive the asymptotic variance for this parameter, which is generally smaller than the Eicker\u2013Huber\u2013White robust variance, and propose a consistent estimator for this asymptotic variance. Supplementary materials for this article are available online."], ["Spherical Regression Models Using Projective Linear Transformations", "This article studies the problem of modeling relationship between two spherical (or directional) random variables in a regression setup. Here the predictor and the response variables are constrained to be on a unit sphere and, due to this nonlinear condition, the standard Euclidean regression models do not apply. Several past papers have studied this problem, termed spherical regression, by modeling the response variable with a von Mises-Fisher (VMF) density with the mean given by a rotation of the predictor variable. The few papers that go beyond rigid rotations are limited to one- or two-dimensional spheres. This article extends the mean transformations to a larger group\u2014the projective linear group of transformations\u2014on unit spheres of arbitrary dimensions, while keeping the VMF density to model the noise. It develops a Newton\u2013Raphson algorithm on the special linear group for estimating the MLE of regression parameter and establishes its asymptotic properties when the sample-size becomes large. Through a variety of experiments, using data taken from projective shape analysis, cloud tracking, etc., and some simulations, this article demonstrates improvements in the prediction and modeling performance of the proposed framework over previously used models. Supplementary materials for this article are available online."], ["Score Estimating Equations from Embedded Likelihood Functions Under Accelerated Failure Time Model", "The semiparametric accelerated failure time (AFT) model is one of the most popular models for analyzing time-to-event outcomes. One appealing feature of the AFT model is that the observed failure time data can be transformed to identically independent distributed random variables without covariate effects. We describe a class of estimating equations based on the score functions for the transformed data, which are derived from the full likelihood function under commonly used semiparametric models such as the proportional hazards or proportional odds model. The methods of estimating regression parameters under the AFT model can be applied to traditional right-censored survival data as well as more complex time-to-event data subject to length-biased sampling. We establish the asymptotic properties and evaluate the small sample performance of the proposed estimators. We illustrate the proposed methods through applications in two examples."], ["Proportional Hazards Model With Covariate Measurement Error and Instrumental Variables", "In biomedical studies, covariates with measurement error may occur in survival data. Existing approaches mostly require certain replications on the error-contaminated covariates, which may not be available in the data. In this article, we develop a simple nonparametric correction approach for estimation of the regression parameters in the proportional hazards model using a subset of the sample where instrumental variables are observed. The instrumental variables are related to the covariates through a general nonparametric model, and no distributional assumptions are placed on the error and the underlying true covariates. We further propose a novel generalized methods of moments nonparametric correction estimator to improve the efficiency over the simple correction approach. The efficiency gain can be substantial when the calibration subsample is small compared to the whole sample. The estimators are shown to be consistent and asymptotically normal. Performance of the estimators is evaluated via simulation studies and by an application to data from an HIV clinical trial. Estimation of the baseline hazard function is not addressed."], ["Dimension-Reduced Modeling of Spatio-Temporal Processes", "The field of spatial and spatio-temporal statistics is increasingly faced with the challenge of very large datasets. The classical approach to spatial and spatio-temporal modeling is very computationally demanding when datasets are large, which has led to interest in methods that use dimension-reduction techniques. In this article, we focus on modeling of two spatio-temporal processes where the primary goal is to predict one process from the other and where datasets for both processes are large. We outline a general dimension-reduced Bayesian hierarchical modeling approach where spatial structures of both processes are modeled in terms of a low number of basis vectors, hence reducing the spatial dimension of the problem. Temporal evolution of the processes and their dependence is then modeled through the coefficients of the basis vectors. We present a new method of obtaining data-dependent basis vectors, which is geared toward the goal of predicting one process from the other. We apply these methods to a statistical downscaling example, where surface temperatures on a coarse grid over Antarctica are downscaled onto a finer grid. Supplementary materials for this article are available online."], ["Meta-Analysis With Fixed, Unknown, Study-Specific Parameters", "Meta-analysis is a valuable tool for combining information from independent studies. However, most common meta-analysis techniques rely on distributional assumptions that are difficult, if not impossible, to verify. For instance, in the commonly used fixed-effects and random-effects models, we take for granted that the underlying study-level parameters are either exactly the same across individual studies or that they are realizations of a random sample from a population, often under a parametric distributional assumption. In this article, we present a new framework for summarizing information obtained from multiple studies and make inference that is not dependent on any distributional assumption for the study-level parameters. Specifically, we assume the study-level parameters are unknown, fixed parameters and draw inferences about, for example, the quantiles of this set of parameters using study-specific summary statistics. This type of problem is known to be quite challenging (see Hall and Miller). We use a novel resampling method via the confidence distributions of the study-level parameters to construct confidence intervals for the above quantiles. We justify the validity of the interval estimation procedure asymptotically and compare the new procedure with the standard bootstrapping method. We also illustrate our proposal with the data from a recent meta-analysis of the treatment effect from an antioxidant on the prevention of contrast-induced nephropathy."], ["Generalized Ordinary Differential Equation Models", "Existing estimation methods for ordinary differential equation (ODE) models are not applicable to discrete data. The generalized ODE (GODE) model is therefore proposed and investigated for the first time. We develop the likelihood-based parameter estimation and inference methods for GODE models. We propose robust computing algorithms and rigorously investigate the asymptotic properties of the proposed estimator by considering both measurement errors and numerical errors in solving ODEs. The simulation study and application of our methods to an influenza viral dynamics study suggest that the proposed methods have a superior performance in terms of accuracy over the existing ODE model estimation approach and the extended smoothing-based (ESB) method. Supplementary materials for this article are available online."], ["Structural Pursuit Over Multiple Undirected Graphs", "Gaussian graphical models are useful to analyze and visualize conditional dependence relationships between interacting units. Motivated from network analysis under different experimental conditions, such as gene networks for disparate cancer subtypes, we model structural changes over multiple networks with possible heterogeneities. In particular, we estimate multiple precision matrices describing dependencies among interacting units through maximum penalized likelihood. Of particular interest are homogeneous groups of similar entries across and zero-entries of these matrices, referred to as clustering and sparseness structures, respectively. A nonconvex method is proposed to seek a sparse representation for each matrix and identify clusters of the entries across the matrices. Computationally, we develop an efficient method on the basis of difference convex programming, the augmented Lagrangian method and the blockwise coordinate descent method, which is scalable to hundreds of graphs of thousands nodes through a simple necessary and sufficient partition rule, which divides nodes into smaller disjoint subproblems excluding zero-coefficients nodes for arbitrary graphs with convex relaxation. Theoretically, a finite-sample error bound is derived for the proposed method to reconstruct the clustering and sparseness structures. This leads to consistent reconstruction of these two structures simultaneously, permitting the number of unknown parameters to be exponential in the sample size, and yielding the optimal performance of the oracle estimator as if the true structures were given a priori. Simulation studies suggest that the method enjoys the benefit of pursuing these two disparate kinds of structures, and compares favorably against its convex counterpart in the accuracy of structure pursuit and parameter estimation."], ["Treatment Evaluation With Multiple Outcome Periods Under Endogeneity and Attrition", null], ["Book Reviews", null], ["Correction", null], ["Editorial Collaborators", null], ["Editorial Board EOV", null], ["A Bayesian Nonparametric Modeling Framework for Developmental Toxicity Studies", "We develop a Bayesian nonparametric mixture modeling framework for replicated count responses in dose-response settings. We explore this methodology for modeling and risk assessment in developmental toxicity studies, where the primary objective is to determine the relationship between the level of exposure to a toxic chemical and the probability of a physiological or biochemical response, or death. Data from these experiments typically involve features that cannot be captured by standard parametric approaches. To provide flexibility in the functional form of both the response distribution and the probability of positive response, the proposed mixture model is built from a dependent Dirichlet process prior, with the dependence of the mixing distributions governed by the dose level. The methodology is tested with a simulation study, which involves also comparison with semiparametric Bayesian approaches to highlight the practical utility of the dependent Dirichlet process nonparametric mixture model. Further illustration is provided through the analysis of data from two developmental toxicity studies."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Mechanistic Hierarchical Gaussian Processes", "The statistics literature on functional data analysis focuses primarily on flexible black-box approaches, which are designed to allow individual curves to have essentially any shape while characterizing variability. Such methods typically cannot incorporate mechanistic information, which is commonly expressed in terms of differential equations. Motivated by studies of muscle activation, we propose a nonparametric Bayesian approach that takes into account mechanistic understanding of muscle physiology. A novel class of hierarchical Gaussian processes is defined that favors curves consistent with differential equations defined on motor, damper, spring systems. A Gibbs sampler is proposed to sample from the posterior distribution and applied to a study of rats exposed to noninjurious muscle activation protocols. Although motivated by muscle force data, a parallel approach can be used to include mechanistic information in broad functional data analysis applications."], ["Identifying Genetic Variants for Addiction via Propensity Score Adjusted Generalized Kendall\u2019s Tau", null], ["Optimizing Sedative Dose in Preterm Infants Undergoing Treatment for Respiratory Distress Syndrome", null], ["Estimating Ocean Circulation: An MCMC Approach With Approximated Likelihoods via the Bernoulli Factory", "We provide a Bayesian analysis of ocean circulation based on data collected in the South Atlantic Ocean. The analysis incorporates a reaction-diffusion partial differential equation that is not solvable in closed form. This leads to an intractable likelihood function. We describe a novel Markov chain Monte Carlo approach that does not require a likelihood evaluation. Rather, we use unbiased estimates of the likelihood and a Bernoulli factory to decide whether or not proposed states are accepted. The variates required to estimate the likelihood function are obtained via a Feynman\u2013Kac representation. This lifts the common restriction of selecting a regular grid for the physical model and eliminates the need for data preprocessing. We implement our approach using the parallel graphic processing unit (GPU) computing environment."], ["Multivariate Analysis of Longitudinal Ordinal Data With Mixed Effects Models, With Application to Clinical Outcomes in Osteoarthritis", "Our objective was to evaluate the efficacy of robenacoxib in osteoarthritic dogs using four ordinal responses measured repeatedly over time. We propose a multivariate probit mixed effects model to describe the joint evolution of endpoints and to evidence the intrinsic correlations between responses that are not due to treatment effect. Maximum likelihood computation is intractable within reasonable time frames. We therefore use a pairwise modeling approach in combination with a stochastic EM algorithm. Multidimensional ordinal responses with longitudinal measurements are a common feature in clinical trials. However, the standard methods for data analysis use unidimensional models, resulting in a loss of information. Our methodology provides substantially greater insight than these methods for the evaluation of treatment effects and shows a good performance at low computational cost. We thus believe that it could be used in routine practice to optimize the evaluation of treatment efficacy."], ["Causal Inference for fMRI Time Series Data With Systematic Errors of Measurement in a Balanced On/Off Study of Social Evaluative Threat", "Functional magnetic resonance imaging (fMRI) has facilitated major advances in understanding human brain function. Neuroscientists are interested in using fMRI to study the effects of external stimuli on brain activity and causal relationships among brain regions, but have not stated what is meant by causation or defined the effects they purport to estimate. Building on Rubin\u2019s causal model, we construct a framework for causal inference using blood oxygenation level dependent (BOLD) fMRI time series data. In the usual statistical literature on causal inference, potential outcomes, assumed to be measured without systematic error, are used to define unit and average causal effects. However, in general the potential BOLD responses are measured with stimulus dependent systematic error. Thus we define unit and average causal effects that are free of systematic error. In contrast to the usual case of a randomized experiment where adjustment for intermediate outcomes leads to biased estimates of treatment effects, here the failure to adjust for task dependent systematic error leads to biased estimates. We therefore adjust for systematic error using measured \u201cnoise covariates,\u201d using a linear mixed model to estimate the effects and the systematic error. Our results are important for neuroscientists, who typically do not adjust for systematic error. They should also prove useful to researchers in other areas where responses are measured with error and in fields where large amounts of data are collected on relatively few subjects. To illustrate our approach, we reanalyze data from a social evaluative threat task, comparing the findings with results that ignore systematic error."], ["Bayesian Generalized Low Rank Regression Models for Neuroimaging Phenotypes and Genetic Markers", "We propose a Bayesian generalized low-rank regression model (GLRR) for the analysis of both high-dimensional responses and covariates. This development is motivated by performing searches for associations between genetic variants and brain imaging phenotypes. GLRR integrates a low rank matrix to approximate the high-dimensional regression coefficient matrix of GLRR and a dynamic factor model to model the high-dimensional covariance matrix of brain imaging phenotypes. Local hypothesis testing is developed to identify significant covariates on high-dimensional responses. Posterior computation proceeds via an efficient Markov chain Monte Carlo algorithm. A simulation study is performed to evaluate the finite sample performance of GLRR and its comparison with several competing approaches. We apply GLRR to investigate the impact of 1071 SNPs on top 40 genes reported by AlzGene database on the volumes of 93 regions of interest (ROI) obtained from Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI). Supplementary materials for this article are available online."], ["Estimation and Accuracy After Model Selection", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Bayesian Aggregation of Order-Based Rank Data", "Rank aggregation, that is, combining several ranking functions (called base rankers) to get aggregated, usually stronger rankings of a given set of items, is encountered in many disciplines. Most methods in the literature assume that base rankers of interest are equally reliable. It is very common in practice, however, that some rankers are more informative and reliable than others. It is desirable to distinguish high quality base rankers from low quality ones and treat them differently. Some methods achieve this by assigning prespecified weights to base rankers. But there are no systematic and principled strategies for designing a proper weighting scheme for a practical problem. In this article, we propose a Bayesian approach, called Bayesian aggregation of rank data (BARD), to overcome this limitation. By attaching a quality parameter to each base ranker and estimating these parameters along with the aggregation process, BARD measures reliabilities of base rankers in a quantitative way and makes use of this information to improve the aggregated ranking. In addition, we design a method to detect highly correlated rankers and to account for their information redundancy appropriately. Both simulation studies and real data applications show that BARD significantly outperforms existing methods when equality of base rankers varies greatly."], ["Inference From Intrinsic Bayes\u2019 Procedures Under Model Selection and Uncertainty", null], ["Adaptive Confidence Bands for Nonparametric Regression Functions", "This article proposes a new formulation for the construction of adaptive confidence bands (CBs) in nonparametric function estimation problems. CBs, which have size that adapts to the smoothness of the function while guaranteeing that both the relative excess mass of the function lying outside the band and the measure of the set of points where the function lies outside the band are small. It is shown that the bands adapt over a maximum range of Lipschitz classes. The adaptive CB can be easily implemented in standard statistical software with wavelet support. We investigate the numerical performance of the procedure using both simulated and real datasets. The numerical results agree well with the theoretical analysis. The procedure can be easily modified and used for other nonparametric function estimation models. Supplementary materials for this article are available online."], ["Efficient R-Estimation of Principal and Common Principal Components", null], ["Spatially Varying Coefficient Model for Neuroimaging Data With Jump Discontinuities", "Motivated by recent work on studying massive imaging data in various neuroimaging studies, we propose a novel spatially varying coefficient model (SVCM) to capture the varying association between imaging measures in a three-dimensional volume (or two-dimensional surface) with a set of covariates. Two stylized features of neuorimaging data are the presence of multiple piecewise smooth regions with unknown edges and jumps and substantial spatial correlations. To specifically account for these two features, SVCM includes a measurement model with multiple varying coefficient functions, a jumping surface model for each varying coefficient function, and a functional principal component model. We develop a three-stage estimation procedure to simultaneously estimate the varying coefficient functions and the spatial correlations. The estimation procedure includes a fast multiscale adaptive estimation and testing procedure to independently estimate each varying coefficient function, while preserving its edges among different piecewise-smooth regions. We systematically investigate the asymptotic properties (e.g., consistency and asymptotic normality) of the multiscale adaptive parameter estimates. We also establish the uniform convergence rate of the estimated spatial covariance function and its associated eigenvalues and eigenfunctions. Our Monte Carlo simulation and real-data analysis have confirmed the excellent performance of SVCM. Supplementary materials for this article are available online."], ["Testing Second-Order Dynamics for Autoregressive Processes in Presence of Time-Varying Variance", null], ["Filtering With Heavy Tails", null], ["Generalized Gaussian Process Regression Model for Non-Gaussian Functional Data", "In this article, we propose a generalized Gaussian process concurrent regression model for functional data, where the functional response variable has a binomial, Poisson, or other non-Gaussian distribution from an exponential family, while the covariates are mixed functional and scalar variables. The proposed model offers a nonparametric generalized concurrent regression method for functional data with multidimensional covariates, and provides a natural framework on modeling common mean structure and covariance structure simultaneously for repeatedly observed functional data. The mean structure provides overall information about the observations, while the covariance structure can be used to catch up the characteristic of each individual batch. The prior specification of covariance kernel enables us to accommodate a wide class of nonlinear models. The definition of the model, the inference, and the implementation as well as its asymptotic properties are discussed. Several numerical examples with different non-Gaussian response variables are presented. Some technical details and more numerical examples as well as an extension of the model are provided as supplementary materials."], ["Space-Filling Fractional Factorial Designs", null], [null, null], ["Multiply Robust Estimation in Regression Analysis With Missing Data", "Doubly robust estimators are widely used in missing-data analysis. They provide double protection on estimation consistency against model misspecifications. However, they allow only a single model for the missingness mechanism and a single model for the data distribution, and the assumption that one of these two models is correctly specified is restrictive in practice. For regression analysis with possibly missing outcome, we propose an estimation method that allows multiple models for both the missingness mechanism and the data distribution. The resulting estimator is consistent if any one of those multiple models is correctly specified, and thus provides multiple protection on consistency. This estimator is also robust against extreme values of the fitted missingness probability, which, for most doubly robust estimators, can lead to erroneously large inverse probability weights that may jeopardize the numerical performance. The numerical implementation of the proposed method through a modified Newton\u2013Raphson algorithm is discussed. The asymptotic distribution of the resulting estimator is derived, based on which we study the estimation efficiency and provide ways to improve the efficiency. As an application, we analyze the data collected from the AIDS Clinical Trials Group Protocol 175."], ["Targeted Local Support Vector Machine for Age-Dependent Classification", "We develop methods to accurately predict whether presymptomatic individuals are at risk of a disease based on their various marker profiles, which offers an opportunity for early intervention well before definitive clinical diagnosis. For many diseases, existing clinical literature may suggest the risk of disease varies with some markers of biological and etiological importance, for example, age. To identify effective prediction rules using nonparametric decision functions, standard statistical learning approaches treat markers with clear biological importance (e.g., age) and other markers without prior knowledge on disease etiology interchangeably as input variables. Therefore, these approaches may be inadequate in singling out and preserving the effects from the biologically important variables, especially in the presence of potential noise markers. Using age as an example of a salient marker to receive special care in the analysis, we propose a local smoothing large margin classifier implemented with support vector machine (SVM) to construct effective age-dependent classification rules. The method adaptively adjusts age effect and separately tunes age and other markers to achieve optimal performance. We derive the asymptotic risk bound of the local smoothing SVM and perform extensive simulation studies to compare with standard approaches. We apply the proposed method to two studies of premanifest Huntington\u2019s disease (HD) subjects and controls to construct age-sensitive predictive scores for the risk of HD and risk of receiving HD diagnosis during the study period. Supplementary materials for this article are available online."], ["On an Additive Semigraphoid Model for Statistical Networks With Application to Pathway Analysis", "We introduce a nonparametric method for estimating non-Gaussian graphical models based on a new statistical relation called additive conditional independence, which is a three-way relation among random vectors that resembles the logical structure of conditional independence. Additive conditional independence allows us to use one-dimensional kernel regardless of the dimension of the graph, which not only avoids the curse of dimensionality but also simplifies computation. It also gives rise to a parallel structure to the Gaussian graphical model that replaces the precision matrix by an additive precision operator. The estimators derived from additive conditional independence cover the recently introduced nonparanormal graphical model as a special case, but outperform it when the Gaussian copula assumption is violated. We compare the new method with existing ones by simulations and in genetic pathway analysis. Supplementary materials for this article are available online."], ["Functional Principal Component Analysis of Spatiotemporal Point Processes With Applications in Disease Surveillance", "In disease surveillance applications, the disease events are modeled by spatiotemporal point processes. We propose a new class of semiparametric generalized linear mixed model for such data, where the event rate is related to some known risk factors and some unknown latent random effects. We model the latent spatiotemporal process as spatially correlated functional data, and propose Poisson maximum likelihood and composite likelihood methods based on spline approximations to estimate the mean and covariance functions of the latent process. By performing functional principal component analysis to the latent process, we can better understand the correlation structure in the point process. We also propose an empirical Bayes method to predict the latent spatial random effects, which can help highlight hot areas with unusually high event rates. Under an increasing domain and increasing knots asymptotic framework, we establish the asymptotic distribution for the parametric components in the model and the asymptotic convergence rates for the functional principal component estimators. We illustrate the methodology through a simulation study and an application to the Connecticut Tumor Registry data. Supplementary materials for this article are available online."], ["Optimal Tests of Treatment Effects for the Overall Population and Two Subpopulations in Randomized Trials, Using Sparse Linear Programming", "We propose new, optimal methods for analyzing randomized trials, when it is suspected that treatment effects may differ in two predefined subpopulations. Such subpopulations could be defined by a biomarker or risk factor measured at baseline. The goal is to simultaneously learn which subpopulations benefit from an experimental treatment, while providing strong control of the familywise Type I error rate. We formalize this as a multiple testing problem and show it is computationally infeasible to solve using existing techniques. Our solution involves a novel approach, in which we first transform the original multiple testing problem into a large, sparse linear program. We then solve this problem using advanced optimization techniques. This general method can solve a variety of multiple testing problems and decision theory problems related to optimal trial design, for which no solution was previously available. In particular, we construct new multiple testing procedures that satisfy minimax and Bayes optimality criteria. For a given optimality criterion, our new approach yields the optimal tradeoff between power to detect an effect in the overall population versus power to detect effects in subpopulations. We demonstrate our approach in examples motivated by two randomized trials of new treatments for HIV. Supplementary materials for this article are available online."], ["Sequential Lasso Cum EBIC for Feature Selection With Ultra-High Dimensional Feature Space", "In this article, we propose a method called sequential Lasso (SLasso) for feature selection in sparse high-dimensional linear models. The SLasso selects features by sequentially solving partially penalized least squares problems where the features selected in earlier steps are not penalized. The SLasso uses extended BIC (EBIC) as the stopping rule. The procedure stops when EBIC reaches a minimum. The asymptotic properties of SLasso are considered when the dimension of the feature space is ultra high and the number of relevant feature diverges. We show that, with probability converging to 1, the SLasso first selects all the relevant features before any irrelevant features can be selected, and that the EBIC decreases until it attains the minimum at the model consisting of exactly all the relevant features and then begins to increase. These results establish the selection consistency of SLasso. The SLasso estimators of the final model are ordinary least squares estimators. The selection consistency implies the oracle property of SLasso. The asymptotic distribution of the SLasso estimators with diverging number of relevant features is provided. The SLasso is compared with other methods by simulation studies, which demonstrates that SLasso is a desirable approach having an edge over the other methods. The SLasso together with the other methods are applied to a microarray data for mapping disease genes. Supplementary materials for this article are available online."], ["Segmented Model Selection in Quantile Regression Using the Minimum Description Length Principle", "This article proposes new model-fitting techniques for quantiles of an observed data sequence, including methods for data segmentation and variable selection. The main contribution, however, is in providing a means to perform these two tasks simultaneously. This is achieved by matching the data with the best-fitting piecewise quantile regression model, where the fit is determined by a penalization derived from the minimum description length principle. The resulting optimization problem is solved with the use of genetic algorithms. The proposed, fully automatic procedures are, unlike traditional break point procedures, not based on repeated hypothesis tests, and do not require, unlike most variable selection procedures, the specification of a tuning parameter. Theoretical large-sample properties are derived. Empirical comparisons with existing break point and variable selection methods for quantiles indicate that the new procedures work well in practice."], ["The Sparse MLE for Ultrahigh-Dimensional Feature Screening", "Feature selection is fundamental for modeling the high-dimensional data, where the number of features can be huge and much larger than the sample size. Since the feature space is so large, many traditional procedures become numerically infeasible. It is hence essential to first remove most apparently noninfluential features before any elaborative analysis. Recently, several procedures have been developed for this purpose, which include the sure-independent-screening (SIS) as a widely used technique. To gain computational efficiency, the SIS screens features based on their individual predicting power. In this article, we propose a new screening method via the sparsity-restricted maximum likelihood estimator (SMLE). The new method naturally takes the joint effects of features in the screening process, which gives itself an edge to potentially outperform the existing methods. This conjecture is further supported by the simulation studies under a number of modeling settings. We show that the proposed method is screening consistent in the context of ultrahigh-dimensional generalized linear models. Supplementary materials for this article are available online."], ["Nonparametric Independence Screening in Sparse Ultra-High-Dimensional Varying Coefficient Models", "The varying coefficient model is an important class of nonparametric statistical model, which allows us to examine how the effects of covariates vary with exposure variables. When the number of covariates is large, the issue of variable selection arises. In this article, we propose and investigate marginal nonparametric screening methods to screen variables in sparse ultra-high-dimensional varying coefficient models. The proposed nonparametric independence screening (NIS) selects variables by ranking a measure of the nonparametric marginal contributions of each covariate given the exposure variable. The sure independent screening property is established under some mild technical conditions when the dimensionality is of nonpolynomial order, and the dimensionality reduction of NIS is quantified. To enhance the practical utility and finite sample performance, two data-driven iterative NIS (INIS) methods are proposed for selecting thresholding parameters and variables: conditional permutation and greedy methods, resulting in conditional-INIS and greedy-INIS. The effectiveness and flexibility of the proposed methods are further illustrated by simulation studies and real data applications."], ["Interaction Screening for Ultrahigh-Dimensional Data", null], ["Martingale Difference Correlation and Its Use in High-Dimensional Variable Screening", null], ["Some Comments on Copula-Based Regression", "In a recent article, Noh, El Ghouch, and Bouezmarni proposed a new semiparametric estimate of a regression function with a multivariate predictor, which is based on a specification of the dependence structure between the predictor and the response by means of a parametric copula. This comment investigates the effect which occurs under misspecification of the parametric model. We demonstrate by means of several examples that even for a one or two-dimensional predictor the error caused by a \u201cwrong\u201d specification of the parametric family is rather severe, if the regression is not monotone in one of the components of the predictor. Moreover, we also show that these problems occur for all of the commonly used copula families and we illustrate in several examples that the copula-based regression may lead to invalid results even when flexible copula models such as vine copulas (with the common parametric families) are used in the estimation procedure."], ["Book Reviews", null], ["Flexible Marginal Structural Models for Estimating the Cumulative Effect of a Time-Dependent Treatment on the Hazard: Reassessing the Cardiovascular Risks of Didanosine Treatment in the Swiss HIV Cohort Study", "The association between antiretroviral treatment and cardiovascular disease (CVD) risk in HIV-positive persons has been the subject of much debate since the Data collection on Adverse events of Anti-HIV Drugs (D:A:D) study reported that recent use of two antiretroviral drugs, abacavir (ABC) and didanosine (DDI), was associated with increased risk. We focus on the potential impact of DDI use, as this drug has not been as studied intensively as ABC. We propose a flexible marginal structural Cox model with weighted cumulative exposure modeling (Cox WCE MSM) to address two key challenges encountered when using observational longitudinal data to assess the adverse effects of medication: (1) the need to model the cumulative effect of a time-dependent treatment and (2) the need to control for time-dependent confounders that also act as mediators of the effect of past treatment. Simulations confirm that the Cox WCE MSM yields accurate estimates of the causal treatment effect given complex exposure effects and time-dependent confounding. We then use the new flexible Cox WCE MSM to assess the association between DDI use and CVD risk in the Swiss HIV Cohort Study. In contrast to the nonsignificant results obtained with conventional parametric Cox MSMs, our new Cox WCE MSM identifies a significant short-term risk increase due to DDI use in the previous year. Supplementary materials for this article are available online."], ["To Fuel or Not to Fuel? Is that the Question?", "According to the International Air Transport Association, the industry fuel bill accounts for more than 25% of the annual airline operating costs. In times of severe economic constraints and increasing fuel costs, air carriers are looking for ways to reduce costs and improve fuel efficiency without putting flight safety into jeopardy. In particular, this is inducing discussions on how much additional fuel to put in a planned route to avoid diverting to an alternate airport due to Air Traffic Flow Management delays. We provide here a general model to support such decisions. We illustrate it with a case study and provide comparison with the current practice, showing the relevance of our approach."], ["A Bayesian Nonparametric Regression Model With Normalized Weights: A Study of Hippocampal Atrophy in Alzheimer\u2019s Disease", "Hippocampal volume is one of the best established biomarkers for Alzheimer\u2019s disease. However, for appropriate use in clinical trials research, the evolution of hippocampal volume needs to be well understood. Recent theoretical models propose a sigmoidal pattern for its evolution. To support this theory, the use of Bayesian nonparametric regression mixture models seems particularly suitable due to the flexibility that models of this type can achieve and the unsatisfactory predictive properties of semiparametric methods. In this article, our aim is to develop an interpretable Bayesian nonparametric regression model which allows inference with combinations of both continuous and discrete covariates, as required for a full analysis of the dataset. Simple arguments regarding the interpretation of Bayesian nonparametric regression mixtures lead naturally to regression weights based on normalized sums. Difficulty in working with the intractable normalizing constant is overcome thanks to recent advances in MCMC methods and the development of a novel auxiliary variable scheme. We apply the new model and MCMC method to study the dynamics of hippocampal volume, and our results provide statistical evidence in support of the theoretical hypothesis."], ["Automated Tsunami Source Modeling Using the Sweeping Window Positive Elastic Net", "In response to hazards posed by earthquake-induced tsunamis, the National Oceanographic and Atmospheric Administration developed a system for issuing timely warnings to coastal communities. This system, in part, involves matching data collected in real time from deep-ocean buoys to a database of precomputed geophysical models, each associated with a geographical location. Currently, trained operators must handpick models from the database using the epicenter of the earthquake as guidance, which can delay issuing of warnings. In this article, we introduce an automatic procedure to select models to improve the timing and accuracy of these warnings. This procedure uses an elastic-net-based penalized and constrained linear least-squares estimator in conjunction with a sweeping window. This window ensures that selected models are close spatially, which is desirable from geophysical considerations. We use the Akaike information criterion to settle on a particular window and to set the tuning parameters associated with the elastic net. Test data from the 2006 Kuril Islands and the devastating 2011 Japan tsunamis show that the automatic procedure yields model fits and verification equal to or better than those from a time-consuming hand-selected solution."], ["Bayesian Forecasting of Cohort Fertility", "There are signs that fertility in rich countries may have stopped declining, but this depends critically on whether women currently in reproductive ages are postponing or reducing lifetime fertility. Analysis of average completed family sizes requires forecasts of remaining fertility for women born 1970\u20131995. We propose a Bayesian model for fertility that incorporates a priori information about patterns over age and time. We use a new dataset, the Human Fertility Database (HFD), to construct improper priors that give high weight to historically plausible rate surfaces. In the age dimension, cohort schedules should be well approximated by principal components of HFD schedules. In the time dimension, series should be smooth and approximately linear over short spans. We calibrate priors so that approximation residuals have theoretical distributions similar to historical HFD data. Our priors use quadratic penalties and imply a high-dimensional normal posterior distribution for each country's fertility surface. Forecasts for HFD cohorts currently aged 15\u201344 show consistent patterns. In the United States, Northern Europe, and Western Europe, slight rebounds in completed fertility are likely. In Central and Southern Europe, East Asia, and Brazil, there is little evidence for a rebound. Our methods could be applied to other forecasting and missing-data problems with only minor modifications."], ["Estimating Risk With Time-to-Event Data: An Application to the Women\u2019s Health Initiative", "Accurate and individualized risk prediction is critical for population control of chronic diseases such as cancer and cardiovascular disease. Large cohort studies provide valuable resources for building risk prediction models, as the risk factors are collected at the baseline and subjects are followed over time until disease occurrence or termination of the study. However, for rare diseases the baseline risk may not be estimated reliably based on cohort data only, due to sparse events. In this article, we propose to make use of external information to improve efficiency for estimating time-dependent absolute risk. We derive the relationship between external disease incidence rates and the baseline risk, and incorporate the external disease incidence information into estimation of absolute risks, while allowing for potential difference of disease incidence rates between cohort and external sources. The asymptotic properties, namely, uniform consistency and weak convergence, of the proposed estimators are established. Simulation results show that the proposed estimator for absolute risk is more efficient than that based on the Breslow estimator, which does not use external disease incidence rates. A large cohort study, the Women\u2019s Health Initiative Observational Study, is used to illustrate the proposed method. Supplementary materials for this article are available online."], ["Using Data Augmentation to Facilitate Conduct of Phase I\u2013II Clinical Trials With Delayed Outcomes", "A practical impediment in adaptive clinical trials is that outcomes must be observed soon enough to apply decision rules to choose treatments for new patients. For example, if outcomes take up to six weeks to evaluate and the accrual rate is one patient per week, on average three new patients will be accrued while waiting to evaluate the outcomes of the previous three patients. The question is how to treat the new patients. This logistical problem persists throughout the trial. Various ad hoc practical solutions are used, none entirely satisfactory. We focus on this problem in phase I\u2013II clinical trials that use binary toxicity and efficacy, defined in terms of event times, to choose doses adaptively for successive cohorts. We propose a general approach to this problem that treats late-onset outcomes as missing data, uses data augmentation to impute missing outcomes from posterior predictive distributions computed from partial follow-up times and complete outcome data, and applies the design\u2019s decision rules using the completed data. We illustrate the method with two cancer trials conducted using a phase I\u2013II design based on efficacy\u2013toxicity trade-offs, including a computer stimulation study. Supplementary materials for this article are available online."], ["A Bivariate Model for Simultaneous Testing in Bioinformatics Data", "We develop a novel approach for testing treatment effects in high-throughput data. Most previous works on this topic focused on testing for differences between the means, but recently it has been recognized that testing for differential variation is probably as important. We take it a step further, and introduce a bivariate model modeling strategy which accounts for both differential expression and differential variation. Our model-based approach, in which the differential mean and variance are considered random effects, results in shrinkage estimation and powerful tests as it borrows strength across levels. We show in simulations that the method yields a substantial gain in the power to detect differential means when differential variation is present. Our case studies show that the model is realistic in a wide range of applications. Furthermore, a hierarchical estimation approach implemented using the EM algorithm results in a computationally efficient method which is particularly well-suited for \u201cmultiple testing\u201d situations. Finally, we develop a power and sample size calculation tool that mirrors the estimation and inference method described in this article, and can be used to design experiments involving thousands of simultaneous tests."], ["Object-Oriented Data Analysis of Cell Images", "This article discusses a study of cell images in cell culture biology from an object-oriented point of view. The motivation of this research is to develop a statistical approach to cell image analysis that better supports the automated development of stem cell growth media. A major hurdle in this process is the need for human expertise, based on studying cells under the microscope, to make decisions about the next step of the cell culture process. We aim to use digital imaging technology coupled with statistical analysis to tackle this important problem. The discussion in this article highlights a common critical issue: choice of data objects. Instead of conventionally treating either the individual cells or the wells (a container in which the cells are grown) as data objects, a new type of data object is proposed, that is the union of a well with its corresponding set of cells. The image data analysis suggests that the cell\u2013well unions can be a better choice of data objects than the cells or the wells alone. The data are available in the online supplementary materials."], ["Source-Sink Reconstruction Through Regularized Multicomponent Regression Analysis\u2014With Application to Assessing Whether North Sea Cod Larvae Contributed to Local Fjord Cod in Skagerrak", "The problem of reconstructing the source-sink dynamics arises in many biological systems. Our research is motivated by marine applications where newborns are passively dispersed by ocean currents from several potential spawning sources to settle in various nursery regions that collectively constitute the sink. The reconstruction of the sparse source-sink linkage pattern, that is, to identify which sources contribute to which regions in the sink, is a challenging task in marine ecology. We derive a constrained nonlinear multicomponent regression model for source-sink reconstruction, which is capable of simultaneously selecting important linkages from the sources to the sink regions and making inference about the unobserved spawning activities at the sources. A sparsity-inducing and nonnegativity-constrained regularization approach is developed for model estimation, and theoretically we show that our estimator enjoys the oracle properties. The empirical performance of the method is investigated via simulation studies mimicking real ecological applications. We examine the transport hypothesis that Atlantic cod larvae were transported by sea currents from the North Sea to a few exposed coastal fjords along the Norwegian Skagerrak. Our findings of the spawning date distribution is consistent with results from previous studies, and the proposed approach for the first time provides valid statistical support for the larval drift conjecture. Supplementary materials for this article are available online."], ["Variable Selection in Nonparametric Classification Via Measurement Error Model Selection Likelihoods", null], ["Group LASSO for Structural Break Time Series", null], ["Testing Independence Among a Large Number of High-Dimensional Random Vectors", null], ["Adaptive Multivariate Global Testing", null], ["Adaptive Global Testing for Functional Linear Models", "This article studies global testing of the slope function in functional linear regression models. A major challenge in functional global testing is to choose the dimension of projection when approximating the functional regression model by a finite dimensional multivariate linear regression model. We develop a new method that simultaneously tests the slope vectors in a sequence of functional principal components regression models. The sequence of models being tested is determined by the sample size and is an integral part of the testing procedure. Our theoretical analysis shows that the proposed method is uniformly powerful over a class of smooth alternatives when the signal to noise ratio exceeds the detection boundary. The methods and results reflect the deep connection between the functional linear regression model and the Gaussian sequence model. We also present an extensive simulation study and a real data example to illustrate the finite sample performance of our method. Supplementary materials for this article are available online."], ["Local Empirical Likelihood Inference for Varying-Coefficient Density-Ratio Models Based on Case-Control Data", "In this article, we develop a varying-coefficient density-ratio model for case-control studies. The case and control samples come from two different distributions. Under the model assumption, the ratio of the two densities is related to the linear combination of covariates with varying coefficients through a known function. A special case is the exponential tilt model where the log ratio of the two densities is a linear function of covariates. We propose a local empirical likelihood (EL) approach to estimate the nonparametric coefficient functions. Under some regularity assumptions, the proposed estimators are shown to be consistent and asymptotically normally distributed. The sieve empirical likelihood ratio (SELR) test statistic for detecting whether the varying-coefficients are really constant and other related hypotheses is constructed and it follows approximately a chi-squared distribution. We introduce a modified bootstrap procedure to estimate the null distribution of the SELR when sample size is small. We also examine the performance of proposed method for finite sample sizes through simulation studies and illustrate it with a real dataset. Supplementary materials for this article are available online."], ["Enriched Stick-Breaking Processes for Functional Data", "In many applications involving functional data, prior information is available about the proportion of curves having different attributes. It is not straightforward to include such information in existing procedures for functional data analysis. Generalizing the functional Dirichlet process (FDP), we propose a class of stick-breaking priors for distributions of functions. These priors incorporate functional atoms drawn from constrained stochastic processes. The stick-breaking weights are specified to allow user-specified prior probabilities for curve attributes, with hyperpriors accommodating uncertainty. Compared with the FDP, the random distribution is enriched for curves having attributes known to be common. Theoretical properties are considered, methods are developed for posterior computation, and the approach is illustrated using data on temperature curves in menstrual cycles."], ["A Smooth Simultaneous Confidence Corridor for the Mean of Sparse Functional Data", "Functional data analysis (FDA) has become an important area of statistics research in the recent decade, yet a smooth simultaneous confidence corridor (SCC) does not exist in the literature for the mean function of sparse functional data. SCC is a powerful tool for making statistical inference on an entire unknown function, nonetheless classic \u201cHungarian embedding\u201d techniques for establishing asymptotic correctness of SCC completely fail for sparse functional data. We propose a local linear SCC and a shoal of confidence intervals (SCI) for the mean function of sparse functional data, and establish that it is asymptotically equivalent to the SCC of independent regression data, using new results from Gaussian process extreme value theory. The SCC procedure is examined in simulations for its superior theoretical accuracy and performance, and used to analyze growth curve data, confirming findings with quantified high significance levels. Supplementary materials for this article are available online."], ["Convex Optimization, Shape Constraints, Compound Decisions, and Empirical Bayes Rules", null], ["A Generic Path Algorithm for Regularized Statistical Estimation", null], ["Sparse Additive Ordinary Differential Equations for Dynamic Gene Regulatory Network Modeling", "The gene regulation network (GRN) is a high-dimensional complex system, which can be represented by various mathematical or statistical models. The ordinary differential equation (ODE) model is one of the popular dynamic GRN models. High-dimensional linear ODE models have been proposed to identify GRNs, but with a limitation of the linear regulation effect assumption. In this article, we propose a sparse additive ODE (SA-ODE) model, coupled with ODE estimation methods and adaptive group least absolute shrinkage and selection operator (LASSO) techniques, to model dynamic GRNs that could flexibly deal with nonlinear regulation effects. The asymptotic properties of the proposed method are established and simulation studies are performed to validate the proposed approach. An application example for identifying the nonlinear dynamic GRN of T-cell activation is used to illustrate the usefulness of the proposed method."], ["Parametrically Assisted Nonparametric Estimation of a Density in the Deconvolution Problem", "Nonparametric estimation of a density from contaminated data is a difficult problem, for which convergence rates are notoriously slow. We introduce parametrically assisted nonparametric estimators which can dramatically improve on the performance of standard nonparametric estimators when the assumed model is close to the true density, without degrading much the quality of purely nonparametric estimators in other cases. We establish optimal convergence rates for our problem and discuss estimators that attain these rates. The very good numerical properties of the methods are illustrated via a simulation study. Supplementary materials for this article are available online."], ["Estimation for General Birth-Death Processes", "Birth-death processes (BDPs) are continuous-time Markov chains that track the number of \u201cparticles\u201d in a system over time. While widely used in population biology, genetics, and ecology, statistical inference of the instantaneous particle birth and death rates remains largely limited to restrictive linear BDPs in which per-particle birth and death rates are constant. Researchers often observe the number of particles at discrete times, necessitating data augmentation procedures such as expectation-maximization (EM) to find maximum likelihood estimates (MLEs). For BDPs on finite state-spaces, there are powerful matrix methods for computing the conditional expectations needed for the E-step of the EM algorithm. For BDPs on infinite state-spaces, closed-form solutions for the E-step are available for some linear models, but most previous work has resorted to time-consuming simulation. Remarkably, we show that the E-step conditional expectations can be expressed as convolutions of computable transition probabilities for any general BDP with arbitrary rates. This important observation, along with a convenient continued fraction representation of the Laplace transforms of the transition probabilities, allows for novel and efficient computation of the conditional expectations for all BDPs, eliminating the need for truncation of the state-space or costly simulation. We use this insight to derive EM algorithms that yield maximum likelihood estimation for general BDPs characterized by various rate models, including generalized linear models (GLM). We show that our Laplace convolution technique outperforms competing methods when they are available and demonstrate a technique to accelerate EM algorithm convergence. We validate our approach using synthetic data and then apply our methods to cancer cell growth and estimation of mutation parameters in microsatellite evolution."], ["Nonparametric Regression for Spherical Data", "We develop nonparametric smoothing for regression when both the predictor and the response variables are defined on a sphere of whatever dimension. A local polynomial fitting approach is pursued, which retains all the advantages in terms of rate optimality, interpretability, and ease of implementation widely observed in the standard setting. Our estimates have a multi-output nature, meaning that each coordinate is separately estimated, within a scheme of a regression with a linear response. The main properties include linearity and rotational equivariance. This research has been motivated by the fact that very few models describe this kind of regression. Such current methods are surely not widely employable since they have a parametric nature, and also require the same dimensionality for prediction and response spaces, along with nonrandom design. Our approach does not suffer these limitations. Real-data case studies and simulation experiments are used to illustrate the effectiveness of the method."], ["Spectral Density Ratio Models for Multivariate Extremes", "The modeling of multivariate extremes has received increasing recent attention because of its importance in risk assessment. In classical statistics of extremes, the joint distribution of two or more extremes has a nonparametric form, subject to moment constraints. This article develops a semiparametric model for the situation where several multivariate extremal distributions are linked through the action of a covariate on an unspecified baseline distribution, through a so-called density ratio model. Theoretical and numerical aspects of empirical likelihood inference for this model are discussed, and an application is given to pairs of extreme forest temperatures. Supplementary materials for this article are available online."], ["Self-Excited Threshold Poisson Autoregression", "This article studies theory and inference of an observation-driven model for time series of counts. It is assumed that the observations follow a Poisson distribution conditioned on an accompanying intensity process, which is equipped with a two-regime structure according to the magnitude of the lagged observations. Generalized from the Poisson autoregression, it allows more flexible, and even negative correlation, in the observations, which cannot be produced by the single-regime model. Classical Markov chain theory and Lyapunov\u2019s method are used to derive the conditions under which the process has a unique invariant probability measure and to show a strong law of large numbers of the intensity process. Moreover, the asymptotic theory of the maximum likelihood estimates of the parameters is established. A simulation study and a real-data application are considered, where the model is applied to the number of major earthquakes in the world. Supplementary materials for this article are available online."], ["Selection of Mixed Copula Model via Penalized Likelihood", "A fundamental issue of applying a copula method in applications is how to choose an appropriate copula function for a given problem. In this article we address this issue by proposing a new copula selection approach via penalized likelihood plus a shrinkage operator. The proposed method selects an appropriate copula function and estimates the related parameters simultaneously. We establish the asymptotic properties of the proposed penalized likelihood estimator, including the rate of convergence and asymptotic normality and abnormality. Particularly, when the true coefficient parameters may be on the boundary of the parameter space and the dependence parameters are in an unidentified subset of the parameter space, we show that the limiting distribution for boundary parameter estimator is half-normal and the penalized likelihood estimator for unidentified parameter converges to an arbitrary value. Finally, Monte Carlo simulation studies are carried out to illustrate the finite sample performance of the proposed approach and the proposed method is used to investigate the correlation structure and comovement of financial stock markets."], ["A Class of Hazard Rate Mixtures for Combining Survival Data From Different Experiments", "Mixture models for hazard rate functions are widely used tools for addressing the statistical analysis of survival data subject to a censoring mechanism. The present article introduced a new class of vectors of random hazard rate functions that are expressed as kernel mixtures of dependent completely random measures. This leads to define dependent nonparametric prior processes that are suitably tailored to draw inferences in the presence of heterogenous observations. Besides its flexibility, an important appealing feature of our proposal is analytical tractability: we are, indeed, able to determine some relevant distributional properties and a posterior characterization that is also the key for devising an efficient Markov chain Monte Carlo sampler. For illustrative purposes, we specialize our general results to a class of dependent extended gamma processes. We finally display a few numerical examples, including both simulated and real two-sample datasets: these allow us to identify the effect of a borrowing strength phenomenon and provide evidence of the effectiveness of the prior to deal with datasets for which the proportional hazards assumption does not hold true. Supplementary materials for this article are available online."], ["Fused Estimators of the Central Subspace in Sufficient Dimension Reduction", null], ["EMVS: The EM Approach to Bayesian Variable Selection", "Despite rapid developments in stochastic search algorithms, the practicality of Bayesian variable selection methods has continued to pose challenges. High-dimensional data are now routinely analyzed, typically with many more covariates than observations. To broaden the applicability of Bayesian variable selection for such high-dimensional linear regression contexts, we propose EMVS, a deterministic alternative to stochastic search based on an EM algorithm which exploits a conjugate mixture prior formulation to quickly find posterior modes. Combining a spike-and-slab regularization diagram for the discovery of active predictor sets with subsequent rigorous evaluation of posterior model probabilities, EMVS rapidly identifies promising sparse high posterior probability submodels. External structural information such as likely covariate groupings or network topologies is easily incorporated into the EMVS framework. Deterministic annealing variants are seen to improve the effectiveness of our algorithms by mitigating the posterior multimodality associated with variable selection priors. The usefulness of the EMVS approach is demonstrated on real high-dimensional data, where computational complexity renders stochastic search to be less practical."], ["Simulated Stochastic Approximation Annealing for Global Optimization With a Square-Root Cooling Schedule", null], ["Book Reviews", null], ["\u201cEnsemble Subsampling for Imbalanced Multivariate Two-Sample Tests,\u201d", null], ["Bayesian Models for Multiple Outcomes in Domains With Application to the Seychelles Child Development Study", "The Seychelles Child Development Study (SCDS) examines the effects of prenatal exposure to methylmercury on the functioning of the central nervous system. The SCDS data include 20 outcomes measured on 9-year-old children that can be classified broadly in four outcome classes or \u201cdomains\u201d: cognition, memory, motor, and social behavior. Previous analyses and scientific theory suggest that these outcomes may belong to more than one of these domains, rather than only a single domain as is frequently assumed for modeling. We present a framework for examining the effects of exposure and other covariates when the outcomes may each belong to more than one domain and where we also want to learn about the assignment of outcomes to domains. Each domain is defined by a sentinel outcome, which is preassigned to that domain only. All other outcomes can belong to multiple domains and are not preassigned. Our model allows exposure and covariate effects to differ across domains and across outcomes within domains, and includes random subject-specific effects that model correlations between outcomes within and across domains. We take a Bayesian MCMC approach. Results from the Seychelles study and from extensive simulations show that our model can effectively determine sparse domain assignment, and at the same time give increased power to detect overall, domain-specific, and outcome-specific exposure and covariate effects relative to separate models for each endpoint. When fit to the Seychelles data, several outcomes were classified as partly belonging to domains other than their originally assigned domains. In retrospect, the new partial domain assignments are reasonable and, as we discuss, suggest important scientific insights about the nature of the outcomes. Checks of model misspecification were improved relative to a model that assumes each outcome is in a single domain. Supplementary materials for this article are available online."], ["A New Estimation Approach for Combining Epidemiological Data From Multiple Sources", "We propose a novel two-step procedure to combine epidemiological data obtained from diverse sources with the aim to quantify risk factors affecting the probability that an individual develops certain disease such as cancer. In the first step, we derive all possible unbiased estimating functions based on a group of cases and a group of controls each time. In the second step, we combine these estimating functions efficiently to make full use of the information contained in data. Our approach is computationally simple and flexible. We illustrate its efficacy through simulation and apply it to investigate pancreatic cancer risks based on data obtained from the Connecticut Tumor Registry, a population-based case\u2013control study, and the Behavioral Risk Factor Surveillance System which is a state-based system of health surveys. Supplementary materials for this article are available online."], ["Estimating the Lifetime Risk of Dementia in the Canadian Elderly Population Using Cross-Sectional Cohort Survival Data", "Dementia is one of the world\u2019s major public health challenges. The lifetime risk of dementia is the proportion of individuals who ever develop dementia during their lifetime. Despite its importance to epidemiologists and policy-makers, this measure does not seem to have been estimated in the Canadian population. Data from a birth cohort study of dementia are not available. Instead, we must rely on data from the Canadian Study of Health and Aging, a large cross-sectional study of dementia with follow-up for survival. These data present challenges because they include substantial loss to follow-up and are not representatively drawn from the target population because of structural sampling biases. A first bias is imparted by the cross-sectional sampling scheme, while a second bias is a result of stratified sampling. Estimation of the lifetime risk and related quantities in the presence of these biases has not been previously addressed in the literature. We develop and study nonparametric estimators of the lifetime risk, the remaining lifetime risk, and cumulative risk at specific ages, accounting for these complexities. In particular, we reveal the fact that estimation of the lifetime risk is invariant to stratification by current age at sampling. We present simulation results validating our methodology, and provide novel facts about the epidemiology of dementia in Canada using data from the Canadian Study of Health and Aging. Supplementary materials for this article are available online."], ["Statistical Modeling Methodology for the Voting Rights Act Section 203 Language Assistance Determinations", "Section 203 of the Voting Rights Act includes provisions requiring the use of election materials in languages other than English for states or political subdivisions, specifically, when a minimum number of voting age U.S. citizens of specified language minority groups who are unable to speak English very well and have obtained less than a fifth-grade education is met. Data on these characteristics are provided by the 2010 Census and the American Community Survey (ACS), a general purpose sample survey designed to produce a large volume of estimates across the spectrum of the nation\u2019s geographic areas and subgroups of the population. This article describes the small-area model and the estimation methods that were developed and applied to create the list of 2011 political subdivisions that were subject to the provisions."], ["A Locally Convoluted Cluster Model for Nucleosome Positioning Signals in Chemical Maps", "The nucleosome is the fundamental packing unit of DNA in eukaryotic cells, and its positioning plays a critical role in regulation of gene expression and chromosome functions. Using a recently developed chemical mapping method, nucleosomes can be potentially mapped with an unprecedented single-base-pair resolution. Existence of overlapping nucleosomes due to cell mixture or cell dynamics, however, causes convolution of nucleosome positioning signals. In this article, we introduce a locally convoluted cluster model and a maximum likelihood deconvolution approach, and illustrate the effectiveness of this approach in quantification of the nucleosome positional signal in the chemical mapping data. Supplementary materials for this article are available online."], ["A Methodology for Robust Multiproxy Paleoclimate Reconstructions and Modeling of Temperature Conditional Quantiles", "Great strides have been made in the field of reconstructing past temperatures based on models relating temperature to temperature-sensitive paleoclimate proxies. One of the goals of such reconstructions is to assess if current climate is anomalous in a millennial context. These regression-based approaches model the conditional mean of the temperature distribution as a function of paleoclimate proxies (or vice versa). Some of the recent focus in the area has considered methods that help reduce the uncertainty inherent in such statistical paleoclimate reconstructions, with the ultimate goal of improving the confidence that can be attached to such endeavors. A second important scientific focus in the subject area is the area of forward models for proxies, the goal of which is to understand the way paleoclimate proxies are driven by temperature and other environmental variables. One of the primary contributions of this article is novel statistical methodology for (i) quantile regression (QR) with autoregressive residual structure, (ii) estimation of corresponding model parameters, (iii) development of a rigorous framework for specifying uncertainty estimates of quantities of interest, yielding (iv) statistical byproducts that address the two scientific foci discussed above. We show that by using the above statistical methodology, we can demonstrably produce a more robust reconstruction than is possible by using conditional-mean-fitting methods. Our reconstruction shares some of the common features of past reconstructions, but we also gain useful insights. More importantly, we are able to demonstrate a significantly smaller uncertainty than that from previous regression methods. In addition, the QR component allows us to model, in a more complete and flexible way than least squares, the conditional distribution of temperature given proxies. This relationship can be used to inform forward models relating how proxies are driven by temperature."], ["Some Statistical Strategies for DAE-seq Data Analysis: Variable Selection and Modeling Dependencies Among Observations", "In DAE (DNA after enrichment)-seq experiments, genomic regions related with certain biological processes are enriched/isolated by an assay and are then sequenced on a high-throughput sequencing platform to determine their genomic positions. Statistical analysis of DAE-seq data aims to detect genomic regions with significant aggregations of isolated DNA fragments (\u201cenriched regions\u201d) versus all the other regions (\u201cbackground\u201d). However, many confounding factors may influence DAE-seq signals. In addition, the signals in adjacent genomic regions may exhibit strong correlations, which invalidate the independence assumption employed by many existing methods. To mitigate these issues, we develop a novel autoregressive Hidden Markov model (AR-HMM) to account for covariates effects and violations of the independence assumption. We demonstrate that our AR-HMM leads to improved performance in identifying enriched regions in both simulated and real datasets, especially in those in epigenetic datasets with broader regions of DAE-seq signal enrichment. We also introduce a variable selection procedure in the context of the HMM/AR-HMM where the observations are not independent and the mean value of each state-specific emission distribution is modeled by some covariates. We study the theoretical properties of this variable selection procedure and demonstrate its efficacy in simulated and real DAE-seq data. In summary, we develop several practical approaches for DAE-seq data analysis that are also applicable to more general problems in statistics. Supplementary materials for this article are available online."], ["Uncertainty in Propensity Score Estimation: Bayesian Methods for Variable Selection and Model-Averaged Causal Effects", null], ["Modeling Bivariate Longitudinal Hormone Profiles by Hierarchical State Space Models", "The hypothalamic-pituitary-adrenal (HPA) axis is crucial in coping with stress and maintaining homeostasis. Hormones produced by the HPA axis exhibit both complex univariate longitudinal profiles and complex relationships among different hormones. Consequently, modeling these multivariate longitudinal hormone profiles is a challenging task. In this article, we propose a bivariate hierarchical state space model, in which each hormone profile is modeled by a hierarchical state space model, with both population-average and subject-specific components. The bivariate model is constructed by concatenating the univariate models based on the hypothesized relationship. Because of the flexible framework of state space form, the resultant models not only can handle complex individual profiles, but also can incorporate complex relationships between two hormones, including both concurrent and feedback relationship. Estimation and inference are based on marginal likelihood and posterior means and variances. Computationally efficient Kalman filtering and smoothing algorithms are used for implementation. Application of the proposed method to a study of chronic fatigue syndrome and fibromyalgia reveals that the relationships between adrenocorticotropic hormone and cortisol in the patient group are weaker than in healthy controls. Supplementary materials for this article are available online."], ["Detection of Unusual Increases in MRI Lesion Counts in Individual Multiple Sclerosis Patients", "Data Safety and Monitoring Boards (DSMBs) for multiple sclerosis clinical trials consider an increase of contrast-enhancing lesions on repeated magnetic resonance imaging an indicator for potential adverse events. However, there are no published studies that clearly identify what should be considered an \u201cunexpected increase\u201d of lesion activity for a patient. To address this problem, we consider as an index the likelihood of observing lesion counts as large as those observed on the recent scans of a patient conditional on the patient\u2019s lesion counts on previous scans. To estimate this index, we rely on random effects models. Given the patient-specific random effect, we assume that the repeated lesion counts from the same patient follow a negative binomial distribution and may be correlated over time. We fit the model using data collected from the trial under DSMB review and update the estimation when new data are to be reviewed. We consider two estimation procedures: maximum likelihood for a fully parameterized model and a simple semiparametric method for a model with an unspecified distribution for the random effects. We examine the performance of our methods using simulations and illustrate the approach using data from a clinical trial. Supplementary materials for this article are available online."], ["Clustered Treatment Assignments and Sensitivity to Unmeasured Biases in Observational Studies", null], ["A Generalized Least-Square Matrix Decomposition", null], ["Bootstrap Testing of the Rank of a Matrix via Least-Squared Constrained Estimation", null], ["Parametric Estimation of Ordinary Differential Equations With Orthogonality Conditions", null], ["Joint Estimation of the Mean and Error Distribution in Generalized Linear Models", null], ["The Estimation of Leverage Effect With High-Frequency Data", "The leverage effect has become an extensively studied phenomenon that describes the (usually) negative relation between stock returns and their volatility. Although this characteristic of stock returns is well acknowledged, most studies of the phenomenon are based on cross-sectional calibration with parametric models. On the statistical side, most previous works are conducted over daily or longer return horizons, and few of them have carefully studied its estimation, especially with high-frequency data. However, estimation of the leverage effect is important because sensible inference is possible only when the leverage effect is estimated reliably. In this article, we provide nonparametric estimation for a class of stochastic measures of leverage effect. To construct estimators with good statistical properties, we introduce a new stochastic leverage effect parameter. The estimators and their statistical properties are provided in cases both with and without microstructure noise, under the stochastic volatility model. In asymptotics, the consistency and limiting distribution of the estimators are derived and corroborated by simulation results. For consistency, a previously unknown bias correction factor is added to the estimators. Applications of the estimators are also explored. This estimator provides the opportunity to study high-frequency regression, which leads to the prediction of volatility using not only previous volatility but also the leverage effect. The estimator also reveals a theoretical connection between skewness and the leverage effect, which further leads to the prediction of skewness. Furthermore, adopting the ideas similar to the estimation of the leverage effect, it is easy to extend the methods to study other important aspects of stock returns, such as volatility of volatility."], ["Model Selection via Bayesian Information Criterion for Quantile Regression Models", "Bayesian information criterion (BIC) is known to identify the true model consistently as long as the predictor dimension is finite. Recently, its moderate modifications have been shown to be consistent in model selection even when the number of variables diverges. Those works have been done mostly in mean regression, but rarely in quantile regression. The best-known results about BIC for quantile regression are for linear models with a fixed number of variables. In this article, we investigate how BIC can be adapted to high-dimensional linear quantile regression and show that a modified BIC is consistent in model selection when the number of variables diverges as the sample size increases. We also discuss how it can be used for choosing the regularization parameters of penalized approaches that are designed to conduct variable selection and shrinkage estimation simultaneously. Moreover, we extend the results to structured nonparametric quantile models with a diverging number of covariates. We illustrate our theoretical results via some simulated examples and a real data analysis on human eye disease. Supplementary materials for this article are available online."], ["Quantile Association Regression Models", "It is often important to study the association between two continuous variables. In this work, we propose a novel regression framework for assessing conditional associations on quantiles. We develop general methodology which permits covariate effects on both the marginal quantile models for the two variables and their quantile associations. The proposed quantile copula models have straightforward interpretation, facilitating a comprehensive view of association structure which is much richer than that based on standard product moment and rank correlations. We show that the resulting estimators are uniformly consistent and weakly convergent as a process of the quantile index. Simple variance estimators are presented which perform well in numerical studies. Extensive simulations and a real data example demonstrate the practical utility of the methodology. Supplementary materials for this article are available online."], ["Predictor Selection for Positive Autoregressive Processes", null], ["A Model-Averaging Approach for High-Dimensional Regression", null], ["Feature Selection for Varying Coefficient Models With Ultrahigh-Dimensional Covariates", "This article is concerned with feature screening and variable selection for varying coefficient models with ultrahigh-dimensional covariates. We propose a new feature screening procedure for these models based on conditional correlation coefficient. We systematically study the theoretical properties of the proposed procedure, and establish their sure screening property and the ranking consistency. To enhance the finite sample performance of the proposed procedure, we further develop an iterative feature screening procedure. Monte Carlo simulation studies were conducted to examine the performance of the proposed procedures. In practice, we advocate a two-stage approach for varying coefficient models. The two-stage approach consists of (a) reducing the ultrahigh dimensionality by using the proposed procedure and (b) applying regularization methods for dimension-reduced varying coefficient models to make statistical inferences on the coefficient functions. We illustrate the proposed two-stage approach by a real data example. Supplementary materials for this article are available online."], ["Scale-Invariant Sparse PCA on High-Dimensional Meta-Elliptical Data", "We propose a semiparametric method for conducting scale-invariant sparse principal component analysis (PCA) on high-dimensional non-Gaussian data. Compared with sparse PCA, our method has a weaker modeling assumption and is more robust to possible data contamination. Theoretically, the proposed method achieves a parametric rate of convergence in estimating the parameter of interests under a flexible semiparametric distribution family; computationally, the proposed method exploits a rank-based procedure and is as efficient as sparse PCA; empirically, our method outperforms most competing methods on both synthetic and real-world datasets."], ["Large Sample Randomization Inference of Causal Effects in the Presence of Interference", "Recently, there has been increasing interest in making causal inference when interference is possible. In the presence of interference, treatment may have several types of effects. In this article, we consider inference about such effects when the population consists of groups of individuals where interference is possible within groups but not between groups. A two-stage randomization design is assumed where in the first stage groups are randomized to different treatment allocation strategies and in the second stage individuals are randomized to treatment or control conditional on the strategy assigned to their group in the first stage. For this design, the asymptotic distributions of estimators of the causal effects are derived when either the number of individuals per group or the number of groups grows large. Under certain homogeneity assumptions, the asymptotic distributions provide justification for Wald-type confidence intervals (CIs) and tests. Empirical results demonstrate that the Wald CIs have good coverage in finite samples and are narrower than CIs based on either the Chebyshev or Hoeffding inequalities provided the number of groups is not too small. The methods are illustrated by two examples which consider the effects of cholera vaccination and an intervention to encourage voting."], ["Accurate Directional Inference for Vector Parameters in Linear Exponential Families", null], ["Expectation Propagation for Likelihood-Free Inference", null], ["A Nonparametric Approach for Multiple Change Point Analysis of Multivariate Data", "Change point analysis has applications in a wide variety of fields. The general problem concerns the inference of a change in distribution for a set of time-ordered observations. Sequential detection is an online version in which new data are continually arriving and are analyzed adaptively. We are concerned with the related, but distinct, offline version, in which retrospective analysis of an entire sequence is performed. For a set of multivariate observations of arbitrary dimension, we consider nonparametric estimation of both the number of change points and the positions at which they occur. We do not make any assumptions regarding the nature of the change in distribution or any distribution assumptions beyond the existence of the \u03b1th absolute moment, for some \u03b1 \u2208 (0, 2). Estimation is based on hierarchical clustering and we propose both divisive and agglomerative algorithms. The divisive method is shown to provide consistent estimates of both the number and the location of change points under standard regularity assumptions. We compare the proposed approach with competing methods in a simulation study. Methods from cluster analysis are applied to assess performance and to allow simple comparisons of location estimates, even when the estimated number differs. We conclude with applications in genetics, finance, and spatio-temporal analysis. Supplementary materials for this article are available online."], ["Probit Transformation for Kernel Density Estimation on the Unit Interval", "Kernel estimation of a probability density function supported on the unit interval has proved difficult, because of the well-known boundary bias issues a conventional kernel density estimator would necessarily face in this situation. Transforming the variable of interest into a variable whose density has unconstrained support, estimating that density, and obtaining an estimate of the density of the original variable through back-transformation, seems a natural idea to easily get rid of the boundary problems. In practice, however, a simple and efficient implementation of this methodology is far from immediate, and the few attempts found in the literature have been reported not to perform well. In this article, the main reasons for this failure are identified and an easy way to correct them is suggested. It turns out that combining the transformation idea with local likelihood density estimation produces viable density estimators, mostly free from boundary issues. Their asymptotic properties are derived, and a practical cross-validation bandwidth selection rule is devised. Extensive simulations demonstrate the excellent performance of these estimators compared to their main competitors for a wide range of density shapes. In fact, they turn out to be the best choice overall. Finally, they are used to successfully estimate a density of nonstandard shape supported on [0, 1] from a small-size real data sample."], ["Gradient-Based Kernel Dimension Reduction for Regression", null], ["Efficient Estimation of Semiparametric Transformation Models for Two-Phase Cohort Studies", "Under two-phase cohort designs, such as case\u2013cohort and nested case\u2013control sampling, information on observed event times, event indicators, and inexpensive covariates is collected in the first phase, and the first-phase information is used to select subjects for measurements of expensive covariates in the second phase; inexpensive covariates are also used in the data analysis to control for confounding and to evaluate interactions. This article provides efficient estimation of semiparametric transformation models for such designs, accommodating both discrete and continuous covariates, and allowing inexpensive and expensive covariates to be correlated. The estimation is based on the maximization of a modified nonparametric likelihood function through a generalization of the expectation\u2013maximization algorithm. The resulting estimators are shown to be consistent, asymptotically normal and asymptotically efficient with easily estimated variances. Simulation studies demonstrate that the asymptotic approximations are accurate in practical situations. Empirical data from Wilms\u2019 tumor studies and the Atherosclerosis Risk in Communities (ARIC) study are presented. Supplementary materials for this article are available online."], ["Landmark Estimation of Survival and Treatment Effect in a Randomized Clinical Trial", null], ["Kernels, Degrees of Freedom, and Power Properties of Quadratic Distance Goodness-of-Fit Tests", null], ["Multivariate Functional Halfspace Depth", "This article defines and studies a depth for multivariate functional data. By the multivariate nature and by including a weight function, it acknowledges important characteristics of functional data, namely differences in the amount of local amplitude, shape, and phase variation. We study both population and finite sample versions. The multivariate sample of curves may include warping functions, derivatives, and integrals of the original curves for a better overall representation of the functional data via the depth. We present a simulation study and data examples that confirm the good performance of this depth function. Supplementary materials for this article are available online."], ["Principal Flows", "We revisit the problem of extending the notion of principal component analysis (PCA) to multivariate datasets that satisfy nonlinear constraints, therefore lying on Riemannian manifolds. Our aim is to determine curves on the manifold that retain their canonical interpretability as principal components, while at the same time being flexible enough to capture nongeodesic forms of variation. We introduce the concept of a principal flow, a curve on the manifold passing through the mean of the data, and with the property that, at any point of the curve, the tangent velocity vector attempts to fit the first eigenvector of a tangent space PCA locally at that same point, subject to a smoothness constraint. That is, a particle flowing along the principal flow attempts to move along a path of maximal variation of the data, up to smoothness constraints. The rigorous definition of a principal flow is given by means of a Lagrangian variational problem, and its solution is reduced to an ODE problem via the Euler\u2013Lagrange method. Conditions for existence and uniqueness are provided, and an algorithm is outlined for the numerical solution of the problem. Higher order principal flows are also defined. It is shown that global principal flows yield the usual principal components on a Euclidean space. By means of examples, it is illustrated that the principal flow is able to capture patterns of variation that can escape other manifold PCA methods."], ["Bayes Variable Selection in Semiparametric Linear Models", null], ["Book Reviews", null]]}