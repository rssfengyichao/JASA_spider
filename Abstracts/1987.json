{"1987": [["Census Undercount Adjustment and the Quality of Geographic Population Distributions", "Our principal analytical result shows that a necessary but not sufficient condition for adjustment to fail to improve the quality of the geographic distribution is that either blacks are most heavily undercounted where they are least prevalent or whites are most heavily undercounted where they are most prevalent. According to the best available empirical evidence, such patterns of covariation do not prevail. To derive the result, we assume that national undercounts are measured perfectly, although simulation results suggest that this is not critical."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Tests of Forecast Accuracy and Bias for County Population Projections", "This article deals with the forecast accuracy and bias of population projections for 2,971 counties in the United States. It uses three different projection techniques and data from 1950, 1960, 1970, and 1980 to make two sets of 10-year projections and one set of 20-year projections. These projections are compared with census counts to determine forecast errors. The size, direction, and distribution of forecast errors are analyzed by size of place, rate of growth, and length of projection horizon. A number of consistent patterns are noted, and an extension of the empirical results to the production of confidence intervals for population projections is considered."], ["Comment", null], ["Rejoinder", null], ["A Comparison of Household and Provider Reports of Medical Conditions", "Levels of provider agreement with household reports ranged from 1% for upper extremity and shoulder impairments to 86% for pregnancy care and delivery. Over all 63 categories, providers concurred with household condition reports 40% of the time. Broader categories created by collapsing conditions substantially increased agreement levels. Analyses of household-provider agreement suggest that questionnaire revisions and revised condition categories would have increased correspondence between survey responses and provider records."], ["Time Series Analysis of a Contagious Process", "Parameter estimates for the models suggest that successful hijackings in the United States did indeed increase the subsequent rate of hijacking attempts during the 1968\u20131972 period; each successful hijacking generated about .5 new attempts, with a median delay of about 33 days. Unsuccessful attempts had neither a stimulating nor an inhibiting effect."], ["Best Approximate Aggregation of Input\u2014Output Systems", "A method of constructing aggregate input\u2014output systems is proposed. The procedure minimizes the mean squared error of aggregate predictions. The range of data on which the aggregate model is to be applied influences the best approximate aggregate. Best approximate aggregate models maximize Ijiri's coefficient of aggregation. In a special case, Fisher's aggregated model emerges: if consistent aggregation is possible it gives the best approximate aggregate. Computations reveal best approximate aggregations to be feasible and to yield considerably better predictions than alternative methods in realistic cases."], ["Non-Gaussian State\u2014Space Modeling of Nonstationary Time Series", "The algorithms herein can be easily extended to a wider class of models. As an example, the smoothing of nonhomogeneous binomial mean function is shown, where the observation is distributed according to a discrete random variable. Extension to a nonlinear system is also straightforward."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Benchmarking of Economic Time Series", "This article develops a statistical model-based approach to the benchmarking problem. Benchmarking is done when data from a monthly sample survey are combined with data from an annual census for the purpose of improving the survey estimates. Previous authors have used numerical analysis techniques to derive methods to perform benchmarking. This article formulates the benchmarking problem in a statistical framework and uses modern times series methods to derive a solution. This solution is based in part upon the statistical properties of the time series being benchmarked and upon the properties of the survey errors associated with that time series. The article makes use of the theory of signal extraction that has been derived for nonstationary time series. Two common types of benchmarking problems are studied in greater detail. The results of the theory derived in the article are illustrated by an example."], ["Estimating Properties of Autoregressive Forecasts", null], ["Variance Function Estimation", null], ["Analysis of Covariance Structures under Elliptical Distributions", "This article examines the adjustment of normal theory methods for the analysis of covariance structures to make them applicable under the class of elliptical distributions. It is shown that if the model satisfies a mild scale invariance condition and the data have an elliptical distribution, the asymptotic covariance matrix of sample covariances has a structure that results in the retention of many of the asymptotic properties of normal theory methods. If a scale adjustment is applied, the likelihood ratio tests of fit have the usual asymptotic chi-squared distributions. Difference tests retain their property of asymptotic independence, and maximum likelihood estimators retain their relative asymptotic efficiency within the class of estimators based on the sample covariance matrix. An adjustment to the asymptotic covariance matrix of normal theory maximum likelihood estimators for elliptical distributions is provided. This adjustment is particularly simple in models for patterned covariance or correlation matrices. These results apply not only to normal theory maximum likelihood methods but also to a class of minimum discrepancy methods. Similar results also apply when certain robust estimators of the covariance matrix are employed."], ["Optimal Designs for Binary Data", "The results presented here attempt to strengthen those introduced by Abdelbasit and Plackett (1983), who derived optimal allocation for the local criterion under the restriction of symmetry and suggested a two-stage procedure. Here it is shown that although for even sample size the optimal allocation is in fact symmetric, for odd sample size the restriction of symmetry leads to suboptimal designs (one important case is only 93% efficient). For the two-stage procedure, their proposal does not attempt to compensate in the second stage. This may lead to a serious loss of efficiency (below 80% in extreme cases). The numerical results corresponding to the global criterion (i.e., using the likelihood regions) indicate that the globally best allocation corresponds to less extreme probabilities than those prescribed by the local criterion."], ["The Power of the Mantel\u2014Haenszel Test", "Simulation studies show that for the CB the new approximations for power are closer to the exact values than previously obtained formulas. We also present designs in which the CB power differs markedly from that of the DD, and we show that our approximations do an excellent job of tracking these differences. This comparison also indicates that the strategy of poststratification may in some situations lead to an appreciable loss of power."], ["Computing Distributions for Exact Logistic Regression", "The algorithm given in this article enables the data analyst to perform exact inference for models with or without interaction terms and for matched as well as unmatched designs. Exact analysis proposed by Cox (1970) was restricted to a single parameter. Since our algorithm can be used to generate any combination of joint and conditional distributions of the sufficient statistics, it paves the way for multiparametric exact inference. Further, this algorithm also provides a tool for comparing exact and asymptotic inferential procedures. Such comparisons would, it is hoped, provide statisticians with guidelines stating when each of the procedures should be preferred."], ["The Efficiency of Multinomial Logistic Regression Compared with Multiple Group Discriminant Analysis", "In this article detailed evaluations are given for two specific cases of interest in which the ARE is parameterized only by the response group frequencies and the generalized distances between them. For the case of collinear logistic slope vectors, the presence of additional response groups can have substantial influence on the ARE, especially for equal response group frequencies. The range of the ARE changes from 50% to 65% with two groups to 35% to 95% with four groups for generalized distances of 3.0 to 3.5 between the first two response groups. For the case of orthogonal logistic slope vectors, the ARE generally decreases in the presence of an additional response group."], ["An Approach to Inference following Model Selection with Applications to Transformation-Based and Adaptive Inference", "Hogg, Uthoff, Randles, and Davenport (1972) did adaptive inference. They selected from among a finite number of possible location-scale families and then estimated the location of the chosen family. Our formulation is appropriate for this situation. We address the issue of whether Hogg's intuitive adaptive procedure is generalized Bayes and/or admissible for a suitable loss function. We offer a loss function for which a modification of Hogg's procedure has such optimality properties."], ["Biased and Unbiased Cross-Validation in Density Estimation", null], ["Fine-Tuning Some Resistant Rules for Outlier Labeling", null], ["Least-Absolute-Value Prediction Lines in Closed Form", null], ["Robust Empirical Bayes Estimation of Means from Stratified Samples", "It is interesting to note that when samples of equal size are drawn from different strata, the empirical Bayes estimators of Ghosh and Meeden (1986) include as special cases the James\u2014Stein estimators, the positive-part James\u2014Stein estimators, and the estimators of Lindley and Smith (1972). A new feature of the present article is that the Bayes risks of such estimators, calculated without the normality assumption, indicate a clear robustness of procedures motivated originally under normality."], ["Inference and Prediction for a General Order Statistic Model with Unknown Population Size", null], ["Restricted Mean Life with Adjustment for Covariates", null], ["Book Reviews", null], ["Letters to the Editor", null], ["Editorial Board Page", "This article has no abstract"], ["An Application of Bayes Methodology to the Analysis of Diary Records from a Water Use Study", "One of the objectives of a comprehensive domestic water use study conducted in Perth, Western Australia, was the quantification of components of water usage. Each household in a sample of about 3,000 was asked to record all water uses in a specially provided diary for a 2-week period, and the total water usage, both in-house and ex-house, was metered daily during this time. This article describes an application of Bayesian methodology to the estimation of usage for each in-house water-using activity for each house on each day of participation. The estimation procedure allows for inaccuracies in the diary records and incorporates prior information on the distributions of certain rates and volumes over houses. The prior information came from intensive data collection on a prior sample of about 150 households."], ["Restricted Randomization: A Practical Example", "Once a good initial arrangement is known, plans can be quickly produced for a large number of trials with the same experimental structure."], ["On Concurrent Seasonal Adjustment", "As all models are but approximations to the truth, and as \u201clinearized X-11-ARIMA\u201d is an approximation to actual seasonal adjustment procedures employed, an example is included of an industrial production series published by the Federal Reserve Board. As expected, and as shown in Table 4, concurrent seasonal adjustment of the series is more accurate than year-ahead adjustment, but to a smaller degree because of the preliminary-data error in the NSA series."], ["The Effects of Annual Accounting Data on Stock Returns and Trading Activity: A Causal Model Study", "The model is estimated, and the hypothesized model configuration recreates 82% of the generalized variance in the observed data. The individual parameter estimates show that the major source of variation in abnormal returns is the unexpected changes in profitability. None of the unexpected changes in the financial dimensions are found to be significantly linked directly to abnormal trading volume. Instead, the link is indirect through the relationship between abnormal returns and abnormal volume."], ["Empirical Bayes Confidence Intervals Based on Bootstrap Samples", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["A Unified Treatment of Integer Parameter Models", "All but the first three models have additional parameters \u03b8\u2014which leads to a variety of approaches to inferential problems. For instance, the mark\u2013recapture model has a conditional likelihood free from \u03b8 and so exact conditional methods apply. In this case exact, conditional likelihood and profile likelihood methods are all available to construct confidence intervals. On the other hand, conditional methods are not available in the capture\u2013removal model."], ["Minimum Norm Quadratic Estimation of Spatial Variograms", null], ["Bayesian Methods for Censored Categorical Data", "Such a likelihood is ordinarily considered intractable and unsuited for Bayesian conjugate prior inference. We develop a Bayesian conjugate theory, however, by recognizing the complete integrals of such functions as Carlson functions and the posterior distributions resulting from Dirichlet prior distributions as known generalized Dirichlet distributions. The corresponding posterior density functions are similar in form to the likelihood, and these constitute a family of distributions closed under sampling and tractable in various senses, including the convenient computability of moments and modes."], ["An Exact Test for Multiple Inequality and Equality Constraints in the Linear Regression Model", null], ["Time- and Space-Efficient Algorithms for Least Median of Squares Regression", null], ["Minimum Hellinger Distance Estimation for the Analysis of Count Data", "Minimum Hellinger distance (MHD) estimation is studied in the context of discrete data. The MHD estimator is shown to provide an effective treatment of anomalous data points, and its properties are illustrated using short-term mutagenicity test data. Asymptotic normality for a discrete distribution with countable support is derived under a readily verified condition on the model. Breakdown properties of the MHD estimator and an outlier screen are compared. Count data occur frequently in statistical applications. For instance, in chemical mutagenicity studies, which comprise an important step in the identification of environmental carcinogens, much of the resultant data are counts. Woodruff, Mason, Valencia, and Zimmering (1984) reported anomalous counts in the sex-linked recessive lethal test in drosophila. These outliers can have a substantial impact on the experimental conclusions. MHD estimation provides a means for reliable inference when modeling count data that are prone to outliers. The MHD fit gives little weight to counts that are improbable relative to the model. On the other hand, the MHD estimator is asymptotically equivalent to the maximum likelihood estimator when the model is correct. This latter result, long known for a parametric multinomial model with finite support, is extended here to models with countable support. The breakdown point provides a quantification of outlier resistence. Roughly, it is the smallest proportion of outliers in the data that can cause an arbitrarily large shift in the estimate (Donoho and Huber 1983). Here the MHD estimator is shown to have an asymptotic breakdown point of \u00bd at the model."], ["Regression Methods for Poisson Process Data", null], ["Test Statistics Derived as Components of Pearson's Phi-Squared Distance Measure", null], ["Conditioning Ratio Estimates under Simple Random Sampling", "Roy all and Cumberland (1981) illustrated empirically that the usual random sampling estimates and estimated variances could be badly conditionally biased when samples were obtained that were unbalanced in the auxiliary variate. So if the sample mean and population mean, both of which are known at the time of analysis, are not close, it is known that the results are biased, but the usual random sampling formulas, which are purely unconditional, do not take account of this. They suggested that prediction models were necessary to adjust these. If the joint distribution of the sample means of the variate and covariate are considered together, then using the fact that these are asymptotically bivariate normal, it is possible to adjust approximately for the linear regression of the variate mean on the mean of the auxiliary variate. In this way a bias-adjusted estimate is suggested and formulas for the conditional variances of the estimates are obtained. A simulation study of two populations, one where the model on which the ratio estimate is based holds and one where it does not, is carried out to illustrate the improvements given by the results and to compare them with results based on the formulas obtained by Royall and Cumberland (1981) by using prediction models, showing that results based on the assumption of simple random sampling can provide adequate theoretical results."], ["A Frequency-Domain Median Time Series", "This article discusses the median of a batch of time series. We focus on the frequency-domain median (FDM) series (Zeger 1985), which is calculated from the \u201cmediancenters\u201d (Gower 1974) of the discrete Fourier transforms. Like the univariate median, it has a breakdown value of 50%. Like the mean, the FDM time series of independent realizations from a stationary Gaussian process has a spectrum proportional to that of the component series. Analogs of the univariate median have been described for multivariate (e.g., Brown 1983), survival (Brookmeyer and Crowley 1982), and spherical (Fisher 1985) data. As in the multivariate case, there is not a unique definition of a median time series. One possibility is the median value at each time or time-domain median (TDM). A drawback of the TDM is that it can be qualitatively dissimilar in appearance to its component series because of discontinuities when the median value switches from one realization to another. This is particularly disturbing for smooth time series. Our alternative, the FDM, avoids this problem for nearly Gaussian processes. We show that the breakdown value of the mediancenter of spherically symmetric random variables is 50%. The result for the FDM then follows. We consider a set of time series in which each is composed of a common signal and an iid stationary error series. Two asymptotic scenarios are discussed\u2014the number of iid series of finite length getting large and the length increasing for a finite number of realizations. In the former case, the FDM is asymptotically a Gaussian process with a spectrum similar to that of the components. In the latter, we show that the FDM of stationary Gaussian processes is unbiased and has a spectrum proportional to the spectrum of a single realization. An example illustrates the FDM."], ["Identifying a Simplifying Structure in Time Series", "This article studies how to identify hidden factors in multivariate time series process. This problem is important because, when the series are driven by a set of common factors, (a) a large number of parameters may be needed to obtain an adequate representation of the system and (b) the estimated parameters will be highly correlated. Therefore, a complex and badly defined relationship can appear when, in fact, a simpler and parsimonious model in terms of a few common factors can be operating. This article develops a methodology to identify the number of factors and to build a simplifying transformation to represent the series. It is proved that the number of factors is equal to the rank of the covariance matrices and the parameter matrices of the infinite moving average representation of the process. The eigenvectors of these matrices will provide the canonical transformation. The method is illustrated with one example, using series of the price of wheat in five provinces of Spain in the 19th century. The standard approach to build a vector autore-gressive integrated moving average model showed a complex relationship with all kinds of feedback operating. When the methodology developed in the article was applied, however, two factors were identified and a clearer and simpler representation of the system was achieved."], ["Marginal Curvatures and Their Usefulness in the Analysis of Nonlinear Regression Models", null], [null, null], ["A Semiparametric Approach to Density Estimation", null], ["A Comparison of Variance Component Estimates for Arbitrary Underlying Distributions", null], ["Quick Simultaneous Confidence Intervals for Multinomial Proportions", "Opinion polls often give an indication of sampling error based on the standard conservative confidence intervals for a single binomial proportion. We give a lower bound for the simultaneous coverage probability when such intervals are applied to all of the proportions in a poll. This bound is equal to 1 \u2013 2\u03b1. for small values of \u03b1. The same result is shown to apply to the standard confidence intervals for changes in proportions between surveys."], ["Model Robustness for Simultaneous Confidence Bands", "Confidence bands for the simple linear model are examined to assess their degrees of robustness to departures from the model. All calculations are made under an interval constraint on the range of interest for the predictor variable. The true model is taken to be a quadratic polynomial, and departure from the linear case is considered in terms of increasing magnitude of a curvature parameter. Proposed measures of robustness include the actual coverage probability under the true model and a measure of percentage coverage over the predictor variable axis when the band fails to cover the true quadratic model. The different band functions considered include hyperbolic bands constructed from Scheffd's 5 method, linear-segmented bands, and fixed-width (minimax) and minimax-regret bands. In terms of preserving coverage probability under quadratic mis-specification, the fixed-width and linear-segment bands perform best, the former being preferred when the constraint interval on the predictor variable is small. When coverage is lost over some portion of the constraint interval, the fixed-width bands are also shown to preserve the greatest percentage of covered (predictor) axis values. The favorable performance of the fixed-width bands may be due to their generally wider, more rigid shape, relative to more curvilinear competitors. This may allow the bands to retain more extreme quadratic misspecifications than other bands."], ["Best Median-Unbiased Estimation in Linear Regression with Bounded Asymmetric Loss Functions", null], ["Minimax Regret Simultaneous Confidence Bands for Multiple Regression Functions", "Formulas for computing critical points for minimax regret bands are given for two special cases: for simple linear regression, when the predictor variable is constrained to lie in an interval, and for (balanced) multiple regression with intercept, when the constraint region is one of those defined by Halperin and Gurian (1968). Tables of critical points are presented for balanced simple linear regression over a finite interval. The following application is presented to illustrate the use of these results. A fitted simple linear regression model based on data from Duncan (1974) is to be used to relate strengths of weldings to their diameters, and a simultaneous confidence band for expected welding strength is to be constructed over a practical range of welding diameters. For this problem, a comparison between confidence intervals defined by the 95% minimax regret band, the Scheff\u00e9-type band (Scheff\u00e9 1953, 1959), and the constant-width band is given."], ["Comparison of Several Treatments with a Control Using Multiple Contrasts", "The choice of the particular convex combination depends on the experimenter's belief regarding the relative magnitudes of the \u201ccommon\u201d effectiveness of the treatments and their individual effectiveness. If, for example, in a drug experiment each treatment is a combination of a common drug, known to be effective, and some individual new drug, the experimenter may wish to determine the convex combination based on his belief of the relative effectiveness of the new drugs. In the absence of such prior information we recommend the orthogonal contrast test for its simplicity and uniform power."], ["Influence Analysis of Generalized Least Squares Estimators", "Influence analysis and leverage analysis are important and well-established adjuncts to ordinary least squares (OLS) regression, but analogous regression diagnostics are not generally available for multivariate regression problems. This article describes measures of influence and leverage for a generalized least squares (GLS) estimator of the regression coefficients in a class of multivariate linear models for repeated measurements. When the covariance matrix of the observation vectors is known, the GLS estimator can be expressed as an OLS estimator calculated from transformed variables. In this situation, the multivariate regression diagnostics are directly related to the univariate measures. When the covariance matrix is unknown, however, multistage estimation procedures are required. We describe measures of influence and leverage for a three-step estimator that uses the residuals from an initial OLS regression to estimate the covariance matrix of the response vectors. These diagnostics account for the influence of individual data values on the estimated covariance matrix. The article reports on two different measures of influence and the relation between them: (a) derivative influence, the differential change in an estimated parameter or predicted value resulting from a slight perturbation in the weight assigned to a given observation or vector of observations, and (b) deletion influence, the change in a parameter estimate resulting from the deletion of the vector of observations. Both of these measures can be represented as the sum of components that correspond to the stages of the GLS estimation. Examination of the relative asymptotic sizes of these components and of the differences between derivative and deletion influence leads to modified versions of these diagnostics that require less computation than deletion influence but are more appropriate than derivative influence for small samples. This approach yields computationally feasible diagnostics for the three-step estimator under study and for a larger class of non-iterative GLS estimators. An example illustrates their usefulness."], [null, "The test statistics are essentially based on a doubly weighted sum of integrated squared differences between the empirical distribution functions of the individual samples and that of the pooled sample. One weighting adjusts for the possibly different sample sizes, and the other is inside the integration placing more weight on tail differences of the compared distributions. The two versions differ mainly in the definition of the empirical distribution function. These tests are consistent against all alternatives. The use of these tests is two-fold: (a) in a one-way analysis of variance to establish differences in the sampled populations without making any restrictive parametric assumptions or (b) to justify the pooling of separate samples for increased sample size and power in further analyses. Exact finite sample mean and variance formulas for one of the two statistics are derived in the continuous case. It appears that the asymptotic standardized percentiles serve well as approximate critical points of the appropriately standardized statistics for individual sample sizes as low as 5. The application of the tests is illustrated with an example. Because of the convolution nature of the asymptotic distribution, a further use of these critical points is possible in combining independent Anderson\u2013Darling tests by simply adding their test statistics."], ["How Much Better are Better Estimators of a Normal Variance", null], ["Small Sample Properties of Probit Model Estimators", "We find that, on average, the Hessian matrix and the information matrix give almost identical results and lead to more accurate estimates of the asymptotic covariance matrix than does the estimator based on first derivatives. The finite sample mean squared error of the maximum likelihood estimator, however, is considerably greater than the asymptotic covariance matrix, and the estimator based on first derivatives provides a better estimate of finite sample mean squared error. All three estimators lead to empirical distributions that can be approximated by an asymptotic normal distribution. The pretest estimator formed by testing for the omission of an explanatory variable is reasonably efficient, but its distribution is seldom well approximated by a normal distribution."], ["Parameter Estimation for the Sichel Distribution and its Multivariate Extension", "Examples of application for both univariate and bivariate cases are given. Since the Sichel distribution encompasses a number of the well-known discrete distributions as limiting forms (Sichel 1971), the estimates of the parameters sometimes suggest an appropriate limiting form for the data. This is illustrated in one of the examples."], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Editors' Report for 1986", null], ["Generalized Additive Models: Some Applications", "The local scoring algorithm is analogous to the iterative reweighted least squares algorithm for solving likelihood and nonlinear regression equations. At each iteration, an adjusted dependent variable is formed and an additive regression model is fit using the backfitting algorithm. The backfitting algorithm cycles through the variables and estimates each coordinated function by smoothing the partial residuals."], ["Model-Based Direct Adjustment", "Direct adjustment or standardization applies population weights to subclass means in an effort to estimate population quantities from a sample that is not representative of the population. Direct adjustment has several attractive features, but when there are many subclasses it can attach large weights to small quantities of data, often in a fairly erratic manner. In the extreme, direct adjustment can attach infinite weight to nonexistent data, a noticeable inconvenience in practice. This article proposes a method of model-based direct adjustment that preserves the attractive features of conventional direct adjustment while stabilizing the weights attached to small subclasses. The sample mean and conventional direct adjustment are both special cases of model-based direct adjustment under two different extreme models for the subclass-specific selection probabilities. The discussion of this method provides some insights into the behavior of true and estimated propensity scores: the estimated scores are better than the true ones for almost the same reason that direct adjustment can outperform the sample mean in a simple random sample. The method is applied to a nonrandom sample in an effort to estimate a discrete distribution of essay scores in the College Board's Advanced Placement Examination in Biology."], ["Uncertainty in Model-Based Seasonal Adjustment Procedures and Construction of Minimax Filters", "Unobserved component autoregressive integrated moving average models are often the cornerstone of model-based seasonal adjustment procedures. Unfortunately these models are inherently underidentified and ad hoc assumptions must be made prior to the analysis. This article investigates the effect of seasonal adjustment filters on a class of observationally equivalent models. Bounds on the mean squared error (MSE) associated with arbitrary linear filters are derived. The article also derives robust seasonal adjustment filters. The filters are robust in the sense that they minimize the maximum MSE from the set of observationally equivalent models. The article shows that the minimax and minimum extraction filters are equivalent for a certain class of models. Empirical results for a number of economic time series are presented."], ["Some Generalizations of the Mitofsky-Waksberg Technique for Random Digit Dialing", "To provide greater flexibility and improved methodology for random digit dialing for telephone surveys, this article proposes a class of two-stage sampling techniques for random digit dialing that generalizes the Mitofsky\u2013Waksberg technique in two directions. First, different ways of classifying telephone numbers are allowed; the classification need not be according to whether or not the number is residential. Second, drawing more than one phone number per primary sampling unit is allowed in the first stage, with the result that complexities in the second stage can be reduced."], ["Research in Statistical Graphics", "Three active areas of research in statistical graphics are methodology, computing, and graphical perception. The five articles in this special section provide a good representation of these areas. Dynamic graphical methods, which involve a tight weaving of methodological and computing issues, are a particularly exciting topic of research; dynamic graphics will soon have a major impact on how data are analyzed. Evaluation of graphical inventions is difficult. Certain visualization issues can be evaluated by studies in graphical perception. At the moment, however, the performance of most graphical methods can be studied only by rigorous field testing in which the methods are used to analyze a large number of data sets. A bibliography is provided to allow readers to begin a study of any of the major areas of statistical graphics."], [null, "High-performance interaction with scatterplot matrices is a powerful approach to exploratory multivariate data analysis. For a small number of data points, real-time interaction is possible and overplotting is usually not a major problem. When the number of plotted points is large, however, display techniques that deal with overplotting and slow production are important. This article addresses these two problems. Topics include density representation by gray scale or by symbol area, alternatives to brushing, and animation sequences. We also discuss techniques that are generally applicable, including interactive graphical subset selection from any plot in a collection of scatterplots and comparison of scatterplot matrices."], ["The Geometric Interpretation of Correspondence Analysis", "Correspondence analysis is an exploratory multivariate technique that converts a data matrix into a particular type of graphical display in which the rows and columns are depicted as points. The method has a long and varied history and has appeared in different forms in the psychometric and ecological literature, among others. In this article we review the geometry of correspondence analysis and its geometric interpretation. We also discuss various extensions of correspondence analysis to multivariate categorical data (multiple correspondence analysis) and a variety of other data types."], ["Experiences with Three-Dimensional Scatterplots", "This article is a brief, subjective account of my experiences with various implementations and uses of three-dimensional scatterplots."], ["An Information-Processing Analysis of Graph Perception", "Recent work on graph perception has focused on the nature of the processes that operate when people decode the information represented in graphs. We began our investigations by gathering evidence that people have generic expectations about what types of information will be the major messages in various types of graphs. These graph schemata suggested how graph type and judgment type would interact to determine the speed and accuracy of quantitative information extraction. These predictions were confirmed by the finding that a comparison judgment was most accurate when the judgment required assessing position along a common scale (simple bar chart), had intermediate accuracy on length judgments (divided bar chart), and was least accurate when assessing angles (pie chart). In contrast, when the judgment was an estimate of the proportion of the whole, angle assessments (pie chart) were as accurate as position (simple bar chart) and more accurate than length (divided bar chart). Proposals for elementary information processes involving anchoring, scanning, projection, superimposition, and detection operators were made to explain this interaction."], ["Plot Windows", "Personal workstations with bitmap displays and window systems are becoming a widely used computing tool. This article describes some of the capabilities of an experimental package for drawing scatterplots and histograms that has been implemented on such a workstation. An example illustrates how those capabilities can be used in the graphical analysis of a data set."], ["Repeated Sampling in the Presence of Publication Effects", null], ["Comment", null], ["Comment", null], ["The Analysis of Two-Stage Sampling Data by Ordinary Least Squares", "A reasonably broad class of models for analyzing two-stage samples is considered. For models in this class, ordinary least squares (OLS) estimates are best linear unbiased estimates (BLUE's) and a large variety of exact tests and confidence intervals are available. The necessary computations can be made simply by fitting two models in the ordinary fashion using least squares. The most stringent assumption made is that there are equal numbers of observations taken in each cluster. For such a sampling scheme, OLS estimates are BLUE's iff for each variable in the model, the variable obtained by replacing each component with the corresponding cluster average is also, either implicitly or explicitly, contained in the model. A quite general condition on the parameterization (actually the design matrix) of such models is given that, if satisfied, assures the existence of exact inferential procedures for a wide variety of parametric functions. For any model in which OLS estimates are BLUE's there exist parameterizations that satisfy the condition in question. Results are also given for the problem of testing a given model against a reduced model."], ["Generalized Variance Functions in Stratified Two-Stage Sampling", null], ["Conditional Properties of Some Estimators in Stratified Sampling", null], ["A Constructive Procedure for Unbiased Controlled Rounding", null], ["Compound Random Number Generators", "The present article describes an alternative class of generators, which produce random values by compounding, or intermixing, several different sequences of random values. These \u201ccompound\u201d generators are easier to implement, especially on smaller computers, than are Marsaglia's recommended generators. Five specific compound generators, two standard generators, and Marsaglia's two combination generators were subjected to several tests for randomness, including Marsaglia's stringent tests. The standard generators each failed at least one of the tests; Marsaglia's generators and all of the compound generators passed all of the tests."], ["The Calculation of Posterior Distributions by Data Augmentation", "There are several examples given in this article. First, the algorithm is introduced and motivated in the context of a genetic linkage example. Second, we apply this algorithm to an example of inference from incomplete data regarding the correlation coefficient of the bivariate normal distribution. It is seen that the algorithm recovers the bimodal nature of the posterior distribution. Finally, the algorithm is used in the analysis of the traditional latent-class model as applied to data from the General Social Survey."], ["Comment", null], ["Comment", null], ["Comment", "A Noniterative Sampling/Importance Resampling Alternative to the Data Augmentation Algorithm for Creating a Few Imputations When Fractions of Missing Information Are Modest: The SIR Algorithm"], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["The Nonparametric Estimation of Branching Curves", "The estimation technique is illustrated by data from an experiment on sunflowers in which each of four treatments diverges from the control at a different specified time. The branching curve estimates give an informative presentation of the data and of the structure of the experiment and indicate the relative merits of the treatments."], ["Local Likelihood Estimation", "In some real data examples, the local likelihood technique proves to be effective in uncovering nonlinear dependencies. It is useful as a descriptive tool or to suggest transformations of the covariates. We also discuss some methods for inference."], ["Estimation of Variance of the Regression Estimator", null], ["Spatial Autocorrelation among Errors and the Relative Efficiency of OLS in the Linear Regression Model", "In practice, the relevant eigenvector will often be a column of ones. This implies that OLS is in the limit as good as GLS whenever there is a constant in the regression. We conclude, however, from several concrete examples that the loss in efficiency can still be substantial for intermediate degrees of autocorrelation."], ["Bayes Linear Estimators for Randomized Response Models", "Two versions of randomized response are examined: the original formulation of Warner (1965) and the unrelated question method of Simmons (Horvitz, Shah, and Simmons 1967). Other Bayesian analyses of these models have been proposed by Winkler and Franklin (1979) and Pitz (1980), respectively, using parametric models of prior information. Using the Bayes linear estimator, generally similar results are obtained from a nonparametric model. This model extends the applicability of the Bayesian approach, while also offering estimates that are trivial to compute."], ["Computable MINQUE-Type Estimates of Variance Components", "The procedure yields computationally convenient estimates in the general mixed ANOVA model. Computing formulas are given, and it is noted that the computational complexity of these estimates is comparable with the computational complexity of a special case of MINQUE demonstrated by Wansbeek (1980)."], ["Conditional Heteroscedastic Time Series Models", "Properties of the proposed general models are discussed, and a model specification procedure is suggested. The information criterion approach is adopted to specify the order of the transfer function equation, and simulation is used to illustrate the performance of the criteria, for example, Akaike's information criterion (AIC) and the Bayesian information criterion (BIC). Finally, two applications of the proposed model are considered. The first application uses the model as an alternative for handling outliers in time series analysis. An apparent advantage here is that the model does not require identification of the outliers, for example, location, type, and number of outliers. Series C of Box and Jenkins (1976) provides a good example. The second application of the proposed model is to refine the traditional ARMA technique so that volatility of a series can be adequately addressed. An advantage here is the ability to provide adaptive forecasting intervals. The weekly spot rate of the British pound provides a clear illustration. This example also gives justifications for the model generalization considered in the article."], ["Asymptotic Properties of Maximum Likelihood Estimators and Likelihood Ratio Tests under Nonstandard Conditions", "A variety of examples is provided for which the limiting distributions of likelihood ratio statistics are mixtures of chi-squared distributions. One example is provided with a nuisance parameter on the boundary for which the asymptotic distribution is not a mixture of chi-squared distributions."], ["Chi-Squared Tests for and against an Order Restriction on Multinomial Parameters", null], ["Order-Restricted Score Parameters in Association Models for Contingency Tables", "The row effects model discussed in this article has been proposed by Haberman (1974), Simon (1974), and Goodman (1979), among others. This model contains parameters for the rows in the contingency table that describe the structure of the association and can be used to describe dependence in corresponding logit models. This article deals with applications of the model in which there is a monotonic relationship between the variables, in the sense that the population values of the local log-odds ratios are uniformly nonnegative or uniformly nonpositive. For instance, one might expect a nonnegative relationship for the data analyzed by Haberman (1974) and by Goodman (1979) on mental health and socioeconomic status, and a nonpositive relationship for the data analyzed in Section 2 of this article on age and severity of disturbances in dreams. By using the methods described in this article, one can obtain monotone estimates of the association parameters, which imply a monotone relationship between the variables. With this approach, one obtains a simpler description of the relationship, and better estimates, when the parameter scores truly are ordered."], ["Goodness-of-Fit Tests for Large Sparse Multinomial Distributions", null], ["Small-Sample Comparisons of Level and Power for Simple Goodness-of-Fit Statistics under Cluster Sampling", null], ["Efficiency and Optimality Results for Tests Based on Weighted Rankings", "The Tukey\u2013Quade method of weighted rankings for the analysis of complete blocks is considered. Assuming only that the within-block error components are exchangeable, the limiting distribution of a large class of test statistics based on this method is obtained under the null hypothesis and under nearby alternatives as the number of blocks increases to infinity. This makes it possible to derive the asymptotic efficiency of the allied tests relative to the maximin most powerful test based on block-location-free statistics. In particular, the Quade test is seen to be more efficient than the Friedman test when there are less than eight observations per block and the errors are assumed to be normally distributed. It is also possible to determine which within-block scores, which measure of block variability, and which between-block score-generating function maximize the asymptotic efficiency. It is seen that the class of test statistics based on the method of weighted rankings compares favorably with the class of test statistics based on the method of unweighted rankings. The intrinsically optimal asymptotic efficiencies of both classes are studied in detail when the errors are normally distributed and when they have the extreme value distribution."], ["Sample Size Determination for Some Common Nonparametric Tests", "The article discusses the problem of determining the number of observations required by some common nonparametric tests, so that the tests have power at least 1 \u2013 \u03b2 against alternatives that differ sufficiently from the hypothesis being tested. It is shown that the number of observations depends on certain simple probabilities. A method is suggested for fixing the value of the appropriate probability when determining sample size."], ["Combined Rank Tests for the Two-Sample Problem with Randomly Censored Data", "In the two-sample problem under random censorship we consider the uncensored observations and the censored ones as two separate groups. For each group a suitable rank statistic is obtained, and these two are then combined to a final one. This idea, which was first employed in Akritas (1983), is shown here to produce tests that (a) closely resemble the ordinary rank tests for the uncensored case and do not require the calculation of the Kaplan-Meier estimator, (b) are comparatively easy to apply and to understand, and (c) allow results on asymptotic normality to follow simply from standard results for the uncensored case. It is shown that the loss due to using two separate rankings rather than one complete ranking is asymptotically negligible. The optimal score function for each of the two separate rank statistics is seen to depend on the censoring distribution. Whereas in Akritas (1983) no assumption on the form of the censoring distribution was made (unrestricted adaptation), here we pursue restricted adaptation that results in simple score functions. In particular, motivated by the model of Koziol and Green (1976), we assume that the censoring distribution is a suitable function of the survival distribution. An example related to the Wilcoxon test is used for illustration. The effect of restricted adaptation on the efficiency of the test is examined by both exact efficiency calculations and simulation results."], ["A Rank Correlation Coefficient Resistant to Outliers", null], ["A Continuous Bivariate Exponential Distribution", null], ["Book Reviews", null], ["Letters to the Editor", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["The Importance of Statisticians", null], ["Stochastic Blockmodels for Directed Graphs", null], ["Impact of Chlorofluoromethanes on Stratospheric Ozone", "Monthly averages of data on stratospheric total ozone and vertical distribution ozone profiles from a global collection of ground-based recording stations are analyzed for the detection of trend in ozone that may be associated with possible effects of the release of chlorofluoromethanes (CFM's). Regression time series models that include seasonal, trend, and other factors are estimated from the data at each individual station. A factor to explicitly account empirically for the effects of atmospheric aerosols due to volcanic activity on the ozone profile measurements is also included in the models. A random effects model for the individual station trend estimates is used to combine the individual trend estimates in the form of an overall global estimate of trend in ozone. Based on analysis of ozone data during the period 1960-1983, there is little evidence of any trend in total ozone. But a statistically significant negative trend of about \u2212.3% change per year for the period 1970-1981 has been obtained for profile ozone data at about the 40 km altitude region. These empirical findings are in reasonably good agreement with current theoretical photochemical model predictions of ozone change."], ["Markovian Forecast Processes", "A sequential forecast process arises when forecasts of a fixed but uncertain state are prepared with decreasing lead times, each subsequent forecast incorporating additional information and, therefore, updating the previous forecast. Bayesian Markov models of such a process are investigated for purposes of rational decision making. The Markov structure, although not universally valid, seems plausible, leads to tractable models, and implies sequential sufficiency of forecasts (roughly speaking, each subsequent forecast is less uncertain than the previous one). Sequential sufficiency, in turn, has implications on modeling Markov stopping-control processes. The normal model for point (categorical) forecasts is studied in detail."], ["Estimating a Common Relative Risk: Application in Equal Employment", null], ["A Model for Multinomial Response Error Applied to Labor Flows", "A model for the response error associated with reported categorical data is developed. The model is used to construct estimators for the interior cells of a two-way table with marginals subject to independent response error. The estimation procedure is applied to the month-to-month table of employment status obtained from the U.S. Current Population Survey."], ["Comparison of Purposive and Random Sampling Schemes for Estimating Capital Expenditure", "This article reports results of a large-scale study of various sampling strategies. Conventional sampling strategies are compared with model-based strategies on data from over 12,000 businesses included in the annual Manufacturing Census of the Australian Bureau of Statistics. The study has been designed to replicate the quarterly Survey of Capital Expenditure. The results show that for the given data a stratified sample consisting of units with the largest values of the auxiliary variable in each stratum and simple ratio estimation is by far the most efficient of the strategies considered. A meaningful estimate of sampling error can be derived."], ["Editing and Imputation for Quantitative Survey Data", null], ["Market Transactions and Hypothetical Demand Data: A Comparative Study", "Empirical demand studies have been based on data from (a) actual market transactions or (b) hypothetical questions. Many social scientists are skeptical of the accuracy of responses to hypothetical questions, yet few studies assess the quality of this type of data. This article directly compares the demand relations obtained from actual market transactions and hypothetical survey responses using primary field data and limited dependent variable regression analysis. Using a log-likelihood ratio test, the null hypothesis that the two demand relations are statistically identical cannot be rejected at the 1% level of significance."], ["Probabilistic Solution of Ill-Posed Problems in Computational Vision", "Computational vision is a set of inverse problems. We review standard regularization theory, discuss its limitations, and present new stochastic (in particular, Bayesian) methods for their solution. We derive efficient algorithms and describe parallel implementations on digital parallel SIMD architectures, as well as a new class of parallel hybrid computers."], ["Inference for Discrete Markov Fields: The Simplest Nontrivial Case", "Markov fields provide a general context for describing the strength and structure of spatial interactions. The Gibbs\u2014Markov equivalence theorem (Preston 1974) parameterizes Markov fields via their neighborhood structures, yielding exponential families in canonical form. Likelihood inference is, therefore, apparently straightforward. The requisite normalizing constants, however, are obstreperous. Even when asymptotic characterizations can be obtained, substantial location errors arise during implementation. Moreover, Markov fields can exhibit phase transitions and long-range interactions, thereby creating identifiability problems. These issues are illustrated in the simplest nontrivial case\u2014the classical Ising model of ferromagnetism."], ["Maximum Likelihood Computations with Repeated Measures: Application of the EM Algorithm", "The rate of convergence of the EM algorithm is generally linear. The actual speed of convergence in two data examples is shown to depend heavily on both the actual data set and the assumed structure for the covariance matrix. We discuss two methods for accelerating convergence, which we find are most useful when the covariance matrix is assumed to have a random effects structure. When the covariance matrix is assumed to be arbitrary, the EM iterations reduce to familiar iteratively reweighted least squares (IRLS) computations. The EM algorithm has the unusual property in this setting that when all of the data are complete (no missing observations), the iterations are still IRLS, but the rate of convergence changes from linear to quadratic."], ["Reconciling Bayesian and Frequentist Evidence in the One-Sided Testing Problem", null], [null, null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Rejoinder", null], ["Bayesian Models for Directed Graphs", null], ["Assessing the Accuracy of Normal Approximations", "The first method gives the smallest approximate likelihood region that contains the exact likelihood region and the largest approximate likelihood region contained in the exact region."], ["Calibrating Confidence Coefficients", "The second approach is more traditional. It is a procedure for modifying an interval to yield improved coverage properties. Given a confidence interval, its estimated coverage probability obtained in the first approach is used to alter the nominal level of the interval. The interval with this modified nominal level is called a calibrated interval. In the case that the given interval is the normal-theory interval for the estimation of variance, the calibrated interval is proved to be asymptotically robust as long as sixth moments exist. As another application, the method is used to modify a bootstrap interval procedure for variance estimation. This leads to the derivation of a new bootstrap interval."], ["Bootstrap Confidence Intervals and Bootstrap Approximations", null], ["Better Bootstrap Confidence Intervals", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Outer and Inner Confidence Intervals for Finite Population Quantile Intervals", null], ["A Fast Model Selection Procedure for Large Families of Models", "An efficient procedure for model selection from large families of models is described. It is closely related to the all possible models approach but is considerably faster. It is based on two principles: first, if a model is accepted, then all models that include it are considered to be accepted; second, if a model is rejected, then all of its submodels are considered to be rejected. Application of the procedure to variable selection in multiple regression is illustrated. General algorithms are described that enable the procedure to be applied to any family of models that forms a lattice. As an example, a problem in multiple comparisons is considered."], ["Simultaneous Confidence Bounds in Multiple Regression Using Predictor Variable Constraints", "This article presents a method for approximating the coverage probability of simultaneous confidence bounds for multiple regression functions when the vector of predictor variables is constrained to lie in a polyhedral convex set. The method is useful because it allows one to construct simultaneous confidence intervals with prescribed coverage probability for the regression function evaluated at various settings of the predictor variables, which are narrower than bounds obtained without using the predictor variable constraints. For a family of two-sided simultaneous confidence bounds that includes Scheff\u00e9-type and constant-width bounds, the probability of coverage is related to the distribution of the maximum Euclidean norm of the projection onto a polyhedral cone for a pair of random vectors with known joint distribution. An analogous relation holds for one-sided bounds. If an algorithm for computing the projection onto the cone is available, then these results enable one to use the Monte Carlo method to approximate the coverage probabilities of bounds. Particular attention is given to the case in which lower and upper bounds can be specified for each of the predictor variables so that the constraint region for bounding the regression function is rectangular. An efficient algorithm for calculating projections onto the appropriate cone for rectangular constraint regions, which facilitates coverage probability approximation of Schefff\u00e9-type bounds, is presented. This algorithm is used to calculate approximate critical points of Scheff\u00e9-type bounds for some specific designs and constraint regions and we show that substantial improvements for one-sided bounds and modest improvements for two-sided bounds over the conservative bounds of Casella and Strawderman (1980) may be obtained."], ["A Minimax Property of Linear Regression", null], ["Approximate Confidence Limits for a Parameter Function in Nonlinear Regression", null], ["Weighted Local Regression and Kernel Methods for Nonparametric Curve Fitting", "Weighted local regression, a popular technique for smoothing scatterplots, is shown to be asymptotically equivalent to certain kernel smoothers. Since both methods are local weighted averages of the data, it is proved that in the fixed design regression model, given a weighted local regression procedure with any weight function, there is a corresponding kernel method such that the quotients of weights distributed by both methods tend uniformly to 1 as the number of observations increases to infinity. It is demonstrated by examples that in some instances the weights are nearly the same for both methods, even for small samples. The asymptotic equivalence allows the derivation of the leading terms of the mean squared error and of the local limit distribution for weighted local regression. Further, a close correspondence is found between the orders of the polynomial to be locally fitted in weighted local regression and the order (number of vanishing moments) of the kernel employed in the kernel smoother and between the shapes of the kernel and the weight function."], ["Inequality-Constrained Multivariate Smoothing Splines with Application to the Estimation of Posterior Probabilities", null], ["Exploratory Projection Pursuit", "A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods. A number of practical issues concerning its application are addressed. A connection to multivariate density estimation is established, and its properties are investigated through simulation studies and application to real data. The goal of exploratory projection pursuit is to use the data to find low- (one-, two-, or three-) dimensional projections that provide the most revealing views of the full-dimensional data. With these views the human gift for pattern recognition can be applied to help discover effects that may not have been anticipated in advance. Since linear effects are directly captured by the covariance structure of the variable pairs (which are straightforward to estimate) the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables. Although arbitrary nonlinear effects are impossible to parameterize in full generality, they are easily recognized when presented in a low-dimensional visual representation of the data density. Projection pursuit assigns a numerical index to every projection that is a functional of the projected data density. The intent of this index is to capture the degree of nonlinear structuring present in the projected distribution. The pursuit consists of maximizing this index with respect to the parameters defining the projection. Since it is unlikely that there is only one interesting view of a multivariate data set, this procedure is iterated to find further revealing projections. After each maximizing projection has been found, a transformation is applied to the data that removes the structure present in the solution projection while preserving the multivariate structure that is not captured by it. The projection pursuit algorithm is then applied to these transformed data to find additional views that may yield further insight. This projection pursuit algorithm has potential advantages over other dimensionality reduction methods that are commonly used for data exploration. It focuses directly on the \u201cinterestingness\u201d of a projection rather than indirectly through the interpoint distances. This allows it to be unaffected by the scale and (linear) correlational structure of the data, helping it to overcome the \u201ccurse of dimensionality\u201d that tends to plague methods based on multidimensional scaling, parametric mapping, cluster analysis, and principal components."], ["Estimation of a Convex Density Contour in Two Dimensions", null], ["Simultaneous Confidence Regions for the Frequency Analysis of Multiple Time Series", null], ["Estimating Trend and Growth Rates in Seasonal Time Series", "We consider the problem of trend estimation from an ARIMA-model-based perspective. Given that the time series of interest is well represented by a multiplicative seasonal ARIMA model (Box and Jenkins 1970), a method is presented for estimating the trend of this series as a component of the model's forecast function. This method is applied to the \u201cairline model,\u201d a commonly occurring model form for economic and social time series. For those numerous series that are rendered stationary by logging and first-differencing, the resulting quantity supplies an estimate of the rate of change, or growth rate, of the series. As this trend estimate is given in terms of the underlying series' forecast function, we analyze forecast functions of ARIMA models in general and of the airline model in particular, extending the development in Box and Jenkins (1970, chaps. 5 and 9). Representing the forecast function as the solution of a difference equation, the trend estimate is the component of the forecast function that is a linear combination of those roots of the observable series' autoregressive operator that are associated with trend (an allocation that is usually standard and unambigous in practice). Thus it is important to determine the coefficients in this linear combination, which are adaptive in the time origin of the forecast, and we describe three general methods for doing this: in terms of initial forecasts, directly from the present and recent values of the observed series, and recursively. For the airline model the trend estimate is a straight line, with both intercept and slope changing in each time period, adapting to the new information becoming available. We also connect this trend estimate with trend forecasts in unobserved-components ARIMA models, which underlie much seasonal adjustment research of the last decade. Trend components in such models are not unique, even given the association of certain of the autoregressive roots with the trend. We show, however, that all admissible trend-seasonal-irregular decompositions have identical trend-component forecasts and that this common forecast is supplied by our trend estimate. Thus the \u201ctrue trend\u201d (that which our trend estimate estimates) is given only with reference to a particular decomposition. One such decomposition, commonly used in model-based seasonal adjustment, is the one (whose existence and uniqueness are known under fairly general conditions) that maximizes the irregular-component variance or, equivalently, minimizes the trend- and seasonal-component innovation variances. We show that, among all admissible decompositions, this \u201ccanonical decomposition\u201d minimizes the mean squared error of our trend estimate."], ["Chi-Squared-Type Tests for Ordered Alternatives in Contingency Tables", null], [null, "How to adjust when covariates are present is discussed. Also discussed is the particular case of testing for umbrella alternatives, as are inherent differences with a competing procedure of Mack and Wolfe (1981)."], ["Minimax Estimation of the Mixing Proportion of Two Known Distributions", null], ["Nonparametric Estimation of the Probability of Discovering a New Species", null], ["Supremum Versions of the Log-Rank and Generalized Wilcoxon Statistics", "Considerable progress has been made in generalizing to censored data hypothesis tests based on rank statistics. The most commonly used generalizations are the log-rank test statistic, which extends the Savage exponential scores statistic (Cox 1972; Mantel 1966) and the generalized Wilcoxon statistics (Gehan 1965; Peto and Peto 1972). Gill (1980) showed that in the two-sample problem the asymptotic efficiency properties of the Savage and Wilcoxon statistics are maintained in censored data by the log-rank and Peto and Peto Wilcoxon statistics, respectively, when censoring is the same in both samples. Heuristics and simulations have implied that some recently proposed supremum statistics, closely related to these two popular linear rank statistics, can be used to construct tests that provide good power against a much wider range of nonlocal alternatives (see, e.g., Fleming and Harrington 1981; Fleming, O'Brien, O'Fallon, and Harrington 1980; Gill 1980; Schumacher 1984). In this article we consider new and previously discussed classes of rank statistics that are related to the log-rank and generalized Wilcoxon tests. We focus on linear rank statistics and their Renyi-type supremum versions. These classes are either of the \u201casymptotically fully efficient\u201d type or the \u201capproximately distribution-free\u201d type discussed by Leurgans (1983). The large sample distributions of these statistics are established for the general setting in which ties can be present in the data. By restricting attention to the general random censorship model, we obtain the asymptotic results by stating and applying a simplified version of Gill's (1980) limit theorems. We examine more thoroughly than in previous literature the relative operating characteristics of these newly and previously proposed censored data rank statistics. Important insights are obtained from simulations performed for small and moderate sample sizes, in equal and unequal sample sizes, and across a range of survival differences and degrees of censorship. For example, results reveal that supremum versions of the log-rank statistic are nearly as sensitive to proportional-hazards alternatives as the efficient log-rank test. In addition, these supremum versions provide greater sensitivity across a wide range of nonproportional-hazards configurations. This increase in power from the supremum statistics is less evident, however, when data are so heavily censored that one can only estimate early survival differences. Another important insight is obtained from the comparison in small samples of asymptotically fully efficient versus approximately distribution-free statistics. Although the former class has received much greater attention, the latter has the desirable property that the types of survival differences detected with highest power are less affected by degree of censorship. In certain circumstances, it would be appealing to apply simultaneously a linear rank statistic and its supremum version. Distributional results are obtained to show how this can be done appropriately and in a very straightforward manner."], ["Introductory Textbooks: A Framework for Evaluation", null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"]]}