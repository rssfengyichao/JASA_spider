{"1992": [["On the Quality of Reinterview Data with Application to the Current Population Survey", "The Current Population Survey (CPS) reinterview sample consists of two subsamples: (a) a sample of CPS households is reinterviewed and the discrepancies between the reinterview responses and the original interview responses are reconciled for the purpose of obtaining more accurate responses (i.e., response bias estimates), and (b) a sample of CPS households, nonoverlapping with sample (a), is reinterviewed \u201cindependently\u201d of the original interview for the purpose of estimating simple response variance (SRV). In this article a model and estimation procedure are proposed for obtaining estimates of SRV from subsample (a) as well as the customary estimates of SRV from subsample (b). In this way, an improved estimator of SRV that combines data from both subsamples can be computed. Additionally, under conditions that are usually satisfied in practice, several inequalities involving statistics computed from both subsamples are derived. These inequalities can be used to check the validity of the reinterview assumptions and the quality of the estimates of SRV and response bias from the reinterview program. Data from the CPS reinterview program for both subsamples (a) and (b) are analyzed both (1) to illustrate the methodology and (2) to check the validity of the CPS reinterview data. Our results indicate that data from subsample (a) are not consistent with the data from subsample (b) and provide convincing evidence that errors in subsample (a) are the source of the inconsistency."], ["Estimation Using Multiyear Rotation Design Sampling in Agricultural Surveys", "In the annual June Enumerative Surveys (JES) of the U.S. Department of Agriculture (USDA), the area frame sampling involves multiyear rotation designs with 20% replacement of sample units each year. Currently, USDA uses the latest year sample data almost exclusively in its estimation procedure. We propose that the multiyear sample survey data should be used for estimation of crop acreages and livestock. We develop a multiyear estimation method based on an analysis of variance model that takes into account the successive sampling of units in the area frame from year to year. The proposed method is applied to estimate the 1989 hogs and soybean acreages using JES data for three years: 1987, 1988, and 1989. These estimates are compared with those obtained using the current USDA estimation method. We give relative efficiencies of the multiyear estimates compared to the single-year estimates, often showing a substantial improvement in precision. We also make an evaluation study of the proposed estimation method using simulations and show it to be fairly robust to misspecification of the model parameter."], ["The Chilean Plebiscite: Projections without Historic Data", "On October 5, 1988, Chileans decided by plebiscite to oust General Pinochet from power and have free presidential elections in 1989. This article describes the projections that the authors made for the results of the plebiscite from early returns. From a statistical point of view, what made these projections different from those made in other countries was the complete lack of historic data. Furthermore, the Pinochet government carried out a campaign to discredit the projection effort. Uncertainty about both the data and the unpredictable political climate on the night of the plebiscite influenced the choice of the statistical methodology. The predictions, based on a 10% sample of the first one-third of the votes counted, were within one-half a percentage point of the true outcome. The described methodology could prove useful in projections of other elections that will take place under similar conditions (e.g., in Eastern Europe)."], ["Flexible Methods for Analyzing Survival Data Using Splines, with Applications to Breast Cancer Prognosis", "In this article some flexible methods for modeling censored survival data using splines are applied to the problem of modeling the time to recurrence of breast cancer patients. The basic idea is to use fixed knot splines with a fairly modest number of knots to model aspects of the data, and then to use penalized partial likelihood to estimate the parameters of the model. Test statistics are proposed which are analogs of those used in traditional likelihood analysis, and approximations to the distributions of these statistics are suggested. In an analysis of a large data set taken from clinical trials conducted by the Eastern Cooperative Oncology Group, these methods are seen to give useful insight into how prognosis varies as a function of continuous covariates, and also into how covariate effects change with follow-up time."], ["Secondary Data Analysis when there are Missing Observations", "A data set having missing observations often is completed by using imputed values. Our objective is to improve the practice of secondary data analysis by looking at the interplay of different imputation techniques and different methods that secondary data analysts use when there are both observed and imputed values. Secondary data analysts typically either treat the completed data set as if it has only observed values or ignore the imputations and analyze only the observed values. The first objective of our research is to investigate the effect on the properties of standard statistical techniques of proceeding in these ways. We assume that the missing data cannot be regarded as missing at random (MAR), and that the secondary data analyst's objectives are confidence intervals for the regression coefficients in a simple linear regression. Standard, \u201cgeneral purpose\u201d imputation methods are emphasized. The second objective is to investigate the performance of confidence intervals based on multiple imputations. Obtaining moments of statistics requires averaging using a weighted distribution. Because analytical results typically cannot be obtained, we show how to obtain lower and upper bounds that can be computed easily. We also present a simple parametric function for the probability of response given variables of interest, and validate it using data from the 1987 Economic Censuses. We also summarize our findings and make recommendations to the organizations providing the imputations. Finally, we delineate the options available to secondary data analysts."], ["Estimating the Size-selectivity of Fishing Gear by Conditioning on the Total Catch", null], ["Estimating a Multivariate Proportional Hazards Model for Clustered Data Using the EM Algorithm, with an Application to Child Survival in Guatemala", "This article discusses a random-effects model for the analysis of clustered survival times, such as those reflecting the mortality experience of children in the same family. We describe parametric and nonparametric approaches to the specification of the random effect and show how the model may be fitted using an accelerated EM algorithm. We then fit two specifications of the model to child survival data from Guatemala. These data had been analyzed before using standard hazard models that ignore cluster effects."], ["Age Patterns of Marital Fertility: Revising the Coale\u2013Trussell Method", null], ["Leverage and Superleverage in Nonlinear Regression", "Several measures of the leverage of an observation in a nonlinear regression model are defined and developed. In contrast to the upper bound on the leverage in a linear model, it is found that in a nonlinear model the leverage of an observation may exceed 1. Such a case is said to exhibit superleverage. Relationships between the leverage measures are explored, and several examples are developed to illustrate the proposed methodology."], ["Breakdown in Nonlinear Regression", null], ["Design-adaptive Nonparametric Regression", "In this article we study the method of nonparametric regression based on a weighted local linear regression. This method has advantages over other popular kernel methods. Moreover, such a regression procedure has the ability of design adaptation: It adapts to both random and fixed designs, to both highly clustered and nearly uniform designs, and even to both interior and boundary points. It is shown that the local linear regression smoothers have high asymptotic efficiency (i.e., can be 100% with a suitable choice of kernel and bandwidth) among all possible linear smoothers, including those produced by kernel, orthogonal series, and spline methods. The finite sample property of the local linear regression smoother is illustrated via simulation studies. Nonparametric regression is frequently used to explore the association between covariates and responses. There are many versions of kernel regression smoothers. Some estimators are not good for random designs, such as in observational studies, and others are not good for nonequispaced designs. Furthermore, most nonparametric regression smoothers have \u201cboundary effects\u201d and require modifications at boundary points. However, the local linear regression smoothers do not share these disadvantages. They adapt to almost all regression settings and do not require any modifications even at boundary. Besides, this method has higher efficiency than other traditional nonparametric regression methods."], ["Preaveraged Localized Orthogonal Polynomial Estimators for Surface Smoothing and Partial Differentiation", null], ["Kernel Regression When the Boundary Region is Large, with an Application to Testing the Adequacy of Polynomial Models", "It is well known that kernel regression estimators are subject to so-called boundary or edge effects, a phenomenon in which the bias of an estimator increases near the endpoints of the estimation interval. When the regression curve is linear or nearly linear, the requisite amount of smoothing is so great that the boundary region is effectively the entire estimation interval. Special boundary kernels are proposed here to deal with such cases. It is shown that the proposed kernel estimator has a property also enjoyed by cubic smoothing splines; namely, as the estimator's smoothing parameter becomes large, the estimator tends to a straight line. The limiting straight line is essentially the least squares line when the design points are equally spaced. A simple generalization of ideas in the linear case leads to kernel estimates that are polynomials of any given degree for large bandwidths. Such estimates are an important component of a proposed test for the adequacy of a polynomial model. The test statistic is the bandwidth chosen to minimize an estimated risk function. An example illustrates the usefulness of the new boundary kernels."], ["On Principal Hessian Directions for Data Visualization and Dimension Reduction: Another Application of Stein's Lemma", "Modern graphical tools have enhanced our ability to learn many things from data directly. With much user-friendly graphical software available, we are encouraged to plot a lot more often than before. The benefits from direct interaction with graphics have been enormous. But trailing behind these high-tech advances is the issue of appropriate guidance on what to plot. There are too many directions to project a high-dimensional data set and unguided plotting can be time-consuming and fruitless. In a recent article, Li set up a statistical framework for study on this issue, based on a notion of effective dimension reduction (edr) directions. They are the directions to project a high dimensional input variable for the purpose of effectively viewing and studying its relationship with an output variable. A methodology, sliced inverse regression, was introduced and shown to be useful in finding edr directions. This article introduces another method for finding edr directions. It begins with the observation that the eigenvectors for the Hessian matrices of the regression function are helpful in the study of the shape of the regression surface. A notation of principal Hessian directions (pHd's) is defined that locates the main axes along which the regression surface shows the largest curvatures in an aggregate sense. We show that pHd's can be used to find edr directions. We further use the celebrated Stein lemma for suggesting estimates. The sampling properties of the estimated pHd's are obtained. A significance test is derived for suggesting the genuineness of a view found by our method. Some versions for implementing this method are discussed, and simulation results and an application to real data are reported. The relationship of this method with exploratory projection pursuit is also discussed."], ["Measurement Error Regression with Unknown Link: Dimension Reduction and Data Visualization", null], ["Diagnostics for Nonparametric Regression Models with Additive Terms", "Recent developments of multivariate smoothing methods provide a rich collection of feasible models for nonparametric multivariate data analysis. Among the most interpretable are models with additive terms. Construction of various models and algorithms for computing the models have been the main concern of the existing literature in this area. Few results are available on the validation of computed fits, and many applications of nonparametric methods unfortunately end up interpreting the noise. This article proposes and illustrates some simple retrospective diagnostics to help data analysts in detecting possible aliasing effects in computed nonparametric fits and in building parsimonious models in an interactive fashion. It also discusses the concepts and rationale behind the proposal, including concurvity, diagnostics versus tests, and so forth. For their ready availability, interaction splines are used in the illustrations."], ["Diagnostics for a Cumulative Multinomial Generalized Linear Model, with Applications to Grouped Toxicological Mortality Data", "Toxicologists frequently conduct toxicity experiments in which different treatment conditions are applied to groups of animals and the resulting mortality in each group is measured at a number of discrete time points over the course of the experiment. In this article, we develop and extend a number of diagnostic tools for the detection of mean misspecification, or systematic departures of the mean-link specification, in cumulative multinomial generalized linear models fit to such data. Several real data sets are used to illustrate these diagnostics. These tools help the analyst to differentiate between two sources of lack of fit in such models: mean misspecification and extra-multinomial variation or overdispersion."], ["Frequency Domain Diagnostics for Linear Smoothers", "Frequency domain analysis is used to examine estimates from linear smoothers operating on realizations of random fields over space and/or time. The estimates are expressed in terms of the Fourier transforms of the dependent variable and of the smoother weights. The latter is referred to as the equivalent transfer function. The data do not need to be evenly spaced to perform this analysis. The modulus of the equivalent transfer function characterizes the spectral content of an estimate and may reveal subtle sampling properties of the design. Frequency domain bias calculations are useful for comparing different smoothers and for assessing the resolution capabilities of a data set. These methods are used to compare six one-dimensional smoothers and analyze a complex three-dimensional example using data from a satellite altimeter."], ["Testing Causality between Two Vectors in Multivariate Autoregressive Moving Average Models", "In the analysis of economic time series, a question often raised is whether a vector of variables causes another one in the sense of Granger. Most of the literature on this topic is concerned with bivariate relationships or uses finite-order autoregressive specifications. The purpose of this article is to develop a causality analysis in the sense of Granger for general vector autoregressive moving average (ARMA) models. We give a definition of Granger noncausality between vectors, which is a natural and simple extension of the notion of Granger noncausality between two variables. In our context, this definition is shown to be equivalent to a more complex definition proposed by Tjostheim. For the class of linear invertible processes, we derive a necessary and sufficient condition for noncausality between two vectors of variables when the latter do not necessarily include all the variables considered in the analysis. This result is then specialized to the class of stationary invertible ARMA processes. Further, relatively simple necessary and sufficient conditions are obtained for two important cases: (1) the case where the two vectors reduce to two variables inside a larger vector including other variables; and (2) the case where the two vectors embody all the variables considered. Test procedures for these necessary and sufficient conditions are discussed. Among other things, it is noted that the necessary and sufficient conditions for noncausality may involve singularities at which standard asymptotic regularity conditions do not hold. To deal with such situations, we propose a sequential approach that leads to bounds tests. Finally, the tests suggested are applied to Canadian money and income data. The tests are based on bivariate and trivariate models of changes in nominal income and two money stocks (M1 and M2). In contrast with the evidence based on bivariate models, we find from the trivariate model that money causes income unidirectionally."], ["Linear Regression Analysis for Multivariate Failure Time Observations", "In this article we consider the case that each patient in a longitudinal study may experience two or more distinct failures. The corresponding failure times, which are possible censored, are recorded for each patient. The logarithm of each marginal failure time is assumed to be linearly related to its covariates; however, the distributional form of the error term in the model does not have to be specified in the analysis. Furthermore, no specific structure of dependence among the distinct failure times on each subject has to be imposed. Various linear regression methods for analyzing multivariate failure time observations are proposed. Our procedures do not involve the unstable nonparametric hazard function estimation. Extensive numerical studies are conducted to evaluate the new proposals. Recommendations are also made for their practical usage."], ["Propagation of Probabilities, Means, and Variances in Mixed Graphical Association Models", null], ["Parameter Updating in a Bayes Network", "A Bayes network is a directed acyclic graph in which the links are quantified by fixed conditional probabilities and the nodes represent random variables. The primary use of the network is to provide an efficient method for updating conditional probabilities in the graph. We consider the consequences of using the network as the computational device for updating parameter estimates in the dynamic linear model, a discrete time Bayesian model. We show that using the network characterizes the dynamic linear model and its computations in a unified way. The generality of the network permits nonsequential data collection and thereby provides a straightforward method of incorporating delayed data. An on-line diagnostic is offered to complement the conventional forecast error and an approximation to the posterior distribution is proposed. Algorithms for data propagation in multivariate Gaussian causal trees are presented."], ["De Finetti-type Representations for Life Distributions", null], ["Conjugate Priors for Exponential Families Having Quadratic Variance Functions", null], ["Ranking and Estimation of Related Means in the Presence of a Covariate\u2014A Bayesian Approach", "Choosing the largest of several means can be a demanding problem, especially in the presence of a covariate. A hierarchical Bayesian approach to ranking and selection, as well as estimation of related means in the presence of a covariate, is considered. For the multiple slopes model we compute, in addition to the posterior means and standard deviations of the parameters, the posterior probabilities that each mean, at a given value of the covariate, is the largest. The vector of posterior probabilities thus obtained provides an easily understandable answer to the selection problem. Although calculation of the posterior probabilities may involve four-dimensional numerical integration in the difficult unbalanced design and unknown variance case, an efficient Monte Carlo method of evaluation has been developed and is given in the article. By reanalyzing a well-known data set on the breaking strength and thickness of starch films, we illustrate how our Bayesian approach produces meaningful conclusions, some of which would perhaps be difficult to obtain otherwise. For the starch film example, we found that it took only 1.4 seconds to compute the quantities of interest using an IBM 3090\u2013600S machine. Because the computation time is quite small, it is apparent that the Bayesian procedure can be implemented for everyday use."], ["Improved Tests for Comparing Treatments against a Control and other One-Sided Problems", null], ["Normal Goodness-of-Fit Tests for Multinomial Models with Large Degrees of Freedom", null], ["The Optimal Size of a Preliminary Test of Linear Restrictions in a Misspecified Regression Model", "When the choice of estimator for the coefficients in a linear regression model is determined by the outcome of a prior test of the validity of restrictions on the model, it is well known that a minimax (risk) regret criterion leads to the simple rule that the optimal critical value for the preliminary test is approximately two in value, regardless of the degrees of freedom. We show that this result no longer holds in the (likely) event that relevant regressors are excluded from the model at the outset."], ["Multidimensional Designs for Two-Factor Experiments", null], ["Conditionally Consistent Estimators Using Only Probabilities of Selection in Complex Sample Surveys", null], ["Outlier Resistant Alternatives to the Ratio Estimator", null], ["Characterizing Linear Birth and Death Processes", null], ["Information Ratios for Validating Mixture Analyses", null], ["The Use of Names for Linking Personal Records", "The skill of a human who searches large files of personal records depends much on prior knowledge of how the names vary in successive documents pertaining to the same individuals (e.g., as with ANTHONY\u2013TONY, JOSEPH\u2013JOE, WILLIAM\u2013BILL). Now, an essentially exact procedure enables computers to make similar use of an accumulated memory of their own past experiences when searching for, and linking, records that relate to particular persons. This knowledge is further applied to quantify the benefits from various refinements of the rules by which the discriminating powers of names are calculated when they do not precisely agree or are substantially dissimilar. Of the six refinements tested, by far the most important is the recently developed exact approach for calculating the ODDS associated with comparisons of names that are possible synonyms."], ["Comment", null], ["Rejoinder", null], ["Power Calculations for General Linear Multivariate Models Including Repeated Measures Applications", "We conclude that power analysis catalyzes the interaction of statisticians and subject matter specialists. Using the recent advances for power analysis in linear models can further invigorate the interaction."], [null, null], ["Book Reviews", null], ["Publications Received", null], ["Correction", null], ["Editorial Board Page", "This article has no abstract"], ["The Bayes/Non-Bayes Compromise: A Brief Review", null], ["Some Statistical Issues in Medicine and Forensics", "We discuss and comment on the use of statistics in a number of topical issues in forensics and medicine. The forensic issues are scientific misconduct, DNA pattern matching, and causal analysis. In the biomedical area, particular statistical approaches are suggested for problems of mass screening, interim analysis, and the regulation of chronic diseases."], ["Hierarchical Bayes Models for the Progression of HIV Infection Using Longitudinal CD4 T-Cell Numbers", "Taking the absolute number of CD4 T-cells as a marker of disease progression for persons infected with the human immunodeficiency virus (HIV), we model longitudinal series of such counts for a sample of 327 subjects in the San Francisco Men's Health Study (Waves 1\u20138, excluding zidovudine cases). We conduct a fully Bayesian analysis of these data. We employ individual level nonlinear models incorporating such critical features as incomplete and unbalanced data, population covariates (age at study entry and an indicator of self-reported herpes simplex virus infection), unobserved random change points, heterogeneous variances, and errors in variables. We construct prior distributions using results of previously published work from several different sources and data from HIV-negative men in this study. We also develop an approach to Bayesian model choice and individual prediction. Our analysis provides marginal posterior distributions for all population parameters in our model for this cohort. Using an inverse prediction approach, we also develop the posterior distributions of time for CD4 T-cell number to reach a specified level."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Pediatric AIDS in New York City: Estimating the Distributions of Infection, Latency, and Reporting Delay and Projecting Future Incidence", "Maternal-to-infant transmission of the human immunodeficiency virus (HI V) accounts for most cases of acquired immune deficiency syndrome (AIDS) in children. In New York City more than 500 cases of AIDS attributed to such transmission were reported before the end of 1989; this accounts for 80% of the cases of pediatric AIDS in the United States. In addition to surveillance of AIDS cases, the New York State Health Department instituted a program to screen the blood of newborns for presence of HIV antibodies in December 1987. Such antibodies imply that the mother, although not necessarily the child, is infected with HIV. The new methods we propose in this article allow us to combine the two types of information to estimate the risk of AIDS for the first 10 years of life. They also make it possible to estimate the number of children born to infected mothers since the start of the epidemic and to project the future number of cases of pediatric AIDS. From these projections it appears that the case load and the average age at diagnosis will increase steadily throughout the early 1990s. The statistical problem that we address concerns making inferences about processes of infection, disease, and reporting for which realizations are right- (and perhaps left-) truncated. Further complication arises from the fact that the reporting delay distribution appears to be changing over chronologic time."], ["Time-Dependent Association Measures for Bivariate Survival Distributions", "We propose time-dependent association measures for application to bivariate survival analysis. Such association measures provide informative summaries for data on twins, ophthalmic and auditory studies, and for other matched-pair designs. We develop several desirable properties of time-dependent association measures and study three measures motivated by these properties. We examine the measures from a general bivariate survival perspective and for the proportional hazards frailty model. We use monozygotic (MZ) and dizygotic (DZ) twin data from the Danish Twin Registry to illustrate how these measures depend on the specification of the proportional hazards frailty model. This model consists of two components: a baseline hazard function and a frailty distribution. We produce gamma and nonparametric maximum likelihood estimates of the frailty distribution and estimate a Gompertz baseline hazard function. For two of the measures, a nonparametric estimate provides a comparison to the model-based estimates. As expected, the MZ twins display greater association at all ages, but the association measures give different insights into the association structure."], ["Bivariate Latent Variable Models for Clustered Discrete and Continuous Outcomes", "We use the concept of a latent variable to derive the joint distribution of a continuous and a discrete outcome, and then extend the model to allow for clustered data. The model can be parameterized in a way that allows one to write the joint distribution as a product of a standard random effects model for the continuous variable and a correlated probit model for the discrete variable. This factorization suggests a convenient approach to parameter estimation using quasi-likelihood techniques. Our approach is motivated by the analysis of developmental toxicity experiments for which a number of discrete and continuous outcomes are measured on offspring clustered within litters. Fetal weight and malformation data illustrate the results."], ["Modeling and Forecasting U.S. Mortality", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["A Feasible Bayesian Estimator of Quantiles for Projectile Accuracy from Non-iid Data", "An important measure of accuracy for problems of directing projectiles at targets is the circular error probable (CEP), a bivariate version of a 50% quantile point. This article presents a Bayesian procedure for estimating CEP when the projectile impact measurements are not iid, which is the case of usual practical interest. Our interest in a Bayesian procedure is motivated by a desire to combine accuracy information from several different sources. Except for the simplest problem settings, however, it is not possible to compute the standard Bayesian conditional mean estimate due to the associated computationally infeasible high-dimensional integrals. Thus we present an estimator that is closely related to the conditional mean (based on asymptotic theory and empirical experience) but is computationally feasible in all settings of practical interest. We demonstrate the procedure on a problem in missile accuracy analysis. The article also includes some comments on the potential application of several other Bayesian techniques\u2014namely the Laplace and Gibbs sampling integration methods\u2014in the CEP estimation problem, as well as some comments on how our technique could apply in certain other (i.e., non-CEP) high-dimensional estimation problems."], ["Estimating the Lyapunov Exponent of a Chaotic System with Nonparametric Regression", null], ["The Importance of Assessing Measurement Reliability in Multivariate Regression", null], ["Estimation of within Model Parameters in Regression Models with a Nested Error Structure", "Restricted randomizations, similar to those in split-plot type experiments, often are adapted to assign quantitative treatment factors to experimental units. Such restrictions result in the experiment having a nested error structure. Sufficient conditions are presented under which ordinary least squares (OLS) estimates of regressor parameters are uniformly minimum variance unbiased (UMVU). If one designs experiments so that these conditions are satisfied, the analysis is straightforward and easy. When these conditions are not met, three different estimators of nested regressor parameters are suggested and compared."], ["Identifiability in Multivariate Dynamic Linear Errors-in-Variables Models", null], ["Mean Squared Error of Estimation or Prediction under a General Linear Model", "The problem considered is that of predicting a linear combination of the fixed and random effects of a mixed-effects linear model. More generally, the problem considered is that of predicting an unobservable random variable from a set of observable random variables. The best linear-unbiased predictor depends on parameters which generally are unknown. Various exact or approximate expressions are given for the mean squared error (MSE) of the predictor obtained by replacing the unknown parameters with estimates. Several estimators of the MSE are investigated."], ["On the Distributional Properties of Model Selection Criteria", "It is commonly accepted that statistical modeling should follow the parsimony principle; namely, that simple models should be given priority whenever possible. But little quantitative knowledge is known concerning the amount of penalty (for complexity) regarded as allowable. We try to understand the parsimony principle in the context of model selection. In particular, the generalized final prediction error criterion is considered, and we argue that the penalty term should be chosen between 1.5 and 5 for most practical situations. Applying our results to the cross-validation criterion, we obtain insights into how the partition of data should be done. We also discuss the small sample performance of our methods."], ["The Little Bootstrap and other Methods for Dimensionality Selection in Regression: X-Fixed Prediction Error", null], ["A Resampling Procedure for Complex Survey Data", null], ["Testing the Rank and Definiteness of Estimated Matrices with Applications to Factor, State-Space and ARMA Models", null], ["ARMA Covariance Structures with Time Heteroscedasticity for Repeated Measures Experiments", "Rochon and Helms (1989) presented a model for analyzing repeated measures experiments. The general linear model was used to assess the influence of covariate information, and ARMA time series models were put forward to characterize the covariance matrix among the repeated measures. Practical experience has suggested, however, that the ARMA assumption of constant variances and autocovariances over time is too restrictive for many applications. For example, observations may be relatively stable toward the beginning of the study but become more variable toward the end. This article presents a modification to this structure, which provides for heteroscedasticity over time. Maximum likelihood (ML) estimation procedures are considered, and the estimators are found to enjoy optimal large sample properties. A scoring algorithm is described for iterating to a solution of the ML equations. The model is illustrated with data from a clinical trial investigating human erythropoietin for treating anemia in end-stage renal disease."], ["Residual Diagnostics for Mixture Models", null], ["Diagnostics for Overdispersion", "Diagnostic tools are proposed for assessing the dependence of extrabinomial or extra-Poisson variation on explanatory variables and for comparing several common models for overdispersion. These tools are based on tests for regression terms in the dispersion parameter of a generalized linear model, using double exponential family and \u201cpseudolikelihood\u201d formulations. Score tests do not require the full fitting of models for variation and lead to easy graphical and numerical procedures based on squared residuals (deviance or Pearson). Robust modifications of these are motivated by Levene's test in linear models. The diagnostic tools are intended primarily to ensure prudent modeling of the variance to make correct inferences about parameters in the mean."], ["Existence and Uniqueness of the Maximum Likelihood Estimator for a Multivariate Probit Model", "The multivariate probit model (MPM) is a particular case of the class of correlated prediction models. A correlated prediction model is especially useful when prediction or classification is envisaged into diagnostic classes that are combinations of binary responses. The parameter vector consists of a \u201clocation\u201d part and an \u201cassociation\u201d part. The location part accounts for the effect the regressors have on the marginal probabilities of the binary responses. The association part corrects these probabilities, taking into account that the responses are related. This article investigates conditions for the existence and unicity of the maximum likelihood estimator (MLE) of the parameter vector. It turns out that the existence and uniqueness of the MLE for the location parameters when the association parameters are known are related to those of the multigroup logistic model. Necessary and sufficient conditions are given for the existence of the MLE of the association part. On the other hand the conditions for the unicity of the MLEs of the association parameters are much more complicated and not yet established. Finally, the article shows that for an MPM the estimates of the regression parameters for the location part exist and are unique if and only if they exist and are unique for each marginal univariate probit model. This result provides practical guidelines to detect early divergence Good starting values are essential; this problem is touched on briefly. The theoretical results are illustrated by a medical example."], ["A Generalizable Formulation of Conditional Logit with Diagnostics", null], ["The Analysis of Repeated Categorical Measurements Subject to Nonignorable Nonresponse", "Conditional likelihood methods have been proposed for analyzing the repeated measurement of categorical responses. This article extends the methods to include partially classified subjects. The extension allows for an ignorable or nonignorable nonresponse mechanism and uses standard statistical software for the computations. Two examples of incomplete longitudinal categorical data illustrate the method."], ["Testing Hypotheses about an Identified Treatment when there are Multiple Endpoints", null], ["Minimum Distance Estimators of Scale with Censored Data", "This article examines how the level of censoring affects the performance of estimators of scale that minimize a weighted Cram\u00e9rvon Mises distance between the Kaplan-Meier product-limit estimator of the survival distribution function and an assumed model. The article shows that under certain conditions these estimators are asymptotically normal if the survival function is a member of the assumed scale family. Weights that minimize the asymptotic variance under these conditions also are found. Under stronger conditions, the minimum distance estimators (MDE's) are asymptotically normal even if the survival function is not a member of the assumed scale family. Asymptotic coverage probabilities of confidence intervals are found for percentiles constructed from exponential MDE's of scale under departures from the exponential distribution, and what happens in the limiting cases of no censoring and complete censoring is examined. Using the asymptotic results and Monte Carlo experiments, several exponential MDE's of scale are compared to the exponential maximum likelihood estimator (MLE) of scale. The confidence intervals constructed from some of the MDE's have coverage probabilities closer to that expected under exponentiality than does the MLE at most censoring levels."], ["Adaptive Bayesian Classification of Spatial Data", null], ["A Technique for Estimating Marginal Posterior Densities in Hierarchical Models Using Mixtures of Conditional Densities", "A technique called quantile integration is proposed for the estimation of marginal posterior densities arising in Bayesian models having hierarchical representations. The method is based on approximating marginal densities as mixtures of conditional densities, where the conditioning variables are selected deterministically from the mixing distributions. The form of the approximation makes it easy to implement, and the resulting approximations are computationally efficient to obtain. The technique leads to particularly simple approximations for the predictive and posterior densities in Kalman filter or state-space models, and specific formulas are provided for the special case in which innovations belong to location-scale families. Other applications include a hierarchical empirical Bayes model for Poisson rates and a hierarchical linear model with exchangeable regression parameters and unknown variance components."], ["Facilitating the Gibbs Sampler: The Gibbs Stopper and the Griddy-Gibbs Sampler", "The article briefly reviews the history, literature, and form of the Gibbs sampler. An importance sampling device is proposed for converting the output of the Gibbs sampler to a sample from the exact posterior. This Gibbs stopper technique is also useful for assessing convergence of the Gibbs sampler for moderate sized problems. Also presented is an approach for implementing the Gibbs sampler in nonconjugate situations. The basic idea is to approximate the true cdf of each conditional distribution by a piecewise linear function and then sample from the approximation. Questions relating to the number of nodes in the approximation, gap size between successive nodes, and the treatment of unbounded intervals for a given conditional are discussed. The methodology is illustrated using a genetic linkage model, a nonlinear regression model, and the Cox model."], ["An Elementary Approach to Weak Convergence for Quantile Processes, with Applications to Censored Survival Data", null], ["Observational Studies of Rare Events: A Subset Selection Approach", "We propose a subset selection procedure for use in observational studies of rare events. Departing from traditional selection applications, which necessarily involve experimental settings, these problems are characterized by large, uncontrollable sample sizes and extremely low incidence rates. Although these characteristics impose increased analytical and computational burdens, they are readily apparent in many problems of practical importance and thus warrant investigation. To this end we present an approximation of a bound on the selection probability function. We empirically demonstrate its quality and conclude our study by offering methods for determining the size of the selected subset that are consistent with the observational nature of our problem context. We begin with an analysis of the behavior of a bound on the selection probability function as the number of populations from which the selection is to be made increases. A simply defined function that emerges from this asymptotic analysis can significantly reduce the computational burden associated with the selection procedure. We illustrate the performance of the approximation using data associated with a study of urban traffic hazards, for which sample sizes vary by an order of magnitude and incidence rates are virtually 0. These empirical results indicate that our asymptotically based function provides a remarkably accurate approximation of the bound even when a reasonably small number of populations are considered. These data also demonstrate that the bound may be quite conservative, but that judgments concerning the composition of the selected subset are similar to those that would be obtained by estimating the achieved selection probability."], ["Rank Procedures for the Two-Factor Mixed Model", "The test problem of fixed treatment effects is considered in the two-factor mixed model with interaction and unequal cell frequencies when the classical assumptions of normality do not hold. An explicit form of a test statistic is derived using a partial rank transform (ranking all observations within each block), and the asymptotic distribution of the statistic is determined under the assumption that the number of blocks tends to infinity and the cell frequencies are bounded. The statistic reduces to Friedman's statistic if no interactions are involved in the model and all cell frequencies are equal; hence the proposed test can be regarded as a generalization of Friedman's test for repeated observations when the cell frequencies are not equal. The test is compared to a corresponding test that can be used under the assumption of normality by the criterion of asymptotic relative efficiency. In the case of two treatments, the exact conditional distribution is determined and estimators and confidence intervals for the shift effect are proposed."], ["A New Look at Warning and Action Lines of Surveillance Schemes", "Some authors have suggested an efficient dynamic monitoring procedure for detection of a change in the drift of a Brownian motion. Their procedure can be described as a sequence of extremely short sequential probability ratio tests (SPRT's) done in zero time with infinitesimal time between consecutive SPRT's. The drawbacks of this procedure are that it can be described only as a limit of practical procedures and it does not take into account the cost of initiating a test. In this article, we suggest a procedure that takes both sampling and overhead costs into account and that can reasonably be carried out in practice. In this procedure the process is monitored continuously with a constant sampling rate. The accumulated data are analyzed by the standard cumulative sum (CUSUM) statistics. Whenever the CUSUM procedure raises an alarm, data are accumulated as fast as possible until either the alarm is relaxed or the process is stopped."], ["Book Reviews", null], ["Publications Received", null], ["Accounting for Interactions", null], ["Comment on Efron", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["Editors' Report for 1991", null], ["Nerve Cell Spike Train Data Analysis: A Progression of Technique", "Collections of occurrence times of events taking place irregularly in time provide a fairly common, but not broadly discussed, data type. This article is concerned with the particular circumstance of firing times in nerve cells that interact and form networks. The article reviews a progression of statistical analysis techniques: description, association as measured by moments and correlation, regression, and finally likelihood. The data is point process, but may be seen as that of regression and of multivariate analysis in standard parlance. A simple description of data collected simultaneously for one or more cells is provided."], ["A Spatial Statistical Analysis of Tumor Growth", null], ["Accelerated Failure-Time Regression Models with a Regression Model of Surviving Fraction: An Application to the Analysis of \u201cPermanent Employment\u201d in Japan", "Accelerated failure-time regression models with an additional regression model for the surviving fraction are proposed for the analysis of events that may never occur, regardless of censoring, for some people in the population risk set. The models attempt to estimate simultaneously the effects of covariates on the acceleration/deceleration of the timing of a given event and the surviving fraction; that is, the proportion of the population for which the event never occurs. The extended family of the generalized Gamma distribution is used for the accelerated failure-time regression model; the logistic function is used for the regression model of the surviving fraction. The models are applied to the data of interfirm job mobility in Japan to assess variability in \u201cpermanent employment\u201d among white collar and blue collar employees in firms of different sizes, independent from their variability in the timing of interfirm job separations."], ["Calibrated Seismic Verification of the Threshold Test Ban Treaty", "To improve verification of the Threshold Test Ban Treaty, the United States and Russia have embarked on an effort to make on-site yield measurements of each other's nuclear tests. Beyond their direct use in verification, these measurements also may prove useful in calibrating a monitoring system based on seismic magnitudes. The relative merits of seismic monitoring vis-a-vis on-site measurement have been at the core of a long-standing controversy. Many seismic verification problems hinge on statistical issues, including linear calibration based on a small data set and the formal use of expert opinion."], ["Estimating Price Indices for Residential Property: A Comparison of Repeat Sales and Assessed Value Methods", "Accurate estimation of price indices for residential property is an essential feature of real estate research, especially in view of recent efforts to forecast price trends for the 1990s. In this article, price trends are estimated by using the sales price, assessed value and date of sale for every residential property transaction between independent parties. This assessed value (AV) methodology is compared to the repeat sales (RS) method. This article develops a simple method for correcting the effect of the measurement errors associated with assessed value. We demonstrate that the large samples available with the AV method allow the measurement error problem to be reduced to negligible proportions. Using data on the Hartford, Connecticut metropolitan area, we find that price trends estimated from the AV and RS methods are substantially similar over a seven-year period. But the RS method is inefficient because it uses a relatively small subset of the data. Our results indicate that it remains inefficient when the researcher has a dataset much richer in repeat sales than ours."], ["Use of Nested Orthogonal Contrasts in Analyzing Rank Data", "A data set consisting of 143 rankings of 10 occupations from a survey of Goldberg has been analyzed in a number of recent papers. The purpose of this paper is to use so-called \u201cnested orthogonal contrasts\u201d of the occupations to gain further insight into the data. A contrast is a comparison of subsets of the occupations based on their relative ranks; contrasts are orthogonal if the comparisons they represent are not confounded. Various models based on sets of orthogonal contrasts\u2014including contingency table models, models analogous to those of Fligner and Verducci, and latent class models\u2014are applied to the data. It is found that there are three main groups of occupations based on overall prestige, and within each group are distinctions between managerial and technical occupations."], ["Predicting Working Memory Failure: A Subjective Bayesian Approach to Model Selection", "We use Bayes factors to compare two alternative characterizations of human working memory load in their ability to predict errors in database query-writing tasks. The first measures working memory load by the number of different features each task contains, while the second attempts instead to measure the complexity of the task by giving more weight to features that require more mental time for their correct execution. We reanalyze data from a previously conducted experiment using two logistic regression models with random subject effects nested within an experimental condition factor. The two models have alternative covariates based on the alternative measures of working memory load. We construct prior distributions based on our subjective knowledge gleaned from related experiments, providing details of the elicitation process. We examine sensitivity of our results to the effects of prior misspecification and case deletion. Asymptotic approximations are used throughout to facilitate computations. Finally, we comment on the strengths and limitations of the approach in light of our experience."], ["The Effect of Age at School Entry on Educational Attainment: An Application of Instrumental Variables with Moments from Two Samples", "We present a model in which compulsory school attendance laws, which typically require school attendance until a specified birthday, induce a relationship between years of schooling and age at school entry. Variation in school starting age created by children's dates of birth provides a natural experiment for estimating the effect of age at school entry. Because no large data set contains information on both age at school entry and educational attainment, we use an instrumental variables (IV) estimator with data derived from the 1960 and 1980 Censuses to estimate and test the age-at-entry/compulsory schooling model. In most IV applications, the two covariance matrices that form the estimator are constructed from the same sample. We use a method-of-moments framework to discuss IV estimators that combine moments from different data sets. In our application, quarter of birth dummies are the instrumental variables used to link the 1960 Census, from which age at school entry can be derived for one cohort of students, to the 1980 Census, which contains educational attainment for the same cohort of students. The results suggest that compulsory attendance laws constrain roughly 10% of students to stay in school."], ["Forensic Inference from DNA Fingerprints", null], ["The Errors-in-Variables Problem: Considerations Provided by Radiation Dose-Response Analyses of the A-Bomb Survivor Data", "Some basic issues in the errors-in-variables problem are discussed, in terms of considerations that arose in analyses of radiation effects on atomic bomb survivors. The setting essentially involves generalized linear models for the response variables, a very nonnormal distribution of the true covariable, and multiplicative errors in the observed covariable. Consideration is given to distinctions between structural and functional modeling. It is argued that careful attention to the apparent distribution of true covariables is critical in either case, and a quasi-structural approach to functional models is suggested. The focus is on the case in which the expected response is linear in the true covariable and strong assumptions are tentatively made about the model for covariate errors. For settings such as just described, which differ from that of much of the classical work in the area, it is emphasized that an attractive approach is based on weighted regression of the response on the expected values of the true covariable, given the observed values."], ["Statistical Analysis of the Time Dependence of HIV Infectivity Based on Partner Study Data", "Statistical analyses of data from studies of human immunodeficiency virus (HIV) transmission in partners of infected individuals often focus on estimation of the per contact probability of virus transmission, or infectivity. Of particular interest is evaluating whether the infectivity changes during the course of a partnership and identifying factors that influence the infectiousness of the initially infected partner (called the index case) and the susceptibility of the uninfected partner. Estimation and inference are complicated by limitations in partner study data, which may include unknown time of infection for either or both partners and inaccurate or incomplete information on the number and frequency of contacts. Using techniques from survival analysis, we extend earlier work of Jewell and Shiboski by developing semiparametric models for partner study data that allow variation in the infectivity according to time since infection of the index case. These models provide a unifying framework for investigations of infectivity based on data from various types of partner studies. The necessary statistical methodology requires analysis of binary regression models with complementary log-log links, where components of the regression function are subject to smoothness or isotonicity constraints. The methods are illustrated on data sets from studies of heterosexual transmission."], ["Votes or Competitions Which Determine a Winner by Estimating Expected Plurality", null], ["Calibration Estimators in Survey Sampling", null], ["\u201cEquivalent Sample Size\u201d and \u201cEquivalent Degrees of Freedom\u201d Refinements for Inference Using Survey Weights under Superpopulation Models", null], ["The Analysis of Retrospectively Ascertained Data in the Presence of Reporting Delays", "Suppose the progress of a disease consists of two chronologically ordered events, termed the starting event and the failure event. In retrospective sampling, the sampling scheme under which observations in the data set are identified retrospectively, individuals who experienced the starting event but not the failure event are excluded and thus are truncated from the data set, and only those who experienced both the starting event and the failure event before a given time are observed. The problem of reporting delays arises when some of the failure events are not reported before the given time and thus the corresponding cases also are excluded from the data set. In survival studies failure time data sometimes are collected under the retrospective sampling scheme subject to reporting delays. This article explores nonparametric and semiparametric methods of dealing with this type of data. The results generalize some existing nonparametric and semiparametric methods for analyzing right-truncated data when reporting delays are absent. Estimation of the expected number of events is studied in detail, interpretation of the proposed estimates is discussed, and an analysis of the blood transfusion data from the Centers for Disease Control is presented."], ["Tree-Structured Methods for Longitudinal Data", "The thrust of tree techniques is the extraction of meaningful subgroups characterized by common covariate values and homogeneous outcome. For longitudinal data, this homogeneity can pertain to the mean and/or to covariance structure. The regression tree methodology is extended to repeated measures and longitudinal data by modifying the split function so as to accommodate multiple responses. Several split functions are developed based either on deviations around subgroup mean vectors or on two sample statistics measuring subgroup separation. For the methods to be computationally feasible, it is necessary to devise updating algorithms for the split function. This has been done for some commonly used covariance specifications: independence, compound symmetry, and first-order autoregressive models. Data analytic issues, such as handling missing values and time-varying covariates and determining appropriate tree size are discussed. An illustrative example concerning immune function loss in a cohort of human immunodeficiency virus (HIV)-seropositive gay men also is presented."], ["Comparison of Model Misspecification Diagnostics Using Residuals from Least Mean of Squares and Least Median of Squares Fits", "This article explores model misspecification diagnostics based on least squares and least median of squares fits. It shows that in some circumstances, least median of squares methods (or any other estimator with the exact fit property) fail to reveal an incorrectly specified mean function, but least squares methods succeed."], ["Bootstrap Critical Values for Testing Homogeneity of Covariance Matrices", null], ["Boundary Estimation", "A data set consists of independent observations taken at the nodes of a grid. An unknown boundary partitions the grid into two regions. All the observations coming from a particular region share a common distribution, but the distributions are different for the two different regions. These two distributions are entirely unknown and need not differ in their means, medians, or any other measure of \u201clevel.\u201d The grid is of arbitrary dimension, and its mesh is rectangular. Our objective is to estimate the boundary without making any distributional assumptions. We propose a class of estimators and obtain strong consistency for them (including rates of convergence and a bound on the error probability). The boundary estimate is selected from an appropriate collection of candidate boundaries, which must be specified by the user. The candidate boundaries as well as the true boundary must satisfy certain intuitively natural regularity assumptions, including a \u201csmoothness\u201d condition. The boundary estimation problem has applications in diverse fields, including quality control, epidemiology, forestry, marine science, meteorology, and geology. Our method provides (as special cases) estimators for the change point problem, the epidemic change model, templates, linear bisection of the plane, and Lipschitz boundaries. Each of these examples is explicitly analyzed. A simulation study provides numerical evidence that the boundary estimators work well; in this simulation, the two distributions actually share the same mean, median, variance, and skewness. Finally, as an illustration, a boundary estimate is calculated on a data grid of cancer mortality rates in the United States."], ["On One-Step GM Estimates and Stability of Inferences in Linear Regression", null], ["Testing for Overdispersion in Poisson and Binomial Regression Models", "In this article a method for obtaining tests for overdispersion with respect to a natural exponential family is derived. The tests are designed to be powerful against arbitrary alternative mixture models where only the first two moments of the mixed distribution are specified. Various tests for extra-Poisson and extra-binomial variation are obtained as special cases; the use of a particular test may be motivated by a consideration of the mechanism through which the overdispersion may arise. The common occurrence of extra-Poisson and extra-binomial variation has been noted by several authors. However, the Poisson and binomial models remain valid in many instances and, because of their simplicity and appeal, it is of real interest to ascertain when they apply. This paper develops a unifying theory for testing for overdispersion and generalizes tests previously derived, including those by Fisher (1950), Collings and Margolin (1985), and Prentice (1986). It also shows the Pearson statistic to be a score test for overdispersion in a certain situation."], ["A Chi-Squared Goodness-of-Fit Test for Randomly Censored Data", "In this article, procedures analogous to Karl Pearson's well-known chi-squared goodness-of-fit test for a simple null hypothesis are developed under the random censorship model. It is shown that one straightforward analog of Pearson's statistic is diminished in applicability due to the form of its limiting distribution. This leads to the development of an asymptotically exact test based on a Wald-type statistic with a chi-squared limiting null distribution. This test is compared and contrasted theoretically and via a simulation with Akritas\u2019 test with respect to significance levels, asymptotic local powers, and finite sample powers. The general conclusions from the simulation study are that the proposed test usually achieves the desired significance levels when the probability of observing a censored or an uncensored value in the last interval is not small, whereas Akritas\u2019 test tends to be a bit anticonservative. On the other hand, Akritas\u2019 test is more powerful than the proposed test in a model with Weibull lifetimes, but in models with exponential and normal lifetimes neither test dominates the other."], ["Methods for Exact Goodness-of-Fit Tests", "Numerous goodness-of-fit tests with asymptotic chi-squared distributions have been proposed for discrete multivariate data, and there has been much discussion about using asymptotic results for computing critical values when there are small expected cell values. Although exact methods would be preferred in these situations, it generally is believed that such methods are computationally intractable. We propose methods for calculating exact distributions and significance levels for goodness-of-fit statistics that are computationally feasible over a wide range of models. In particular, the distribution for a simple multinomial model can be evaluated in polynomial time. For composite null hypotheses, we calculate the distribution conditional on the sufficient statistics for the nuisance parameters. We calculate the characteristic function of a distribution and invert the characteristic function using the fast Fourier transform (FFT). Our approach emphasizes the relationship between exact methods and probability formulas. Our technique, transforming the domain of the problem, is interesting for two reasons: First, algorithms that use the FFT and the convolution theorem are efficient for calculating the distribution of sums of independent statistics; and second, less storage is needed when working in the frequency domain than in the probability domain. The algorithms can be applied to general goodness-of-fit statistics and are parallelizable."], ["An Evaluation of Some Tests of Trend in Contingency Tables", null], ["Linear Logistic Latent Class Analysis for Polytomous Data", null], ["Computing Exact Distributions for Polytomous Response Data", "This article presents an efficient method for computing exact conditional distributions of the sufficient statistics for the parameters of four polytomous response models. For nominal response, two baseline category logit models and, for ordinal response, two adjacent categories logit models are considered. The method consists of recursive generation of the joint distribution of the sufficient statistics, augmented by a technique to slice off portions of the evolving sample space so as to eventually yield the required conditional distribution. Two actual data sets are analyzed to illustrate the method."], ["A Monte Carlo Approach to Nonnormal and Nonlinear State-Space Modeling", null], ["Posterior Mode Estimation by Extended Kalman Filtering for Multivariate Dynamic Generalized Linear Models", "A family of multivariate dynamic generalized linear models is introduced as a general framework for the analysis of time series with observations from the exponential family. Besides common conditionally Gaussian models, this article deals with univariate models for counted and binary data and, as the most interesting multivariate case, models for nonstationary multicategorical time series. For univariate responses, a related yet different class of models has been introduced in a Bayesian setting by West, Harrison and Migon. Assuming conjugate prior-posterior distributions for the natural parameter of the exponential family, they derive an approximate filter for estimation of time-varying states or parameters. However, their method raises some problems; in particular, in extending it to the multivariate case. A different approach to filtering and smoothing is chosen in this article. To avoid a full Bayesian analysis based on numerical integration, which becomes computationally critical for higher dimensions, we propose to estimate time-varying parameters by posterior modes. A generalization of the extended Kalman filter and smoother for conditionally Gaussian observations is suggested for approximate posterior mode estimation. For the purpose of comparison, it is applied to data sets analyzed by the authors mentioned earlier. The quality of approximation is also studied by simulation experiments, indicating good estimation behavior, and an application to multicategorical business test data is given."], ["Bayesian Designs for Maximizing Information and Outcome", null], ["Computing Bounds on Expectations", "One method for evaluating the sensitivity of a Bayesian analysis is to embed the prior into a class of priors. Then bounds on prior and posterior quantities of interest must be computed. This approach to inference, often called robust Bayesian inference, has received much attention lately. Implementing robust Bayesian methods entails difficult computations, especially if the parameter space is high dimensional. In this article we develop a Monte Carlo approach to computing these bounds and also explore some interesting theoretical properties of certain classes of priors. The methods can be useful in other situations in which bounds on expectations are required."], ["Bayesian Analysis of Constrained Parameter and Truncated Data Problems Using Gibbs Sampling", null], ["Constrained Bayes Estimation with Applications", "Bayesian techniques are widely used in these days for simultaneous estimation of several parameters in compound decision problems. Often, however, the main objective is to produce an ensemble of parameter estimates whose histogram is in some sense close to the histogram of population parameters. This is for example the situation in subgroup analysis, where the problem is not only to estimate the different components of a parameter vector, but also to identify the parameters that are above, and the others that are below a certain specified cutoff point. We have proposed in this paper Bayes estimates in a very general context that meet this need. These estimates are obtained by matching the first two moments of the histogram of the estimates, and the posterior expectations of the first two moments of the histogram of the parameters, and minimizing, subject to these conditions, the posterior expectation of the Euclidean distance between the estimates and the parameters. Several applications of the main result are provided in the normal and other models. Also, the results are applied to an actual data set."], ["Bayes Factors for Outlier Models Using the Device of Imaginary Observations", null], ["Confidence Intervals for Partial Rank Correlations", null], ["Nonparametric Two-Sample Procedures for Ranked-Set Samples Data", "Ranked-set samples have been shown to lead to improved methods of estimation in parametric settings under specific distributional forms when actual measurement of the sample observations is difficult but ranking them is relatively easy. The earliest work with ranked-set data concentrated on estimating a population mean or variance. More recently, a ranked-set sample estimator of a cumulative distribution function was developed and used to obtain a simultaneous confidence interval for the function. In this article, we take the next logical step and use this ranked-set empirical distribution function to construct distribution-free competitors to the standard Mann\u2013Whitney\u2013Wilcoxon estimation and testing procedures. The appropriate null distribution tables for the associated test are presented for the case of perfect ranking. Asymptotic relative efficiency comparisons between the simple random sample Mann\u2013Whitney\u2013Wilcoxon procedures and their ranked-set analogues are discussed, and the results of a small-sample Monte Carlo simulation study of the same are presented."], ["Statistical Issues Arising in AIDS Clinical Trials", "In the 11 years since AIDS became a defined disease, programs for the development and evaluation of new drugs for this disease have grown rapidly. Although the fundamental principles that drive the design, conduct, and analysis of clinical trials are as applicable to AIDS as to other diseases, there is no question that we have been confronted with unusually difficult challenges in studying therapeutic approaches for this disease. These include the multiple treatment needs of individual patients, identification of appropriate endpoints, rapidly changing \u201cnatural history,\u201d and the need for interaction with an informed and vocal patient community that continues to express dissatisfaction with the pace of research. In this context, statisticians have taken a leadership role in identifying and addressing important methodological issues in the evaluation of AIDS drugs."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Book Reviews", null], ["Publications Received", null], ["Comment on Samuels, Casella, and McCabe", null], ["Reply to Nelder's Letter", null], ["Response to Kadane's Book Review", null], ["Reply to Mallios", null], ["Note from the Theory & Methods Editor", null], ["Note from the Theory & Methods Editor", null], ["Editorial Board Page", "This article has no abstract"], ["Statistics, Science and Public Policy", null], ["Ice Floe Identification in Satellite Images Using Mathematical Morphology and Clustering about Principal Curves", "Identification of ice floes and their outlines in satellite images is important for understanding physical processes in the polar regions, for transportation in ice-covered seas, and for the design of offshore structures intended to survive in the presence of ice. At present this is done manually, a long and tedious process that precludes full use of the great volume of relevant images now available. We describe an accurate and almost fully automatic method for identifying ice floes and their outlines. Floe outlines are modeled as closed principal curves, a flexible class of smooth nonparametric curves. We propose a robust method of estimating closed principal curves that reduces both bias and variance. Initial estimates of floe outlines come from the erosion-propagation (EP) algorithm, which combines erosion from mathematical morphology with local propagation of information about floe edges. The edge pixels from the EP algorithm are grouped into floe outlines using a new clustering algorithm. This extends existing clustering methods by allowing groups to be centered about principal curves rather than points or lines. This may open the way to efficient feature extraction using cluster analysis in images more generally. The method is implemented in an object-oriented programming environment, for which it is well suited, and is quite computationally efficient."], ["Measuring the Similarities between the Lifetimes of Adult Danish Twins Born between 1881\u20131930", "The survival of like-sex twins born between 1881 and 1930 in Denmark, of which both were alive by the age of 15, is studied by means of models for bivariate survival data, with special reference to the degree of dependence. The 8,985 pairs were followed until 1980. The dependence is assumed to be generated by a common unobserved risk level. Several families of distributions for this level, including positive stable and gamma, are compared. General aspects of such data are discussed, including how to measure the degree of dependence. A simple choice is Kendall's coefficient of concordance, which for monozygotic males and females is about .17 and .15 and for dizygotic .09 and .10. Adjusting for cohort effects by a covariate describing year of birth reduces the dependence measure only slightly. The dependence is so small that a prediction for one twin is only slightly improved if the survival status of the other is known."], ["Alternative Estimates of the Effect of Family Structure during Adolescence on High School Graduation", "Many studies have reported significant empirical associations between family structure during childhood and children's outcomes later in life. It may be that living in a nonintact family has adverse consequences for children. On the other hand, it may be that some unobserved process jointly determines family structure and children's outcomes. How then should one interpret the empirical evidence on the relationship between family structure and children's outcomes? The answer depends on the question asked and on the prior information available to the researcher. We seek to interpret the association between family structure and high school graduation found among respondents in the National Longitudinal Study of Youth. We seek to answer the traditional question of the literature on treatment effects: How would the probability of high school graduation vary with family structure if family structure were not selected by parents but were, instead, an exogenously assigned \u201ctreatment,\u201d as in a clinical trial or other controlled experiment? The inferential problem is that the data alone do not suffice to identify the treatment effect. Hence any attempt to estimate a treatment effect depends critically on the prior information available to the researcher. We develop alternative estimates of the effect of family structure on high school graduation, obtained under differing assumptions about the actual process generating family structure and high school outcomes. We first assume strong prior information and present estimates of a set of parametric latent-variable models explaining family structure and children's outcomes. We then assume no prior information at all and report estimates of nonparametric bounds on the graduation probabilities. Finally, we give non-parametric estimates obtained under the assumption that family structure is exogenous with respect to high school graduation. Our empirical analysis strengthens the evidence that living in an intact family increases the probability that a child will graduate from high school. We also report that the probability of high school graduation increases markedly with both parents\u2019 education, regardless of family structure. At the same time, we stress that prior information is necessary if one is to do more than bound the effect of family structure on children's outcomes. Any point estimate embodies prior information about the process generating family structure and children's outcomes. As long as social scientists are heterogenous in their beliefs about this process, their estimates of family-structure effects may vary."], ["Modeling Household Fertility Decisions: A Nonlinear Simultaneous Probit Model", "This article proposes new methods for modeling household fertility decisions. Much of the demographic literature on this subject suggests that decisions relating to fertility are influenced by the orientations (attitudes, desires, intentions) of both husbands and wives, but the methods used in previous work do not indicate how wife's (husband's) orientation influences husband's (wife's) orientation, nor how these separate phenomena are combined to produce a joint decision. As such, these methods cannot be used to model the process by which husbands and wives come to have similar or dissimilar orientations or to assess one of the key issues in the literature, namely, the relative influence of the husband (wife) on the subsequent decision. To address these issues, we construct and estimate a nonlinear simultaneous equation probit that has not been considered in previous statistical work. Specifically, we model the trivariate distribution of wife's stated desire for additional children, husband's stated desire for additional children, and subsequent fertility. In the model, the stated desire of the husband (wife) is viewed as an indicator of the husband's (wife's) latent disposition toward subsequent fertility. The husband's (wife's) disposition is allowed to depend on the wife's (husband's) disposition. The two dispositions are then combined to generate the couple's propensity for subsequent fertility. We show how such models can be estimated and tested and how the parameters can be used to assess the relative influence of each partner on the propensity. To illustrate the approach, we reanalyze data on post\u2013World War II fertility from the Princeton Fertility Study. We find that recursive models do not fit the data, that both partners\u2019 dispositions influence subsequent fertility and (under additional assumptions) that the relative influence of each partner on the couple's propensity is .5. Next, we show how to extend the basic framework to treat other types of decision making. Specifically, we take up the case where the observed variables are not necessarily binary, as in the example, and we discuss differences between the binary case and the more general cases. For such cases we also propose a number of new models that have not been considered in previous work. We also show how the framework can be extended to the case where the latent variables have multiple indicators, the case where multiple decisions are jointly made, and the case where the decision(s) depends on three or more decision makers."], ["The Effects of Helmet Use on the Severity of Head Injuries in Motorcycle Accidents", "In 1976 the U.S. Congress removed the threat of withdrawal of certain highway funds from states that failed to enact motorcycle helmet laws. Since then over half the states have either repealed or weakened these laws. Most researchers in the field agree that this has lead to a significant increase in injuries and fatalities among motorcyclists involved in accidents. Potential limitations of many of the studies on which these conclusions are based include the facts that fatalities can result from injuries to parts of the body not protected by helmets and that other factors, such as speed and alcohol use, are not taken into account, usually due to lack of data. The former will result in a loss of power and the latter in the introduction of bias. In this article I model the level of head injury rather than the fatality rate and build a multivariate model that includes the other factors. The basic model is an ordered probit model with heteroscedasticity in the errors. The adequacy of the model is tested by Lagrange multiplier and goodness-of-fit tests. The former include tests for the normality of the errors and the specification of the regressors. Predictions from the model include that helmets lead to a 42% increase in the number of riders with no head injury and a $1,700 per rider decrease in the direct medical cost of treating the riders."], ["Comment on Maxwell and Delaney", null], ["Hierarchical Spline Models for Conditional Quantiles and the Demand for Electricity", "Methods for estimating nonparametric models for conditional quantiles are suggested based on the regression quantile methods of Koenker and Bassett. Spline parameterizations of the conditional quantile functions are used. The methods are illustrated by estimating hierarchical models for household electricity demand using data from the Chicago metropolitan area. The empirical results show that lower quantiles of demand (\u201cbase-load\u201d) vary only slightly across residential households. This variability is difficult to explain using household characteristics. However, upper quantiles of the demand distribution vary considerably and are systematically related to household characteristics and appliance ownership. The implications of analyzing mean demand behavior rather than various quantiles of the distribution of demand are also discussed."], ["Frequency Domain Estimation of the Parameters of Human Brain Electrical Dipoles", "Human brain evoked potentials are elicited by a stimulus and can be recorded by scalp electrodes. Many researchers have proposed models in which evoked potentials are generated by equivalent electrical dipoles in the brain. Each dipole is defined by a set of parameters that specify its location, orientation, and magnitude. Existing approaches to estimation of dipole parameters do not realistically account for errors resulting from background brain electrical activity (\u201cnoise\u201d) and thus lead to inefficient estimators and incorrect confidence sets. As an alternative, we derive frequency domain maximum likelihood estimators of the dipole parameters. The frequency domain approach simplifies the representation of the noise process and leads to substantial data reduction. The Fourier coefficients of the noise are approximately complex normal and independent across frequencies. This leads to a multivariate complex normal likelihood with a mean vector that is a nonlinear function of the dipole parameters. We compute the maximum likelihood estimates using iterative Fisher scoring. The results of a simulation study demonstrate that the parameter and standard error estimators are approximately unbiased when the model is correctly specified. An application to data from four subjects indicates that electrical activity approximately 50 milliseconds following an auditory click stimulus can be represented by an equivalent dipole in midline subcortical structures. We discuss the problem of model misspecification in applications to real data and describe possible approaches to improving the model and reducing bias due to misspecification."], [null, null], ["Nonparametric Estimation of Specific Occurrence/Exposure Rate in Risk and Survival Analysis", null], ["Variable Selection in Nonparametric Regression with Categorical Covariates", null], ["Poisson Overdispersion Estimates Based on the Method of Asymmetric Maximum Likelihood", null], ["Nonparametric Estimation of Nonstationary Spatial Covariance Structure", null], ["An Algorithm for Computing the Nonparametric MLE of a Mixing Distribution", null], ["On Certain Bivariate Sign Tests and Medians", null], ["Sequential Rank Tests with Repeated Measurements in Clinical Trials", "For comparing responses in two groups of subjects observed repeatedly, we propose a group sequential procedure based on linear rank statistics. The asymptotic normality of the sequentially computed linear rank statistics is obtained, and construction of the group sequential boundaries is based on this distribution theory. By virtue of this asymptotic approximation, the proposed procedure can be applied to interim analyses with either continuous or discrete repeated measurements. Even for staggered patient entry, simulation results suggest the theory is approximately correct. It can also be useful for testing the equality of two changes and rates of change, as well as the equality of two means of the responses. This procedure is illustrated with a real example."], [null, null], [null, null], ["Robust Wald-Type Tests of One-Sided Hypotheses in the Linear Model", null], ["A Step-Up Multiple Test Procedure", null], ["A Test for Extreme Value Domain of Attraction", null], ["Generalized Collinearity Diagnostics", null], ["General Classes of Influence Measures for Multivariate Regression", null], ["Exact and Optimum Tests in Unbalanced Split-Plot Designs under Mixed and Random Models", "Unbalanced split-plot designs are considered, where the whole-plot treatments could be replicated an unequal number of times in the design, but each split-plot treatment occurs exactly once in every whole plot. Analysis of such designs is provided when the various effects could be fixed or random. In each case, valid exact tests and optimum invariant tests (whenever they exist) are derived for testing the significance of the various effects. In some cases optimum tests exist only under additional conditions on the design, and these conditions are specified for the various setups."], ["Nonlinear Regression with Variance Components", "The nonlinear model with variance components, which combines a nonlinear model for the mean with additive random effects, is applicable to split-plot and nested experiments. We propose two methods of estimation for the parameters of the nonlinear model for the mean: (1) estimated generalized least squares (EGLS), and (2) maximum likelihood (MLE) by the method of scoring. Using a generalization of Klimko and Nelson's theorem on strong consistency of least squares estimators, it is possible to show that both the MLE and the EGLS estimators are strongly consistent, asymptotically normal, and asymptotically efficient."], ["Estimating the Number of Classes via Sample Coverage", "Assume that a random sample is drawn from a population with unknown number of classes and possibly unequal class probabilities. A nonparametric estimation technique is proposed to estimate the number of classes using the idea of sample coverage, which is defined as the sum of the cell probabilities of the observed classes. Since expected sample coverage can be well estimated, we were motivated to find its role in the estimation of the number of classes. This work generalizes the result of Esty to a nonparametric approach and extends Darroch and Ratcliff to incorporate the heterogeneity of the class probabilities. The coefficient of variation of the class sizes is shown to play an important role in the recommended estimation procedures. The performance of the proposed estimators is investigated by means of Monte Carlo simulations."], ["Bandwidth Choice for Average Derivative Estimation", null], ["Regression Smoothing Parameters that are not Far from their Optimum", null], ["A Generalized Moments Specification Test of the Proportional Hazards Model", null], ["Min and Max Scorings for Two-Sample Ordinal Data", null], ["Book Reviews", null], ["Publications Received", null], ["Corrections", null], ["An Extension to Wittkowski", null], ["Editorial Board Page", "This article has no abstract"]]}