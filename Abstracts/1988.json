{"1988": [["Miracles and Statistics: The Casual Assumption of Independence", "The primary theme of this address is cautionary: Statistical independence is far too often assumed casually, without serious concern for how common is dependence and how difficult it can be to achieve independence (or related structures). After initial discussion of statistics and religion, the address turns to miracles, especially Hume's critique and Babbage's reply. Stress is given the often tacit or unexamined assumption of independence among witnesses of a putative miracle. Other contexts of multiple testimony are treated, and the address ends with contemporary casual assumptions of independence: nuclear reactor safety, repeated measurements, and so forth. Other topics include prayer, circularity of argument, and the tension between skepticism about testimony and the pragmatic need to accept most of it provisionally."], ["Philatelic Mixtures and Multimodal Densities", null], ["A Walsh\u2014Fourier Analysis of the Effects of Moderate Maternal Alcohol Consumption on Neonatal Sleep-State Cycling", "Recent discussions of the functional significance of ultradian rhythms emphasize their importance to human physiology. Over the past 25 years, electroencephalographic (EEG) sleep patterns have been used in assessing the cerebral and central nervous system maturation of neonates. Through an interdisciplinary effort, spectral (Fourier) methods have been developed to discriminate between the various stages of sleep based on EEG recordings. Nevertheless, there has been little effort to develop methods for the statistical analysis of sleep-state cycling. In particular, attention has primarily been focused on the ultradian rhythm of sleep as it cycles between two states, active or rapid eye movement (REM) and quiet or non-REM sleep. There are, however, several components of REM and non-REM sleep, as well as a transitional state (indeterminate sleep) and abrupt alterations of state (arousal). Moreover, few studies have investigated the effects of prenatal alcohol exposure on the neurophysiological development of infants as assessed through sleep or EEG patterns. In this article the theory of Walsh\u2014Fourier analysis for discrete-valued (categorical) time series is used to investigate the spectral components of EEG sleep-state patterns of infants whose mothers abstained from drinking during pregnancy, and infants of mothers who used moderate amounts of alcohol continuously during pregnancy. The sample for this study is part of a larger cohort of women participating in a longitudinal study of substance use during pregnancy. The analysis is based on the finite Walsh\u2014Fourier transform that is defined in terms of the Walsh functions. The square-wave Walsh functions form a complete orthonormal sequence on (0, 1) and take on only two values, +1 and \u20131 (\u201con\u201d and \u201coff\u201d). This allows correlation of per-minute infant sleep-state records with square waveforms, and analysis of sleep-state cycles relative to per-minute switches from one sleep stage to another (termed sequency). The analysis consists of two parts. First, the article considers the problem of detecting whether a common sleep pattern exists in the unexposed and exposed groups of neonates separately. As expected, it is concluded that a common signal does exist in each group. Next, the spectral components of the sleep-state signals of the two groups are compared, and the differences between the two groups are discussed."], ["Estimating Proportions from Randomized Response Data Using the EM Algorithm", "The computation of maximum likelihood estimates for proportions from randomized response (RR) data may require numerical methods, especially for the more complex RR designs involving multiple proportions and samples. It is shown that one can apply the EM algorithm by viewing the RR data as mixture data, and taking the proportions to be estimated as the mixing proportions. A general formulation is presented for both related-question and unrelated-question RR designs, and illustrated with a real data set."], ["Covariate Randomized Response Models", null], ["Computational Statistics", null], ["Applications of Parallel Computation to Statistical Inference", null], ["A New Family of Multivariate Distributions with Applications to Monte Carlo Studies", null], ["Maximum Likelihood and Quasi-Likelihood for Nonlinear Exponential Family Regression Models", "Linear and nonlinear exponential family and quasi-likelihood regression models form a class of models with a structure that invites using one algorithmic framework to compute parameter estimates and regression diagnostics. This framework extends our work on nonlinear least squares; it includes iteratively reweighted least squares but also encompasses secant updates for part of the Hessian matrix of the likelihood or quasi-likelihood function along with tests for when to use this information. The framework also provides basic machinery for computing \u201cleave one out\u201d-style regression diagnostics. We describe the framework, discuss some implementation details, and present some numerical experience."], ["Importance Sampling for Estimating Exact Probabilities in Permutational Inference", null], ["Methods for the Analysis of Contingency Tables with Large and Small Cell Counts", null], ["Newton\u2014Raphson and EM Algorithms for Linear Mixed-Effects Models for Repeated-Measures Data", null], ["Bayesian Variable Selection in Linear Regression", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Computational Methods Using a Bayesian Hierarchical Generalized Linear Model", null], ["The Effect of Estimating Weights in Weighted Least Squares", null], ["The Effect of Truncation on the Identifiability of Regression Coefficients", "When data are truncated and hence partly unobservable, parameters identifiable under the complete observation may not be identifiable. A class of distributions is characterized, under which regression coefficients are unidentifiable by truncation of a dependent variable in the regression model. In a class of strongly unimodal densities regression coefficients are shown to be unidentifiable only when the underlying distribution is proportional to the exponential distribution in support of the truncated observation. The exponential, double exponential, and Huber's least-informative distribution for a location parameter are apparent examples of the distributions under which the unidentifiability can occur. In a more restrictive class of symmetric and strongly unimodal densities the unidentifiability arises only when the error distribution has exponential tails and the degree of truncation is at least 50%. The likelihood function becomes completely flat with respect to the regression coefficients and hence does not provide any power for the inference of the parameters here."], ["Approximations for Regression with Covariate Measurement Error", null], ["Regression Transformation Diagnostics Using Local Influence", "Following Box and Cox (1964), the use of transformations in regression analysis is now common; recently there has been emphasis on diagnostic methods for transformation, much of which has involved deletion of data cases. Summaries were given by Cook and Weisberg (1982) and Atkinson (1985). This article obtains diagnostics for the estimated regression parameter of the Box-Cox transformation of the response variable in the linear model. Instead of deleting cases, the more general notions of perturbing assumptions of the model, or components of the data, are employed, as in the local-influence approach of Cook (1986). The diagnostics then arise from local changes to the transformation parameter estimate caused by small perturbations; the case direction in which small perturbations have the greatest effect is the main diagnostic quantity. An appeal of the approach is that it allows simultaneous perturbations affecting all data cases, not just one-at-a-time deletions; it can thus point to groups of influential cases, giving some local indications of possible masking effects. These are usually said to occur when single deletions produce small changes in parameter estimates, whereas deletions of pairs or small groups of cases cause large changes. Any outlying direction cosines with similar signs are indications of cases possibly associated with masking. In the transformation problem, diagnostics are first obtained from perturbing the constant model variances, a general way of detecting case sensitivity. The diagnostics are shown to be functions of the residuals after transformation and their derivatives with respect to the transformation parameter, a second set of residuals. By allowing data perturbations, the approach can also be used to produce more specific diagnostics directed at sensitive values in either the response or explanatory data. The methods are illustrated on the poison data originally used by Box and Cox (1964). General sensitivity is attributed to two particular pairs; their deletion changes the maximum likelihood estimate of the transformation parameter from \u2013 .75 to \u2013 .55 and \u2013 .97, respectively. Only one of these pairs is sensitive to its response values alone being perturbed. In this instance, as in general, diagnostics cannot explain (in subject-matter terms) the reasons for the influential cases. Their purpose is to alert the investigator to possible difficulties with the data in relation to the model being fitted. Early notions of this work were reported in the author's discussion contribution to Cook (1986)."], ["A Recursive Regression for High-Dimensional Models, with Application to Growth Curves and Repeated Measures", "A fixed-effects formulation of repeated-measures and growth-curve problems usually leads to an unwieldy linear model, so mixed models are widely used for inference that the conditional linear error model could otherwise support with weaker distributional assumptions. Very high-dimensional regressions (not necessarily arising this way) can be fitted by the proposed alternating algorithm, which partitions the design matrix into singly manageable strips and recursively calls a regression routine with low-dimensional subproblems. Convergence to the full least squares solution with modest memory and time requirements is a consequence of the behavior of cyclically iterated projections of linear spaces. The partitioning can be implicitly and transparently done for a wide class of growth-curve problems. The method does not hinge on any balance or completeness properties of the design. In all cases coefficients and residuals are recoverable from standard regression output after convergence, but package-supplied covariances of fitted coefficients are not directly usable. Circumvention of the difficulty is described in conjunction with a case study involving a special error structure."], ["The Cost of Generalizing Logistic Regression", null], ["A Unifying Approach to Nonparametric Regression Estimation", null], ["Minimax Estimates in a Semiparametric Model", null], ["Testing for Common Trends", null], ["A Graphical Procedure for Determining Nonstationarity in Time Series", null], ["Rank-Based Tests for Randomness against First-Order Serial Dependence", "Optimal rank-based procedures were derived in Hallin, Ingenbleek, and Puri (1985, 1987) and Hallin and Puri (1988) for some fundamental testing problems arising in time series analysis. The optimality properties of these procedures are of an asymptotic nature, however, whereas much of the attractiveness of rank-based methods lies in their small-sample applicability and robustness features. Accordingly, the objective of this article is twofold: (a) a study of the finite-sample behavior of the asymptotically optimal tests for randomness against first-order autoregressive moving average dependence proposed in Hallin et al. (1985), both under the null hypothesis (tables of critical values) and under alternatives of serial dependence (evaluation of the power function), and (b) an (heuristic) investigation of the robustness properties of the proposed procedures (with emphasis on the identification problem in the presence of \u201coutliers\u201d). We begin (Sec. 2) with a brief description of the rank-based measures of serial dependence to be considered throughout: (a) Van der Waerden, (b) Wilcoxon, (c) Laplace, and (d) Spearman\u2014Wald\u2014Wolfowitz autocorrelations. The article is mainly concerned with first-order (lag 1) coefficients of these types. Tables of the critical values required for performing tests of randomness are provided (Sec. 3), and the finite-sample power of the resulting tests is compared with that of their parametric competitors (Sec. 4). Although the exact level of classical parametric procedures is only approximately correct (whereas the distribution-free rank tests are of the correct size), the proposed rank-based tests compare quite favorably with the classical ones, and appear to perform at least as well as (often strictly better than) their classical counterparts. The examples of Section 5 emphasize the robustness properties of rank-based tests with respect to departures from modeling assumptions, outliers, and gross errors (Secs. 5.1 and 5.3), as well as their insensitivity to spurious end effects (Sec. 5.2). Discrepancies between rank-based and the usual parametric tests also may provide an indication that an intervention analysis should be considered (Sec. 5.4), and rank-based correlograms may detect serial dependence in series where standard methods fail to do so (Sec. 5.5). These examples show how classical Gaussian methods that take normal white noise for granted can yield misleading diagnostic information\u2014spurious autocorrelation or failure to detect significant serial dependencies\u2014when the data have outliers, atypical start-up behavior, and so on. Rank-based tests exhibit much better resistance to aberrations of this type, and the conclusions drawn from the methods proposed here are thus likely to be more reliable in the model-identification process than those resulting from an inspection of traditional correlograms."], ["A Bayesian Approach to the Best-Choice Problem", "This article considers a version of the so-called best-choice problem. The buyer receives a random sample of price quotations for a good and desires to buy the good at as low a price as possible. After each price quotation is received, the buyer must decide either to accept the price or not, with the objective of maximizing the probability of buying the good at the lowest price. When the distribution of the price quotation is completely known, the optimal buying policy is myopic. The main purpose of this article is to show that this myopic property holds for some interesting price distributions that are updated in a Bayesian manner as successive prices are received."], ["Bayesian Confidence Intervals for Smoothing Splines", "The frequency properties of Wahba's Bayesian confidence intervals for smoothing splines are investigated by a large-sample approximation and by a simulation study. When the coverage probabilities for these pointwise confidence intervals are averaged across the observation points, the average coverage probability (ACP) should be close to the nominal level. From a frequency point of view, this agreement occurs because the average posterior variance for the spline is similar to a consistent estimate of the average squared error and because the average squared bias is a modest fraction of the total average squared error. These properties are independent of the Bayesian assumptions used to derive this confidence procedure, and they explain why the ACP is accurate for functions that are much smoother than the sample paths prescribed by the prior. This analysis accounts for the choice of the smoothing parameter (bandwidth) using cross-validation. In the case of natural splines an adaptive method for avoiding boundary effects is considered. The main disadvantage of this approach is that these confidence intervals are only valid in an average sense and may not be reliable if only evaluated at peaks or troughs in the estimate."], ["Nonparametric Tests under Restricted Treatment-Assignment Rules", null], ["The Construction of Trend-Free Run Orders of Two-Level Factorial Designs", null], ["Combined Rank Tests for Randomly Censored Paired Data", "Many authors have dealt with the problem of extending ordinary two-sample rank tests to cases where censoring occurs. Albers and Akritas (1987) proposed simple tests for this purpose, based on the idea of making separate rankings for uncensored and censored observations and subsequently combining the resulting two rank statistics. Similarly, some papers appeared that studied the problem of censoring for the paired-data case rather than the two-sample case. This article indicates how the approach of Albers and Akritas can be adapted to the paired-data case. The tests involved are not based on the ranks of the differences, but the differences of the ranks in the pooled sample. In this way, use is made of interblock information. The asymptotic distribution of the new statistic is obtained under the null hypothesis and contiguous location alternatives. By way of example, Wilcoxon- and Savage-type scores are introduced that are optimal for logistic location and exponential scale alternatives, respectively. The two corresponding tests are applied to some real data, and comparisons are made with the results obtained with competing tests on the same data set."], ["Friedman-Type Statistics and Consistent Multiple Comparisons for Unbalanced Designs with Missing Data", "A generalization of the Friedman test using the marginal likelihood principle (Kalbfleisch and Prentice 1973) is presented and its asymptotic power given. It allows use of a variety of score functions and handles ties and unbalanced designs. Some well-known statistics [including the tests of Kruskal and Wallis (1952), Prentice (1979), and Rinaman (1983)] are proven to be special cases, whereas others (e.g., Klotz 1980; Groggel and Skillings 1986; Rai 1987; Skillings and Mack 1981) are shown to be less appropriate. Multiple comparisons are considered under both the global hypothesis and alternatives. Evaluating non-centrality parameters under local shift alternatives, the procedures of Klotz (1980) and (for a special case) Skillings and Mack (1981) can be either anticonservative (showing differences that do not exist) or insensitive (ignoring differences that do exist), depending on the distribution of missing data. A new Scheff\u00e9-type procedure for arbitrarily missing data is presented and recommended as consistent and more powerful."], ["Comparison of Two Treatments in Animal Carcinogenicity Experiments", null], ["Estimating the Relative Rotation of Two Tectonic Plates from Boundary Crossings", null], ["Optimal Rates of Convergence for Deconvolving a Density", null], ["Sign-Preserving Unbiased Estimators in Linear Exponential Families", null], ["Approximations to the Distribution Function of the Anderson\u2014Darling Test Statistic", "A theoretical approximation to the upper tail area of the Anderson-Darling test statistic is derived by applying a result of Zolotarev (1961) to the characteristic function. This approximation is particularly good in the upper tail. An empirical distribution is given that is accurate over the complete domain of the distribution."], ["Tables and Large-Sample Distribution Theory for Censored-Data Correlation Statistics for Testing Normality", "Plotting order statistics versus some variant of the normal scores is a standard graphical technique for assessing the assumption of normality. To obtain an objective evaluation of the normal assumption, it is customary to calculate the correlation coefficient associated with this plot. The Shapiro\u2014Francia statistic is the square of the correlation between the observed order statistics and the expected values of standard normal order statistics, whereas the Shapiro\u2014Wilk statistic also involves the covariances of the standard normal order statistics. In a wide variety of applications, an investigation of the plausibility of the normal (or lognormal) model is needed when the observations on strength or life length are right-censored. The plotting procedure still applies if the observations are censored at a fixed order statistic or a fixed time. Here, the corresponding distribution theory for some modified versions of the Shapiro\u2014Wilk correlation statistic is investigated. Because the asymptotic theory used in this article shows a surprisingly slow rate of convergence even for complete samples, a table of critical values based on a Monte Carlo study is provided. Results from an empirical power study are also presented. Finally, large-sample critical values are obtained and compared with the Monte Carlo values."], ["A Test of Missing Completely at Random for Multivariate Data with Missing Values", null], [null, null], ["Book Reviews", null], ["Letters to the Editor", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["A Look Backward on the Occasion of the Centenary of JASA", null], ["An Archaeological Inference Problem", "An archaeological dating problem is presented and analyzed. The analysis is based on a data-generating model, which takes careful account of the various kinds of uncertainty involved in relating observed radiocarbon dates of artifacts to successive chronological beginning and ending dates of significant archaeological phases. In addition to its intrinsic interest as a possible model for many kinds of archaeological data of this type, the model poses some challenging inference problems. Maximum likelihood techniques do not seem adequate to the task, and a Bayesian approach has been adopted."], ["Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting", null], ["Evaluating Logistic Models for Large Contingency Tables", "This article discusses through three examples several new methods to aid in the analysis of large contingency tables. The general goal is to give better understanding of specific contingency tables, both by comparing how various log-linear/logistic models fit and through clearer interpretations of the resulting fits. For model selection, we show how to focus on a subset of simple, good-fitting models, beginning with a plot of a goodness-of-fit statistic versus residual degrees of freedom for all of the fitted models. To assess whether a particular model is adequate, we demonstrate that certain plots of residuals can reveal interesting effects that are often otherwise hidden. For model summarization and interpretation, we plot odds-ratio factors with confidence intervals to show the effects of explanatory variables in a concise and appealing way. The first example involves the relationship of job satisfaction to demographic variables for craft employees of a large corporation. The data presented consist of a five-way contingency table with about 10,000 counts. Job satisfaction for such employees increased with age and was higher in the Southwest and West than in the Northeast. Of four race-by-sex groups, the most satisfied was nonwhite males; the least satisfied was nonwhite females. Another example gives a six-way table with about 1,200 counts concerning whether or not high-school students think they will need mathematics in their future work. Among other results, for students planning to take a job right after graduation, those from suburban schools had odds about 2.6 times those from urban schools of thinking that mathematics will be useful. Moreover, among urban students, males had odds of finding mathematics useful about 2.1 times those for females, but there was little difference between the odds for males and females among suburban students. The third example, drawn from the literature, relates knowledge about cancer to four dichotomous variables. We compare our analysis with earlier ones."], ["Estimation of Interviewer Effects for Categorical Items in a Random Digit Dial Telephone Survey", null], ["Analysis of Repeated Ordered Categorical Outcomes with Possibly Missing Observations and Time-Dependent Covariates", "This article describes a method for comparing responses in two groups of subjects observed repeatedly at a common set of observation times when the response is an ordered categorical outcome. The method, which allows both time-dependent covariates and missing observations, consists of two analytic steps. In the first step, the data are analyzed separately at each occasion using a regression model chosen from the class of models for ordinal data proposed by McCullagh (1980). The joint asymptotic distribution of the estimates of these occasion-specific regression coefficients and a consistent estimator of their asymptotic covariance matrix are obtained without imposing any parametric model of dependence on the repeated observations. In the second step, this asymptotic distribution, together with appropriate simultaneous inference procedures, is used to characterize the overall difference between groups and the variation in group differences over time. The missing-data process may differ between groups to be compared, but it must be independent of response given the covariate values. The new procedures are illustrated by an analysis of annual reports of severity of wheezing in a cohort of preadolescent children participating in a longitudinal study of air pollution and respiratory health."], ["Simple Parametric Analysis of Animal Tumorigenicity Data", null], ["A Bayesian Approach to Nonlinear Calibration Problems", "In bioassays, enzyme assays, or radioimmunoassays the concentration response relationships are mostly nonlinear. Very often because of physical restrictions or for biological reasons, the prediction samples cannot be diluted and the determination of very low concentrations is of interest. Thus one cannot be restricted to the \u201calmost linear\u201d part of the response curve and standard linear calibration methods are not applicable. The nonlinear calibration problem is of great importance in these types of applications. To implement the Bayesian paradigm in nonlinear calibration problems, a substantial amount of numerical integration is necessary. In practice, when concentrations of hundreds of samples have to be determined routinely, even with a highly efficient integration method (Naylor and Smith 1982), the numerical effort becomes prohibitive. An approximation method is proposed, therefore, to reduce the calculation. The precision of the approximation method is examined by validation subsamples and comparison with an integration routine based on the Gaussian quadrature strategy. An example from agricultural research is used to illustrate the underlying problem. To determine the concentrations of an agrochemical present in soil samples taken at different time points from the field, two different bioassays were performed. The plant used in one assay is extremely sensitive at low concentrations, whereas the plant used in the other assay reacts at higher concentrations. Combination of results from the two assays is also demonstrated."], ["Bispectral-Based Tests for the Detection of Gaussianity and Linearity in Time Series", "Statistical techniques have been developed that use estimated bispectrum values to test whether a sample of a time series is consistent with the hypothesis that the observations are generated by a linear process. The magnitude of the test statistics indicates the amount of divergence between the observations and the linear model hypothesis. It is important to investigate such a divergence, since the usual linear model coefficients can be shown to be biased in the face of nonlinear time series structure. The tests presented here can thus be considered diagnostic as well as confirmatory. These tests are applied to a variety of real series previously modeled with linear models. The results indicate nonlinear models may yield better results, because many of the series analyzed appear to have considerable nonlinear lagged interactions."], ["A Model for Ex Ante Real Interest Rates and Derived Inflation Forecasts", "A methodology for estimating expected real interest rates and making inflation forecasts, together with appropriate confidence intervals, is presented. The role of price expectations in determining real interest rates is also analyzed. Structural breaks in the process generating real interest rates are detected in 1980 and 1983 by tracking the accuracy of interest-rate forecasts. Analysis of these structural breaks reveals that the break at the beginning of 1983 returns the process to its pre-1980 formulation. During 1953-1985, price forecasts derived from the interest-rate model are found to be unbiased once the structural breaks in the real rate process are handled through heteroscedastic corrections. Invoking the rational-expectations hypothesis, we construct a general yet parsimonious dynamic model for estimating economic agents' anticipations of the real rate of interest. By using quarterly data for 1953-1985 the model is efficiently estimated by two-step two-stage least squares. Results provide support for a random-walk process and for the existence of a negative price expectations (Tobin-Mundell) effect on real interest rates. The U.S. economy was subjected to a number of shocks in the early 1980s. A new chairman took the helm of the Federal Reserve System (Fed) in 1979, and almost immediately the Fed announced that it would change its operating procedure from controlling interest rates to targeting the money supply. The Fed then returned to its pre-1979 operating procedure in October 1982. Added to these changes were the tightening of credit controls in the first half of 1980, the advent of the large fiscal deficits of the Reagan administration, and a fall in petroleum prices. To some degree, all of these shocks probably contributed to the greater volatility of the economy in the first few years of the 1980s. In fact, some researchers have recently argued that there have been shifts in the process generating real interest rates in October 1979 and 1982 coincident with the Fed's changes in operating procedure. To investigate the existence and timing of any breaks in the early 1980s we constructed exact (small-sample) confidence intervals for the interest-rate forecasts of investors and tracked them with actual interest rates. We identified a break in the first quarter of 1980 when the actual interest rate no longer fell within the confidence interval for the forecast, and another break in the first quarter of 1983. Adapting the model to account for these structural shifts, we show that, although the 1980-1982 period is significantly different from the pre-1980 period (in terms of parameter values), the post-1982 period is not significantly different from the pre-1980 period. We also construct investors' ex ante forecasts of inflation from the interest-rate model and show them to be unbiased, whereas the inflation forecasts from the Livingston survey are biased. The mean squared prediction error of inflation forecasts improves significantly when price expectations are included as a determinant of ex ante real interest rates. We also find that the forecasting performance of this model compares favorably with the inflation forecasts from the American Statistical Association/National Bureau of Economic Research Business Outlook Surveys."], ["Measuring Migration Distances: Self-Reporting and Indirect Methods", "Distance is a critical concept in the measurement and analysis of geographical mobility, but statistical offices rarely provide data on distance moved. Two approaches for doing so are simply to ask movers how far they moved or to infer distance from localities of origin and destination. The former has been used in Health Interview Surveys, and the latter is applied to Current Population Surveys; both are national surveys conducted by the U.S. Census Bureau. The two approaches appear to produce consistent results and offer ways of increasing comparability of data and research findings on geographical mobility."], ["Balanced Simultaneous Confidence Sets", null], ["Prepivoting Test Statistics: A Bootstrap View of Asymptotic Refinements", null], ["A Bootstrap Revival of Some Nonparametric Distance Tests", null], ["Importance Sampling for Bootstrap Confidence Intervals", null], ["Tree-Structured Classification via Generalized Discriminant Analysis", null], ["Comment", null], ["Rejoinder", null], ["Identification of Binary Response Models", null], ["On Errors-in-Variables in Binary Regression\u2014Berkson Case", null], ["Estimation and Testing in a Two-Sample Generalized Odds-Rate Model", null], ["Kernel and Probit Estimates in Quantal Bioassay", "A kernel method for the estimation of quantal dose-response curves is considered. In contrast to parametric modeling, this local smoothing method does not require any assumptions beyond smoothness of the dose-response curve and, in this sense, is nonparametric. In finite-sample situations, the kernel estimate of the dose-response curve is not necessarily monotone and, therefore, if the additional assumption of monotonicity of the dose-response curve is made, a monotonized version is discussed. Bias, variance, asymptotic normality, and uniform consistency, including rates of convergence of kernel estimates, are derived and applied to establish consistency and limiting distribution of kernel estimates of the ED\u03b1. Here, ED\u03b1 is the effective dose at level \u03b1, that is, the dose where 100\u03b1% of the subjects show a response. The properties of the kernel estimated ED\u03b1 are compared with the corresponding properties of the maximum likelihood estimator assuming the probit model. Practical application of the kernel method requires choice of a kernel function and of a bandwidth (smoothing window), and methods for global as well as local bandwidth selection are discussed. Another point of interest is the construction of confidence intervals for the estimated ED\u03b1. For the kernel method, two asymptotically consistent approaches are presented. These are compared in a simulation study, where the behavior of kernel and probit estimates of the ED\u03b1, \u03b1 = .01, .05, .1, and .5, and of corresponding confidence intervals is observed for four different models (two probit models, one Weibull model, and one normal mixture model). The choice of different kernels and of local bandwidths was compared in another Monte Carlo study. The observed finite sample behavior of the kernel estimate of the ED50 was considerably better (gain of 40%-70% in terms of mean squared error) than that of the probit method under the two nonsigmoid models, whereas it was not drastically worse under the probit models (loss of 20%-30% in terms of mean squared error). Application of higher-order kernels and local bandwidth choice turned out to yield favorable versions of the kernel method. The results concerning observed coverage probabilities for the confidence intervals based on different approaches were more mixed."], ["The Analysis of Multivariate Contingency Tables by Restricted Canonical and Restricted Association Models", "Restricted canonical models and restricted association models are proposed and applied to multiway contingency tables. These models have been previously applied to two-way contingency tables; however, multivariate generalization has been impeded in the past, since canonical and association models both depend on singular value decompositions that apply only to two-way arrays. In this article, this restriction to two-way arrays is overcome by division of the cross-classified variables into explanatory and response variables. The explanatory variables are treated as a single polytomous variable, and the response variables are treated as a second single polytomous variable. In this fashion, the multiway table is reduced to a two-way array to which traditional canonical and association models may be applied. Use of linear restrictions on parameters in canonical and association models is especially important in multiway tables if useful models are to be constructed. The class of models considered in this article is sufficiently broad to include models of conditional independence, homogeneity models, conditional symmetry models, log-linear models of no three-factor interaction, conditional quasisymmetry models, and models that express association in terms of preassigned scores. The proposed models can be applied to tables in which some or all variables are ordered as well as to tables with no ordered variables. To illustrate results, appropriate restricted canonical and restricted association models are applied to a three-way cross-classification of abortion attitudes (a response variable) and religion and education (explanatory variables). Insights into the table are obtained that are not readily available from the analysis of Haberman (1979, chap. 6)."], ["Bayesian Nonparametric Survival Analysis", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["A Nonparametric Approach to the Truncated Regression Problem", null], ["Equivalence of Smoothing Parameter Selectors in Density and Intensity Estimation", "Kernel smoothing is an attractive method for the nonparametric estimation of either a probability density function or the intensity function of a nonstationary Poisson process. In each case the amount of smoothing, controlled by the bandwidth, that is, smoothing parameter, is crucial to the performance of the estimator. Bandwidth selection by cross-validation has been widely studied in the context of density estimation. A bandwidth selector in the intensity estimation case has been proposed that minimizes an estimate of the mean squared error under the assumption that the data are generated by a stationary Cox process. This article shows that these two methods each select the same bandwidth, even though they are motivated in much different ways. In addition to providing further justification of each method, this equivalence of smoothing parameter selectors yields new insights for both density and intensity estimation. A benefit for intensity estimation is that this equivalence makes it clear how the Cox process method may be applied to kernels that are nonuniform, or even of higher order. Another benefit is that this duality between problems makes it clear how to apply the well-developed asymptotic methods for understanding density estimation in the intensity setting. A benefit for density estimation is that it motivates an analog of the Cox process method, which provides a useful nonasymptotic means of studying that problem. The specific forms of the estimators and smoothing parameter selectors are introduced in Section 1. The basic equivalence result is stated in Section 2. Sections 3 and 4 describe new insights that follow for intensity and density estimation, respectively. Section 5 discusses modification of these ideas to take boundary effects into consideration and shows how they can be used to motivate new boundary adjustments in intensity estimation."], ["Simultaneous Tolerance Intervals for the Linear Regression Model", null], ["Mallows-Type Bounded-Influence-Regression Trimmed Means", "The influence functions of the regression trimmed-mean estimators proposed by Koenker and Bassett (1978) and Welsh (1987) are bounded in the dependent-variable space but not in the independent-variable space. This article follows the approach of Mallows (1973, 1975) and modifies these estimators so that the resulting estimators have bounded-influence functions. The large-sample behavior of these estimators is studied, and it is shown that they have the same asymptotic distribution. The small-sample behaviors of the ordinary-regression and bounded-influence-regression trimmed means are then investigated by means of a Monte Carlo study and by applying the estimators to water-salinity data (see Ruppert and Carroll 1980). Based on these results we conclude that one can potentially gain much by using bounded-influence-regression trimmed means over ordinary-regression trimmed means; however, there does not seem to be a clear choice between the Koenker-Bassett and Welsh versions."], ["An Approximate Test for Comparing Heteroscedastic Regression Models", null], ["A General Nested Split-Plot Analysis of Covariance", null], ["The Effect of Sampling Design and Response Mechanism on Multivariate Regression-Based Predictors", null], ["Families of Multivariate Distributions", "For many years there has been an interest in families of bivariate distributions with marginals as parameters. Genest and MacKay (1986a,b) showed that several such families that appear in the literature can be derived by a unified method. A similar conclusion is obtained in this article through the use of mixture models. These models might be regarded as multivariate proportional hazards models with random constants of proportionality. The mixture models are useful for two purposes. First, they make some properties of the derived distributions more transparent; the positive-dependency property of association is sometimes exposed, and a method for simulation of data from the distributions is suggested. But the mixture models also allow derivation of several new families of bivariate distributions with marginals as parameters, and they indicate obvious multivariate extensions. Some of the new families of bivariate distributions given in this article extend known distributions by adding a parameter to make them more flexible. Other families are derived that appear to be entirely new."], ["The Bias of Autoregressive Coefficient Estimators", null], ["Nested Reduced-Rank Autoregressive Models for Multiple Time Series", null], ["The Parameter Inference for Nearly Nonstationary Time Series", null], ["Efficiency Bounds Implied by Multiperiod Conditional Moment Restrictions", null], ["When Should One Stop Testing Software?", null], ["Granularity and Efficiency", null], ["Demographic Reproduction Rates and the Estimation of an Expected Total Count per Person in an Open Population", null], ["Multistage Ranking Models", null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Editors' Report for 1987", null], ["The Shape Parameter of a Two-Variable Graph", "The shape parameter of a two-variable graph is the ratio of the horizontal and vertical distances spanned by the data. For at least 70 years this parameter has received much attention in writings on data display, because it is a critical factor on two-variable graphs that show how one variable depends on the other. But despite the attention, there has been little systematic study. In this article the shape parameter and its effect on the visual decoding of slope information are studied through historical, empirical, theoretical, and experimental investigations. These investigations lead to a method for choosing the shape that maximizes the accuracy of slope judgments."], ["A Method for Obtaining Short-Term Projections and Lower Bounds on the Size of the AIDS Epidemic", "A methodology is proposed for obtaining short-term projections of the acquired immunodeficiency syndrome (AIDS) epidemic by projecting the number of cases from those already infected with the AIDS virus. This is a lower bound on the size of the AIDS epidemic, because even if future infections could be prevented, one could still anticipate this number of cases. The methodology is novel in that no assumptions are required about either the number of infected individuals in the population or the probability of an infected individual eventually developing AIDS. The methodology presupposes knowledge of the incubation distribution, however, among those destined to develop AIDS. Although the method does not account for new infections, it may produce accurate short-term projections because of the relatively long incubation period from infection to clinically diagnosed AIDS. The estimation procedure \u201cback-calculates\u201d from AIDS incidence data to numbers previously infected. The number of cases diagnosed in each calendar period has a multinomial distribution with cell probabilities that can be expressed as a convolution of the density of infection times and the incubation distribution. The problem is shown to reduce to estimating the size of a multinomial population. A simple EM algorithm is developed for obtaining maximum likelihood estimates when the density of infection times is parameterized as a step function. The methodology is applied to AIDS incidence data in the United States, to obtain short-term projections and an estimate of the minimum size of the epidemic by assuming no new infections in 1987 and after. The sensitivity of the projections to the assumed incubation distribution is investigated. It was found that short-term projections are not nearly as sensitive to the assumed incubation distribution as long-term projections. Although some information on the incubation distribution has been reported from an analysis of transfusion-associated AIDS cases, we discuss several issues and caveats associated with estimates of the incubation distribution from such data. An important point is that short-term projections are considerably less sensitive to the assumed incubation-period distribution and the parametric model for the density of infection times than long-term projections. There is a temptation to use back-calculation to estimate current human immunodeficiency virus seroprevalence, and to make longer-term projections; however, such estimates are highly uncertain both because they are sensitive to model assumptions and because the AIDS incidence data provide more information about infections in the distant past than the near past."], ["Predicting Accident Frequencies for Drivers Classified by Two Factors", "For predicting accident frequencies, a succession of log-linear models for Poisson data, some of which include nested random effects, is introduced. By applying maximum likelihood and empirical Bayes estimation techniques to these models, one can incorporate the actuarial notions of risk classification, model-based smoothing, credibility theory, and experience rating under a unified statistical approach to loss prediction. The performance of these methods is evaluated by using accident data from California."], ["Finding the Location of a Signal: A Bayesian Analysis", "We study the problem of determining the location of an emergency transmitter in a downed aircraft. The observations are bearings read at fixed stations. A Bayesian approach, yielding a posterior map of probable locations, seems reasonable in this situation. We therefore develop conjugate prior distributions for the von Mises distribution, which we use to compute a posterior distribution of the location. An approximation to the posterior distribution yields accurate, rapidly computable answers. A common problem with this kind of data is the possibility that signals will reflect off orographic terrain features, resulting in wild bearings. Such bearings can affect the posterior distribution severely. We develop a sensitivity analysis, based on the idea of predictive distribution, to reject wild bearings. The method, which is based on an asymptotic argument, nonetheless performs well in a small simulation study. When the preceding approximation is used, the sensitivity analysis is practical in terms of computation time."], ["Forecasting Records by Maximum Likelihood", "A maximum likelihood method of fitting a model to a series of records is proposed, using ideas from the analysis of censored data to construct a likelihood function based on observed records. This method is tried out by fitting several models to series of athletics records for mile and marathon races. A form of residual analysis is proposed for testing the models. Forecasting consequences are also considered. In the case of mile records, a steady linear improvement since 1931 is found. The marathon data are harder to interpret, with a steady improvement until 1965 with only slight improvement in world records since then. In both cases, the normal distribution appears at least as good as extreme-value distributions for the distribution of annual best performances. Short-term forecasts appear satisfactory, but serious reservations are expressed about using regression-type methods to predict long-term performance limits."], ["Analysis of Recurrent Events: Nonparametric Methods for Random-Interval Count Data", null], ["Predicting the Outcome of Intensive Care Unit Patients", null], ["Separating Probability Elicitation from Utilities", null], ["A Bayesian Approach to Ranking and Selection of Related Means with Alternatives to Analysis-of-Variance Methodology", null], ["Characterization of a Ranked-Set Sample with Application to Estimating Distribution Functions", "Ranked-set sampling has been shown to provide improved estimators of the mean and variance when actual measurement of the observations is difficult but ranking of the elements in a sample is relatively easy. This result holds even if the ranking is imperfect. In this article, we provide a characterization of a ranked-set sample that makes the source of this additional information intuitively clear. It is applied to show that the empirical distribution function of a ranked-set sample is unbiased and has greater precision than that from a random sample. The null distribution of a Kolmogorov-Smirnov statistic based on this empirical distribution function is derived for the case in which perfect ranking is possible. It is seen to be stochastically smaller than the usual Kolmogorov-Smirnov statistic based on a simple random sample, resulting in a smaller simultaneous confidence interval for the cumulative distribution function."], ["Optimal Sequential Selection from a Known Distribution with Holding Costs", null], ["Richardson Extrapolation and the Bootstrap", null], ["Estimating Transformations for Regression via Additivity and Variance Stabilization", "I propose a method for the nonparametric estimation of transformations for regression. It is much more flexible than the familiar Box-Cox procedure, allowing general smooth transformations of the variables, and is similar to the ACE (alternating conditional expectation) algorithm of Breiman and Friedman (1985). The ACE procedure uses scatterplot smoothers in an iterative fashion to find the maximally correlated transformations of the variables. Like ACE, my proposal can incorporate continuous, categorical, or periodic variables, or any mixture of these types. The method differs from ACE in that it uses a (nonparametric) variance-stabilizing transformation for the response variable. The technique seems to alleviate many of the anomalies that ACE suffers with regression data, including the inability to reproduce model transformations and sensitivity to the marginal distribution of the predictors. I provide several examples, including an analysis of the \u201cbrain and body weight\u201d data and some data on telephone-call load. I also discuss the relationship of the proposed technique to the Box-Cox and ACE procedures. Efron's work on transformations provides some of the theoretical basis for the methodology."], ["High Breakdown-Point Estimates of Regression by Means of the Minimization of an Efficient Scale", null], ["Logistic Regression, Survival Analysis, and the Kaplan-Meier Curve", "We discuss the use of standard logistic regression techniques to estimate hazard rates and survival curves from censored data. These techniques allow the statistician to use parametric regression modeling on censored data in a flexible way that provides both estimates and standard errors. An example is given that demonstrates the increased structure that can be seen in a parametric analysis, as compared with the nonparametric Kaplan-Meier survival curves. In fact, the logistic regression estimates are closely related to Kaplan-Meier curves, and approach the Kaplan-Meier estimate as the number of parameters grows large."], ["Generalized Logistic Models", "A class of models indexed by two shape parameters is introduced, both to extend the scope of the standard logistic model to asymmetric probability curves and improve the fit in the noncentral probability regions. One-parameter subclasses can be used to examine symmetric or asymmetric deviations from the logistic model. The delta algorithm is adapted to obtain maximum likelihood estimates of the parameters. A review is made of other proposed generalizations. The standard linear logistic model is widely used for modeling the dependence of binary data on explanatory variables. Its success is due to its broad applicability, simplicity of form, and ease of interpretation. This model works well for many common applications; however, it assumes that the expected probability curve \u03bc(\u03b7) is skew-symmetric about \u03bc = \u00bd and that the shape of \u03bc(\u03b7) is the cumulative distribution function of the logistic distribution. Symmetric data with a shallower or steeper slope of ascent may not be fitted well by this model, nor is there any provision for treating the two tails of the estimated curve \u03bc(\u03b7) asymmetrically or fitting different distributions for \u03bc(\u03b7). This article introduces a class of models, indexed by one or two shape parameters, that encompasses a wider range of situations than the standard logistic model (although the standard model is included). The shape parameters have been specifically designed to modify the behavior of the curve in the extreme-probability regions where problems of lack of fit may occur, while allowing for asymmetric treatment of the two tails. Members of this family approximate the Gaussian, Laplace, and extreme minimum and maximum distributions up to the first four moments. The model can be collapsed to several simpler one-parameter symmetric and asymmetric formulations."], ["Prediction and Estimation of Growth Curves with Special Covariance Structures", null], ["Linear Interpolation with a Nonparametric Accelerated Failure-Time Model", null], ["Robust Permutation Tests for Matched-Pairs Designs", null], ["Nonparametric Methods for Detecting Treatment Effects in Repeated-Measures Designs", null], ["Asymptotic Relative Efficiencies of the Rank-Transformation Procedure in Randomized Complete Block Designs", null], ["Generalizations of Steel's Treatments-Versus-Control Multivariate Sign Test", null], ["Optimal Step-Type Designs for Comparing Test Treatments with a Control", null], ["Nonorthogonal Analysis of Variance Using Gradient Methods", null], ["Regression Procedures for ARMA Estimation", null], ["A Test for Aliasing Using Bispectral Analysis", "Aliasing is a signal-confounding problem that arises when a continuous-time signal is sampled at a rate slower than twice the highest frequency component of a Fourier series representation of the signal. Aliasing can be especially serious for social-science time series applications, since the sampling designs used to construct most social-science data bases are fixed by considerations other than the nature of the underlying continuous-time mechanisms. After collecting sampled data, it is of value to test the observations for the presence of aliasing. It is shown that the nature of the support set of a sampled band-limited stationary signal can be used to motivate an amended version of the Hinich bispectrum test for Gaussianity (Hinich 1982) as a test for aliasing."], ["Ranges of Posterior Probabilities for Quasiunimodal Priors with Specified Quantiles", null], ["The Logistic Normal Distribution for Bayesian, Nonparametric, Predictive Densities", "This article models the common density of an exchangeable sequence of observations by a generalization of the process derived from a logistic transform of a Gaussian process. The support of the logistic normal includes all distributions that are absolutely continuous with respect to the dominating measure of the observations. The logistic-normal family is closed in the prior to posterior Bayes analysis, with the observations entering the posterior distribution through the covariance function of the Gaussian process. The covariance of the Gaussian process plays the role of a smoothing kernel. Three features of the model provide a flexible structure for computing the predictive density: (a) The mean of the Gaussian process corresponds to the prior mean of the random density: (b) The prior variance of the Gaussian process controls the influence of the data in the posterior process. As the variance increases, the predictive density has greater fidelity to the data, (c) The prior covariance of the Gaussian process controls the smoothness of its sample paths and the amount of pooling of the sample information. For iid observations the empirical distribution function (edf) is a sufficient statistic for all inference. Since the human eye finds it difficult to distinguish important features of distribution functions, their densities often are plotted instead. Unfortunately, the edf does not possess a density function with respect to Lebesgue measure; consequently, many techniques have been proposed to smooth the edf so that its modification does possess a proper density. From the subjective, Bayesian perspective, the data are iid given a common but unspecified density. Beliefs about the unknown density are modeled through probability statements. When the density has a known functional form, the prior distribution concerns the density's parameters, which describe important, unknown features of the density. In this article the density is not constrained to a functional form, so it becomes the parameter of interest. Its posterior distribution becomes the mechanism for smoothing the edf so that the density estimator (the posterior mean) evaluated at a point can use nearby data."], ["Parameter Orthogonality for a Family of Discrete Distributions", "The standard contagious distributions (see Douglas 1980) have been used in such varied fields as biology and automobile insurance, often to model various physical phenomena as well as provide a good fit to count data when other models are inadequate. Unfortunately, the parameterizations often used when working with these distributions normally lead to extremely high correlations of the maximum likelihood estimators (MLE's). This tends to lead to mathematical complexities, and causes difficulty or even errors in their interpretation. Furthermore, numerical difficulties may arise when using numerical procedures to locate the estimates. Some of these difficulties were discussed by Douglas (1980, pp. 171, 204-205), who suggested that a reparameterization to reduce or even eliminate such correlation is desirable. If the MLE's are asymptotically uncorrelated, the parameterization is orthogonal. Philpot (1964) derived an orthogonal parameterization for the Neyman Type A distribution; Stein, Zucchini, and Juritz (1987) derived for the Poisson mixture by the inverse Gaussian distribution. Parameter orthogonality has several attractive features in the present context. Since there is no correlation asymptotically, the estimates (with their standard errors) provide a simpler summary of the data than in the absence of such orthogonality. The use of a parameterization where the MLEs are highly correlated can lead to a misleading analysis, or at best a more complicated analysis that would be necessary if an orthogonal parameterization had been used. To the extent that a high correlation exists, the parameters involved tend to measure similar quantities, and orthogonality separates information about the parameters from each other. This article gives an orthogonal parameterization for a large family of discrete distributions, including many of the contagious distributions, some Poisson mixtures, and some other generalized distributions. The previously cited works are unified and extended, for example, to the Polya-Aeppli, Poisson-binomial, and Sichel's Poisson-generalized inverse Gaussian distribution. One of the orthogonal parameters is the mean, and in many applications it is of interest to express the mean as a function of relevant covariate information. For example, Hinde (1982) considered some of these distributions in a regression context. This article shows how the results may be extended to deal with the covariate case in a relatively straightforward manner. Consequently, a convenient parameterization exists for a large family of distributions in a wide variety of situations. Some numerical examples are given, and a simple algorithm is given to find the maximum likelihood estimates in the case of no covariates."], ["Bivariate Distributions with Exponential Conditionals", null], ["The Shapiro-Wilk Test for Exponentiality Based on Censored Data", "In this article we present a modification of the Shapiro-Wilk (1972) exponentiality test when the sample is censored. The test statistic is constructed by using the normalized waiting times based on the sample data, and is shown to have the same null distribution as the uncensored case with a corresponding reduction in sample size. Stephens's (1978) modification of the Shapiro-Wilk statistic for known origin is also modified, to allow censoring in the sample. Again, the test is constructed using the normalized waiting times; it also has the same null distribution as the uncensored case, with a reduction in the sample size. We compare the power of our test with the power of the Brain and Shapiro (1983) regression tests, using Monte Carlo simulation. Our test compares favorably to the regression test, and it often does better in the case of left censoring. We then demonstrate how our results may be used in a test for uniformity, using the probability integral transformation. Finally, we give an explicit expression for the null distribution of the Shapiro-Wilk test in the neighborhood of its upper tail."], [null, null], ["Ordinal Association in Contingency Tables: Some Interpretive Aspects", "Two families of models for ordered contingency tables\u2014Goodman's association models and canonical correlation models\u2014are investigated and compared with respect to the interpretation of their parameters. We show that the two families of models actually refer to different kinds of ordinal association: stochastic order extremity for correlation models and stochastic order entropy for association models. This difference is related to the way the two models scale interaction. The scale difference is proven to be of substantial consequence, especially under strong association."], [null, null], ["A Warning on the Use of Chi-Squared Statistics with Frequency Tables with Small Expected Cell Counts", "When applied to frequency tables with small expected cell counts, Pearson chi-squared test statistics may be asymptotically inconsistent even in cases in which a satisfactory chi-squared approximation exists for the distribution under the null hypothesis. This problem is particularly important in cases in which the number of cells is large and the expected cell counts are quite variable. To illustrate this bias of the chi-squared test, this article considers the Pearson chi-squared test of the hypothesis that the cell probabilities for a multinomial frequency table have specified values. In this case, the expected value and variance of the Pearson chi-square may be evaluated under both the null and alternative hypotheses. When the number of cells is large, normal approximations and discrete Edgeworth expansions may also be used to assess the size and power of the Pearson chi-squared test. These analyses show that unless all cell probabilities are equal, it is possible to select a significance level and cell probabilities under the alternative hypothesis such that the power is less than the size of the test. As shown by exact calculations, the difference may be substantial even in cases in which all expected cell sizes are at least 5 under the null hypothesis. The use of moments shows that given any minimum expected cell size under the null hypothesis and given any significance level, it is possible to make the power arbitrarily close to 0 by the selection of a large enough number of cells in the table and suitable cell probabilities for the null and alternative hypotheses. The normal approximations for the distribution of the Pearson chi-squared statistic permit the size of this bias to be assessed in less-extreme cases involving tables with many cells. These results imply that caution must be exercised in the application of Pearson chi-squared statistics to sparse contingency tables with many cells. An alternative to the Pearson chi-square, proposed by Zelterman (1986), avoids some of the problems. Exact calculation, however, shows that the alternative statistic does not eliminate all problems of bias. The problems described in this article clearly extend to more general applications of the Pearson chi-squared statistic."], ["Book Reviews", null], ["Letters to the Editor", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["Statistical Practice and Research: The Essential Interactions", null], ["Statistical Models for Earthquake Occurrences and Residual Analysis for Point Processes", null], ["An Error-Components Model for Prediction of County Crop Areas Using Survey and Satellite Data", "Knowledge of the area under different crops is important to the U.S. Department of Agriculture. Sample surveys have been designed to estimate crop areas for large regions, such as crop-reporting districts, individual states, and the United States as a whole. Predicting crop areas for small areas such as counties has generally not been attempted, due to a lack of available data from farm surveys for these areas. The use of satellite data in association with farm-level survey observations has been the subject of considerable research in recent years. This article considers (a) data for 12 Iowa counties, obtained from the 1978 June Enumerative Survey of the U.S. Department of Agriculture and (b) data obtained from land observatory satellites (LANDSAT) during the 1978 growing season. Emphasis is given to predicting the area under corn and soybeans in these counties. A linear regression model is specified for the relationship between the reported hectares of corn and soybeans within sample segments in the June Enumerative Survey and the corresponding satellite determination for areas under corn and soybeans. A nested-error model defines a correlation structure among reported crop hectares within the counties. Given this model, the mean hectares of the crop per segment in a county is defined as the conditional mean of reported hectares, given the satellite determinations and the realized (random) county effect. The mean hectares of the crop per segment is the sum of a fixed component, involving unknown parameters to be estimated and a random component to be predicted. Variance-component estimators in the nested-error model are defined, and the generalized least-squares estimators of the parameters of the linear model are obtained. Predictors of the mean crop hectares per segment are defined in terms of these estimators. An estimator of the variance of the error in the predictor is constructed, including terms arising from the estimation of the parameters of the model. Predictions of mean hectares of corn and soybeans per segment for the 12 Iowa counties are presented. Standard errors of the predictions are compared with those of competing predictors. The suggested predictor for the county mean crop area per segment has a standard error that is considerably less than that of the traditional survey regression predictor."], ["Mixed Model for Analyzing Geographic Variability in Mortality Rates", "A mixed model is proposed for the analysis of geographic variability in mortality rates. In addition to demographic parameters and random geographic parameters, the model includes additional random-effects parameters to adjust for extra-Poisson variability. The model uses a gamma-Poisson distribution with a random scale parameter having an inverse gamma prior. An empirical Bayes approach is used to estimate relative risks for geographic regions and annual rates for demographic groups within each region. Lung cancer in Missouri is used to motivate and illustrate the procedure. Observed disease-specific death rates of specific age/sex groups, within regional units such as counties or cities, are generally quite unreliable for all but the largest units. The amount of information available from any one unit is generally limited. But modeling the variability between and within units can improve estimates, as demonstrated frequently in empirical Bayes examples. A numerical comparison with the fixed effects multiplicative Poisson model demonstrates the considerable flexibility the random effects model has in showing how geographic effects change over different age/sex groups. Computing maximum likelihood estimates of hyper-parameters requires a fair amount of work, since the solution is iterative and requires numerical integration. Expressions are provided to facilitate computation for similar problems."], ["An Interactive PC-Based Procedure for Reliability Assessment Incorporating Expert Opinion and Survival Data", "In this article I present a new approach for the analysis of life-length data that can be described by a Weibull distribution. The novel feature of my approach pertains to the incorporation of informed judgment or expert opinion into the analysis, using a personal computer in an interactive and user-friendly manner. This approach also provides for incorporating the analyst's opinions on the expertise of the experts. Many of the analyses result in graphical displays, making the development attractive to users not comfortable with analytical expressions."], [null, null], ["Regression Analysis for Categorical Variables with Outcome Subject to Nonignorable Nonresponse", "We develop a log-linear model for categorical response subject to nonignorable nonresponse. The paper differs from Fay (1986) in its focus on estimation and hypothesis testing in a regression setting, as opposed to imputation in a multivariate setting. We present several new results concerning the existence of solutions on the boundary of the parameter space and the construction of confidence intervals for estimates. We illustrate the method by estimating the proportion of voters preferring Truman in a 1948 preelection poll (Mosteller, Hyman, McCarthy, Marks, and Truman 1949). Results may depend strongly on the model assumed for nonresponse; goodness-of-fit tests are available for comparing alternative models."], ["Police Responses to Family Violence Incidents", null], ["Estimating Continuous-Time Processes Subject to Time Deformation", "A class of time series models is presented in which variables evolve on a data-based rather than calendar time scale. The discrete calendar-time model thus obtained exhibits time-varying parameters and conditional heteroscedasticity. Using a procedure based on the Kalman filter, univariate models are estimated for postwar U.S. real gross national product (GNP) and short- and long-term interest rates. The results indicate significant time scale nonlinearities."], ["How Far are Automatically Chosen Regression Smoothing Parameters from their Optimum?", "We address the problem of smoothing parameter selection for nonparametric curve estimators in the specific context of kernel regression estimation. Call the \u201coptimal bandwidth\u201d the minimizer of the average squared error. We consider several automatically selected bandwidths that approximate the optimum. How far are the automatically selected bandwidths from the optimum? The answer is studied theoretically and through simulations. The theoretical results include a central limit theorem that quantifies the convergence rate and gives the differences asymptotic distribution. The convergence rate turns out to be excruciatingly slow. This is not too disappointing, because this rate is of the same order as the convergence rate of the difference between the minimizers of the average squared error and the mean average squared error. In some simulations by John Rice, the selectors considered here performed quite differently from each other. We anticipated that these differences would be reflected in different asymptotic distributions for the various selectors. It is surprising that all of the selectors have the same limiting normal distribution. To provide insight into the gap between our theoretical results and these simulations, we did a further Monte Carlo study. Our simulations support the theoretical results, and suggest that the differences observed by Rice seemed to be principally due to the choice of a very small error standard deviation and the choice of error criterion. In the example considered here, the asymptotic normality result describes the empirical distribution of the automatically chosen bandwidths quite well, even for small samples."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Bootstrapping in Nonparametric Regression: Local Adaptive Smoothing and Confidence Bands", "The operation of the bootstrap in the context of nonparametric regression is considered. Bootstrap samples are taken from estimated residuals to study the distribution of a suitably recentered kernel estimator. The application of this principle to the problem of local adaptive choice of bandwidth and to the construction of confidence bands is investigated and compared with a direct method based on asymptotic means and variances. The technique of the bootstrap is to replace any occurrence of the unknown distribution in the definition of the statistical function of interest by the empirical distribution function of the observed errors. In a regression context these errors are not directly observed, although their role can be played by the residuals from the fitted model. In this article the fitted model is a kernel nonparametric regression estimator. Since nonparametric smoothing is involved, an additional difficulty is created by the bias incurred in smoothing. This bias, however, can be estimated in a consistent fashion. These considerations suggest the way in which the distribution of the nonparametric estimate about the true curve at some point of interest may be approximated by suitable recentering of the nonparametric estimates based on bootstrap samples. The bootstrap samples are constructed by adding to the observed estimate errors, which are randomly chosen without replacement from the collection of recentered and bias-corrected residuals from the original data. A theorem is proved to establish that the bootstrap distribution approximates the distribution of interest in terms of the Mallows metric. Two applications are considered. The first uses bootstrap sampling to approximate the mean squared error of the nonparametric estimate at some point of interest. This can then be minimized over the smoothing parameter to adapt the degree of smoothing applied at any point to the local behavior of the underlying curve. The second application uses the percentiles of the approximate distribution to construct confidence intervals for the curve at specific design points. In both of these cases the performance of the bootstrap is compared with a simple \u201cplug-in\u201d method based on direct estimation of the terms in an asymptotic expansion. The performances of the two methods are in general very similar. The bootstrap, however, has the slight advantage of not being as sensitive as the direct method to second derivatives near 0 in the local adaptive smoothing problem. In addition, in the construction of confidence intervals the bootstrap is able to reflect features such as skewness but falls slightly short of target confidence intervals as a result of inaccuracies in centering when the second derivative of the curve is high."], ["Asymptotic Equivalence of Ordinary Least Squares and Generalized Least Squares in Regressions with Integrated Regressors", null], ["The Reduction of the Width of Confidence Bands in Linear Regression", "When constructing a confidence band in the linear regression model, there have been various efforts made to reduce the width of the band. One of them is to change the shape of a confidence band. Another effort is to restrict the region of the independent variables. This article proposes another possibility to narrow down the width of a band, which is to change the curvature of the band. Following that a theorem is given to combine these devices. Numerical comparisons are carried out to see how the improvement due to the restriction of the independent variable works depending on the value of the curvature and the size of the restriction region for a couple of given band shapes. The results are shown by graphs. The following facts are observed. An improvement by restricting the region of the independent variables without changing the curvature of the band, which is my predecessors' method, does not bring the minimum average width. We must adjust the curvature at the same time to obtain the best band. Moreover, for a certain size of the restriction region, the average band width can be considerably reduced by choosing a suitable curvature of the band, even if the restriction is not taken into consideration. Especially when the shape of the confidence band is trapezoidal, the restriction of the independent variables may not work successfully for the purpose of reducing the band width, whereas the mere change of the curvature does. Provided we choose a suitable value of the curvature, the choice of the band shape does not affect the reduction of the band width."], ["Alternative Transformations to Handle Extreme Values of the Dependent Variable", null], ["How Nearly Can Model-Based Prediction and Design-Based Estimation Be Reconciled?", "Two general linear model-based predictors, one of the expectation of a finite population total and one of that total itself, are compared with the design-based generalized regression estimator (GRE). First, the predictors are made to conform to the GRE by modifying the regression parameter estimators but retaining the same (optimal) inclusion probabilities. Second, the GRE is made to conform with each of the predictors in turn by modifying the inclusion probabilities but retaining the generalized least squares (GLS) or best linear unbiased form for the estimators of the regression parameters. It is shown that the choice of inclusion probabilities is more important asymptotically than the choice of estimator for the regression parameters and hence that predictors obtained by the first method generally have smaller asymptotic expected variances than those obtained by the second method. Using the first method, certain special cases are shown to correspond to familiar estimators. If there is only one explanatory variable and no constant term in the model, the predictor of the expectation of the finite population total obtained by the first procedure is identical to the Horvitz-Thompson ratio estimator. If the finite population total itself is to be predicted, the estimator is that suggested by Brewer (1979). If there is a constant term in the model, the GLS predictor can be conformed to the GRE by replacing the inverse-variance weights used to estimate the regression coefficients by functions of the (optimal) inclusion probabilities. It is shown further that appropriate estimators can be constructed in the general case by the use of an appropriate instrumental variable. Under fairly weak conditions this variable can be constructed by deleting a column from a matrix used in the calculation of the GLS estimator and replacing it by a column of weights that are simple functions of the inclusion probabilities. Illustrative examples are given."], ["Optimum Tests for Fixed Effects and Variance Components in Balanced Models", null], ["The Equivalence of Backward Elimination and Multiple Comparisons", null], ["A Bayesian Solution to the Multivariate Behrens\u2014Fisher Problem", null], [null, null], ["Negative Regret, Optional Stopping, and the Elimination of Outliers", null], ["A Generalization of the Kalman Filter for Models with State-Dependent Observation Variance", null], ["A Study toward a Dynamic Theory of Seasonality for Economic Time Series", "Several economists, notably Plosser (1978), Sargent (1978), and Wallis (1978) refuted the assumption that the seasonal component of endogenous variables in a dynamic economic model has almost all of its power restricted to what is termed seasonal frequency and its harmonics. In this article, a model with a closed-form solution is formulated to provide more insight into the arguments put forward by economists. It is concluded that univariate seasonal adjustment cannot be considered a harmless simplification of data without loss of information, neither for the interpretation of economic time series nor for regression analysis."], ["Concomitant-Variable Latent-Class Models", "This article introduces and illustrates a new type of latent-class model in which the probability of latent-class membership is functionally related to concomitant variables with known distribution. The function (or so-called submodel) may be logistic, exponential, or another suitable form. Concomitant-variable models supplement latent-class models incorporating grouping by providing more parsimonious representations of data for some cases. Also, concomitant-variable models are useful when grouping models involve a greater number of parameters than can be meaningfully fit to existing data sets. Although parameter estimates may be calculated using standard iterative procedures such as the Newton\u2014Raphson method, sample analyses presented here employ a derivative-free approach known as the simplex method. A general procedure for imposing linear constraints on the parameter estimates is introduced. A data set involving arithmetic test items in a mastery testing context is used to illustrate fitting and comparison of concomitant-variable models."], ["Homogeneity Tests against Central-Mixture Alternatives", null], ["Statistical Models for Intercepted Data", null], ["The Power Function of Conditional Log-Linear Model Tests", null], ["A Graphical Method for Assessing Goodness of Fit in Cox's Proportional Hazards Model", "Suggested here is a simple graphical method for studying the goodness of fit in Cox's regression model for survival data. The method is easy to use, as it does not require the estimation of alternative models and only involves quantities similar to those already appearing in the partial likelihood expression that is needed in the parameter estimation. The rationale behind the graphs is very intuitive: They make a direct comparison between observed and expected failure frequencies, as estimated from the model. In a correctly specified model one anticipates an approximate balance between such frequencies; otherwise there will typically be groups of individuals for which the expected frequencies are systematically too high or too low to match with the data, and this shows in the graphs introduced here. In the concrete applications of the method the individuals are stratified in a way that depends on what aspect of the model is being checked against data. There is always one graph for each stratum. Simulated and real data are used to illustrate the method. In the simulations two types of defect that can come up in a Cox's model are considered: (a) an influential covariate has been deleted from the model, and (b) a common baseline hazard for all individuals has been assumed in a case in which the individuals should be stratified according to baseline hazard. Serious defects in the model are relatively easy to detect from the diagnostic graphs. As concrete applications of the method, studied briefly are the fitting of Cox's model to the well-known Stanford heart transplant data and to a data set describing the survival of malignant melanoma patients after operation. The article concludes with some general observations concerning the randomness in the graphs."], ["Exact Kolmogorov-Type Tests for Left-Truncated and/or Right-Censored Data", "The tests proposed in this article are natural generalizations of the ordinary Kolmogorov\u2014Smirnov one-sample tests based on the product-limit estimator. They can be used with left-truncated and/or right-censored data and, as the ordinary tests, they have the nice properties of being valid for any sample size and any underlying distribution specified by the null hypothesis. The acceptance region for the product-limit estimator consists of a Kolmogorov\u2014Smirnov-type region that is suitably reduced or enlarged to the right of each point at which there is a change in the number \u201cat risk\u201d due to entries and/or censorship."], ["Pearson-Type Goodness-of-Fit Tests: The Univariate Case", null], ["Resampling Inference with Complex Survey Data", null], ["Some Results on Robust Estimation in Finite Population Sampling", "The present article examines the robustness of sampling strategies, that is, estimators and sample designs, from the point of view of attaining the minimum value of the Godambe\u2014Joshi lower bound to the expected variance of a strategy. It is shown that asymptotic design unbiasedness is necessary but insufficient for robustness defined in the above sense. The concepts of weak and strong robustness of estimators, for given sampling strategies, are introduced. The article establishes that a sufficient condition for a sampling strategy to be robust is that the estimator is model unbiased and weakly robust. Sufficient conditions are also given for an estimator to be weakly robust. These conditions are then used to examine the robustness of strategies when the design matrix or the covariance matrix of the superpopulation model is misspecified. The robustness of model-based predictors is also examined."], ["A Biweight Prediction Interval for Random Samples", null], ["The Bahadur Efficiency of Tests of Some Joint Hypotheses", "This article examines the relative efficiency of finite induced and Wald-like (or \u201cinfinite induced\u201d) tests of some commonly encountered joint hypotheses. One two-sided testing problem and two types of two-sided testing problems concerning two parameters and normally distributed estimates are considered. The exact slopes of six test statistics are derived, and from these the Bahadur relative efficiency of tests based on any pair of the statistics can be easily computed. For the problems considered, Bahadur relative efficiency is equal to limiting Pitman efficiency, whereas Pitman relative efficiency coincides with exact finite sample relative efficiency. I present numerical results showing that Bahadur relative efficiency generally approximates Pitman efficiency rather poorly, although the patterns exhibited by these two efficiency criteria bear a reasonable similarity in most cases."], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"]]}