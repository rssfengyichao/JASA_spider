{"2005": [["Estimating Risks of Identification Disclosure in Microdata", "When statistical agencies release microdata to the public, malicious users (intruders) may be able to link records in the released data to records in external databases. Releasing data in ways that fail to prevent such identifications may discredit the agency or, for some data, constitute a breach of law. To limit disclosures, agencies often release altered versions of the data; however, there usually remain risks of identification. This article applies and extends the framework developed by Duncan and Lambert for computing probabilities of identification for sampled units. It describes methods tailored specifically to data altered by recoding and topcoding variables, data swapping, or adding random noise (and combinations of these common data alteration techniques) that agencies can use to assess threats from intruders who possess information on relationships among variables and the methods of data alteration. Using data from the Current Population Survey, the article illustrates a step-by-step process for evaluating identification disclosure risks for competing releases under varying assumptions of intruders' knowledge. Risk measures are presented for individual units and for entire datasets."], ["Forecasts From Nonrandom Samples", "The 1990s was not the best of decades for electoral polls, with striking errors occurring in, among others, the British, French, and Spanish elections, including election night, when errors are more evident. This article proposes a model for predicting final election outcomes based on the consistency that polling stations show between elections. Using both past and incoming polling station vote proportions, the model produces continuously revised predictions. The method is validated predicting the 1995 Corts Valencianes (Valencia regional parliament) elections and displaying the real-time experience of the 1999 Corts Valencianes election night. The case study is completed by demonstrating the technique's efficacy in three additional elections. The results confirm that the procedure generates quick, highly reliable, and accurate forecasts. In fact, only a few minutes after starting the scrutiny, the proposal permits one to approximate the final results with great precision, even with only a small percentage of votes polled. The great flexibility of the procedure makes it possible to use the method under a wide variety of circumstances and electoral systems. Furthermore, this procedure has additional advantages, including robustness and lower cost, over other methods which can also be implemented during election night with the objective of forecasting final outcomes, like exit polls or quick counts of a meaningful sample of polling stations."], ["Imputation of Binary Treatment Variables With Measurement Error in Administrative Data", "Administrative systems\u2014specifically, cancer registries\u2014can be a valuable data source for studies of health care; however, provision of adjuvant chemotherapy or radiation therapy is often underreported in such databases. In a study of colorectal cancer in California, a relatively small physician follow-back survey allowed us to model the probability of underreporting. We then wished to model the relationship of true treatment status to covariates in the full database. We developed hierarchical models for imputation of corrected data using data recorded with error in the administrative system and the \u201cvalidation sample\u201d from the survey. The model includes a model for the probability of receipt of chemotherapy and a model for the probability of reporting given that chemotherapy was received. This factorization of the joint distribution of the true status and reported data is designed to permit generalization from the validation sample to a larger population in which the reporting process is similar but the prevalence of treatment may differ. Hospital random effects are included to represent variation in both treatment and reporting patterns across hospitals. We used Markov chain Monte Carlo simulation techniques to estimate model parameters and impute true treatment status. Valid inferences are obtained by combining the results from multiply imputed datasets. In an analysis of predictors of survival using imputed data that corrected for bias due to underreporting, uncertainty due to underreporting of chemotherapy substantially inflated the variance of estimates of the chemotherapy effect but had little effect on the estimation of coefficients of other characteristics."], ["A Kernel-Based Spatio-Temporal Dynamical Model for Nowcasting Weather Radar Reflectivities", "A good short-period forecast of heavy rainfall is essential for many meteorological and hydrological applications. Traditional deterministic and stochastic nowcasting methodologies have been inadequate in their characterization of pixelwise rainfall reflectivity propagation, intensity, and uncertainty. The methodology presented herein uses an approach that efficiently parameterizes spatio-temporal dynamic models in terms of integro-difference equations within a hierarchical framework. The approach accounts for the uncertainty in the prediction and provides relevant distributional information concerning the nowcast. An application is presented that shows the effectiveness of the technique and its potential for nowcasting weather radar reflectivities."], ["Maximization by Parts in Likelihood Inference", "This article presents and examines a new algorithm for solving a score equation for the maximum likelihood estimate in certain problems of practical interest. The method circumvents the need to compute second-order derivatives of the full likelihood function. It exploits the structure of certain models that yield a natural decomposition of a very complicated likelihood function. In this decomposition, the first part is a log-likelihood from a simply analyzed model, and the second part is used to update estimates from the first part. Convergence properties of this iterative (fixed-point) algorithm are examined, and asymptotics are derived for estimators obtained using only a finite number of iterations. Illustrative examples considered in the article include multivariate Gaussian copula models, nonnormal random-effects models, generalized linear mixed models, and state-space models. Properties of the algorithm and of estimators are evaluated in simulation studies on a bivariate copula model and a nonnormal linear random-effects model."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Artificially Augmented Samples, Shrinkage, and Mean Squared Error Reduction*", "An inequality is provided that determines when shrinkage reduces the mean squared error (MSE) of an unbiased estimate. Artificially augmented samples are then used to obtain, among others, shrinkage estimates of the population's variance and covariance, which improve the unbiased estimates for all parameter values and for all probability models with marginals having finite second moments, and alternative jackknife estimates that complement the usual jackknife estimates in reducing the MSE."], ["Robust Estimation in Generalized Partial Linear Models for Clustered Data", "In this article we consider robust generalized estimating equations for the analysis of semiparametric generalized partial linear models (GPLMs) for longitudinal data or clustered data in general. We approximate the nonparametric function in the GPLM by a regression spline, and use bounded scores and leverage-based weights in the estimating equation to achieve robustness against outliers. We show that the regression spline approach avoids some of the intricacies associated with the profile-kernel method, and that robust estimation and inference can be carried out operationally as if a generalized linear model were used."], ["Univariate Nonparametric Regression in the Presence of Auxiliary Covariates", "This article addresses the problem of finding a relationship between the univariate predictor and the response when regression errors, created in part by known auxiliary covariates, are too large for a reliable regression estimation. A typical example is a controlled random design experiment with a large number of covariates, where the statistician is interested in the effect of a particular covariate and this effect is blurred by a large regression noise created by other covariates. This article develops a theory of asymptotically optimal nonparametric univariate regression estimation in the presence of auxiliary covariates. Here optimality means mimicking the performance of an oracle that knows the effects of auxiliary covariates on the response. The asymptotic theory shows that such an optimal estimation is possible, and also explains how to evaluate the noise created by auxiliary covariates and how to develop an estimator for the interesting case of small sample sizes. The concept of modeling regression noise is well known in analysis of covariance (ANCOVA), and here it is applied in the optimal way to a nonparametric regression setting. A procedure for small sample sizes, denoised scattergram, is tested on simulated examples and a real dataset with 84 observations and 9 auxiliary covariates; the results justify the practical feasibility of the developed method. The method also allows a practitioner to visualize how a dataset would appear if the effects of auxiliary covariates were eliminated and to determine why an exhibited regression function has any given particular shape. Many practical recommendations (in particular, how to use known shape restrictions) are presented and discussed. The asymptotic theory, a numerical study, and analysis of a real dataset indicate that the proposed method of reducing the variance of regression errors created by auxiliary covariances is feasible, is easy to implement, and improves the likelihood of a meaningful regression analysis."], ["Combining Linear Regression Models", "Model-combining (i.e., mixing) methods have been proposed in recent years to deal with uncertainty in model selection. Even though advantages of model combining over model selection have been demonstrated in simulations and data examples, it is still unclear to a large extent when model combining should be preferred. In this work, first we propose an instability measure to capture the uncertainty of model selection in estimation, called perturbation instability in estimation (PIE), based on perturbation of the sample. We demonstrate that estimators from model selection can have large PIE values and that model combining substantially reduces the instability for such cases. Second, we propose a model combining method, adaptive regression by mixing with model screening (ARMS), and derive a theoretical property. In ARMS, a screening step is taken to narrow down the list of candidate models before combining, which not only saves computing time, but also can improve estimation accuracy. Third, we compare ARMS with EBMA (an empirical Bayesian model averaging) and model selection methods in a number of simulations and real data examples. The comparison shows that model combining produces better estimators when the instability of model selection is high and that ARMS performs better than EBMA in most such cases in our simulations. With respect to the choice between model selection and model combining, we propose a rule of thumb in terms of PIE. The empirical results support that PIE is a sensible indicator of model selection instability in estimation and is useful for understanding whether model combining is a better choice over model selection for the data at hand."], ["Efficient Empirical Bayes Variable Selection and Estimation in Linear Models", "We propose an empirical Bayes method for variable selection and coefficient estimation in linear regression models. The method is based on a particular hierarchical Bayes formulation, and the empirical Bayes estimator is shown to be closely related to the LASSO estimator. Such a connection allows us to take advantage of the recently developed quick LASSO algorithm to compute the empirical Bayes estimate, and provides a new way to select the tuning parameter in the LASSO method. Unlike previous empirical Bayes variable selection methods, which in most practical situations can be implemented only through a greedy stepwise algorithm, our method gives a global solution efficiently. Simulations and real examples show that the proposed method is very competitive in terms of variable selection, estimation accuracy, and computation speed compared with other variable selection and estimation methods."], ["Quantiles for Counts", "This article studies the estimation of conditional quantiles of counts. Given the discreteness of the data, some smoothness must be artificially imposed on the problem. We show that it is possible to smooth the data in a way that allows inference to be performed using standard quantile regression techniques. The performance and implementation of the estimators are illustrated by simulations and an application."], ["Nonparametric Estimation of an Additive Quantile Regression Model", null], ["Weighted Estimators for Proportional Hazards Regression With Missing Covariates", "Missing covariate data are common in epidemiologic studies and disease prevention trials. In this article regression parameter estimation in the Cox proportional hazards model is considered when certain covariates are observed for all study subjects and other covariate data are collected only for a subset. The article presents both simple weighted and kernel-assisted fully augmented weighted estimators that use the partially incomplete data nonparametrically. We use nonparametric methods to estimate selection probabilities in the simple weighted estimating functions. We also use nonparametric kernel smoothing techniques to estimate certain conditional expectations in fully augmented weighted estimating functions. The proposed methods are nonparametric in the sense that they require neither a model for the missing-data mechanism nor specification of the conditional distribution of missing covariates given observed covariates. These estimators allow the missing-data mechanism to depend on outcome variables and observed covariates, and they are applicable to various cohort sampling procedures, including case-cohort and nested case-control designs. We show that the simple and the kernel-assisted fully augmented weighted estimators are typically consistent and asymptotically normal. Moreover, the proposed estimators are more efficient than the simple weighted estimator with the inverse of true selection probability as weight. They also correct the bias of estimates from analysis of the complete data alone when the missing-data mechanism depends on outcome variables. In addition, when covariates are time-independent, certain simple weighted estimators are shown to be asymptotically equivalent to the kernel-assisted fully augmented weighted estimators. Moderate sample size performance of the estimators is examined via simulation and by application to two real datasets."], ["A Pseudo\u2013Partial Likelihood Method for Semiparametric Survival Regression With Covariate Errors", "This article presents an estimator for the regression coefficient vector in the Cox proportional hazards model with covariate error. The estimator is obtained by maximizing a likelihood-type function similar to the Cox partial likelihood. The likelihood function involves the cumulative baseline hazard function, for which a simple estimator is substituted. The method is capable of handling general covariate error structures; it is not restricted to the independent additive error model. It can be applied to studies with either an external or internal validation sample, and also to studies with replicate measurements of the surrogate covariate. The estimator is shown to be consistent and asymptotically normal, and an estimate of the asymptotic covariance matrix is derived. Some extensions to general transformation survival models are indicated. Simulation studies are presented for a setup with a single error-prone binary covariate and a setup with a single error-prone normally distributed covariate. These simulation studies show that the method typically produces estimates with low bias and confidence intervals with accurate coverage rates. Efficiency results relative to fully parametric maximum likelihood are also presented. The method is applied to data from the Framingham Heart Study."], ["Hierarchical Mixture Modeling With Normalized Inverse-Gaussian Priors", "In recent years the Dirichlet process prior has experienced a great success in the context of Bayesian mixture modeling. The idea of overcoming discreteness of its realizations by exploiting it in hierarchical models, combined with the development of suitable sampling techniques, represent one of the reasons of its popularity. In this article we propose the normalized inverse-Gaussian (N\u2013IG) process as an alternative to the Dirichlet process to be used in Bayesian hierarchical models. The N\u2013IG prior is constructed via its finite-dimensional distributions. This prior, although sharing the discreteness property of the Dirichlet prior, is characterized by a more elaborate and sensible clustering which makes use of all the information contained in the data. Whereas in the Dirichlet case the mass assigned to each observation depends solely on the number of times that it occurred, for the N\u2013IG prior the weight of a single observation depends heavily on the whole number of ties in the sample. Moreover, expressions corresponding to relevant statistical quantities, such as a priori moments and the predictive distributions, are as tractable as those arising from the Dirichlet process. This implies that well-established sampling schemes can be easily extended to cover hierarchical models based on the N\u2013IG process. The mixture of N\u2013IG process and the mixture of Dirichlet process are compared using two examples involving mixtures of normals."], ["On Consistency of Nonparametric Normal Mixtures for Bayesian Density Estimation", "The past decade has seen a remarkable development in the area of Bayesian nonparametric inference from both theoretical and applied perspectives. As for the latter, the celebrated Dirichlet process has been successfully exploited within Bayesian mixture models, leading to many interesting applications. As for the former, some new discrete nonparametric priors have been recently proposed in the literature that have natural use as alternatives to the Dirichlet process in a Bayesian hierarchical model for density estimation. When using such models for concrete applications, an investigation of their statistical properties is mandatory. Of these properties, a prominent role is to be assigned to consistency. Indeed, strong consistency of Bayesian nonparametric procedures for density estimation has been the focus of a considerable amount of research; in particular, much attention has been devoted to the normal mixture of Dirichlet process. In this article we improve on previous contributions by establishing strong consistency of the mixture of Dirichlet process under fairly general conditions. Besides the usual Kullback\u2013Leibler support condition, consistency is achieved by finiteness of the mean of the base measure of the Dirichlet process and an exponential decay of the prior on the standard deviation. We show that the same conditions are also sufficient for mixtures based on priors more general than the Dirichlet process. This leads to the easy establishment of consistency for many recently proposed mixture models."], ["Outlier Robust Model Selection in Linear Regression", "We propose a new approach to the selection of regression models based on combining a robust penalized criterion and a robust conditional expected prediction loss function that is estimated using a stratified bootstrap. Both components of the procedure use robust criteria (i.e., robust \u03c1-functions) rather than squared error loss to reduce the effects of large residuals and poor bootstrap samples. A key idea is to separate estimation from model selection by choosing estimators separately from the \u03c1-function. Using the stratified bootstrap further reduces the likelihood of obtaining poor bootstrap samples. We show that the model selection procedure is consistent under some conditions and works well in our simulations. In particular, we find that simultaneous minimization of prediction error and conditional expected prediction loss is better than separate minimization of the prediction error or the conditional expected prediction loss."], ["A Generalized Wang\u2013Landau Algorithm for Monte Carlo Computation", "Inference for a complex system with a rough energy landscape is a central topic in Monte Carlo computation. Motivated by the successes of the Wang\u2013Landau algorithm in discrete systems, we generalize the algorithm to continuous systems. The generalized algorithm has some features that conventional Monte Carlo algorithms do not have. First, it provides a new method for Monte Carlo integration based on stochastic approximation; second, it is an excellent tool for Monte Carlo optimization. In an appropriate setting, the algorithm can lead to a random walk in the energy space, and thus it can sample relevant parts of the sample space, even in the presence of many local energy minima. The generalized algorithm can be conveniently used in many problems of Monte Carlo integration and optimization, for example, normalizing constant estimation, model selection, highest posterior density interval construction, and function optimization. Our numerical results show that the algorithm outperforms simulated annealing and parallel tempering in optimization for the system with a rough energy landscape. Some theoretical results on the convergence of the algorithm are provided."], ["Linear Unmixing of Multivariate Observations", "In many fields of science there are multivariate observations that may be assumed to be generated by a (physical) linear mixing process of contributions from different sources. If the compositions of the sources are constant for different observations, then these observations are, up to a random error term, nonnegative linear combinations of a fixed set of so-called \u201csource profiles\u201d that characterize the sources. The goal of linear unmixing is to recover both the source profiles and the source activities (also called scores) from a multivariate dataset. We present a new parametric mixing model that assumes a multivariate lognormal distribution for the scores. This model is proved to be identifiable. Moreover, consistency and asymptotic normality of the maximum likelihood estimator (MLE) are established in special cases. To calculate the MLE, we propose the combination of two variants of the Monte Carlo EM algorithm. The proposed model is applied to simulated datasets and to a set of air pollution measurements. In addition to the basic model, several extensions are discussed."], ["A Fast, Optimal Spatial-Prediction Method for Massive Datasets", "This article considers a class of multiresolution tree-structured models that are spatially shifted versions of each other and proposes a new spatial-prediction method that averages over the optimal spatial predictors produced from members of this class of models. As a consequence, the resulting predicted surface is smooth, even when the predictors generated separately from individual multiresolution tree-structured models are not. We call the new predictor the multiresolution spatial (MURS) predictor and develop a computationally efficient algorithm for it. The algorithm can handle massive datasets even when some observations are missing. Moreover, the MURS predictor can be shown to be the minimum mean squared error predictor for a large class of covariance functions. A simulation example for massive datasets shows that the MURS method consistently outperforms two commonly used filtering methods. Total column ozone data remotely sensed from a satellite are analyzed using the new methodology."], ["Multiscale, Multigranular Statistical Image Segmentation", null], ["Optimal Design for Goodness-of-Fit of the Michaelis\u2013Menten Enzyme Kinetic Function", null], ["Diverging Moments and Parameter Estimation", null], ["A Tale of Two Time Scales", "It is a common practice in finance to estimate volatility from the sum of frequently sampled squared returns. However, market microstructure poses challenges to this estimation approach, as evidenced by recent empirical studies in finance. The present work attempts to lay out theoretical grounds that reconcile continuous-time modeling and discrete-time samples. We propose an estimation approach that takes advantage of the rich sources in tick-by-tick data while preserving the continuous-time assumption on the underlying returns. Under our framework, it becomes clear why and where the \u201cusual\u201d volatility estimator fails when the returns are sampled at the highest frequencies. If the noise is asymptotically small, our work provides a way of finding the optimal sampling frequency. A better approach, the \u201ctwo-scales estimator,\u201d works for any size of the noise."], ["Independent Particle Filters", "Sequential Monte Carlo methods, especially the particle filter (PF) and its various modifications, have been used effectively in dealing with stochastic dynamic systems. The standard PF samples the current state through the underlying state dynamics, then uses the current observation to evaluate the sample's importance weight. However, there is a set of problems in which the current observation provides significant information about the current state but the state dynamics are weak, and thus sampling using the current observation often produces more efficient samples than sampling using the state dynamics. In this article we propose a new variant of the PF, the independent particle filter (IPF), to deal with these problems. The IPF generates exchangeable samples of the current state from a sampling distribution that is conditionally independent of the previous states, a special case of which uses only the current observation. Each sample can then be matched with multiple samples of the previous states in evaluating the importance weight. We present some theoretical results showing that this strategy improves efficiency of estimation as well as reduces resampling frequency. We also discuss some extensions of the IPF, and use several synthetic examples to demonstrate the effectiveness of the method."], ["A Family of Symmetric Distributions on the Circle", "We propose a new family of symmetric unimodal distributions on the circle that contains the uniform, von Mises, cardioid, and wrapped Cauchy distributions, among others, as special cases. The basic form of the densities of this family is very simple, although its normalization constant involves an associated Legendre function. The family of distributions can also be derived by conditioning and projecting certain bivariate spherically and elliptically symmetric distributions on to the circle. Trigonometric moments are available, and a measure of variation is discussed. Aspects of maximum likelihood estimation are considered, and likelihood is used to fit the family of distributions to an example set of data. Finally, extension to a family of rotationally symmetric distributions on the sphere is briefly made."], ["Nonparametric Model Calibration Estimation in Survey Sampling", "Calibration is commonly used in survey sampling to include auxiliary information at the estimation stage of a population parameter. Calibrating the observation weights on population means (totals) of a set of auxiliary variables implies building weights that when applied to the auxiliaries give exactly their population mean (total). Implicitly, calibration techniques rely on a linear relation between the survey variable and the auxiliary variables. However, when auxiliary information is available for all units in the population, more complex modeling can be handled by means of model calibration; auxiliary variables are used to obtain fitted values of the survey variable for all units in the population, and estimation weights are sought to satisfy calibration constraints on the fitted values population mean, rather than on the auxiliary variables one. In this work we extend model calibration considering more general superpopulation models and use nonparametric methods to obtain the fitted values on which to calibrate. More precisely, we adopt neural network learning and local polynomial smoothing to estimate the functional relationship between the survey variable and the auxiliary variables. Under suitable regularity conditions, the proposed estimators are proven to be design consistent. The moments of the asymptotic distribution are also derived, and a consistent estimator of the variance of each distribution is then proposed. The performance of the proposed estimators for finite-size samples is investigated by means of simulation studies. An application to the assessment of the ecological conditions of streams in the mid-Atlantic highlands in the United States is also carried out."], ["Structural Equation Models", "Structural equation models (SEMs) have been discussed extensively in the psychometrics and quantitative behavioral sciences literature. However, many statisticians and researchers in other areas of application are relatively unfamiliar with their implementation. Here we review some of the SEM literature and describe basic methods, using examples from environmental epidemiology. We make connections to recent work on latent variable models for multivariate outcomes and to measurement error methods, and discuss advantages and disadvantages of SEMs compared with traditional regressions. We give a detailed example in which two models fit the same data well, yet one is physiologically implausible. This underscores the critical role of subject matter knowledge in the successful implementation of SEMs. A brief discussion on open research areas is included."], ["Book Reviews", null], ["Optimization", null], ["Permutation, Parametric, and Bootstrap Tests of Hypotheses", null], ["Statistics, Econometrics and Forecasting", null], ["Periodic Time Series Models", null], ["Preparing for the Worst: Incorporating Downside Risk in Stock Market Investments", null], ["Feynman\u2013Kac Formulae: Genealogical and Interacting Particle Systems with Applications", null], ["L\u00e9vy Processes and Stochastic Calculus", null], ["Information Theory, Inference, and Learning Algorithms", null], ["Stereology for Statisticians", null], ["Measurement Theory and Practice: The World Through Quantification", null], ["Multiple Analyses in Clinical Trials: Fundamentals for Investigators", null], ["Statistical Estimation of Epidemiological Risk", null], ["Statistical Methods in Genetic Epidemiology", null], ["Analyzing Microarray Gene Expression Data", null], ["Discovering Knowledge in Data: an Introduction to Data Mining", null], ["Kendall's Advanced Theory of Statistics, Vol. 2B: Bayesian Inference", null], ["Telegraphic Reviews", null], ["2005 Editorial Collaborators", null], ["Hierarchical Graphical Models", "There is continued debate regarding the exact relation between lower cholesterol levels and increased respiratory disease mortality. One of the goals of this study is to reveal the relationship between subcomponents of cholesterol and pulmonary function. We consider the subcomponents of total cholesterol, namely high-density lipoprotein cholesterol and low-density lipoprotein cholesterol, to investigate the relationship of cholesterol levels with pulmonary function in a longitudinal study. To answer these questions, we propose new methodology for hierarchical reciprocal graphical models. We consider the identification and estimation of these models, and propose maximum likelihood estimation using a generalized EM algorithm. A simulation study of the algorithm and the corresponding estimates reveals excellent performance of the proposed procedures. Application of this methodology to the Normative Aging Study reveals complicated associations between pulmonary function and the subcomponents of total cholesterol."], ["Frailty Survival Model Analysis of the National Deceased Donor Kidney Transplant Dataset Using Poisson Variance Structures", "In a recent study of transplant outcomes, donor age, cerebrovascular accident as the cause of death (CVA), renal insufficiency (serum creatinine >1.5 mg/dL), and history of hypertension have been identified as donor factors associated with elevated risk of kidney transplant failure. It is of great interest to know whether there remain other unmeasured donor factors associated with elevated risk of graft failure. In this article we study a sample of 6,024 deceased donor kidney transplants performed in 194 centers from 1995 to 2000. In addition to variation among transplant recipients, there are two other random effects: unmeasured donor and unrecorded center factors (data not available at the physician level). These two random effects are crossed, because the two kidneys from the same donor can be transplanted in different centers. Multivariate frailty models are applied to analyze the data. The likelihood functions of both parametric (e.g., with piecewise constant baseline hazard) and semiparametric multivariate frailty models are shown to be proportional to the likelihood functions of a class of mixed Poisson regression models. The penalized quasi-likelihood method is used as the numerical procedure for these mixed Poisson regression models. Thus we are able to estimate and model crossed random-effects structures for survival analysis. Although about 30% of recipient graft survival rate variation due to donor factors is explained by the measured donor characteristics, the remaining variation among donors in graft survival rate is still statistically significant, suggesting that there may be other unmeasured donor factors associated with a reduced graft survival rate. We also find significant variation of graft failure rates among transplant centers due to unrecorded center factors. Therefore, this study suggests that practice patterns at transplant centers and identification of other donor factors may merit further investigation."], ["The Joint Measurement of Technical and Allocative Inefficiencies", "This article estimates technical and allocative inefficiencies and increase in costs therefrom of individual firms using a translog cost system consisting of the cost function and the cost share equations. We call it a nonlinear random-effects system because technical and allocative inefficiencies are random (hence the term random effects) and are separated from the random noise terms appearing in each equation of the system, and because the inefficiency terms appear in the system in a highly nonlinear fashion, which helps in separating them from random errors. We use Bayesian inference procedures based on Markov chain Monte Carlo (MCMC) techniques to estimate the proposed system. Inferences on firm-specific technical inefficiency and both input-specific and firm-specific allocative inefficiencies are developed using MCMC techniques. We apply the new methods to a sample of 500 U.S. commercial banks. We focus on input allocation problem based on the assumption that banks minimize cost. Empirical results show that cost for the top (bottom) 10% of banks is increased by at least 3% (11%) due to technical inefficiency. In contrast, very few banks are found to be efficient in allocating all the inputs. Costs are increased by 13% on average due to input misallocation. Increase in costs due to both technical and allocative inefficiencies for the top (bottom) 10% of the banks is at least 11% (29%). When translated into dollar figures, this result indicates that elimination of technical and allocative inefficiencies would save the top (bottom) 10% of the banks more than $$.20 ($$3.56) million. We also find that none of the banks in our sample exceeded the efficient scale size, although most of them were operating near their optimum scale (unitary returns to scale)."], ["A Hierarchical Framework for Modeling and Forecasting Web Server Workload", "Proactive management of web server farms requires accurate prediction of workload. An exemplary measure of workload is the amount of service requests per unit time. As a time series, the workload exhibits not only short-term random fluctuations, but also prominent periodic (daily) patterns that evolve randomly from one period to another. A hierarchical framework with multiple time scales is proposed to model such time series. This framework leads to an adaptive procedure that provides both long-term (in days) and short-term (in minutes) predictions with simultaneous confidence bands that accommodate not only serial correlation, but also heavy tailedness, heteroscedasticity, and nonstationarity of the data."], ["Spike and Slab Gene Selection for Multigroup Microarray Data", null], ["Semilinear High-Dimensional Model for Normalization of Microarray Data", "Normalization of microarray data is essential for removing experimental biases and revealing meaningful biological results. Motivated by a problem of normalizing microarray data, a semilinear in-slide model (SLIM) has been proposed. To aggregate information from other arrays, SLIM is generalized to account for across-array information, resulting in an even more dynamic semiparametric regression model. This model can be used to normalize microarray data even when there is no replication within an array. We demonstrate that this semiparametric model has a number of interesting features. The parametric component and the nonparametric component that are of primary interest can be consistently estimated, the former having a parametric rate and the latter having a nonparametric rate, whereas the nuisance parameters cannot be consistently estimated. This is an interesting extension of the partial consistent phenomena, which itself is of theoretical interest. The asymptotic normality for the parametric component and the rate of convergence for the nonparametric component are established. The results are augmented by simulation studies and illustrated by an application to the cDNA microarray analysis of neuroblastoma cells in response to the macrophage migration inhibitory factor."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["A Two-Way Semilinear Model for Normalization and Analysis of cDNA Microarray Data", "A basic question in analyzing cDNA microarray data is normalization, the purpose of which is to remove systematic bias in the observed expression values by establishing a normalization curve across the whole dynamic range. A proper normalization procedure ensures that the normalized intensity ratios provide meaningful measures of relative expression levels. We propose a two-way semilinear model (TW\u2013SLM) for normalization and analysis of microarray data. This method does not make the usual assumptions underlying some of the existing methods. For example, it does not assume that the percentage of differentially expressed genes is small or that there is symmetry in the expression levels of up-regulated and down-regulated genes, as required in the lowess normalization method. The TW\u2013SLM also naturally incorporates uncertainty due to normalization into significance analysis of microarrays. We use a semiparametric approach based on polynomial splines in the TW\u2013SLM to estimate the normalization curves and the normalized expression values. We study the theoretical properties of the proposed estimator in the TW\u2013SLM, including the finite-sample distributional properties of the estimated gene effects and the rate of convergence of the estimated normalization curves when the number of genes under study is large. We also conduct simulation studies to evaluate the TW\u2013SLM method and illustrate the proposed method using a published microarray dataset."], ["The Generalized Dynamic Factor Model", "This article proposes a new forecasting method that makes use of information from a large panel of time series. Like earlier methods, our method is based on a dynamic factor model. We argue that our method improves on a standard principal component predictor in that it fully exploits all the dynamic covariance structure of the panel and also weights the variables according to their estimated signal-to-noise ratio. We provide asymptotic results for our optimal forecast estimator and show that in finite samples, our forecast outperforms the standard principal components predictor."], ["Measurement Error in Linear Autoregressive Models", "Time series data are often subject to measurement error, usually the result of needing to estimate the variable of interest. Although it is often reasonable to assume that the measurement error is additive (i.e., the estimator is conditionally unbiased for the missing true value), the measurement error variances often vary as a result of changes in the population/process over time and/or changes in sampling effort. In this article we address estimation of the parameters in linear autoregressive models in the presence of additive and uncorrelated measurement errors, allowing heteroscedasticity in the measurement error variances. We establish the asymptotic properties of naive estimators that ignore measurement error and propose an estimator based on correcting the Yule\u2013Walker estimating equations. We also examine a pseudo-likelihood method based on normality assumptions and computed using the Kalman filter. We review other techniques that have been proposed, including two that require no information about the measurement error variances, and compare the various estimators both theoretically and via simulations. The estimator based on corrected estimating equations is easy to obtain and readily accommodates (and is robust to) unequal measurement error variances. Asymptotic calculations and finite-sample simulations show that it is often relatively efficient."], ["Estimation of Long Memory in the Presence of a Smooth Nonparametric Trend", "We consider semiparametric estimation of the long-memory parameter of a stationary process in the presence of an additive nonparametric mean function. We use a semiparametric Whittle-type estimator, applied to the tapered, differenced series. Because the mean function is not necessarily a polynomial of finite order, no amount of differencing will completely remove the mean. We establish a central limit theorem for the estimator of the memory parameter, assuming that a slowly increasing number of low frequencies are trimmed from the estimator's objective function. We find in simulations that tapering and trimming, applied either separately or together, are essential for the good performance of the estimator in practice. In our simulation study, we also compare the proposed estimator of the long-memory parameter with a direct estimator obtained from the raw data without differencing or tapering, and finally we study the question of feasible inference for the regression function. We find that the proposed estimator of the long-memory parameter is potentially far less biased than the direct estimator, and consequently that the proposed estimator may lead to more accurate inference on the regression function."], ["Dynamical Correlation for Multivariate Longitudinal Data", "Nonparametric methodology for longitudinal data analysis is becoming increasingly popular. The analysis of multivariate longitudinal data, where data on several time courses are recorded for each subject, has received considerably less attention, despite its importance for practical data analysis. In particular, there is a need for measures and estimates to capture dependency between the components of vector-valued longitudinal data. We propose and analyze a simple and effective nonparametric method to quantify the covariation of components of multivariate longitudinal observations, which are viewed as realizations of a random process. This includes the notion of a correlation between derivatives and time-shifted versions. The concept of dynamical correlation is based on a scalar product obtained from pairs of standardized smoothed curves. The proposed method can be used when observation times are irregular and not matching between subjects or between responses within a subject. For higher-dimensional data, one may construct a dynamical correlation matrix that then serves as a starting point for standard multivariate analysis techniques, such as principal components. We iliustrate our methods via simulations as well as with data on five acute-phase blood proteins measured longitudinally from a study of hemodialysis patients."], ["Semiparametric Regression Analysis of Longitudinal Data With Informative Observation Times", "Statistical analysis of longitudinal data has been discussed by many authors, and a number of methods have been proposed. Most of the research have focused on situations where observation times are independent of or carry no information about the response variable and therefore rely on conditional inference procedures given the observation times. This article considers a different situation, where the independence assumption may not hold; that is, the observation times may carry information about the response variable. For inference, estimating equation approaches are proposed, and both large-sample and final-sample properties of the proposed methods are established. The methodology is applied to a bladder cancer study that motivated this investigation."], ["Nonparametric Inferences for Additive Models", "Additive models with backfitting algorithms are popular multivariate nonparametric fitting techniques. However, the inferences of the models have not been very well developed, due partially to the complexity of the backfitting estimators. There are few tools available to answer some important and frequently asked questions, such as whether a specific additive component is significant or admits a certain parametric form. In an attempt to address these issues, we extend the generalized likelihood ratio (GLR) tests to additive models, using the backfitting estimator. We demonstrate that under the null models, the newly proposed GLR statistics follow asymptotically rescaled chi-squared distributions, with the scaling constants and the degrees of freedom independent of the nuisance parameters. This demonstrates that the Wilks phenomenon continues to hold under a variety of smoothing techniques and more relaxed models with unspecified error distributions. We further prove that the GLR tests are asymptotically optimal in terms of rates of convergence for nonparametric hypothesis testing. In addition, for testing a parametric additive model, we propose a bias corrected method to improve the performance of the GLR. The bias-corrected test is shown to share the Wilks type of property. Simulations are conducted to demonstrate the Wilks phenomenon and the power of the proposed tests. A real example is used to illustrate the performance of the testing approach."], ["Rank-Sum Tests for Clustered Data", null], ["Multivariate Nonparametric Tests of Independence", "New test statistics are proposed for testing whether two random vectors are independent. Gieser and Randles, as well as Taskinen, Kankainen, and Oja have introduced and discussed multivariate extensions of the quadrant test of Blomqvist. This article serves as a sequel to this work and presents new multivariate extensions of Kendall's tau and Spearman's rho statistics. Two different approaches are discussed. First, interdirection proportions are used to estimate the cosines of angles between centered observation vectors and between differences of observation vectors. Second, covariances between affine-equivariant multivariate signs and ranks are used. The test statistics arising from these two approaches appear to be asymptotically equivalent if each vector is elliptically symmetric. The spatial sign versions are easy to compute for data in common dimensions, and they provide practical, robust alternatives to normal-theory methods. Asymptotic theory is developed to approximate the finite-sample null distributions as well, as to calculate limiting Pitman efficiencies. Small-sample null permutation distributions are also described. A simple simulation study is used to compare the proposed tests with the classical Wilks test. Finally, the theory is illustrated by an example."], ["A Unified Nonparametric Approach for Unbalanced Factorial Designs", "Motivated by questions arising from the field of statistical genetics, we consider the problem of testing main, nested, and interaction effects in unbalanced factorial designs. Based on the concept of composite linear rank statistics, a new notion of weighted rank is proposed. Asymptotic normality of weighted linear rank statistics is established under mild conditions, and consistent estimators are developed for the corresponding limiting covariance structure. A unified framework to use weighted rank to construct test statistics for main, nested, and interaction effects in unbalanced factorial designs is established. The proposed tests statistics are applicable to unbalanced designs with arbitrary cell replicates greater than one per cell. The limiting distributions under both the null hypotheses and Pitman alternatives are derived. Monte Carlo simulations are conducted to confirm the validity and power of the proposed tests. Genetic datasets from a simulated backcross study are analyzed to demonstrate the application of the proposed tests in quantitative trait loci mapping."], ["A Penalized Nonparametric Maximum Likelihood Approach to Species Richness Estimation", "We propose a class of penalized nonparametric maximum likelihood estimators (NPMLEs) for the species richness problem. We use a penalty term on the likelihood because likelihood estimators that lack it have an extreme instability problem. The estimators are constructed using a conditional likelihood that is simpler than the full likelihood. We show that the full-likelihood NPMLE solution given by Norris and Pollock can be found (with great accuracy) by using an appropriate penalty term on the conditional likelihood, so it is an element of our class of estimators. A simple and fast algorithm for the penalized NPMLE is developed; it can be used to greatly speed up computation of the unconditional NPMLE. It can also be used to find profile mixture likelihoods. Based on our goal of attaining high stability while retaining sensitivity, we propose an adaptive quadratic penalty function. A systematic simulation study, using a wide range of scenarios, establishes the success of this method relative to its competitors. Finally, we discuss an application in the gene number estimation using expressed sequence tag (EST) data from genomics."], ["The Profile Sampler", null], ["Bootstrap Standard Error Estimates for Linear Regression", "Standard errors of parameter estimates are widely used in empirical work. The bootstrap often can provide a convenient means of estimating standard errors. The conditions under which bootstrap standard error estimates are theoretically justified have not received much attention, however. This article establishes conditions for the consistency of the moving blocks bootstrap estimators of the variance of the least squares estimator in linear dynamic models with dependent data. We discuss several applications of this result, in particular, the use of bootstrap standard error estimates for bootstrapping Studentized statistics. A simulation study shows that inference based on bootstrap standard error estimates may be considerably more accurate in small samples than inference based on closed-form asymptotic estimates."], ["Locally Efficient Semiparametric Estimators for Generalized Skew-Elliptical Distributions", "We consider a class of generalized skew-normal distributions that is useful for selection modeling and robustness analysis and derive a class of semiparametric estimators for the location and scale parameters of the central part of the model. We show that these estimators are consistent and asymptotically normal. We present the semiparametric efficiency bound and derive the locally efficient estimator that achieves this bound if the model for the skewing function is correctly specified. The estimators that we propose are consistent and asymptotically normal even if the model for the skewing function is misspecified, and we compute the loss of efficiency in such cases. We conduct a simulation study and provide an illustrative example. Our method is applicable to generalized skew-elliptical distributions."], ["CATS", "CATS\u2014clustering after transformation and smoothing\u2014is a technique for nonparametrically estimating and clustering a large number of curves. Our motivating example is a genetic microarray experiment, but the method is very general. The method includes transformation and smoothing multiple curves, multiple nonparametric testing for screening out flat curves, clustering curves with similar shape, and nonparametrically inferring the clustering estimation error rate."], ["Inference for a Class of Transformed Hazards Models", "A new class of transformed hazard rate models is considered that contains both the multiplicative hazards model and the additive hazards model as special cases. The sieve maximum likelihood estimators are derived for the model parameters, and the estimators for the regression coefficients are shown to be consistent and asymptotically normal with variance achieving the semiparametric efficiency bound. Simulation studies are conducted to examine the small-sample properties of the proposed estimates, and a real dataset is used to illustrate our approach."], [null, null], ["Bayesian Nonparametric Spatial Modeling With Dirichlet Process Mixing", "Customary modeling for continuous point-referenced data assumes a Gaussian process that is often taken to be stationary. When such models are fitted within a Bayesian framework, the unknown parameters of the process are assumed to be random, so a random Gaussian process results. Here we propose a novel spatial Dirichlet process mixture model to produce a random spatial process that is neither Gaussian nor stationary. We first develop a spatial Dirichlet process model for spatial data and discuss its properties. Because of familiar limitations associated with direct use of Dirichlet process models, we introduce mixing by convolving this process with a pure error process. We then examine properties of models created through such Dirichlet process mixing. In the Bayesian framework, we implement posterior inference using Gibbs sampling. Spatial prediction raises interesting questions, but these can be handled. Finally, we illustrate the approach using simulated data, as well as a dataset involving precipitation measurements over the Languedoc-Roussillon region in southern France."], ["Generalized Radius Processes for Elliptically Contoured Distributions", null], ["Estimation in the Mixture of Markov Chains Moving With Different Speeds", "This article considers a new mixture of time-homogeneous finite Markov chains where the mixing is on the rate of movement and develops the EM algorithm for maximum likelihood estimation of the parameters of the mixture. Continuous- and discrete-time versions of the mixture are defined, and their estimation is considered separately. The developed methods are illustrated with an application to modeling bond ratings migration. The class of mixture models proposed in this article provides a framework for modeling population heterogeneity with respect to the rate of movement. The proposed mixture subsumes the mover\u2013stayer model, which has been widely used in applications."], ["Residual Diagnostics for Growth Mixture Models", "Growth mixture modeling has become a prominent tool for studying the heterogeneity of developmental trajectories within a population. In this article we develop graphical diagnostics to detect misspecification in growth mixture models regarding the number of growth classes, growth trajectory means, and covariance structures. For each model misspecification, we propose a different type of empirical Bayes residual to quantify the departure. Our procedure begins by imputing multiple independent sets of growth classes for the sample. Then, from these so-called \u201cpseudoclass\u201d draws, we form diagnostic plots to examine the averaged empirical distributions of residuals in each such class. Our proposals draw on the property that each single set of pseudoclass adjusted residuals is asymptotically normal with known mean and (co)variance when the underlying model is correct. These methods are justified in simulation studies involving two classes of linear growth curves that also differ by their covariance structures. These are then applied to longitudinal data from a randomized field trial that tests whether children's trajectories of aggressive behavior could be modified during elementary and middle school. Our diagnostics lead to a solution involving a mixture of three growth classes. When comparing the diagnostics obtained from multiple pseudoclasses with those from multiple imputations, we show the computational advantage of the former and obtain a criterion for determining the minimum number of pseudoclass draws."], ["Transdimensional Markov Chains", "The last 10 years have witnessed the development of sampling frameworks that permit the construction of Markov chains that simultaneously traverse both parameter and model space. Substantial methodological progress has been made during this period. In this article we present a survey of the current state of the art and evaluate some of the most recent advances in this field. We also discuss future research perspectives in the context of the drive to develop sampling mechanisms with high degrees of both efficiency and automation."], ["Book Reviews", null], ["Introduction to Rare Event Simulation", null], ["Diagnostic Checks in Time Series", null], ["Association Schemes: Designed Experiments, Algebra and Combinatorics", null], ["Ranked Set Sampling: Theory and Applications", null], ["Weibull Models", null], ["Statistical Methods of Analysis", null], ["Causality: Models, Reasoning, and Inference", null], ["Bayesian Artificial Intelligence", null], ["Procrustes Problems", null], ["Convex Optimization", null], ["C++ Design Patterns and Derivatives Pricing: Mark Joshi", null], ["An Introduction to Financial Option Valuation: Mathematics, Stochastics and Computation", null], ["Finite-Sample Econometrics: Advanced Texts in Econometrics", null], ["Practical Genetic Algorithms", null], ["Telegraphic Reviews", null], ["Correction", null], ["Hidden Markov Models for Longitudinal Comparisons", null], ["Contact Surface Models for Infectious Diseases", "Controlling of infectious diseases requires information about the rates at which individuals make contact. We propose a novel approach to modeling contact rates via a continuous contact surface. This provides a more realistic and flexible representation of contact rates than currently used methods. Our approach allows modeling of sources of heterogeneity due to age, individual effects, and gender. The models are fitted to serologic survey data by maximum likelihood. This involves solving an integral equation linking the contact surface to the infection hazards. The method is illustrated with two datasets, on mumps and rubella and on Epstein\u2013Barr virus and herpes simplex virus type 1 infection. The advantages and shortcomings of the method, particularly the identifiability of the contact surface, are discussed."], ["A Bayesian Approach to 2000 Census Evaluation Using ACE Survey Data and Demographic Analysis", "Demographic analysis of data on births, deaths, and migration, together with coverage measurement surveys that use capture-recapture methods, have established that U.S. Census counts are flawed for certain subpopulations. Previous work using 1990 Census data in African-Americans age 30\u201349 proposed a hierarchical Bayesian model that assembled Census, follow-up survey, and demographic data, providing a principled solution to the problem of negative estimated counts in some subpopulations, smoothing highly variable estimates across subpopulations, and providing estimates of precision that incorporate uncertainty in the demographic analysis estimates. This article extends that effort by refining the hierarchical model design, expanding the set of models considered, considering the presence of bias in the Census or follow-up survey counts, obtaining Bayes factors for use in model selection, and applying the methods to the entire 2000 U.S. Census. Comparisons with the 1990 U.S. Census results are included as well."], ["Estimating Size and Composition of Biological Communities by Modeling the Occurrence of Species", "We develop a model that uses repeated observations of a biological community to estimate the number and composition of species in the community. Estimators of community-level attributes are constructed from model-based estimators of occurrence of individual species that incorporate imperfect detection of individuals. Data from the North American Breeding Bird Survey are analyzed to illustrate the variety of ecologically important quantities that are easily constructed and estimated using our model-based estimators of species occurrence. In particular, we compute site-specific estimates of species richness that honor classical notions of species-area relationships. We suggest extensions of our model to estimate maps of occurrence of individual species and to compute inferences related to the temporal and spatial dynamics of biological communities."], ["Population-Calibrated Gene Characterization", "Phenotypic characterization of rare disease genes poses a significant statistical challenge, but the need to do so is clear. Clinical management of patients carrying a disease gene depends crucially on an accurate characterization of the genetically predisposed disease, including its likelihood of occurrence among mutation carriers, natural history, and response to treatment. We propose a formal yet practical method for controlling for bias due to ignoring ascertainment, defined as the sampling mechanism, when quantifying the association between genotype and disease using data on high-risk families. The approach is more statistically efficient than conditioning on the variables used in sampling. In it, the likelihood is adjusted by a factor that is a function of sampling weights in strata defined by those variables. It requires that these variables and the sampling probabilities in the strata they define either are known or can be estimated. The latter requires a second, population-based dataset. As an example, we derive ascertainment-corrected estimates of penetrance for the breast cancer susceptibility genes BRCA1 and BRCA2. The Bayesian analysis that we use incorporates a modified segregation model and prior data on penetrance derived from the literature. Markov chain Monte Carlo methods are used for inference."], ["Sufficient Dimension Reduction via Inverse Regression", "A family of dimension-reduction methods, the inverse regression (IR) family, is developed by minimizing a quadratic objective function. An optimal member of this family, the inverse regression estimator (IRE), is proposed, along with inference methods and a computational algorithm. The IRE has at least three desirable properties: (1) Its estimated basis of the central dimension reduction subspace is asymptotically efficient, (2) its test statistic for dimension has an asymptotic chi-squared distribution, and (3) it provides a chi-squared test of the conditional independence hypothesis that the response is independent of a selected subset of predictors given the remaining predictors. Current methods like sliced inverse regression belong to a suboptimal class of the IR family. Comparisons of these methods are reported through simulation studies. The approach developed here also allows a relatively straightforward derivation of the asymptotic null distribution of the test statistic for dimension used in sliced average variance estimation."], ["Component Identification and Estimation in Nonlinear High-Dimensional Regression Models by Structural Adaptation", null], [null, null], ["An Estimated Likelihood Method for Continuous Outcome Regression Models With Outcome-Dependent Sampling", "Many biomedical observational studies attempt to relate a continuous outcome to an environmental exposure and other important covariates. If the outcome is easier or cheaper to measure relative to the exposure of interest, then the outcome may be observed for every member of a finite-study population, whereas exposure measurements may be obtained only for a relatively small subsample of this population. Rather than selecting a simple random subsample of individuals for exposure measurement, investigators may attempt to enhance study efficiency by allowing the selection probabilities to depend on the observed outcomes; we refer to such sampling schemes as outcome-dependent sampling (ODS). Standard estimation methods that ignore the ODS design will yield biased and inconsistent parameter estimates. Furthermore, it is generally desirable to use estimators that incorporate all available data as analyses restricted to subjects with complete information are inefficient. To this end, we extend an estimated likelihood method, originally developed for discrete outcome measurement error problems in which accurate exposure measurements are made only for a simple random \u201cvalidation\u201d sample, to allow for continuous outcomes and ODS designs. We derive the asymptotic properties of the proposed estimator and use simulated data to show that the asymptotic results closely approximate the finite-sample properties in samples of moderate size. We also use simulated data to compare the performance of our proposed estimator with that of existing methods applicable to the ODS problem."], ["Maximum Likelihood Estimation for the Proportional Odds Model With Random Effects", "In this article we study the semiparametric proportional odds model with random effects for correlated, right-censored failure time data. We establish that the maximum likelihood estimators for the parameters of this model are consistent and asymptotically Gaussian. Furthermore, the limiting variances achieve the semiparametric efficiency bounds and can be consistently estimated. Simulation studies show that the asymptotic approximations are accurate for practical sample sizes and that the efficiency gains of the proposed estimators over those of Cai, Cheng, and Wei can be substantial. A real example is provided to illustrate the proposed methods."], ["Testing Quasi-Independence of Failure and Truncation Times via Conditional Kendall's Tau", "Truncated survival data arise when the failure time is observed only if it falls within a subject-specific truncating set. Most analysis methods rely on the key assumption of quasi-independence, that is, factorization of the joint density of failure and truncation times into a product proportional to the individual densities in the observable region. Unlike independence of failure time and censoring time, quasi-independence can be tested. Tests of quasi-independence are available for one-sided truncation and for truncation that depends on a measured covariate, but not for more complex truncation schemes. Here tests of quasi-independence based on a multivariate conditional Kendall's tau are proposed for doubly truncated data, bivariate left-truncated data, and other forms of truncated survival data that arise when initiating or terminating event times are interval-censored. Asymptotic properties under the null are derived. The tests are illustrated using several real datasets and evaluated via simulation."], ["Phase 2 and 3 Combination Designs to Accelerate Drug Development", "For late-stage clinical development, we propose combining phase 2 and 3 trials via a two-stage adaptive design. In the first stage, short-term safety and efficacy are examined, after which low doses that lack efficacy and high doses that cause safety concerns are eliminated from further evaluation. The trial continues to the second stage with doses that are not eliminated. For the second stage, the required sample size is adjusted to maintain power. All patients, including those enrolled in the first stage, are evaluated using a clinical endpoint requiring a longer follow-up. For the second stage, trend statistics are adaptively chosen based on the estimated dose-response curve in the clinical endpoint of the first-stage patients. At the end of the trial, pairwise statistics for the first stage and adaptive trend statistics for the second stage of the clinical endpoint are combined to establish dose-response and to identify the lowest effective dose. A notable feature of our proposed approach is that the adaptation rule governing dose selection, sample size calculation, and derivation of test statistics for the second stage need not be specified in advance to maintain the validity of the trial. The phase 2/3 combination design is effective in achieving robust statistical power and is also efficient, because the number of patients and time needed are substantially reduced."], ["A Note on Nonparametric Estimation of the Effective Dose in Quantal Bioassay", null], ["Exact, Nonparametric Inference When Doses Are Measured With Random Errors", "Studies that estimate the effects of exposure to a possibly harmful agent often compare exposed subjects who received varied doses with matched controls who received zero dose. If the doses are measured with error, then one may wish to use the fallible doses to estimate a linear relationship between the unobserved true dose and the observed response. If one is willing to assume that the dose errors for exposed subjects are symmetrically distributed about 0\u2014that is, the dose errors are pure errors and not, say, systematic underreporting of exposure\u2014then the presence of zero-dose controls is all that is needed to obtain exact, distribution-free confidence intervals and tests, and consistent point estimates. The method is simpler for matched pairs than for matched sets with two or more matched subjects, and it is illustrated using two studies, one of each kind. With matched pairs, as in this first example, the method uses Wilcoxon's signed rank test as the basis for inference. When there are several zero-dose controls matched to each exposed subject, the familiar null distribution of the signed rank statistic is no longer applicable because of dependence within matched sets, so the appropriate exact distribution and large-sample approximation are developed."], ["SLEX Analysis of Multivariate Nonstationary Time Series", "We develop a procedure for analyzing multivariate nonstationary time series using the SLEX library (smooth localized complex exponentials), which is a collection of bases, each basis consisting of waveforms that are orthogonal and time-localized versions of the Fourier complex exponentials. Under the SLEX framework, we build a family of multivariate models that can explicitly characterize the time-varying spectral and coherence properties. Every model has a spectral representation in terms of a unique SLEX basis. Before selecting a model, we first decompose the multivariate time series into nonstationary components with uncorrelated (nonredundant) spectral information. The best SLEX model is selected using the penalized log energy criterion, which we derive in this article to be the Kullback\u2013Leibler distance between a model and the SLEX principal components of the multivariate time series. The model selection criterion takes into account all of the pairwise cross-correlation simultaneously in the multivariate time series. The proposed SLEX analysis gives results that are easy to interpret, because it is an automatic time-dependent generalization of the classical Fourier analysis of stationary time series. Moreover, the SLEX method uses computationally efficient algorithms and hence is able to provide a systematic framework for extracting spectral features from a massive dataset. We illustrate the SLEX analysis with an application to a multichannel brain wave dataset recorded during an epileptic seizure."], ["Diagnostic Checking in ARMA Models With Uncorrelated Errors", null], ["Bootstrapping Unit Root Tests for Autoregressive Time Series", "The theory developed for bootstrapping unit root tests in an autoregressive (AR) context has been concerned mainly with the large-sample behavior of the methods proposed under the assumption that the null hypothesis is true. No results exist for the relative performance and the power behavior of the bootstrap methods under the alternative. This article studies the properties of different AR bootstrap schemes of the unit root hypothesis, including a new proposal based on unrestricted residuals. It shows that bootstrap procedures based on differencing the observed series suffer from power problems as compared with bootstrap procedures based on unrestricted residuals. Whereas for finite-order AR processes differencing leads to just a loss of power, for infinite-order autoregressions such a differencing makes the application of sieve AR bootstrap schemes inappropriate if the alternative is true. The superiority of the new bootstrap proposal is shown, and some numerical examples illustrate our theoretical findings."], ["Constructing Stationary Time Series Models Using Auxiliary Variables With Applications", "Here we present a novel method for modeling stationary time series. Our approach is to construct the model with a specified marginal family and build the dependence structure around it. We show that the resulting time series is linear with a simple autocorrelation structure. We construct models that parallel existing structures, namely state-space models, autoregressive conditional heteroscedasticity (ARCH) models, and generalized ARCH models. We use Bayesian techniques to estimate the resulting models. We also demonstrate that the models perform well compared with competing methods for the applications considered, count models and volatility models."], ["Functional Adaptive Model Estimation", null], ["Functional Data Analysis for Sparse Longitudinal Data", "We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the covariance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement. A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle."], ["Semiparametric Bayesian Analysis of Matched Case-Control Studies With Missing Exposure", "This article considers Bayesian analysis of matched case-control problems when one of the covariates is partially missing. Within the likelihood context, the standard approach to this problem is to posit a fully parametric model among the controls for the partially missing covariate as a function of the covariates in the model and the variables making up the strata. Sometimes the strata effects are ignored at this stage. Our approach differs not only in that it is Bayesian, but, far more importantly, in the manner in which it treats the strata effects. We assume a Dirichlet process prior with a normal base measure for the stratum effects and estimate all of the parameters in a Bayesian framework. Three matched case-control examples and a simulation study are considered to illustrate our methods and the computing scheme."], ["Bayesian Variable Selection in Clustering High-Dimensional Data", null], ["Bayesian Semiparametric Isotonic Regression for Count Data", null], ["Optimal Conditionally Unbiased Bounded-Influence Inference in Dynamic Location and Scale Models", "This article studies the local robustness of estimators and tests for the conditional location and scale parameters in a strictly stationary time series model. We first derive optimal bounded-influence estimators for such settings under a conditionally Gaussian reference model. Based on these results, we obtain optimal bounded-influence versions of the classical likelihood-based tests for parametric hypotheses. We propose a feasible and efficient algorithm for the computation of our robust estimators, which uses analytical Laplace approximations to estimate the auxiliary recentering vectors, ensuring Fisher consistency in robust estimation. This strongly reduces the computation time by avoiding the simulation of multidimensional integrals, a task that typically must be addressed in the robust estimation of nonlinear models for time series. In some Monte Carlo simulations of an AR(1)\u2013ARCH(1) process, we show that our robust procedures maintain a very high efficiency under ideal model conditions and at the same time perform very satisfactorily under several forms of departure from conditional normality. In contrast, classical pseudo\u2013maximum likelihood inference procedures are found to be highly inefficient under such local model misspecifications. These patterns are confirmed by an application to robust testing for autoregressive conditional heteroscedasticity."], ["Outlier Labeling With Boxplot Procedures", null], ["Analyzing Nonstationary Spatial Data Using Piecewise Gaussian Processes", "In many problems in geostatistics the response variable of interest is strongly related to the underlying geology of the spatial location. In these situations there is often little correlation in the responses found in different rock strata, so the underlying covariance structure shows sharp changes at the boundaries of the rock types. Conventional stationary and nonstationary spatial methods are inappropriate, because they typically assume that the covariance between points is a smooth function of distance. In this article we propose a generic method for the analysis of spatial data with sharp changes in the underlying covariance structure. Our method works by automatically decomposing the spatial domain into disjoint regions within which the process is assumed to be stationary, but the data are assumed independent across regions. Uncertainty in the number of disjoint regions, their shapes, and the model within regions is dealt with in a fully Bayesian fashion. We illustrate our approach on a previously unpublished dataset relating to soil permeability of the Schneider Buda oil field in Wood County, Texas."], ["Estimation of Poisson Intensity in the Presence of Dead Time", "Phase Doppler interferometry (PDI) is a nonintrusive technique frequently used to obtain information about spray characteristics. Understanding spray characteristics is of critical importance in many areas of science, including liquid fuel spray combustion, spray coatings, fire suppression, and pesticides. PDI measures the size and velocity of individual droplets in a spray. Due to the design of the instrument, recordings of the PDI contain gaps, called dead times. The presence of recurring dead times greatly complicates estimation of the diffusion rate of the droplets. Modeling the spray process as a homogeneous Poisson process, we construct consistent and asymptotic normal estimators of the diffusion rate (Poisson intensity) under various conditions. Simulation produced a good agreement between our estimators (in the presence of dead time) and the maximum likelihood estimates obtained without dead time. We use experimental data to illustrate the estimation method."], ["Statistical Methods for Eliciting Probability Distributions", "Elicitation is a key task for subjectivist Bayesians. Although skeptics hold that elicitation cannot (or perhaps should not) be done, in practice it brings statisticians closer to their clients and subject-matter expert colleagues. This article reviews the state of the art, reflecting the experience of statisticians informed by the fruits of a long line of psychological research into how people represent uncertain information cognitively and how they respond to questions about that information. In a discussion of the elicitation process, the first issue to address is what it means for an elicitation to be successful; that is, what criteria should be used. Our answer is that a successful elicitation faithfully represents the opinion of the person being elicited. It is not necessarily \u201ctrue\u201d in some objectivistic sense, and cannot be judged in that way. We see that elicitation as simply part of the process of statistical modeling. Indeed, in a hierarchical model at which point the likelihood ends and the prior begins is ambiguous. Thus the same kinds of judgment that inform statistical modeling in general also inform elicitation of prior distributions. The psychological literature suggests that people are prone to certain heuristics and biases in how they respond to situations involving uncertainty. As a result, some of the ways of asking questions about uncertain quantities are preferable to others, and appear to be more reliable. However, data are lacking on exactly how well the various methods work, because it is unclear, other than by asking using an elicitation method, just what the person believes. Consequently, one is reduced to indirect means of assessing elicitation methods. The tool chest of methods is growing. Historically, the first methods involved choosing hyperparameters using conjugate prior families, at a time when these were the only families for which posterior distributions could be computed. Modern computational methods, such as Markov chain Monte Carlo, have freed elicitation from this constraint. As a result, now both parametric and nonparametric methods are available for low-dimensional problems. High-dimensional problems are probably best thought of as lacking another hierarchical level, which has the effect of reducing the as-yet-unelicited parameter space. Special considerations apply to the elicitation of group opinions. Informal methods, such as Delphi, encourage the participants to discuss the issue in the hope of reaching consensus. Formal methods, such as weighted averages or logarithmic opinion pools, each have mathematical characteristics that are uncomfortable. Finally, there is the question of what a group opinion even means, because it is not necessarily the opinion of any participant."], ["Applied Spatial Statistics for Public Health Data", null], ["Foundations of Risk Analysis: A Knowledge and Decision-Oriented Perspective", null], ["Comparison Methods for Stochastic Models and Risks", null], ["Quantitative Methods in Population Health: Extensions of Ordinary Regression", null], ["The Statistical Evaluation of Medical Tests for Classification and Prediction", null], ["Computational Methods in Statistics and Econometrics", null], ["Numerical Issues in Statistical Computing for the Social Scientist", null], ["Experimental and Quasi-Experimental Designs for Generalized Causal Inference", null], ["Planning, Construction, and Statistical Analysis of Comparative Experiments", null], ["Applied Longitudinal Analysis", null], ["Generalized Latent Variable Modeling: Multilevel, Longitudinal, and Structural Equation Models", null], ["Microarray Quality Control", null], ["Design and Analysis of DNA Microarray Investigations", null], ["Mathematical and Statistical Methods for Genetic Analysis", null], ["Resampling Methods for Dependent Data", null], ["Recurrent Events Data Analysis for Product Repairs, Disease Recurrences, and Other Applications", null], ["Telegraphic Reviews", null], ["Correction", null], ["Bayesians, Frequentists, and Scientists", "Broadly speaking, nineteenth century statistics was Bayesian, while the twentieth century was frequentist, at least from the point of view of most scientific practitioners. Here in the twenty-first century scientists are bringing statisticians much bigger problems to solve, often comprising millions of data points and thousands of parameters. Which statistical philosophy will dominate practice? My guess, backed up with some recent examples, is that a combination of Bayesian and frequentist ideas will be needed to deal with our increasingly intense scientific environment. This will be a challenging period for statisticians, both applied and theoretical, but it also opens the opportunity for a new golden age, rivaling that of Fisher, Neyman, and the other giants of the early 1900s. What follows is the text of the 164th ASA presidential address, delivered at the awards ceremony in Toronto on August 10, 2004."], ["Weather Forecasting for Weather Derivatives", "We take a simple time series approach to modeling and forecasting daily average temperature in U.S. cities, and we inquire systematically as to whether it may prove useful from the vantage point of participants in the weather derivatives market. The answer is, perhaps surprisingly, yes. Time series modeling reveals conditional mean dynamics and, crucially, strong conditional variance dynamics in daily average temperature, and it reveals sharp differences between the distribution of temperature and the distribution of temperature surprises. As we argue, it also holds promise for producing the long-horizon predictive densities crucial for pricing weather derivatives, so that additional inquiry into time series weather forecasting methods will likely prove useful in weather derivatives contexts."], ["Are Maintenance Practices for Railroad Tracks Effective?", "The Association of American Railroads wished to determine the effect of a maintenance practice known as grinding on the occurrence of rail fatigue defects and on the subsequent total traffic usage before a track must be replaced. Because a designed experiment was not practical, an analysis of historical data from the Canadian Northern Railroad is presented. In the analysis, certain covariate data are available, specifically the amount of grinding and some physical characteristics of the rail; other important covariate data are not available, however. A model for the number of defects as a function of traffic usage is developed based on a modulated Poisson point process. The model incorporates the effect of the available covariates and a mixture of Dirichlet processes set-up for the scale parameters of the individual rail sections that allows an assessment of the overall effect of the unavailable covariates. The model is then used to determine an optimal replacement period for a whole rail track. The analysis demonstrates that grinding reduces the expected number of defects and increases the optimal replacement interval."], ["A Space\u2013Time Conditional Intensity Model for Evaluating a Wildfire Hazard Index", "Numerical indices are commonly used as tools to aid wildfire management and hazard assessment. Although the use of such indices is widespread, assessment of these indices in their respective regions of application is rare. We evaluate the effectiveness of the burning index (BI) for predicting wildfire occurrences in Los Angeles County, California using space\u2013time point-process models. These models are based on an additive decomposition of the conditional intensity, with separate terms used to describe spatial and seasonal variability as well as contributions from the BI. We fit the models to wildfire and BI data from the years 1976\u20132000 using a combination of nonparametric kernel-smoothing methods and parametric maximum likelihood. In addition to using the Akaike information criterion (AIC) to compare competing models, we use new multidimensional residual methods based on approximate random thinning and rescaling to detect departures from the models and to ascertain the precise contribution of the BI to predicting wildfire occurrence. We find that although the BI appears to have a positive impact on wildfire prediction, the contribution is relatively small after taking into account natural seasonal and spatial variation. In particular, the BI does not appear to take into account increased activity during the years 1979\u20131981 and can overpredict during the early months of the year."], ["Statistical Analysis of a Telephone Call Center", "A call center is a service network in which agents provide telephone-based services. Customers who seek these services are delayed in tele-queues. This article summarizes an analysis of a unique record of call center operations. The data comprise a complete operational history of a small banking call center, call by call, over a full year. Taking the perspective of queueing theory, we decompose the service process into three fundamental components: arrivals, customer patience, and service durations. Each component involves different basic mathematical structures and requires a different style of statistical analysis. Some of the key empirical results are sketched, along with descriptions of the varied techniques required. Several statistical techniques are developed for analysis of the basic components. One of these techniques is a test that a point process is a Poisson process. Another involves estimation of the mean function in a nonparametric regression with lognormal errors. A new graphical technique is introduced for nonparametric hazard rate estimation with censored data. Models are developed and implemented for forecasting of Poisson arrival rates. Finally, the article surveys how the characteristics deduced from the statistical analyses form the building blocks for theoretically interesting and practically useful mathematical models for call center operations."], ["Classification of Missense Mutations of Disease Genes", "Clinical management of individuals found to harbor a mutation at a known disease-susceptibility gene depends on accurate assessment of mutation-specific disease risk. For missense mutations (MMs)\u2014mutations that lead to a single amino acid change in the protein coded by the gene\u2014this poses a particularly challenging problem. Because it is not possible to predict the structural and functional changes to the protein product for a given amino acid substitution, and because functional assays are often not available, disease association must be inferred from data on individuals with the mutation. Inference is complicated by small sample sizes and by sampling mechanisms that bias toward individuals at high familial risk of disease. We propose a Bayesian hierarchical model to classify the disease association of MMs given pedigree data collected in the high-risk setting. The model's structure allows simultaneous characterization of multiple MMs. It uses a group of pedigrees identified through probands tested positive for known disease associated mutations and a group of test-negative pedigrees, both obtained from the same clinic, to calibrate classification and control for potential ascertainment bias. We apply this model to study MMs of breast-ovarian susceptibility genes BRCA1 and BRCA2, using data collected at the Duke University Medical Center in Durham, North Carolina."], ["Modeling Reporting Delays and Reporting Corrections in Cancer Registry Data", "The Surveillance, Epidemiology, and End Results (SEER) program of the National Cancer Institute is an authoritative source of cancer incidence statistics in the United States. The SEER program is a consortium of population-based cancer registries from different areas of the country. Each registry is charged with collecting data on all cancers that occur within its geographic area. As with any disease registry, there is a delay between the time that the disease (cancer) is first diagnosed and the time that it is reported to the registry. The SEER program has allowed for reporting delays of up to 19-months before releasing data for public use. Nevertheless, additional cases are discovered after the 19-month delay, and these cases are added in subsequent releases of the data. Further, any errors discovered are corrected in subsequent releases. Such reporting delays and corrections typically lead to underestimation of the cancer incidence rates in recent diagnosis years, making it difficult to monitor trends. In this article we study models that account for reporting delays and corrections in predicting eventual cancer counts for a diagnosis year from the preliminary counts. Previous models of this type have been studied, especially as applied to AIDS registries. We offer several additions to existing models. First, we explicitly model the reporting corrections. Second, we model the delay distribution with very general models, combining aspects of previous nonparametric-like models (i.e., models that have a separate parameter for each delay time) with more parametric models. Third, we allow random reporting-year effects in the model. Practical issues of model selection and how the data are classified are also discussed, particularly how the definition of a reporting correction may change depending on how subpopulations are defined. An example with SEER melanoma data is studied in detail."], ["False Discovery Rate\u2013Adjusted Multiple Confidence Intervals for Selected Parameters", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Exact and Approximate Stepdown Methods for Multiple Hypothesis Testing", null], ["Sequential Monte Carlo Methods for Statistical Analysis of Tables", "We describe a sequential importance sampling (SIS) procedure for analyzing two-way zero\u2013one or contingency tables with fixed marginal sums. An essential feature of the new method is that it samples the columns of the table progressively according to certain special distributions. Our method produces Monte Carlo samples that are remarkably close to the uniform distribution, enabling one to approximate closely the null distributions of various test statistics about these tables. Our method compares favorably with other existing Monte Carlo-based algorithms, and sometimes is a few orders of magnitude more efficient. In particular, compared with Markov chain Monte Carlo (MCMC)-based approaches, our importance sampling method not only is more efficient in terms of absolute running time and frees one from pondering over the mixing issue, but also provides an easy and accurate estimate of the total number of tables with fixed marginal sums, which is far more difficult for an MCMC method to achieve."], ["Homogeneous Linear Predictor Models for Contingency Tables", null], ["Multifold Predictive Validation in ARMAX Time Series Models", "This article presents a new procedure for multifold predictive validation in time series. The procedure is based on the so-called \u201cfiltered residuals,\u201d in-sample prediction errors evaluated in such a way that they are similar to out-of-sample ones. The filtered residuals are obtained from parameters estimated by eliminating from the estimation process the estimated innovations at the points to be predicted. Thus, instead of using the deletion of observations to validate the predictions, as in classical cross-validation, the procedure is based on deletion of the estimated innovations. It is proved that the filtered residuals are uncorrelated, up to terms of small order, with the in-sample innovations, a property shared with the out-of-sample residuals. The parameters needed for computing the filtered residuals can be obtained by estimating a model with innovational outliers at the points to be predicted. The proposed multifold predictive validation is asymptotically equivalent to an efficient model selection procedure. Some Monte Carlo evidence of the performance of the procedure is presented, and the application is illustrated in an example."], ["Efficient Semiparametric Marginal Estimation for Longitudinal/Clustered Data", "We consider marginal generalized semiparametric partially linear models for clustered data. Lin and Carroll derived the semiparametric efficient score function for this problem in the multivariate Gaussian case, but they were unable to construct a semiparametric efficient estimator that actually achieved the semiparametric information bound. Here we propose such an estimator and generalize the work to marginal generalized partially linear models. We investigate asymptotic relative efficiencies of the estimators that ignore the within-cluster correlation structure either in nonparametric curve estimation or throughout. We evaluate the finite-sample performance of these estimators through simulations and illustrate it using a longitudinal CD4 cell count dataset. Both theoretical and numerical results indicate that properly taking into account the within-subject correlation among the responses can substantially improve efficiency."], ["A General Statistical Framework for Unifying Interval and Linkage Disequilibrium Mapping", "The nonrandom association between different genes, termed linkage disequilibrium (LD), provides a powerful tool for high-resolution mapping of quantitative trait loci (QTL) underlying complex traits. This LD-based mapping approach can be made more efficient when it is coupled with interval mapping characterizing the genetic distance between markers and QTL. This article describes a general statistical framework for simultaneously estimating the linkage and LD that are related in a two-stage hierarchical sampling scheme. This framework is constructed within a maximum likelihood context and can be expanded to fine-scale mapping of complex traits for different population structures and reproductive behaviors. We provide a closed-form solution for joint estimation of quantitative genetic parameters describing QTL effects, QTL position and residual variances, and population genetic parameters describing allele frequencies and QTL-marker LD. We perform simulation studies to investigate the statistical properties of our joint analysis model for interval and LD mapping. An example using body weights of dogs from a multifamily outcrossed pedigree illustrates the use of the model."], ["On the Cox Model With Time-Varying Regression Coefficients", "In the analysis of censored failure time observations, the standard Cox proportional hazards model assumes that the regression coefficients are time invariant. Often, these parameters vary over time, and the temporal covariate effects on the failure time are of great interest. In this article, following previous work of Cai and Sun, we propose a simple estimation procedure for the Cox model with time-varying coefficients based on a kernel-weighted partial likelihood approach. We construct pointwise and simultaneous confidence intervals for the regression parameters over a properly chosen time interval via a simple resampling technique. We derive a prediction method for future patients' survival with any specific set of covariates. Building on the estimates for the time-varying coefficients, we also consider the mixed case and present an estimation procedure for time-independent parameters in the model. Furthermore, we show how to use an integrated function of the estimate for a specific regression coefficient to examine the adequacy of proportional hazards assumption for the corresponding covariate graphically and numerically. All of the proposals are illustrated extensively with a well-known study from the Mayo Clinic."], ["Functional Association Models for Multivariate Survival Processes", "We consider multivariate temporal processes that are continuously observed within overlapping time windows. The intended application is in censored multistate and multivariate survival settings, where point processes are continuously observed. These data differ from other functional data, like longitudinal data, which are discretely observed at irregular times. Existing functional approaches to survival processes use intensity models, which require smoothing and depend critically on the choice of smoothing parameters, similarly to discretely observed data. In this article we study functional mean and association regression models for the point processes, with unspecified time-varying coefficients. The continuous observation scheme is exploited; the coefficients may be estimated nonparametrically by extending generalized estimating equations to continuously observed data. The estimators automatically converge at the parametric rate without smoothing, unlike with discretely observed data. Uniform consistency and weak convergence is established with empirical process techniques. The nonparametric estimators yield new tests for covariate effects, parametric submodeling of these effects, and goodness-of-fit testing. Simulation studies and an analysis of familial aggregation of alcoholism illustrate the methodology's practical utility."], ["A Calculus for Design of Two-Stage Adaptive Procedures", "We propose calculus for designing two-stage adaptive procedures. We describe the design components that specify a two-stage adaptive design, define their interrelationships, and demonstrate how changes in one design component effect changes in the other components. Our technique allows us to control many aspects of a two-stage adaptive clinical trial, including the type I and type II error rates and the maximum total sample size. We also conduct an ANOVA-type study to understand the effects of different components of the design specification on the performance characteristics of the design. Stage I components\u2014namely sample size, type I error rate, and type II error rate\u2014are found to be the most influential."], ["Constrained Inverse Regression for Incorporating Prior Information", null], ["Nonparametric Identification and Estimation of a Censored Location-Scale Regression Model", "In this article we consider identification and estimation of a censored nonparametric location scale-model. We first show that in the case where the location function is strictly less than the (fixed) censoring point for all values in the support of the explanatory variables, the location function is not identified anywhere. In contrast, when the location function is greater or equal to the censoring point with positive probability, the location function is identified on the entire support, including the region where the location function is below the censoring point. In the latter case we propose a simple estimation procedure based on combining conditional quantile estimators for various higher quantiles. The new estimator is shown to converge at the optimal nonparametric rate with a limiting normal distribution. A small-scale simulation study indicates that the proposed estimation procedure performs well in finite samples. We also present an empirical illustration on unemployment insurance duration using administrative-level data from New Jersey."], ["Regression Analysis With Linked Data", "Record linkage, or exact matching, can be used to join together two files that contain information on the same individuals but lack unique personal identification codes. The possibility of errors in linkage causes problems for estimating the relationships between variables on the two files. The effect is analogous to the impact of measurement error. A model of a linear regression relationship between variables in linked files is proposed. Assuming the probabilities that pairs of records are links are known, an unbiased estimator of the regression coefficients is derived. Methods for estimating the linkage probabilities by using mixture models are discussed. A consistent estimator of the covariance matrix of the proposed estimator is proposed. A bootstrap estimator is used to reflect the impact of the uncertainty in record linkage model parameters on the estimators of the regression parameters. A simulation study compares the performance of the proposed estimator and alternatives."], ["A Statistical Model for Signature Verification", "A Bayesian model for off-line signature verification involving the representation of a signature through its curvature is developed. The prior model makes use of a spatial point process for specifying the knots in an approximation restricted to a buffer region close to a template curvature, along with an independent time-warping mechanism. In this way, prior shape information about the signature can be built into the analysis. The observation model is based on additive white noise superimposed on the underlying curvature. The approach is implemented using Markov chain Monte Carlo and applied to a collection of documented instances of William Shakespeare's signature."], ["A Bayesian Semiparametric Model for Random-Effects Meta-Analysis", "In meta-analysis, there is an increasing trend to explicitly acknowledge the presence of study variability through random-effects models. That is, one assumes that for each study there is a study-specific effect and one is observing an estimate of this latent variable. In a random-effects model, one assumes that these study-specific effects come from some distribution, and one can estimate the parameters of this distribution, as well as the study-specific effects themselves. This distribution is most often modeled through a parametric family, usually a family of normal distributions. The advantage of using a normal distribution is that the mean parameter plays an important role, and much of the focus is on determining whether or not this mean is 0. For example, it may be easier to justify funding further studies if it is determined that this mean is not 0. Typically, this normality assumption is made for the sake of convenience, rather than from some theoretical justification, and may not actually hold. We present a Bayesian model in which the distribution of the study-specific effects is modeled through a certain class of nonparametric priors. These priors can be designed to concentrate most of their mass around the family of normal distributions but still allow for any other distribution. The priors involve a univariate parameter that plays the role of the mean parameter in the normal model, and they give rise to robust inference about this parameter. We present a Markov chain algorithm for estimating the posterior distributions under the model. Finally, we give two illustrations of the use of the model."], ["Inferences Under a Stochastic Ordering Constraint", null], ["Estimating Load-Sharing Properties in a Dynamic Reliability System", null], ["Approximate and Asymptotic Distributions of Chi-Squared\u2013Type Mixtures With Applications", null], ["Bilinear Mixed-Effects Models for Dyadic Data", "This article discusses the use of a symmetric multiplicative interaction effect to capture certain types of third-order dependence patterns often present in social networks and other dyadic datasets. Such an effect, along with standard linear fixed and random effects, is incorporated into a generalized linear model, and a Markov chain Monte Carlo algorithm is provided for Bayesian estimation and inference. In an example analysis of international relations data, accounting for such patterns improves model fit and predictive performance."], ["Variance Estimation in a Model With Gaussian Submodels", "This article considers the problem of estimating the dispersion parameter in a Gaussian model that is intermediate between a model where the mean parameter is fully known (fixed) and a model where the mean parameter is completely unknown. One of the goals is to understand the implications of the two-step process of first selecting a model among a finite number of submodels, then estimating a parameter of interest after the model selection, but using the same sample data. The estimators are classified into global, two-step, and weighted estimators. Whereas the global-type estimators ignore the model space structure, the two-step estimators explore the structure adaptively and can be related to pretest estimators, and the weighted estimators are motivated by the Bayesian approach. Their performances are compared theoretically and through simulations using their risk functions based on a scale-invariant quadratic loss function. It is shown that in the variance estimation problem, efficiency gains arise by exploiting the submodel structure through the use of two-step and weighted estimators, especially when the number of competing submodels is few, but that this advantage may deteriorate or be lost altogether for some two-step estimators as the number of submodels increases or the distance between them decreases. Furthermore, it is demonstrated that weighted estimators, arising from properly chosen priors, outperform two-step estimators when there are many competing submodels or when the submodels are close to each other, whereas two-step estimators are preferred when the submodels are highly distinguishable. The results have implications for model averaging and model selection issues."], ["Space\u2013Time Covariance Functions", null], ["Causal Inference Using Potential Outcomes", "Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism\u2014a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials."], ["Missing-Data Methods for Generalized Linear Models", "Missing data is a major issue in many applied problems, especially in the biomedical sciences. We review four common approaches for inference in generalized linear models (GLMs) with missing covariate data: maximum likelihood (ML), multiple imputation (MI), fully Bayesian (FB), and weighted estimating equations (WEEs). There is considerable interest in how these four methodologies are related, the properties of each approach, the advantages and disadvantages of each methodology, and computational implementation. We examine data that are missing at random and nonignorable missing. For ML, we focus on techniques using the EM algorithm, and in particular, discuss the EM by the method of weights and related procedures as discussed by Ibrahim. For MI, we examine the techniques developed by Rubin. For FB, we review approaches considered by Ibrahim et al. For WEE, we focus on the techniques developed by Robins et al. We use a real dataset and a detailed simulation study to compare the four methods."], ["Nonlinear Time Series: Nonparametric and Parametric Methods", null], ["Statistical Inference and Simulation for Spatial Point Processes", null], ["Distribution Theory of Runs and Patterns and Its Applications: A Finite Markov Chain Imbedding Approach", null], ["Stochastic Models in Queueing Theory", null], ["Highly Structured Stochastic Systems", null], ["Discrete Choice Methods With Simulations", null], ["Applied Longitudinal Data Analysis: Modeling Change and Event Occurrence", null], ["Spatial Data Analysis: Theory and Practice", null], ["Multilevel Statistical Models", null], ["Analysis of Multivariate Survival Data", null], ["Subjective and Objective Bayesian Statistics: Principles, Models, and Applications", null], ["Applied Bayesian Modelling", null], ["Sample Survey Theory: Some Pythagorean Perspectives", null], ["Telegraphic Reviews", null]]}