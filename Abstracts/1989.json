{"1989": [["Methods for Studying Coincidences", "This article illustrates basic statistical techniques for studying coincidences. These include data-gathering methods (informal anecdotes, case studies, observational studies, and experiments) and methods of analysis (exploratory and confirmatory data analysis, special analytic techniques, and probabilistic modeling, both general and special purpose). We develop a version of the birthday problem general enough to include dependence, inhomogeneity, and almost and multiple matches. We review Fisher's techniques for giving partial credit for close matches. We develop a model for studying coincidences involving newly learned words. Once we set aside coincidences having apparent causes, four principles account for large numbers of remaining coincidences: hidden cause; psychology, including memory and perception; multiplicity of endpoints, including the counting of \u201cclose\u201d or nearly alike events as if they were identical; and the law of truly large numbers, which says that when enormous numbers of events and people and their interactions cumulate over time, almost any outrageous event is bound to occur. These sources account for much of the force of synchronicity."], ["Choosing among Alternative Nonexperimental Methods for Estimating the Impact of Social Programs: The Case of Manpower Training", "The recent literature on evaluating manpower training programs demonstrates that alternative nonexperimental estimators of the same program produce an array of estimates of program impact. These findings have led to the call for experiments to be used to perform credible program evaluations. Missing in all of the recent pessimistic analyses of nonexperimental methods is any systematic discussion of how to choose among competing estimators. This article explores the value of simple specification tests in selecting an appropriate nonexperimental estimator. A reanalysis of the National Supported Work Demonstration data previously analyzed by proponents of social experiments reveals that a simple testing procedure eliminates the range of nonexperimental estimators at variance with the experimental estimates of program impact."], ["Comment", null], ["Comment", null], ["Rejoinder", null], [null, null], ["Empirical Comparison of Approaches to Forming Strata", null], ["Binomial Regression with Monotone Splines: A Psychometric Application", null], ["Is Two-Phase Sampling Really Better for Estimating Age Composition?", "The two-phase sampling design has been traditionally used in ecology applications to estimate age composition. With regard to both economic and statistical considerations, however, the two-phase design may yield estimates of the age composition that are no better than can be obtained using simple random sampling. This article shows that the optimality of the two-phase sampling design depends upon (a) survey objectives, (b) per-unit sampling costs, and (c) the predictive power of stratum membership on age. Methods are given for determining whether two-phase sampling yields better estimates of the age composition than simple random sampling, accounting for (a), (b), and (c)."], ["A Markov Mixture Model for Magazine Exposure", null], ["Adjusting the 1980 Census of Population and Housing", "In 1980, several cities and states sued the U.S. Census Bureau to correct census results. This correction would adjust for the differential undercounting of Blacks and Hispanics, especially in cities. In this article, the authors, each of whom testified for New York City and State in their joint lawsuit against the Census Bureau, describe the likely pattern of the undercount and present a method to adjust for it. We first explain why the undercount is concentrated among minority populations living in large cities. We describe the demographic and survey data available for adjustment from the Census Bureau's Post Enumeration Program. We present adjustment results obtained by two simple methods\u2014synthetic estimation, and sample estimation for a few large subclasses. The Census Bureau used the latter method, known as the National Vacancy Check, to adjust the results of the 1970 census. We also describe our regression-based, composite method for adjustment. This method takes sample estimates of the undercount rate for a set of mutually exclusive geographic areas, and regresses these estimates upon available predictor variables. The composite estimates of the undercount rate are matrix-weighted averages of the original sample and regression estimates. We compute estimates for 66 areas: 16 large cities, the remainders of the 12 states in which those cities are located, and 38 whole states. As expected, we find that the highest undercount rates are in large cities, and the lowest are in states and state remainders with small percentages of Blacks and Hispanics. Next, we analyze how sensitive our estimates are to changes in data and modeling assumptions. We find that these changes do not affect the estimates very much. Our conclusion is that regardless of whether we use one of the simple methods or the composite method and regardless of how we vary the assumptions of the composite method, an adjustment reliably reduces population shares in states with few minorities and increases the shares of large cities."], [null, null], ["Forecasting Aggregate Period-Specific Birth Rates: The Time Series Properties of a Microdynamic Neoclassical Model of Fertility", "This article demonstrates the value of microdata for understanding the effect of wages on life cycle fertility dynamics. Conventional estimates of neoclassical economic fertility models obtained from linear aggregate time series regressions are widely criticized for being nonrobust when adjusted for serial correlation. Moreover, the forecasting power of these aggregative neoclassical models has been shown to be inferior when compared with conventional time series models that assign no role to wages. This article demonstrates that, when neoclassical models of fertility are estimated on microdata using methods that incorporate key demographic restrictions and when they are properly aggregated, they have considerable forecasting power."], ["Quantitative Risk Assessment for Teratological Effects", "This article presents a quantitative procedure for using a \u201cbenchmark dose\u201d to obtain low-dose risk estimates for reproductive and developmental toxic effects. This procedure combines the best features of the previously proposed methods for handling litter effects for teratology data and the currently used methods for quantitative risk assessment. The beta-binomial distribution is used to account for litter effects, and the Weibull dose\u2014response model is used for modeling teratogenic effects. A benchmark dose, defined to be the lowest dose at which the excess risk does not exceed 1% with 95% confidence, is proposed to replace the no-observed-effect level (NOEL). The NOEL is generally the highest experimental dose that is not statistically different from the control; the NOEL approach does not use experimental data effectively for quantitative risk estimation. In this article, a lower limit on the safe dose is estimated by linearly extrapolating downward from the benchmark dose; this procedure is recommended because it is not strongly dependent on the model used in the data range. Linear extrapolation from a benchmark dose is essentially equivalent to the use of a safety factor in determining a safe dose from a NOEL. However, the linear extrapolation procedure reflects the risk at the estimated safe dose. Confidence limits on excess risks and safe doses are based on the beta-binomial likelihood ratio criterion rather than on the asymptotic distribution of the maximum likelihood estimates, because the latter have been shown to exhibit poor properties in the low-dose extrapolation problem. The proposed method is illustrated by application to a real data set."], ["Joint Estimation of Incidence and Diagnostic Error Rates from Irregular Longitudinal Data", "Longitudinal studies often involve the repeated diagnosis across time of each patient's status with respect to a progressive categorical process. When the occurrence of a change in status is not readily apparent, two factors can make modeling and assessing the incidence rates of progression difficult. First, because diagnoses may be difficult, they may not be performed with the frequency necessary to pinpoint exact times of incidence. Second, uncertainty in the diagnostic process can obscure identification of the time interval in which incidence occurs. When serial diagnoses are fallible, even small error rates can seriously disrupt interpretation and make using the aforementioned methods difficult or impossible. For example, if false diagnoses (both false positives and negatives) occur independently with probability .05 in a longitudinal study involving four serial diagnoses, 19% of the strings of serial diagnoses would be expected to contain at least one error. If the underlying process is progressive, many of these errors would be noticeable: At face value, some patterns of diagnoses would describe regressions. Errors yielding patterns of diagnoses that are progressive would not be detectable. Simply omitting any subjects with inconsistent patterns from the analysis introduces bias. Another possible approach, using the first reported incidence of progression, also introduces bias (Schlesselman 1977). To analyze clinical data on the diagnosis of sexual maturation among subjects with sickle-cell disease, models are developed for jointly parameterizing incidence and error rates. An EM algorithm is presented that allows tractable maximum likelihood estimation even when the times of diagnoses are irregular and vary among subjects. Likelihood ratio tests are used to assess relationships between categorical covariates and both incidence and error rates. Data from the Cooperative Study of Sickle Cell Disease are analyzed to describe the age distribution for the onset of puberty (according to the Tanner stage index) among homozygous (SS) males. Clear delays in maturation are apparent among SS males. Diagnostic error rates for Tanner staging appear to vary with the subject's age. False-positive diagnoses appear to be more common than false-negative diagnoses."], ["Maximum Entropy and the Lottery", null], ["Investigating Smooth Multiple Regression by the Method of Average Derivatives", null], ["Bootstrap Confidence Regions for Directional Data", null], ["Some Asymptotic Results for Learning in Single Hidden-Layer Feedforward Network Models", null], ["Models for the Analysis of Association in Multivariate Contingency Tables", "The present work describes a family of models that can be used for the analysis of association in multiway cross-classifications. The models are complementary to the usual (i.e., the so-called hierarchical) log-linear models. An advantage of this family of models is that it is frequently possible to give a more parsimonious description of the association than is possible with the usual log-linear models approach. The models will be especially useful when some or all of the variables are ordinal. The models are derived from a reparameterization of the saturated model based on singular value decompositions of interaction terms in the log-linear parameterization. Multivariate association models given in Goodman (1979, 1986), Clogg (1982a,b), Agresti and Kezouh (1983), Gilula and Haberman (1988), and Becker and Clogg (1989) are obtained as special cases. The utility of the models developed in this article is demonstrated with the analysis of two examples. The first example, a cross-classification of Danish youths by education, father's income, and father's social rank, illustrates how association models with multidimensional scores can be used to provide a straightforward description of the association in a seemingly complex table; a graphical display of estimated score parameters facilitates the analysis. The usefulness of association models for modeling three-factor interaction is demonstrated in the second example, a cross-classification of married couples in the 1974 General Social Survey by sex of respondent, husband's highest degree, and wife's highest degree."], ["Probabilistic Measures of Adequacy of a Numerical Search for a Global Maximum", "Measures of the probability of all unobserved species are applied to the problem of assessing the adequacy of a search for a global maximum using random starting points. The measures, as used here, estimate the probability that an iterative algorithm using a randomly selected starting point will find a solution not observed in previous random starting points. The probability of an unobserved global maximum is less than or equal to this probability. We used these measures to evaluate the adequacy of our search procedure for the maximum likelihood estimates of the parameters of a mixture of two normals. These measures indicated that for most problems generated there was little chance that there were unobserved domains of convergence. Occasional problems, however, had appreciable estimated probabilities. In such problems, examination of the data suggested regions where a more focused search for unobserved domains of convergence was warranted."], ["Optimal Matching for Observational Studies", "Matching is a common method of adjustment in observational studies. Currently, matched samples are constructed using greedy heuristics (or \u201cstepwise\u201d procedures) that produce, in general, suboptimal matchings. With respect to a particular criterion, a matched sample is suboptimal if it could be improved by changing the controls assigned to specific treated units, that is, if it could be improved with the data at hand. Here, optimal matched samples are obtained using network flow theory. In addition to providing optimal matched-pair samples, this approach yields optimal constructions for several statistical matching problems that have not been studied previously, including the construction of matched samples with multiple controls, with a variable number of controls, and the construction of balanced matched samples that combine features of pair matching and frequency matching. Computational efficiency is discussed. Extensive use is made of ideas from two essentially disjoint literatures, namely statistical matching in observational studies and graph algorithms for matching. The article contains brief reviews of both topics."], ["Empirical Bayes Estimation of Undercount in the Decennial Census", "On April 1, 1990, the decennial census for the United States will be conducted by the U.S. Bureau of the Census. By December 31, 1990, the Census Bureau is specified by law to submit state population counts for the purpose of reapportionment of the U.S. House of Representatives, and by March 31, 1991, to submit small-area population counts for the purpose of redistricting. Census counts are used in a variety of other ways: for revenue-sharing formulas between different levels of government, for demographic projections, as a base for morbidity and mortality statistics, and so forth. Inaccurate census counts should be cause for concern for the whole nation. It is universally acknowledged that certain groups of people (e.g., young black males, illegal aliens, etc.) are harder to count than others. If the hard-to-count groups are distributed in equal proportions throughout the United States, there would be far less controversy over what to do about the uncounted people. As it is, many large American cities such as Chicago, Detroit, New York, and Los Angeles feel they are losing federal funds because their cities contain larger numbers of the groups that are less well counted. And certain states such as New York and California feel they are underrepresented in Congress, to the benefit of Midwestern states such as Indiana and Iowa. Census undercount is defined simply as the difference between the true count and the census count, expressed as a percentage of the true count. Small-area estimation of this undercount is considered here, using empirical Bayes methods based on a new and, it is argued, more realistic model than has been used before. Grouping of like subareas from areas such as states, counties, and so on into strata is a useful way of reducing the variance of undercount estimators. By modeling the subareas within a stratum to have a common mean and variances inversely proportional to their census counts, and by taking into account sampling of the areas (e.g., by dual-system estimation), empirical Bayes estimators that compromise between the (weighted) stratum average and the sample value can be constructed. The amount of compromise is shown to depend on the relative importance of stratum variance to sampling variance. These estimators are evaluated at the state level (51 states, including Washington, D.C.) and stratified on race/ethnicity (3 strata) using data from the 1980 postenumeration survey (PEP 3\u20138, for the noninstitutional population)."], ["A Distribution-Free Multivariate Sign Test Based on Interdirections", null], ["Bayesian Marginal Inference", null], ["Analysis of Nonadditive Multiway Classifications", null], ["Regression Analysis of Multivariate Incomplete Failure Time Data by Modeling Marginal Distributions", "Many survival studies record the times to two or more distinct failures on each subject. The failures may be events of different natures or may be repetitions of the same kind of event. In this article, we consider the regression analysis of such multivariate failure time observations. Each marginal distribution of the failure times is formulated by a Cox proportional hazards model. No specific structure of dependence among the distinct failure times on each subject is imposed. The regression parameters in the Cox models are estimated by maximizing the failure-specific partial likelihoods. The resulting estimators are shown to be asymptotically jointly normal with a covariance matrix that can be consistently estimated. Simultaneous inferential procedures are then proposed. Extensive Monte Carlo studies indicate that the normal approximation is adequate for practical use. The new methods allow time-dependent covariates, missing observations, and arbitrary patterns of censorship. They are illustrated with two real-life examples. For recurrent failure time data, various regression methods have been proposed in the literature. These methods, however, generally assume stringent structures of dependence among the recurrences of each subject. Moreover, as shown in the present article, they are rather sensitive to model misspecification."], ["The Robust Inference for the Cox Proportional Hazards Model", null], ["Bayesian Estimation and Prediction for Pareto Data", "Data from a classical Pareto distribution are to be used to make inferences about the inequality and precision parameters. In addition, it is desired to predict the behavior of further observations from the distribution. Three typical data configurations are considered (iid and two types of censoring). Dependent conjugate prior analyses are reviewed and are compared with an analysis involving independent priors for the inequality and precision parameters. It is argued that mathematical tractability should be, perhaps, a minor consideration in the choice of priors. A comparative example is included."], ["Smoothing and Interpolation with the State-Space Model", "A new result dealing with smoothing and interpolation in the state-space model is developed and explored. The result simplifies the derivation of existing smoothing algorithms and provides alternate forms that have analytic and practical computing advantages. The connection to signal extraction and interpolation is explored, and diffuse specifications are considered."], ["Median Unbiasedness and Pitman Closeness", "Under certain conditions, a median unbiased estimator of a parameter is shown to be the Pitman-closest estimator within a certain class of estimators. For location, scale, or location-scale families of distributions, the class of equivariant estimators possesses some desirable properties; the median-unbiasedness of equivariant estimators is explored in the characterization of the Pitman closeness property. Illustrative examples bear fruitful implications of this characterization."], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Empirical Bayes Procedures for Stabilizing Maps of U.S. Cancer Mortality Rates", "The geographic mapping of age-standardized, cause-specific death rates is a powerful tool for identifying possible etiologic factors, because the spatial distribution of mortality risks can be examined for correlations with the spatial distribution of disease-specific risk factors. This article presents a two-stage empirical Bayes procedure for calculating age-standardized cancer death rates, for use in mapping, which are adjusted for the stochasticity of rates in small area populations. Using the adjusted rates helps isolate and identify spatial patterns in the rates. The model is applied to sex-specific data on U.S. county cancer mortality in the white population for 15 cancer sites for three decades: 1950\u20131959, 1960\u20131969, and 1970\u20131979. Selected results are presented as maps of county death rates for white males."], [null, null], ["An Analysis of Density-Dependent Viability Selection", null], ["Application of Bayesian Decision Procedure to the Inference of Genetic Linkage", null], ["Patterns of Oppression: An Exploratory Analysis of Human-Rights Data", "This article reports an exploratory analysis of human-rights data compiled for 74 countries during 1982\u20131983. The information is combined with social, demographic, and economic indexes, then subjected to the scrutiny of CART, nearest-neighbor clustering, and other classification and clustering algorithms. Human-rights data separate most nations into one of seven categories; these categories can also be accurately predicted from widely available social and economic indexes. The results suggest that one need monitor only a small number of human-rights indexes to capture most of the original information, thus reducing the cost and complexity of human-rights assessment. Also, the analysis discovers socioeconomic indexes associated with certain patterns of human-rights violation, and this may enable a better understanding of climates that foster compliance with the International Bill of Rights."], ["Discriminating Strata in Scatterplots", "When multiple groups are shown in a scatterplot each stratum is represented by a different symbol; for example, three strata might be coded using red, green, and yellow circles. Various symbol types were compared by behavioral experiment: Subjects were fastest when strata were coded using different colors and slowest when strata were coded with confusable letters\u2014but there were no differences in accuracy. Accuracy differed only when processing time was restricted, again with different colors and confusable letters representing the two extremes. We conclude that color is the optimal symbol type and show that measuring response latency in addition to accuracy is essential in research on graphical perception."], ["Multivariate Time Series Projections of Parameterized Age-Specific Fertility Rates", "Projection of individual age-specific fertility rates is a forecasting problem of high dimension. We solve this dimensionality problem by using parametric curves to approximate the annual age-specific rates and a multivariate time series model to forecast the curve parameters. These yield forecasts of future fertility curves, which are then used to compute age-specific fertility rate forecasts. This reduces the dimensionality of the forecasting problem and also guarantees that long-run projections of age-specific fertility rates will exhibit a smooth shape across age similar to historical data. Short-term projections are improved by also using simple techniques to forecast the deviations of the fitted curves from the actual rates. The article applies this approach to age-specific fertility data for U.S. white women from 1921\u20131984. The resulting forecasts are examined, and the multivariate model is used to investigate possible relations between the curve parameters, expressed as the total fertility rate, the mean age of childbearing, and the standard deviation of age at childbearing. The only strong relationship found is the contemporaneous relationship between the mean and standard deviation of age at childbearing. A variation of this approach, in conjunction with traditional demographic judgment, was used in a recent set of U.S. Census Bureau population projections. We discuss this implementation and compare the Census Bureau projections with those produced directly from the model presented here."], ["An Application of Nonlinear Bounded Influence Estimation to Aggregate Bank Borrowing from the Federal Reserve", "Forecasting aggregate discount window borrowing has posed difficulties for the Federal Reserve, due in part to outliers resulting from numerous institutional changes and special borrowing situations. This article applies bounded influence estimation and influence diagnostics to identify and adjust for outliers in the case of discount window borrowing. Since most banks borrow from the Federal Reserve infrequently, the model is nonlinear and of the switching regression class. We perform case-deletion diagnostics, modifying the empirical influence function of Reid and Crepeau (1985) for the nonlinear regression. The bounded influence estimator (BIE) extends Carroll and Ruppert (1985, 1987). Influence diagnostics and bounded influence estimation contribute to the investigation of discount window borrowing in a number of ways. Examination of weights generated by the estimator reveals that the BIE downweights observations during periods of known institutional change affecting the discount window. Consequently, the relative importance of institutional events can be inferred. Examples of institutional circumstances creating outliers in this application include the Monetary Control Act of 1980, the threatened bond defaults of late 1982, the February 1984 change to Contemporaneous Reserve Accounting, solvency problems of the Continental Bank of Illinois during 1984, and borrowing in May 1985 by Maryland thrift institutions affected by bank runs. Influence diagnostics in turn provide information concerning what parameter estimates have been most altered as a result. An interesting intuitive finding arises from use of the switching regression model: outlying observations tend to influence estimated parameters only from the same regime. When compared with maximum likelihood, we find that the BIE substantially alters some parameter estimates, including the estimated switchpoints. Identifying and correcting for outliers by the BIE improves the ability of the model to discriminate among regimes. Moreover, bounded influence estimation in discount window borrowing increases estimator efficiency, reduces residual pattern, discriminates outliers from large estimated residuals, and slightly improves the goodness of fit. The overall findings indicate that influence diagnostics and bounded influence estimation could significantly assist the Federal Reserve in explaining and predicting discount window borrowing."], ["Fully Exponential Laplace Approximations to Expectations and Variances of Nonpositive Functions", null], ["Approximate Bayesian Inference in Conditionally Independent Hierarchical Models (Parametric Empirical Bayes Models)", null], ["Monte Carlo Approximations in Bayesian Decision Theory", "In decision-making problems, the Bayesian action and its posterior expected loss usually cannot be obtained analytically. In this article we study a Monte Carlo method for approximating the Bayesian action and its posterior expected loss. The Monte Carlo approximation to the Bayesian action is obtained through (a) approximating the posterior expected loss function by using the Monte Carlo integration method and (b) searching the minimum of the approximated posterior expected loss function. As the Monte Carlo sample size diverges to infinity, the Monte Carlo approximations are shown to be convergent in general situations. The rates of the convergence are also obtained under some regularity conditions on the loss function. Two accuracy measures of the Monte Carlo approximations are proposed. Some examples are presented for illustration."], ["Discretized and Interpolated Kernel Density Estimates", "The kernel estimate of a probability density function inherits its smoothness properties from the kernel density chosen by the investigator. Nevertheless, for computational (and especially graphical) reasons, exact kernel density estimates are often presented in (piecewise constant) discretized or (piecewise linear) interpolated form, using the exact estimate only at some grid of points. The asymptotic integrated mean squared error properties of these modifications are studied. It is seen that discretization may adversely affect the order of magnitude of this risk criterion, so care needs to be taken in practice to limit the degree of discretization employed. This roughness effect essentially disappears when interpolation is used instead; then, it takes a remarkably coarse grid to result in more than a negligible deterioration in the estimate's performance. Data discretization prior to applying the kernel density estimation prescription is also addressed. Such prebinning does not affect the smoothness of the resulting estimate and has a reassuringly minor effect on overall performance, both in conjunction with postbinning schemes and without."], ["A Semiparametric Model for Randomly Truncated Data", null], ["A Model for Informative Censoring", null], ["A Relation between Pairwise Balanced and Variance Balanced Block Designs", "We show that the problems of constructing pairwise balanced block designs and variance balanced block designs are equivalent. This provides incentive to study only the direct construction for the simplest of these designs\u2014namely, pairwise balanced designs."], ["The World Fertility Survey: An Appraisal of Methodology", "In the first half of the article, a broad account of content and procedures is given. In conduct of individual surveys, the achievements of the World Fertility Survey were based on thoroughness rather than technical superiority. The later aspects of the program, including analysis, archiving, and data dissemination, were more innovative and represent models of excellence for similar future enquiries. In overall terms, the program is judged to be an expensive success. In the second half of the article, two methodological issues are discussed in more detail: the collection of retrospective birth histories and the translation of survey instruments into local languages."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Design of Group Sequential Clinical Trials with Multiple Endpoints", "Group sequential testing for randomized clinical trials designed with multiple endpoints is considered. Previously computed tables for single endpoints are still useful, with a change in interpretation of certain parameters. The advantage in setting sample size based on multiple endpoints is that the sample size required when a trial is designed using more than one endpoint is smaller than the sample size based on any single endpoint, if the two calculations are undertaken using matching significance levels and powers. An example is provided, and possible extensions of the work are discussed."], [null, null], ["Accounting for Misclassification in the Cause-of-Death Test for Carcinogenicity", null], ["Multiple Test Procedures in Clinical Dose Finding Studies", "Test procedures in clinical dose finding studies are considered when it can be assumed that increasing dose levels lead to a larger or at least equal efficacy. If one is interested in various comparisons within the set of dose levels, one arrives at a problem of multiple testing against ordered alternatives. Some multiple test procedures controlling the multiple level of significance are compared in a simulation study. It follows that a simple procedure that only uses comparisons between neighboring dose levels can be recommended. The degree of approximation of this Bonferroni-type procedure is investigated under normal assumptions."], ["Analysis of Rank Measures of Association for Ordinal Data from Longitudinal Studies", null], ["Response Surface Analysis with Correlated Data: A Nonlinear Model Approach", "Statistical methods for fitting nonlinear functions to data generated by correlated response variates are discussed. Estimation of the model parameters is performed with an iterative two-stage scheme. The estimation procedure accommodates both within-unit and between-unit variability in fitting a response surface. Under regularity conditions the procedure yields asymptotically normal, strongly consistent estimators. If desired a patterned variance-covariance matrix can be assumed and incorporated into the model. The methods are illustrated by an analysis of data from a study of the combined effects of hepatotoxins in which between- and within-subject measurements are recorded."], ["The Analysis of Multiple Correlated Binary Outcomes: Application to Rodent Teratology Experiments", "In a developmental toxicity study, pregnant animals are exposed to the test substance and their offspring are assessed for defects. Often, multiple observations are made on each fetus, in which case the data are doubly nested. In this article we adapt the approach of Liang and Zeger (1986) and Zeger and Liang (1986) to yield an analysis, which appropriately allows for the correlation structure."], ["Contrasts for Identifying the Minimum Effective Dose", "Dose-response studies are frequently used to study the effects of an experimental compound on various responses. One characteristic of the compound that is frequently of interest is the minimum effective dose (MED). Generally, inference about the MED is made by comparing various dose groups with a control (Dunnett 1955; Williams 1971). This article examines two families of contrasts used for related problems, and proposes a new family of contrasts (called basin contrasts) designed specifically for identifying the MED. Monte Carlo studies are used to demonstrate the superiority of the contrast procedures and make recommendations for their use."], ["Estimation of Treatment Difference following a Sequential Clinical Trial", "To reduce the number of patients needed to reach a conclusion in a clinical trial, a sequential trial design may be used. In theory the distribution of responses is biased when a sequential trial is stopped. The usual estimation methods of treatment difference applied in nonsequential situations are, therefore, not necessarily applicable following a sequential test. Whitehead (1983) proposed a method for estimating the treatment effect following some types of two-sample sequential tests. By stochastic simulation we compared this method with an ordinary maximum likelihood estimator for nonsequential two-sample situations, following two different sequential tests. The results show that there is little difference between the methods, although Whitehead's method was developed especially for sequential plans. A correction of the bias of the maximum likelihood estimate proposed by Cox (1952), however, gives results that are much closer to the expected values. We also investigated the estimation of treatment difference following a two-sample sequential Wilcoxon test proposed by Skovlund and Wall\u00f8e (1988). The (nonsequential) estimation methods applied following this test give good results over a range of distributions and for various real treatment differences. This article advocates a class of nonsequential estimators that has generally been considered inappropriate for sequential trials. Simulations demonstrate that these nonsequential estimators are as good as (and in some situations better than) the sequential estimator developed by Whitehead."], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Editors' Report for 1988", null], ["A Non-Gaussian Model for Time Series with Pulses", null], ["Inference Based on Retrospective Ascertainment: An Analysis of the Data on Transfusion-Related AIDS", null], ["Multivariate Methods for Clustered Binary Data with More than One Level of Nesting", "Clustered data occur frequently in statistical practice. In some areas of application, such as ophthalmology, clustered data are the rule rather than the exception. In this setting, standard multivariate methods such as logistic regression are invalid, because of the lack of independence among outcomes for individual sample points within a cluster. In Rosner (1984), a polychotomous logistic regression model was presented to control for the effect of both cluster and individual-specific covariates while accounting for the correlation among units within a cluster. This model reduces to a beta-binomial model in the absence of covariates, and to an ordinary logistic model for clusters of size 1 and for larger clusters when no correlation is present. A 1 level of nesting correlation structure is assumed, as is typically found in ophthalmology, where the person is the cluster and the individual eyes are the units within a cluster. In this article, this model is generalized to a 2 level of nesting structure. This model is motivated by an ophthalmologic application, where data are collected in family units and one wishes to model simultaneously the correlation at the first level of nesting among persons within a family, and at the second level of nesting between eyes within a person. In the absence of covariates, the model is characterized by a compound beta-binomial distribution that generalizes the beta-binomial distribution to more than one level of nesting. The compound beta-binomial distribution is then augmented in a regression setting, to allow for the presence of family-, person-, and eye-specific covariates while controlling for correlation at each level of nesting. Extensions to more than two levels of nesting are also considered."], ["An Assessment of Publication Bias Using a Sample of Published Clinical Trials", "The potential magnitude of publication bias has been examined with a consecutive sample of published cancer clinical trials. The analysis is based on the premise that the magnitude of the true treatment effect is unrelated to design features of the study, in particular sample size. This assumption permits an analysis based only on published studies. Three primary endpoints are examined: overall patient survival, disease-free survival, and tumor response rate. There are striking trends for each endpoint, with small studies appearing to possess large treatment effects and large studies possessing relatively small effects. It is believed that these differences are primarily due to publication bias. The bias is very large: Absolute differences observed were 41% for overall survival, 79% for disease-free survival, and 17% for response rates. Other study features have been examined that might be associated with bias, or that might be responsible for the striking trends regarding sample size. The results indicate that absence of randomization leads to significant bias, and studies conducted in a single institution are somewhat more prone to bias than multiinstitutional studies, though the trends are less consistent in the latter case. No strong trend was observed for journal type. Nevertheless, none of these variables could account for the strong effect of sample size. Sensitivity analyses of the results were conducted and alternative models were considered. These analyses generally support the contention that the magnitude of the bias due to sample size cannot be explained by alternative factors. An implication of this study is that the results of small published studies are typically unreliable, even taking into account the fact that such trials are imprecise due to sampling variation."], ["Spatial Modeling of Regional Variables", "In this article, accumulated sudden infant death syndrome (SIDS) data, from 1974\u20131978 and 1979\u20131984 for the counties of North Carolina, are analyzed. After a spatial exploratory data analysis, Markov random-field models are fit to the data. The (spatial) trend is meant to capture the large-scale variation in the data, and the variance and spatial dependence are meant to capture the small-scale variation. The trend could be a function of other explanatory variables or could simply be modeled as a function of spatial location. Both models are fit and compared. The results give an excellent illustration of a phenomenon already well-known in time series, that autocorrelation in data can be due to an undiscovered explanatory variable. Indeed, for 1974\u20131978 we confirm a dependence of SIDS rate on proportion of nonwhite babies born, along with insignificant spatial correlation. Without this regressor variable, however, the spatial correlation is significant. In 1979\u20131984, perhaps due to reporting bias or the effect of public-education programs in infant health, the proportion of nonwhite babies born is no longer an important explanatory variable. SIDS is currently a leading category of postneonatal death, yet its cause is still a mystery. It accounts for about 7,000 deaths a year in the United States, taking the lives of about two infants per 1,000 live births. In contrast to the usual pathologic and physiologic studies of SIDS, this article takes an epidemiologic approach, using data available at the county level. The SIDS data analyzed are in a form that is representative of many problems encountered in the health and social sciences. Counts of individuals from a known or estimated base occur in epidemiologic studies (e.g., consider cancer incidence in a particular year, from the base of population years at risk, for U.S. counties), census surveys (e.g., for assessing undercount consider the dual-system estimate of uncounted people in a decennial census, from the base of total number of people, for U.S. states), and so forth. It is hoped that the spatial methods presented will prove useful in a variety of such problems."], ["Respondent Behavior in Magnitude Estimation", "Some researchers argue that data obtained using magnitude estimation have metric properties superior to those of data obtained using traditional survey items. The analysis in this article examines evidence relevant to the assumptions that respondents can express ratios using numbers, can locate an internal continuum, and can use the standard stimulus effectively. The data are answers to 5 practice items designed to teach magnitude estimation using line lengths and up to 18 items about stress experienced in caring for a sick person. The five practice items provide evidence of respondents' ability to give numeric estimates: 28% gave at least one answer that falls outside fairly tolerant boundaries that define a reasonable answer. Other results indicate that respondents do not improve with practice, that accuracy on the practice items does not predict good performance on magnitude estimates of a subjective property, and that respondents for whom the standard stress stimulus was hypothetical were more likely than others to answer inappropriately. Scale values calculated as medians or geometric means are accurate for the practice items and at least sensible for the care-giving items. Evaluating individual-level scores is more difficult. On the one hand, using a single point estimate entails considerable error, and unscalable answers may have disproportionate effects on individual level statistics. Furthermore, older, less educated, and male respondents find providing magnitude estimates difficult, possibly biasing analysis of individual scores. On the other hand, magnitude estimates may preserve important distinctions among individuals and may be less susceptible to bias arising from group differences in the use of the phrases attached to category ratings."], ["Advances in Record-Linkage Methodology as Applied to Matching the 1985 Census of Tampa, Florida", "A test census of Tampa, Florida and an independent postenumeration survey (PES) were conducted by the U.S. Census Bureau in 1985. The PES was a stratified block sample with heavy emphasis placed on hard-to-count population groups. Matching the individuals in the census to the individuals in the PES is an important aspect of census coverage evaluation and consequently a very important process for any census adjustment operations that might be planned. For such an adjustment to be feasible, record-linkage software had to be developed that could perform matches with a high degree of accuracy and that was based on an underlying mathematical theory. A principal purpose of the PES was to provide an opportunity to evaluate the newly implemented record-linkage system and associated methodology. This article discusses the theoretical and practical issues encountered in conducting the matching operation and presents the results of that operation. A review of the theoretical background of the record-linkage problem provides a framework for discussions of the decision procedure, file blocking, and the independence assumption. The estimation of the parameters required by the decision procedure is an important aspect of the methodology, and the techniques presented provide a practical system that is easily implemented. The matching algorithm (discussed in detail) uses the linear sum assignment model to \u201cpair\u201d the records. The Tampa, Florida, matching methodology is described in the final sections of the article. Included in the discussion are the results of the matching itself, an independent clerical review of the matches and nonmatches, conclusions, problem areas, and future work required."], ["Composite Estimation of Totals for Livestock Surveys", "Four different preliminary estimators are employed by the National Agricultural Statistics Service of the U.S. Department of Agriculture to obtain the final estimate of livestock inventories of major states. To combine these four estimators a composite estimator (which is a weighted average) is proposed. Two types of composite estimators\u2014one with fixed weights, the other with adaptive weights\u2014are investigated. The composite estimator with adaptive weights is derived by minimizing a quadratic function subject to linear constraints. The variance and mean squared error of the composite estimator with adaptive weights are evaluated by the jackknife method. Numerical results based on the data from the 1984 June Enumerative Survey conducted by the Department of Agriculture support the use of composite estimation."], ["Toward a Methodology for Estimating Temporary Residents", "Most population statistics for states, counties, and cities refer to permanent residents, or persons who spend most of their time in an area. At certain times, however, many states and local areas have large numbers of temporary residents who exert a significant impact on the area's economy, physical environment, and quality of life. Typically, very little is known about the number, timing, and characteristics of these residents. This study discusses the problems of defining and estimating temporary residents, focusing on the strengths and weaknesses of a number of data sources and estimation techniques. It closes with an assessment of the potential usefulness of developing methods to estimate temporary residents."], ["Transformations to Symmetry and Homoscedasticity", null], ["A Class of Logistic Regression Models for Multivariate Binary Time Series", "A logistic model for multivariate binary time series is proposed. First, we establish the equivalence between a log-linear model for the marginal distribution of a multivariate binary random vector and logistic models for the conditional distributions of each component given the others. The logistic formulation is used to describe a Markov chain for each series, which implies a Markov model for the vector of time series. A pseudolikelihood estimation procedure is presented. The methods are illustrated with data on psychosomatic and psychological diagnoses for families in a health-maintenance plan."], ["Models for Longitudinal Data with Random Effects and AR(1) Errors", "For longitudinal data on several individuals, linear models that contain both random effects across individuals and autocorrelation in the within-individual errors are studied. A score test for autocorrelation in the within-individual errors for the \u201cconditional independence\u201d random effects model is first developed. An explicit maximum likelihood estimation procedure using the scoring method for the model with random effects and (autoregressive) AR(1) errors is then derived. Empirical Bayes estimation of the random effects and prediction of future responses of an individual based on this random effects with AR(1) errors model are also considered. A numerical example is presented to illustrate these methods."], ["Conditionally Unbiased Bounded-Influence Estimation in General Regression Models, with Applications to Generalized Linear Models", null], ["Tests for Detecting Overdispersion in Poisson Regression Models", "Poisson regression models are widely used in analyzing count data. This article develops tests for detecting extra-Poisson variation in such situations. The tests can be obtained as score tests against arbitrary mixed Poisson alternatives and are generalizations of tests of Fisher (1950) and Collings and Margolin (1985). Accurate approximations for computing significance levels are given, and the power of the tests against negative binomial alternatives is compared with those of the Pearson and deviance statistics. One way to test for extra-Poisson variation is to fit models that parametrically incorporate and then test for the absence of such variation within the models; for example, negative binomial models can be used in this way (Cameron and Trivedi 1986; Lawless 1987a). The tests in this article require only the Poisson model to be fitted. Two test statistics are developed that are motivated partly by a desire to have good distributional approximations for computing significance levels. Simulations suggest that one of the statistics should be satisfactory for testing extra-Poisson variation in most practical situations involving Poisson regression models."], ["Local Model Influence", "This article develops a general method for assessing the influence of model assumptions in a Bayesian analysis. We assume that model choices are indexed by a hyperparameter with some given initial choice. We use the term \u201cmodel\u201d to encompass both the sampling model and the prior distribution. We wish to assess the effect of changing the hyperparameter away from the initial choice. We are performing a sensitivity analysis, with the hyperparameter defining our perturbations. We use the Kullback\u2014Leibler divergence to measure the difference between posteriors corresponding to different choices of the hyperparameter. We also measure the change in priors. If small changes in the priors lead to large changes in posteriors, the choice of hyperparameter is influential. The second-order difference in the Kullback\u2014Leibler divergence is expressed by Fisher information matrices. The relative change in posteriors compared with priors may be summarized by the relative eigenvalue of the posterior and prior Fisher information matrices. The corresponding eigenvector indicates which aspects of the perturbation hyperparameter are most influential. Examples considered are the choice of conjugate prior in regression, case weights in regression, and the choice of Dirichlet prior for multinomials."], ["Robustification of Kalman Filter Models", null], ["Bivariate Survival Models Induced by Frailties", null], [null, null], ["Principal Curves", null], ["Inference Based on Ranks for the Multiple-Design Multivariate Linear Model", null], ["Improved Estimation in a Contingency Table: Independence Structure", "Estimation of the cell probabilities in a two-way contingency table is considered when it is plausible that the table might have an independence structure relating to the two traits. In a classical nonparametric setup, the unrestricted maximum likelihood estimators of the cell probabilities are the corresponding sample proportions; under the assumption of independence, the restricted estimators are the product of the respective row and column sample proportions. The latter estimators behave better than the former when independence actually holds, but a different picture may emerge for possible departure from the assumed independence structure; the restricted estimators may be heavily biased, inefficient, and even inconsistent. For this reason, a preliminary test on independence based on the classical contingency chi-squared statistic may be conveniently incorporated in the formulation of a preliminary test estimator of the matrix of cell probabilities. Since, typically, we have a multiparameter estimation problem, a pretest or shrinkage estimator based on the classical Stein rule may also be formulated in the same vein. The primary objective of this article is to focus on the asymptotic distribution theory of all four estimators under the null hypothesis of independence as well as local Pitman-type alternatives. The bias and risk based on suitable quadratic loss functions of these estimators are considered in the same asymptotic setup, and these are then incorporated in the study of the asymptotic relative performance of these estimators. In the light of their asymptotic risks, neither the preliminary test nor the shrinkage estimators dominates the other, though each fares well relative to the unrestricted or the restricted maximum likelihood estimators."], ["Tests for Autocorrelation and Randomness in Multiple Time Series", "There exist several tests for autocorrelation and randomness in multiple time series. Unfortunately, the exact distributions of these statistics are unknown and the asymptotic distributions that are known do not provide adequate approximation to the exact ones in small samples. In this article, the test statistics are modified. Asymptotically, these modified statistics are equivalent to their original counterparts; however, it is found that the asymptotic distributions of these statistics provide adequate approximation to the exact ones in relatively small samples and possibly when the time series are nonnormal. The adequacy of the approximations is examined by simulation experiments. The original test statistics are based on sample lag cross-covariances, autocovariances, cross-correlations, and autocorrelations standardized by their asymptotic means and covariances. The modified statistics are obtained when the asymptotic means and covariances in the standardization are replaced by the exact means and covariances. The expressions for these moments are derived on the assumption that the time series is Gaussian. These moments (both asymptotic and exact) involve nuisance parameters. In constructing the test statistics, these nuisance parameters are replaced by their sample counterparts, which are consistent estimates of the parameters."], ["Estimation of Multivariate Distributions under Stochastic Ordering", null], ["Approximate Power for Repeated-Measures ANOVA Lacking Sphericity", null], ["Exact Tests That Recover Interblock Information in Balanced Incomplete Blocks Designs", null], ["Approximations and Bounds for the Distribution of the Scan Statistic", null], ["Nonparametric Policy Analysis", null], ["Experimental Randomization and the Validity of Normal-Theory Inference", null], ["Classification Efficiency of Multinomial Logistic Regression Relative to Ordinal Logistic Regression", "Classification procedures are useful for the prediction of a response (or outcome) as a result of knowledge of the levels of one or more independent (or predictor) variables. The procedure is said to classify the (possibly multivariate) observation to a level of the response variable. An example might be the prediction of whether an individual will be well, suffer a nonfatal heart attack, or suffer a fatal heart attack. This prediction might be made on the basis of the levels of various independent variables, such as weight, blood pressure, and serum cholesterol, to name a few. The three response categories of the aforementioned example are ordinal. An example of three nonordered response categories might be as follows: well, death from heart attack, and death from cancer. There is some recent interest in ordinal classification procedures. It is reasonable to assume that, when the response variable is ordinal, inclusion of ordinality in the classification model to be estimated should improve model performance. It is, however, also true that, if ordinality is indeed the case, it will be evident in the parameters estimated by the usual (unordered) models. If there may be applications where use of the ordinal models could lead to bias, it becomes important to evaluate the magnitude of any advantage these models may have in ordinal situations. That is to say, if ordinal models are measurably superior in ordinal situations, the benefit of using them may outweigh the risk of their misuse. The purpose of this research was to quantify the impact of incorporating an ordinality assumption into a classification model in the case in which ordinality is indeed a correct assumption. We have chosen to compare Anderson's ordinal logistic regression model (Anderson 1984; Greenland 1985) with the multinomial logistic model. The former is a special case of the latter, having been developed by imposing ordinality constraints on the latter. Therefore, their inherent relationship as ordered and unordered versions of essentially the same model makes them attractive to compare; differences between their performances may be attributed to the ordinality assumption rather than other differences. In this article, they are compared on the basis of their asymptotic rates of misclassification error. The ordinal model was found to be more efficient when efficiency was measured in terms of the asymptotic rate of excess classification errors due to estimation of parameters. The relative classification efficiency of the usual multinomial model to the ordinal logistic model was shown to decrease as the number of response categories increases, as the Mahalanobis distance separating response populations decreases, or as the number of independent variables increases."], ["Quasi-Independence in Ordinal Triangular Contingency Tables", null], ["Bootstrap Bartlett Adjustment in Seemingly Unrelated Regression", null], [null, null], ["Statistical \u201cDiscoveries\u201d and Effect-Size Estimation", null], ["Book Reviews", null], ["Letters to the Editor", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["How to Hope with Statistics", null], ["Sensitivity Analysis of Seasonal Adjustments: Empirical Case Studies", "Three detailed case studies illustrating the seasonal analysis of economic time series are presented using component models for seasonal and nonseasonal behavior. Analyses are performed within a semi-Bayesian framework where inferences for target quantities of interest, such as seasonally adjusted values, are obtained as posterior distributions conditional on observed data and fitted parameter values. Such an approach is similar to previous model-based methods of seasonal analysis, but new models and algorithms are used and, more important, a sensitivity analysis is performed to determine the extent to which conclusions vary across a range of plausible fitted models. It is found that sensitivity to variation across plausible models is not unusual in practice. The logical conclusion of the investigation is that a fully Bayesian analysis is required that averages conditional posteriors over a posterior distribution for the model parameters. Such an analysis is necessarily sensitive to the choice of prior distribution. We believe that such dependencies on assumptions external to the data are inevitable in complex problems such as seasonal adjustment. The stochastic components of our nonseasonal and seasonal models are based on modified fractional Gaussian noise, as described in Carlin, Dempster, and Jonas (1985). The models allow for joint estimation of both fixed and random effects, a capability that is used to estimate the initial values of nonstationary components and is further illustrated in the data analysis by the use of trading-day adjustments. Our model structure is described in detail, but technical details of algorithms used to perform likelihood and conditional posterior calculations for fitted models are omitted in favor of the empirical case studies. The examples include some comparisons to the nonparametric X-11 method and to an autoregressive moving average modeling approach. One of the examples exhibits particularly striking differences between the model-based seasonal adjustments and those of X-11."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Methods for Analysis of Longitudinal Data: Blood-Lead Concentrations and Cognitive Development", "This article reports results from a longitudinal study investigating the effects of low-to-moderate prenatal and postnatal lead exposure on the cognitive development of children during the first 18 months of life. Study hypotheses are expressed as a sequence of linear models for the outcome variable adjusted score on the Bayley Scales of Mental Development (MDIA), as a function of cord blood-lead concentration, infant blood-lead concentration at semiannual examinations, and other characteristics of study participants. These models are fitted to MDIA measurements on three occasions for as many as 214 infants, first assuming an arbitrary multivariate covariance structure for the repeated measurements and then with covariance structure arising from a random-effects model for errors. Estimates of the effects of lead exposure are not sensitive to the assumed covariance structure. The article describes several approaches to residual analysis and outlier detection in the longitudinal setting. In particular, it shows how empirical Bayes residuals can be used to estimate the partial regression coefficient of covariates not included in the linear model. The major findings concerning the effect of lead exposure on cognitive development are (a) a clear association between cord blood-lead concentration and Bayley scores in the first 18 months of life, (b) no clear evidence of an effect of cumulative postnatal exposure, and (c) a tendency for children who had higher lead concentrations at 6 months of age to exhibit poorer performance at 18 months than children with low lead concentrations. These findings have implications for acceptable blood-lead concentrations in children and pregnant women."], ["Log-Linear Analysis of Censored Survival Data with Partially Observed Covariates", "Log-linear models provide a flexible means of extending life table techniques for the analysis of censored survival data with categorical covariates, as discussed by Holford (1980) and Laird and Olivier (1981). We extend this methodology to incorporate cases in which one or more of the categorical covariates are sometimes missing. Maximum likelihood estimates of the parameters are calculated using data from all cases. This can result in large gains in efficiency over standard methods that require the exclusion of cases with incomplete data. With this approach, we assume that the hazard function, conditional on the covariates, is a stepwise function over disjoint intervals of time. The model has two parts: a log-linear model describing the hazard parameters, and a multinomial model describing the probabilities in the contingency table defined by the covariates. The main interest is in the model for the hazard parameters. We show how to calculate maximum likelihood estimates of parameters of the model either by an application of the EM algorithm in conjunction with one cycle of iterative proportional fitting in the M step or by using the Newton\u2014Raphson algorithm. Estimates of standard errors are computed from the empirical information matrix. When using our proposed maximum likelihood approach, two additional assumptions are needed in addition to the usual assumptions of noninformative censoring. First, the mechanism causing missing covariates must be ignorable (Rubin 1976) in that the probability that a covariate is missing cannot depend on the covariate itself or on other covariates that are missing. The second assumption is that the distribution of the random censoring variable does not depend on any covariate that is missing. The first example, investigating the influence of several covariates on time to diagnosis of high blood pressure in a large cohort of men, shows clear gains in efficiency of our approach over analysis of complete cases and illustrates the flexibility of the log-linear approach. A second example of survival times of symptomatic and asymptomatic lymphoma patients shows interesting differences between the complete-case analysis and the maximum likelihood approach, which could be due to a nonrandom missing-value mechanism."], ["Analysis of Repeated Categorical Measurements with Conditional Likelihood Methods", "This article presents a method for analyzing repeated measures with categorical responses based on maximizing a conditional likelihood. Many existing methods for analyzing repeated categorical responses, such as those proposed by Duncan (1980, 1985), Bishop, Fienberg, and Holland (1975), and Cochran (1950) are special cases of the method based on the conditional likelihood approach. This article explores the relationships among the various methods and extends several of the existing methods. The conditional likelihood approach is used to analyze data from a four-wave panel study with trichotomous responses. The data, taken from Fienberg, Bromet, Follmann, Lambert, and May (1985), are from an investigation of the psychological effects of the accident at the Three Mile Island nuclear power plant in the spring of 1979. In this example, methods are presented for checking the assumption of local independence in the model used for the conditional likelihood analysis. With an extension of the results of Tjur (1982), it is demonstrated that the conditional estimates can be obtained with standard statistical software."], ["New Methods for Tables of School Life, with Applications to U.S. Data from Recent School Years", "Using current and retrospective school-enrollment data from October Current Population Surveys (CPS's) together with demographic accounts for the U.S. civilian noninstitutional population, this study specifies new methods for the estimation of school-life tables. The new methods combine multistate (increment\u2014decrement) methods for those ages for which CPS data on enrollment flows are available together with prevalence-rate methods for other ages for which CPS data are available on enrollment stocks alone. Applying an algorithm to construct tables of school life for the 1969\u20131970, 1974\u20131975, 1979\u20131980, and 1984\u20131985 school years, it is found that, between the ages of 14 and 34, most sex-, race-, and school-year-specific groups have an average of 1.5 to 2.0 exits per person, from enrolled to not enrolled in school. These exits are interspersed by an average of .5 to 1.0 exits per person, from not enrolled to enrolled in school. Since prevalence-rate life-table methods arbitrarily constrain individuals to only one exit from school enrollment to not enrolled over the life course, this finding verifies the need for increment\u2014decrement methods for the teenage and young-adult years. Several other findings regarding differentials in schooling life by sex and race and trends over time are discussed."], ["A Combined Structural and Flexible Functional Approach for Modeling Energy Substitution", "In theory, a flexible functional form can approximate an arbitrary elasticity of substitution; however, when faced with an actual finite data set, a parsimonious parametric functional form that imposes more structural restrictions may yield a more accurate approximation about underlying substitution. Therefore, for uncovering the underlying degree of input substitution a data-exploration strategy is suggested that combines the parametric family of a system of quadratic log-ratio demand equations and conventional flexible functional forms. This allows testing for more restrictive hypotheses about substitution. These issues are examined in the context of interfuel substitution at the two-digit SIC level for manufacturing industries in Ontario and Quebec, Canada. A basic tenet of the approach of this article is that although letting data speak for itself is preferred, economic theory should be used wherever possible, and a simple model is preferable to complicated models unless the evidence suggests otherwise. Given the variations of technologies, managerial behavior, and speed of adjustment across manufacturing sectors, the existence of a single optimal model is questioned. In fact, out of 39 industries examined, suitable models are found for 30 industries. Out of the 30, 18 can be modeled by the relatively more restricted quadratic log-ratio specifications and 12 by the more general flexible functional forms. By using a combined structural and flexible functional approach, this article obtains more precise estimates of the elasticities of substitution and models more likely to satisfy monotonicity and concavity conditions than those obtained by considering the conventional search procedures within a flexible functional approach. It is also interesting to note that, unlike previous studies of interfuel substitution, some industries display biased technological changes, and some appear to possess a dynamic adjustment process consistent with an error-correction mechanism composed of both derivative and proportional control mechanisms."], ["The Average Workweek of Capital in Manufacturing, 1952\u20131984", "This article revises and updates through 1984 an annual and quarterly series originally constructed from 1952\u20131968 on the average workweek of capital in manufacturing. The series provides estimates of the input of capital services into production and uses data on production-worker employment, shift patterns, and weekly working hours to estimate the number of hours that the capital stock is operated each week. The updated series indicates that the capital stock is working longer and the hours of operation are more cyclically variable than in the earlier period. Shift work is found to be an important source of variation in the hours of operation. The findings imply a change in the relationship of the capital stock to manufacturing production."], ["Confidence Bands for a Distribution Function Using the Bootstrap", null], ["The Influence of Assumptions Implicit in a Model on Parametric Inference", "In choosing a model, there are important considerations beyond the goodness of fit, namely that the subsequent inferences should not rely on questionable assumptions implicit in the model, because otherwise they will be suspect. The two-parameter lognormal distribution provides an example. The symmetry on the log scale leads to the anomaly that relative frequencies of classes in the lower tail can substantially affect inferences on the upper tail. This is contrary to the realities of the alcohol consumption example considered in this article, as well as other practical situations. A mathematical analysis of assumptions is made, using the fact that maximum likelihood estimation is asymptotically equivalent to least squares regression. It is shown that, for inferences regarding the upper tail of the two-parameter lognormal distribution, the introduction of the usual third parameter, or, almost equivalently, censoring the lower tail, can remove the anomaly. The theoretical results are illustrated by a numerical example, using data from an alcohol consumption survey at Busselton, Western Australia, showing how the specification affects the estimates of certain functions of the class probabilities."], ["Hellinger Deviance Tests: Efficiency, Breakdown Points, and Examples", "Hellinger distance analogs of likelihood ratio tests are proposed for parametric inference. The proposed tests are based on minimized Hellinger distances between nonparametric density estimates and densities corresponding to null and unconstrained parametric models. If the parametric model is correct, the Hellinger deviance test is asymptotically equivalent to the likelihood ratio test. A simulation study examines the relative performance of these tests in finite samples. Breakdown properties of the Hellinger deviance tests and likelihood ratio tests are compared. The Hellinger tests are generally high breakdown-point tests, whereas, in many instances, the likelihood ratio tests have breakdown points of 0. Two numerical examples illustrate the implementation and performance of the proposed tests. Likelihood-based methods are widely used in applications. They provide a routine method for generating efficient estimates and inference statements. In many instances, however, these methods are known to be sensitive to anomalous data points, and the careful data analyst will screen for outliers prior to a likelihood-based analysis. In large-scale data analysis, for instance, when processing the results of many similar experiments in a data base, the outlier screening procedure needs to be automated. This type of data processing is common in mutation research, where numerous chemicals are subjected to a battery of mutagenicity and carcinogenicity trials. An automated outlier screen is one method for obtaining robust inferences in this setting. The Hellinger test studied here is proposed as a more direct method for obtaining robust inferences. The test statistic simply measures how much farther the data are from the null model than from the unconstrained model. Regions of low predicted density receive very little weight, so the Hellinger deviance test is able to cope with anomalous data. The method adapts to a variety of parametric hypothesis testing situations."], ["The Efficiency and Consistency of Approximations to the Jackknife Variance Estimators", null], ["Lower Bounds on Bayes Factors for Interval Null Hypotheses", null], ["Some Statistical Properties of a Family of Continuous Univariate Distributions", null], ["Power Approximations to Multinomial Tests of Fit", null], ["Analysis of Sets of Two-Way Contingency Tables Using Association Models", null], ["Compatible Conditional Distributions", null], ["Relative Entropy Measures of Multivariate Dependence", null], ["Regularized Discriminant Analysis", "Linear and quadratic discriminant analysis are considered in the small-sample, high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample-based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved."], ["A Nonmetric Approach to Linear Discriminant Analysis", "A new nonmetric linear discriminant analysis approach is proposed that is based on the maximization of an index of separation differing from that used by the classical method. The possibility of choosing between Fisher's classical discriminant function and the one proposed here enables us to reduce the number of misclassifications for given data. The method is exemplified on empirical data and various simulations and is compared with the classical linear discriminant analysis."], ["Approximate Confidence Intervals for the Number of Clusters", "We consider clustering for the purpose of data reduction. Similar objects are grouped together in clusters so that one can then work with the few cluster descriptors instead of the many data points. The quality of any given clustering is measured by a loss function that takes into account both the parsimony of the clustering and the loss of information due to clustering. An optimal clustering can be obtained by minimizing the theoretical loss function. It is shown that a sample version of the loss function and optimal clustering converge strongly to their theoretical counterparts as the sample size tends to infinity. We then develop a bootstrap-based procedure for obtaining approximate confidence bounds on the number of clusters in the \u201cbest\u201d clustering. The effectiveness of this procedure is evaluated in a simulation study. An application is presented."], ["Uniformly More Powerful Tests for Hypotheses concerning Linear Inequalities and Normal Means", null], ["Estimating a Product of Means: Bayesian Analysis with Reference Priors", null], ["Pairwise Comparisons of Generally Correlated Means", null], ["Optimal Reporting of Predictions", null], ["Testing for the Constancy of Parameters over Time", null], ["Testing and Modeling Threshold Autoregressive Processes", "The threshold autoregressive model is one of the nonlinear time series models available in the literature. It was first proposed by Tong (1978) and discussed in detail by Tong and Lim (1980) and Tong (1983). The major features of this class of models are limit cycles, amplitude dependent frequencies, and jump phenomena. Much of the original motivation of the model is concerned with limit cycles of a cyclical time series, and indeed the model is capable of producing asymmetric limit cycles. The threshold autoregressive model, however, has not received much attention in application. This is due to (a) the lack of a suitable modeling procedure and (b) the inability to identify the threshold variable and estimate the threshold values. The primary goal of this article, therefore, is to suggest a simple yet widely applicable model-building procedure for threshold autoregressive models. Based on some predictive residuals, a simple statistic is proposed to test for threshold nonlinearity and specify the threshold variable. Some supplementary graphic devices are then used to identify the number and locations of the potential thresholds. Finally, these statistics are used to build a threshold model. The test statistic and its properties are derived by simple linear regression. Its performance in the finite-sample case is evaluated by simulation and real-world data analysis. The statistic performs well as compared with an alternative test available in the literature. Further applications of threshold autoregressive models are also suggested, including handling heterogeneous time series and modeling random processes with periodic variances whose periodicity is not fixed. The latter phenomenon is commonly encountered in practice, especially in econometrics and biological sciences."], ["The Effect of Covariance Structure on Variance Estimation in Balanced Growth-Curve Models with Random Parameters", "Intuition suggests that altering the covariance structure of a parametric model for repeated-measures data alters the variances of the model's estimated mean parameters. The purpose of this article is to sharpen such intuition for a family of growth-curve models with differing numbers of random effects for the individual sampling units and with a fixed structure on the mean. For every member of this family, the maximum likelihood (ML) estimator of the fixed effects is identical to the ordinary least squares (OLS) estimator. In addition, simple closed-form ML and restricted maximum likelihood estimators for the variance and covariance parameters exist for every member. As a consequence, closed-form expressions for the estimated variance-covariance matrix of the OLS estimator of the fixed effects also exist for the entire family. We derive explicit relationships between the variance and covariance parameter estimators from different members of the family and thereby extend some familiar results. For example, it is well known that for balanced and complete longitudinal designs the compound symmetry assumption for the covariance structure of the serial observations (i.e., assuming one random effect for each sampling unit) yields a more precise estimate of the slope of the population growth curve than of its intercept. It is also well known that for such designs the diagonal covariance structure assumption of OLS regression (i.e., no random effects for the units) yields a more precise estimate of the intercept than does the compound symmetry assumption, and a less precise estimate of the slope. We extend such relationships as these to growth-curve models whose covariance structures are of increasing linear complexity (i.e., assuming two or more random effects for each of the sampling units). We find that, in general, the variance of the OLS estimator depends strongly on assumed covariance structure. We also find that, specifically, the linear growth-curve assumption (i.e., two random effects for each sampling unit) is a conservative assumption, in that such will not give misleadingly small variance estimates for both intercept and slope even if a more complex covariance structure actually holds. We illustrate these points with a data example. In the conclusion, we extend our results by showing how they may apply to more general families of balanced growth-curve models that allow wider ranges of covariance structures and designs for the random effects, such as when different population subgroups require different covariance structures. We also address several computational issues involved in transformations of growth-curve models to canonical form."], ["Efficiency of Ordinary Least Squares for Linear Models with Autocorrelation", "This article provides a reconsideration of Kramer's (1980) results on least squares estimation in linear models with autocorrelated errors. Kramer's results are shown to be dependent on his measure of efficiency and to understate the advantages of correcting for autocorrelation."], ["Bivariate Sign Tests", "In this article the bivariate location problem is treated. New appealing bivariate analogs of the univariate sign tests are proposed for testing the null hypothesis concerning the unknown symmetry center. These tests remain unaltered under any nonsingular linear transformation. From these promising findings a whole family of locally most powerful invariant sign tests is introduced. The tests proposed earlier (Blumen 1958; Hodges 1955) are specific members of this family. For example, Blumen's test appears to be optimal against bivariate normal (or any other elliptic) alternatives. The limiting distributions are derived both under the null hypothesis and under the contiguous alternatives. These limiting distributions are then used to derive asymptotic relative efficiencies. It is found that Blumen's test has the efficiency .785 relative to Hotelling's test against bivariate normal alternatives. For other locally most powerful sign tests the corresponding efficiency depends on the significance level and the power, but not too strongly. In fact, the value .785 also serves as an approximation for other sign tests. The lower bound for the efficiency of Blumen's test relative to Hotelling's test is established against elliptic alternatives. The restriction to unimodal elliptic alternatives increases the lower bound to \u00bd. Finally, some results on Hodges's test are included. Bivariate sign tests can be applied, for example, in paired comparison situations with a bivariate response variable. The hypothesis of no difference between two treatments then implies that the paired response differences are symmetric about the origin, which can be tested by using bivariate sign tests. No extra assumptions concerning the unknown bivariate distributions are needed. In addition, other examples of applications are included. The proposed tests use the direction angles of the observations. Therefore, they can be used also in testing the uniformity (and a kind of symmetry) of a circular distribution. In Tables 1, 2, and 3 critical values for three different sign tests are given."], ["Minimum Variance Estimation in Stratified Sampling", null], ["Small Domain Estimation: A Conditional Analysis", null], ["The Kernel Estimate of a Regression Function in Likelihood-Based Models", null], ["Local Bandwidth Selection for Kernel Estimates", "A kernel estimate of a curve that uses an adaptive procedure for local selection of the bandwidth is considered here. A two-step procedure is proposed for estimating the local bandwidth that minimizes the mean squared error (MSE) of a kernel estimator for nonparametric regression. First, a consistent estimate of the exact MSE is constructed. Then the bandwidth that minimizes the estimate of the MSE is calculated. Sufficient conditions under which this bandwidth is asymptotically optimal and normally distributed are given. The local bandwidth selection procedure was implemented on some simulated data and compared to a global bandwidth selection procedure proposed by Rice (1984b). A 68%\u201391% reduction in the average MSE of a kernel estimator was realized with the local bandwidth selection procedure. Such a scheme was also studied by M\u00fcller (1985) and termed a direct pilot estimator approach. M\u00fcller derived sufficient conditions similar to those presented here, under which the direct pilot estimator approach provides a consistent estimator of the local bandwidth. His result is slightly more general in the specification of the interval that is searched for the optimizing bandwidth. The asymptotic normality of the optimizing bandwidth and the simulation results presented here have not to our knowledge appeared in the statistical literature. M\u00fcller and Stadtm\u00fcller (1987) proposed a local bandwidth selection procedure for kernel estimates that is based on the asymptotic expression for the MSE of the kernel estimator. Our procedure is based upon the finite sample expression for the MSE. It remains to be shown how the two procedures compare when applied to small data sets."], ["Additive Isotonic Models", "Additive isotonic models generalize linear models by replacing lines with isotonic (nondecreasing) transformations. Fitted transformations of several explanatory variables are added together and then transformed by a known function to yield fitted values of the response variable. The isotonic transformations are chosen to minimize an explicit criterion, such as the negative log-likelihood, by an algorithm that optimizes one transformation at a time while adjusting for the current fitted values of the others, cycling until the criterion converges. This approach can be used in various situations, notably for generalizing ordinary linear regression and linear logistic regression. At each step of the algorithm, the needed optimal isotonic transformation is found using a simple generalization of the standard pool-adjacent-violators algorithm (Ayer, Brunk, Ewing, Reid, and Silverman 1955). The fitted transformations are always made up of flat steps, so the technique is useful for finding optimal stratifications of the explanatory variables, but not for finding smooth transformations. The technique speeds the process of checking variables for possible addition to an existing model, because the possibility of finding a useful isotonic transformation can be ruled out if the best such transformation performs poorly. Isotonic regression is usually extended to multivariate settings by fitting a multivariate function that preserves a partial order on the values of all of the explanatory variables. Such models are more general than additive isotonic models, but they are less interpretable (because of their high dimension) and more prone to overfit the data. Transformations that preserve a partial order on a subset of the explanatory variables can be incorporated into additive isotonic models to model interactions within the subset. More general mixtures of techniques within the additive framework are also possible; smooth, isotonic, and parametric transformations can be applied to different explanatory variables within one additive model."], ["Generalizing Logistic Regression by Nonparametric Mixing", null], ["Estimating the Reliability of Systems Subject to Imperfect Repair", null], [null, null], ["The Contrast Algorithm for Unbalanced Multifactor Analysis of Variance", null], ["Efficacies of Rank-Transform Statistics in Two-Way Models with No Interaction", null], ["Book Reviews", null], ["Letters to the Editor", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"]]}