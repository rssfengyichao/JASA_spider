{"1994": [["Bayesian Faces via Hierarchical Template Modeling", "We consider the problem of directly extracting high-level shape information from images of scenes involving faces. The approach adopted owes much to the work of Grenander and colleagues at Brown University on pattern analysis and involves designing stochastic deformable templates for objects in the underlying image scenes. A wide range of realistic object poses can be captured by imposing a prior probability distribution over the space of allowable deformations. We show how hierarchical models can be used to organize the prior information into a coherent structure. Markov chain Monte Carlo methods are exploited to recover the deformation given observed image data."], ["A Space-Time Survival Point Process for a Longleaf Pine Forest in Southern Georgia", null], ["Stochastic Population Forecasts for the United States: Beyond High, Medium, and Low", "Conventional population projections use \u201chigh,\u201d \u201cmedium,\u201d and \u201clow\u201d scenarios to indicate uncertainty, but probability interpretations are rarely given, and in any event the resulting ranges for vital rates, births, deaths, age groups sizes, age ratios, and population size cannot possibly be probabilistically consistent with one another. This article presents and implements a new method for making stochastic population forecasts that provide consistent probability intervals. We blend mathematical demography and statistical time series methods to estimate stochastic models of fertility and mortality based on U.S. data back to 1900 and then use the theory of random-matrix products to forecast various demographic measures and their associated probability intervals to the year 2065. Our expected total population sizes agree quite closely with the Census medium projections, and our 95 percent probability intervals are close to the Census high and low scenarios. But Census intervals in 2065 for ages 65+ are nearly three times as broad as ours, and for 85+ are nearly twice as broad. In contrast, our intervals for the total dependency and youth dependency ratios are more than twice as broad as theirs, and our ratio for the elderly dependency ratio is 12 times as great as theirs. These items have major implications for policy, and these contrasting indications of uncertainty clearly show the limitations of the conventional scenario-based methods."], ["A Probabilistic Model for the Spatial Distribution of Party Support in Multiparty Electorates", "Spatial models of electoral competition locate voters and parties at points in euclidean space\u2014representing issue positions\u2014and specify utility of voters for parties as functions of these positions. Utility functions may also have stochastic components unassociated with issues. In this article probabilistic models are compared in which the utility function incorporates distance between voter and party positions (proximity model) or a scalar product (directional model). Model specification is significant because of its relation to party strategy and the resulting spatial distribution of parties. Maximum likelihood is used to estimate parameters of a mixed directional and proximity model\u2014with stochastic and strategic components\u2014from data in Norwegian and Swedish election studies. Expected spatial distributions of voters by party support are determined for the multiparty electorates of Norway and Sweden. Unlike previous deterministic work, which strongly favors the directional model, the results obtained here suggest that a mixture of proximity and directional probabilistic models may provide a substantially better fit than either pure model or a deterministic model."], ["Discussion", null], ["Rejoinder", null], ["Statistical Quality Control of HIV-1 ELISA Test Performance", "Ensuring the quality and performance of human immunodeficiency virus, type 1 (HIV-1) enzyme-linked immunosorbent assay (ELISA) testing in routine laboratory situations is an important practical concern. In this article we develop statistical quality control procedures designed to monitor test performance of the microplates being processed. First, the logarithm of measured optical density is shown to be a more appropriate scale on which to work. Shewhart control charts are considered but are not entirely satisfactory because of between-plate variation. Correct classification depends more on within-plate variation than on between-plate variation. Range charts are useful, but they do not directly indicate whether the results from any particular microplate are reliable. Consequently, a new statistical control chart\u2014the separation chart\u2014is proposed, and the assumptions on which it is based are empirically verified. Retrospective analysis of nearly 1,300 microplates, using separation and range charts, demonstrates that these statistical process control methods may represent a potentially useful supplement to the manufacturer's recommended quality control procedures."], ["Characterization and Optimization of a Wave-Soldering Process", "A case study for improving the quality of a wave-soldering process that produced printed circuit boards (PCB's) is presented. A mixed-level fractional factorial design was implemented in a high-volume production system during normal operational hours. The observed ordered-categorical data from the bottom-side soldered leads were weighted to formulate the average, spatial uniformity, and dispersion process performance measurements. For lead classes like the integrated circuit and printed grid array, a polynomial model was established using the least squares method with weights provided by a dispersion function. The main-effect and interaction model terms were selected by forward and all-subsets regressions. Production quotas, topside defects, presoldering board temperature, and variance models were used to set the constraints for simultaneously optimizing predictions from the average and the uniformity models of all leads. A nonlinear optimization routine was used to determine the best and most robust settings for the continuous and discrete process variables. Results from a confirmatory experiment showed an improvement of mean soldering quality by 33% and of uniformity by 39%."], ["Modeling Household Purchase Behavior with Logistic Normal Regression", "The successful development of marketing strategies requires the accurate measurement of household preferences and their reaction to variables such as price and advertising. Manufacturers, for example, often offer products at a reduced price for a limited period. One reason for this practice is that it induces households to try the promoted product with the hope of retaining them as permanent customers. The successful implementation of this strategy requires knowledge of the extent of price sensitivity in the population, effective methods of advertising, and the existence of a carry-over effect in the household's evaluation of the product. Logistic regression models are often used to relate household demographics, prices, and advertising variables to household purchase decisions. In this article we extend the standard model to include cross-sectional and serial correlation in household preferences and provide algorithms for estimating the model with random effects. The model is applied to scanner panel data for ketchup purchases, and substantive insights into household preference, brand switching, and autocorrelated purchase behavior are obtained."], ["Some Applications of Radial Plots", "A radial plot is a graphical display for comparing estimates that have differing precisions. It is a scatter plot of standardized estimates against reciprocals of standard errors, possibly with respect to a transformed scale, designed so that the original estimates can be compared and interpreted. The estimates may be means, regression coefficients, proportions, rates, odds ratios, random effects, or indeed any parameter estimates that merit comparison between individuals or groups. This article illustrates some uses of radial plots by discussing a variety of data examples taken from the literature. The statistical application areas include interlaboratory trials, point process event rates, empirical Bayes estimation, modeling of counting data, analysis of overdispersed and underdispersed binomial and Poisson data, mixture modeling and meta-analysis."], ["Capturing the Intangible Concept of Information", "The purpose of this article is to discuss the intricacies of quantifying information in some statistical problems. The aim is to develop a general appreciation for the meanings of information functions rather than their mathematical use. This theme integrates fundamental aspects of the contributions of Kullback, Lindley, and Jaynes and bridges chaos to probability modeling. A synopsis of information-theoretic statistics is presented in the form of a pyramid with Shannon at the vertex and a triangular base that signifies three distinct variants of quantifying information: discrimination information (Kullback), mutual information (Lindley), and maximum entropy information (Jaynes). Examples of capturing information by the maximum entropy (ME) method are discussed. It is shown that the ME approach produces a general class of logit models capable of capturing various forms of sample and nonsample information. Diagnostics for quantifying information captured by the ME logit models are given, and decomposition of information into orthogonal components is presented. Basic geometry is used to display information graphically in a simple example. An overview of quantifying information in chaotic systems is presented, and a discrimination information diagnostic for studying chaotic data is introduced. Finally, some brief comments about future research are given."], ["Flexible Discriminant Analysis by Optimal Scoring", "Fisher's linear discriminant analysis is a valuable tool for multigroup classification. With a large number of predictors, one can find a reduced number of discriminant coordinate functions that are \u201coptimal\u201d for separating the groups. With two such functions, one can produce a classification map that partitions the reduced space into regions that are identified with group membership, and the decision boundaries are linear. This article is about richer nonlinear classification schemes. Linear discriminant analysis is equivalent to multiresponse linear regression using optimal scorings to represent the groups. In this paper, we obtain nonparametric versions of discriminant analysis by replacing linear regression by any nonparametric regression method. In this way, any multiresponse regression technique (such as MARS or neural networks) can be postprocessed to improve its classification performance."], ["Generalized S-Estimators", "In this article we introduce a new type of positive-breakdown regression method, called a generalized S-estimator (or GS-estimator), based on the minimization of a generalized M-estimator of residual scale. We compare the class of GS-estimators with the usual S-estimators, including least median of squares. It turns out that GS-estimators attain a much higher efficiency than S-estimators, at the cost of a slightly increased worst-case bias. We investigate the breakdown point, the maxbias curve, and the influence function of GS-estimators. We also give an algorithm for computing GS-estimators and apply it to real and simulated data."], ["Bootstrap Methods for Finite Populations", "We show that the familiar bootstrap plug-in rule of Efron has a natural analog in finite population settings. In our method a characteristic of the population is estimated by the average value of the characteristic over a class of empirical populations constructed from the sample. Our method extends that of Gross to situations in which the stratum sizes are not integer multiples of their respective sample sizes. Moreover, we show that our method can be used to generate second-order correct confidence intervals for smooth functions of population means, a property that has not been established for other resampling methods suggested in the literature. A second resampling method is proposed that also leads to second-order correct confidence intervals and is less computationally intensive than our bootstrap. But a simulation study reveals that the second method can be quite unstable in some situations, whereas our bootstrap performs very well."], ["A Comparison of Certain Bootstrap Confidence Intervals in the Cox Model", null], ["The Stationary Bootstrap", null], ["Simulation-Extrapolation Estimation in Parametric Measurement Error Models", "We describe a simulation-based method of inference for parametric measurement error models in which the measurement error variance is known or at least well estimated. The method entails adding additional measurement error in known increments to the data, computing estimates from the contaminated data, establishing a trend between these estimates and the variance of the added errors, and extrapolating this trend back to the case of no measurement error. We show that the method is equivalent or asymptotically equivalent to method-of-moments estimation in linear measurement error modeling. Simulation studies are presented showing that the method produces estimators that are nearly asymptotically unbiased and efficient in standard and nonstandard logistic regression models. An oversimplified but fairly accurate description of the method is that it is method-of-moments estimation using Monte Carlo-derived estimating equations."], ["Fast Very Robust Methods for the Detection of Multiple Outliers", "A few repeats of a simple forward search from a random starting point are shown to provide sufficiently robust parameter estimates to reveal masked multiple outliers. The stability of the patterns obtained is exhibited by the stalactite plot. The robust estimators used are least median of squares for regression and the minimum volume ellipsoid for multivariate outliers. The forward search also has potential as an algorithm for calculation of these parameter estimates. For large problems, parallel computing provides appreciable reduction in computational time."], ["Wavelet Methods for Curve Estimation", "The theory of wavelets is a developing branch of mathematics with a wide range of potential applications. Compactly supported wavelets are particularly interesting because of their natural ability to represent data with intrinsically local properties. They are useful for the detection of edges and singularities in image and sound analysis and for data compression. But most of the wavelet-based procedures currently available do not explicitly account for the presence of noise in the data. A discussion of how this can be done in the setting of some simple nonparametric curve estimation problems is given. Wavelet analogies of some familiar kernel and orthogonal series estimators are introduced, and their finite sample and asymptotic properties are studied. We discover that there is a fundamental instability in the asymptotic variance of wavelet estimators caused by the lack of translation invariance of the wavelet transform. This is related to the properties of certain lacunary sequences. The practical consequences of this instability are assessed."], ["Semiparametric Regression in Likelihood-Based Models", null], ["A Semiparametric Correction for Attenuation", null], ["Joint Continuum Regression for Multiple Predictands", "This article generalizes continuum regression (CR) in the hope that regressors \u201cjointly constructed\u201d for several predictands might improve on the separate prediction of individual predictands. The generalization developed is a mixture of principal components regression and de Jong's modification of partial least squares for multiple predictands. The balance of ingredients can be chosen by cross-validation, as can the number of regressors constructed. The new method has been tested on real and simulated data. The indications are that conditions for the superiority of the joint approach may be rare in practice."], ["Best Nonnegative Invariant Partially Orthogonal Quadratic Estimation in Normal Regression", null], ["Group Duration Analysis of the Proportional Hazard Model: Minimum Chi-squared Estimators and Specification Tests", "This article develops a semiparametric, minimum chi-squared estimation method of the proportional hazard model for the case when durations are grouped and covariates are categorical. The proposed estimator is easy to compute, yet asymptotically as efficient as the maximum likelihood estimator. This article also suggests simple specification tests for the proportional hazard model. If proportionality holds, then two sets of minimum chi-squared estimators, one from a further grouped data and the other from the original grouped data, will converge to the same quantity; otherwise, they will not. Therefore, a test of the equality of these two sets of estimators will offer a test for proportionality. Monte Carlo simulations demonstrate the performance of these estimators and specification tests. In addition, two real data applications illustrate the implementation of the suggested methods and the contexts in which these methods are useful."], ["Nonparametric Identification of Nonlinear Time Series: Projections", "We study the possibility of identifying general linear and nonlinear time series models using nonparametric methods. The kernel estimators of the conditional mean and variance are used as a basis, and the properties of these quantities as model indicators are briefly discussed. Some drawbacks are pointed out, and motivated by these we introduce projections as tools of identification. The projections are especially useful for additive modeling. Expressions for the asymptotic bias and variance are obtained. The projection of the conditional variance is suggested as a tool for identifying heteroscedastic time series. The results are illustrated by simulations for both the estimators of the projections and the estimators of the conditional mean and variance."], ["Nonparametric Identification of Nonlinear Time Series: Selecting Significant Lags", "In this article we suggest a nonparametric procedure for selecting significant lags in the model description of a general nonlinear stationary time series. The procedure can be applied to both the conditional mean and the conditional variance and is valid for heteroscedastic series. The procedure is illustrated by simulations and sunspot data, lynx data, and blowfly data are analyzed. It is indicated that projectors can be used in conjunction with the procedure for selecting significant lags to check the adequacy of an additive time series model."], ["Efficient Tests of Nonstationary Hypotheses", "This article proposes tests for unit root and other forms of nonstationarity that are asymptotically locally most powerful against a certain class of alternatives and have asymptotic critical values given by the chi-squared distribution. Many existing unit root tests do not share these properties. The alternatives include fractionally and seasonally fractionally differenced processes. There is considerable flexibility in our choice of null hypothesis, which can entail one or more integer or fractional roots of arbitrary order anywhere on the unit circle in the complex plane. For example, we can test for a fractional degree of integration of order 1/2; this can be interpreted as a test for nonstationarity against stationarity. \u201cOverdifferencing\u201d stationary null hypotheses can also be tested. The test statistic is derived via the score principle and is conveniently expressed in the frequency domain. The series tested are regression errors, which, when the hypothesized differencing is correct, are white noise or more generally have weak parametric autocorrelation. We establish the null and local limit distributions of the statistic under mild regularity conditions. We find that Bloomfield's exponential spectral model can provide an especially neat form for the test statistic. We apply the tests to a number of empirical time series originally analyzed by Box and Jenkins, and in several cases find that our tests reject the order of differencing that Box and Jenkins proposed. We also report Monte Carlo studies of finite-sample behavior of versions of our tests and comparisons with other tests."], ["Dynamic Stochastic Models for Time-Dependent Ordered Paired Comparison Systems", "When paired comparisons are made sequentially over time as for example in chess competitions, it is natural to assume that the underlying abilities do change with time. Previous approaches are based on fixed updating schemes where the increments and decrements are fixed functions of the underlying abilities. The parameters that determine the functions have to be specified a priori and are based on rational reasoning. We suggest an alternative scheme for keeping track with the underlying abilities. Our approach is based on two components: a response model that specifies the connection between the observations and the underlying abilities and a transition model that specifies the variation of abilities over time. The response model is a very general paired comparison model allowing for ties and ordered responses. The transition model incorporates random walk models and local linear trend models. Taken together, these two components form a non-Gaussian state-space model. Based on recent results, recursive posterior mode estimation algorithms are given and the relation to previous approaches is worked out. The performance of the method is illustrated by simulation results and an application to soccer data of the German Bundesliga."], ["Resolution of Additive Mixtures into Source Components and Contributions: A Compositional Approach", "Methodology is developed for analysis of observations that are random linear combinations of point \u201csource components.\u201d Dual goals are to estimate unknown source identities and to characterize the mixing process by which sources contribute to the observations. Observations are modeled as arising from a mixture distribution, whereby the mixing component characterizes the process of interest and the kernel component captures measurement error. A parametric model is proposed, and maximum likelihood estimates of source and mixing parameters are obtained. Estimate performance is investigated by Monte Carlo simulation. Major results are devoted to studying a constraint framework within which model identifiability is guaranteed. For maximal generality, a compositional framework is applied throughout. The resolution problem discussed in this article is common in the physical sciences. For illustration, an application to air pollution data is presented."], ["Competing Risks on Coherent Reliability Systems: Estimation in the Parametric Case", "A coherent reliability system is a machine composed of a number of components that fails as soon as the set of failed components reaches certain fatal set-thresholds. On failure of the machine, an autopsy report reveals its time of failure and the set of failed components. I use incomplete data methods to obtain maximum likelihood estimates of parameters characterizing the lifetime distributions of the components, based on a sample of autopsy reports."], ["Dependence Properties of Generalized Liouville Distributions on the Simplex", "Compositional data arise naturally in several branches of science, including chemistry, geology, biology, medicine, ecology, and manufacturing design. Thus the correct statistical analysis of this type of data is of fundamental importance. Prior to the pioneering and extensive work of Aitchison, the Dirichlet distribution provided the parametric model of choice when analyzing such data. But Aitchison and others have since pointed out that the Dirichlet distribution is appropriate only for modeling compositional vectors that exhibit forms of extreme independence. Aitchison developed his logistic normal classes partly in response to this shortcoming. Unfortunately, Aitchison's logistic normal classes do not contain the Dirichlet distribution as a special case. As a result, they exhibit interesting dependence structures but are unable to model extreme independence. The generalized Liouville family is studied in this article. This family, which contains the Dirichlet class, is shown to contain densities that can model either complicated dependence or complicated independence structures."], ["Parameterizations for Natural Exponential Families with Quadratic Variance Functions", "Parameterizations for natural exponential families (NEF's) with quadratic variance functions (QVF's) are compared according to the nearness to normality of the likelihood and posterior distribution. Nonnormality of the likelihood (posterior) is measured using two criteria. The first is the magnitude of a standardized third derivative of the log-likelihood (logposterior density); the second is a comparison of the probability of particular tail regions under the normalized likelihood (posterior distribution) and under the corresponding normal approximation. A relationship is given that links these two criteria. Sample sizes are recommended for adequate normality in the likelihood for various parameterizations of the NEF-QVF models, and these results are extended to Bayesian models with a conjugate prior."], ["Adjusted Least Squares Estimates for the Scaled Regression Coefficients with Censored Data", null], ["The Bias of Estimating Equations with Application to the Error Rate of Logistic Discrimination", "The logistic regression classification method uses parameter estimates that are the solution of an estimating equation. This article derives a convenient expression for the bias of a vector estimator defined by estimating equations. The expression and the results of O'Neill are used to derive the bias and the error or misclassification rate of logistic regression classification in two examples where the assumed model for logistic regression does not hold. Logistic regression classification is found to be very robust for the departures considered."], ["Testing for Sufficient Follow-Up and Outliers in Survival Data", "A situation in which a sample of failure times with many of the largest times censored may be taken as evidence of a proportion of \u201cimmune\u201d or \u201ccured\u201d individuals, who are not subject to failure, in the population. A plot of the Kaplan-Meier empirical distribution function for such data will tend to level off near its right extreme, provided that follow-up of individuals has been continued for long enough. This article develops a diagnostic test for sufficient follow-up in samples where there may or may not be immunes and demonstrates its properties. The procedure is illustrated on some criminological data, leading us also to propose a test for outliers in this kind of situation."], ["Permutation Tests for Correlation in Regression Errors", null], ["Controlling Correlations in Latin Hypercube Samples", null], ["Random Effects Models for Combining Results from Controlled and Uncontrolled Studies in a Meta-Analysis", "A random effects model is used to analyze meta-analyses containing both controlled and uncontrolled studies. A noniterative estimator of the treatment effect is derived using least squares, with the between-study variance substituted by an empirical Bayes estimator. The estimator is shown to be strongly consistent when the between-study variance is known, and the variance estimator is also shown to be strongly consistent. The estimator is shown to have desirable heuristic properties."], ["Group Sequential Clinical Trials: A Classical Evaluation of Bayesian Decision-Theoretic Designs", null], ["Model Selection and Accounting for Model Uncertainty in Graphical Models Using Occam's Window", "Three examples are given. The first two concern data sets that have been analyzed by several authors in the context of model selection. The third addresses a urological diagnostic problem. In each example, our model averaging approach provides better out-of-sample predictive performance than any single model that might reasonably have been selected."], ["Statistical Disclosure in Two-Dimensional Tables: General Tables", "Confidentiality protection of data published in tables is a major problem for statistical offices. To obtain full cooperation of the respondents, it is required that information of individual respondents with a confidential character is kept from being disclosed. One method to avoid disclosure is the method of cell suppression, in which the values of a number of statistical cells are not published but are suppressed from publication. We discuss the method of cell suppression in general two-dimensional tables, in which row totals and column totals are always published. The values of the sensitive cells are replaced by a cross (X). Usually, additional suppressions are necessary to prevent the values of the sensitive cells from being calculated from the row or column totals. Because of these additional suppressions, useful information gets lost. We want to minimize the loss of information by making the best choice for the additional suppressions. Therefore, we introduce and compare the performance of some heuristics for solving this problem."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Letters to the Editor: COMMENT ON WHITE (1989)", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["A Stochastic Model for Analysis of Longitudinal AIDS Data", null], ["Adjusting for Differential Rates of Prophylaxis Therapy for PCP in High-Versus Low-Dose AZT Treatment Arms in an AIDS Randomized Trial", null], ["Assessing Secular Trends in Blood Pressure: A Multiple-Imputation Approach", "The National Center for Health Statistics makes available data from three national health evaluation surveys that it has conducted since 1960: NHES I (1960-1962), NHANES I (1971-1975), and NHANES II (1976-1980). There has been considerable interest in using these data to assess secular trends in cardiovascular risk factors such as blood pressure (BP). Unfortunately, underlying trends in BP are confounded with trends in physician treatment of hypertension over the same period; in the early 1960s it was rare to treat hypertension, whereas by the late 1970s it had become quite common. Our approach to estimating the underlying trends is to take untreated BP to be the variable of interest and to consider it missing in those subjects who are under treatment. We then use a multiple-imputation scheme to construct estimates of trend parameters that adjust for the incompleteness of the original data. Because our imputations depend on certain model features that the data cannot address, we form estimates under different models and compare the results. Our analyses suggest that trend estimates are sensitive to the assumed model, and naive estimates that do not adjust for treatment trends appear to be overly optimistic."], ["A Random-Effects Probit Model for Predicting Medical Malpractice Claims", null], ["A Case Study of an Adaptive Clinical Trial in the Treatment of Out-Patients with Depressive Disorder", "Adaptive clinical trials have attracted the attention of statisticians because they allow for information accrued early in the trial to influence the allocation of treatment of later enrolled patients, thus allowing those patients an increased likelihood of receiving the better treatment. Few adaptive clinical trials have been reported in the literature; the purpose of this article is to describe the rationale, design, and analysis of a recently completed adaptive trial. Simulations to address sample size issues are presented. A Bayesian and a frequentist randomization analysis are discussed. The randomization analysis addresses the use of an adaptive allocation scheme, a delayed response, and a surrogate response. In this trial, the differences between the Bayesian and randomization inference were small. Some suggestions for future implementation of adaptive clinical trials and future research areas are provided."], ["Optimal Recursive Estimation of Dynamic Models", "This article checks, using both real and simulated data, the effectiveness of modern adaptive techniques to track the parameters of time-varying dynamic models. The real case studies concern a bone marrow transplant data set published by Tong, the gas furnace model of Box and Jenkins, and two series of West German interest rates. Simulation studies focus on ARX models with smoothly and suddenly changing parameters. The general approach is to compare the fitting-forecasting performance of classical and adaptive methods, holding fixed the order of the models. At the methodological level, the basic step is taken by unifying known estimators, such as recursive least squares and Kalman filter, into a general algorithm. Next, the problem of optimal design of the tracking coefficients (such as discounting factors and learning rates), is solved by optimizing a quadratic functional based on one-step-ahead prediction errors. All applications show that adaptive modeling, based on the design and the optimization of recursive algorithms, leads to significant improvements of the forecasting performance."], ["Robust Estimation in the Analysis of Complex Molecular Spectra", null], ["Adaptive Mixtures", null], ["Cross-Validation of Multivariate Densities", null], ["Importance-Weighted Marginal Bayesian Posterior Density Estimation", null], ["Versions of Kernel-Type Regression Estimators", "We explore the aims of, and relationships between, various kernel-type regression estimators. To do so, we identify two general types of (direct) kernel estimators differing in their treatment of the nuisance density function associated with regressor variable design. We look at the well-known Gasser-M\u00fcller, Nadaraya-Watson, and Priestley-Chao methods in this light. In the random design case, none of these methods is totally adequate, and we mention a novel (direct) kernel method with appropriate properties. Disadvantages of even the latter idea are remedied by kernel-weighted local linear fitting, a well-known technique that is currently enjoying renewed popularity. We see how to fit this approach into our general framework, and hence form a unified understanding of how these kernel-type smoothers interrelate. Though the mission of this article is unificatory (and even pedagogical), the desire for better understanding of superficially different approaches is motivated by the need to improve practical estimators. In the end, we concur with other authors that kernel-weighted local linear fitting deserves much further attention for applications."], [null, null], ["Estimation of Regression Coefficients When Some Regressors are not Always Observed", null], ["Cox Regression in a Markov Renewal Model: An Application to the Analysis of Bone Marrow Transplant Data", "We discuss estimation and prediction in a Markov renewal model where intensities of the one-step transitions follow a Cox regression model with baseline hazards and covariates depending on the backward recurrence time. A modification of the Andersen and Gill results is given to extend the asymptotic properties of the partial likelihood estimates to this setting. We further consider prediction of the realization of the process in a single individual based on the follow-up information. Data of the International Bone Marrow Transplant Registry are used for illustration purposes."], ["Self-Validating Computations of Probabilities for Selected Central and Noncentral Univariate Probability Functions", "Self-validating computation based on interval arithmetic can produce computed values with a guaranteed error bound. Such methods are especially useful whenever the computed results must satisfy given accuracy requirements. This article reports methods for obtaining self-validating results when computing probabilities and percentiles of univariate continuous distributions. Probability functions dealt with explicitly in the article are normal, incomplete gamma, incomplete beta, and noncentral chi-squared."], ["Computable Robust Estimation of Multivariate Location and Shape in High Dimension Using Compound Estimators", "Estimation of multivariate shape and location in a fashion that is robust with respect to outliers and is affine equivariant represents a significant challenge. The use of compound estimators that use a combinatorial estimator such as Rousseeuw's minimum volume ellipsoid (MVE) or minimum covariance determinant (MCD) to find good starting points for high-efficiency robust estimators such as S estimators has been proposed. In this article we indicate why this scheme will fail in high dimension due to combinatorial explosion in the space that must be searched for the MVE or MCD. We propose a meta-algorithm based on partitioning the data that enables compound estimators to work in high dimension. We show that even when the computational effort is restricted to a linear function of the number of data points, the algorithm results in an estimator with good asymptotic properties. Extensive computational experiments are used to confirm that significant benefits accrue in finite samples as well. We also give empirical results indicating that the MCD is preferred over the MVE for this application."], ["Robust Bounded-Influence Tests in General Parametric Models", "We introduce robust tests for testing hypotheses in a general parametric model. These are robust versions of the Wald, scores, and likelihood ratio tests and are based on general M estimators. Their asymptotic properties and influence functions are derived. It is shown that the stability of the level is obtained by bounding the self-standardized sensitivity of the corresponding M estimator. Furthermore, optimally bounded-influence tests are derived for the Wald- and scores-type tests. Applications to real and simulated data sets are given to illustrate the tests' performance."], ["Bootstrap Recycling: A Monte Carlo Alternative to the Nested Bootstrap", null], ["Use of Cumulative Sums of Squares for Retrospective Detection of Changes of Variance", "This article studies the problem of multiple change points in the variance of a sequence of independent observations. We propose a procedure to detect variance changes based on an iterated cumulative sums of squares (ICSS) algorithm. We study the properties of the centered cumulative sum of squares function and give an intuitive basis for the ICSS algorithm. For series of moderate size (i.e., 200 observations and beyond), the ICSS algorithm offers results comparable to those obtained by a Bayesian approach or by likelihood ratio tests, without the heavy computational burden required by these approaches. Simulation results comparing the ICSS algorithm to other approaches are presented."], ["Saddlepoint Approximation for the Distribution of a Ratio of Quadratic Forms in Normal Variables", "In this article, the saddlepoint approximations to the density and tail probability of a ratio of quadratic forms in normal variables are derived. A numerical exposition via the Durbin-Watson test statistic reveals several desirable features. The approximations, which involve only a limited number of computable functions, provide the practitioner with an accessible and a very powerful tool."], ["Efficient Semiparametric Estimation in a Stochastic Frontier Model", "This article considers the semiparametric stochastic frontier model with panel data that arises in the problem of measuring technical inefficiency in production processes. We assume a parametric form for the frontier function, which is linear in production inputs. The density of the individual firm-specific effects is considered to be unknown. We construct an efficient estimator of the slope parameters in the frontier function. We also give an estimator of the level of the frontier function and investigate its asymptotic properties. Furthermore, we provide a predictor of the individual effects that can be directly translated to firm-specific technical inefficiencies. Finally, we illustrate our methods through a real data example."], ["Bayes Optimal Designs for Two- and Three-Level Factorial Experiments", null], ["Toward a Reconciliation of the Bayesian and Frequentist Approaches to Point Estimation", "The Bayesian and frequentist approaches to point estimation are reviewed. The status of the debate regarding the use of one approach over the other is discussed, and its inconclusive character is noted. A criterion for comparing Bayesian and frequentist estimators within a given experimental framework is proposed. The competition between a Bayesian and a frequentist is viewed as a contest with the following components: a random observable, a true prior distribution unknown to both statisticians, an operational prior used by the Bayesian, a fixed frequentist rule used by the frequentist, and a fixed loss criterion. This competition is studied in the context of exponential families, conjugate priors, and squared error loss. The class of operational priors that yield Bayes estimators superior to the \u201cbest\u201d frequentist estimator is characterized. The implications of the existence of a threshold separating the space of operational priors into good and bad priors are explored, and their relevance in areas such as Bayesian robustness and the elicitation of prior distributions is discussed. Both the theoretical and empirical results presented in this article suggest that the method to be favored in a particular application depends crucially on the quality of the prior information available, with Bayesian and frequentist methods each emerging as preferable under specific, and complementary, circumstances."], ["The Collapsed Gibbs Sampler in Bayesian Computations with Applications to a Gene Regulation Problem", null], ["Rationality and Unbiasedness in Hypothesis Testing", "The notion of a \u201crationality criterion\u201d is defined. This criterion reflects a desirable property for statistical procedures that deal with censored data. In hypothesis testing situations, we show that when adapting this criterion to a uniformly most powerful unbiased (UMPU) test of comparing the scale parameters of two distributions, sometimes it is possible to determine a test so that it is unbiased and sometimes this is impossible. The question thus remains: What is more \u201crational\u201d\u2014the \u201crationality criterion\u201d or unbiasedness?"], ["Screening with Cost-Effective Quality Control: Potential Applications to HIV and Drug Testing", "This article investigates the applicability of group testing as a quality control procedure to monitor the sensitivity of screening tests used to check the blood supply for infective agents or to check employees for drug use. The problem is important, as the accuracy of screening tests in the field may deteriorate over time. In the blood screening application, our results demonstrate that group testing the screened negatives provides a procedure with high power to detect a decline of .02 in the sensitivity of the original test when the prevalence in the population is quite low (.0001). Moreover, the procedure is cost-effective in the sense that the expected cost per human immunodeficiency virus (HIV) infection avoided could be less than $1 million, in contrast to much higher economic valuations of life that are used in regulatory analyses. The statistical properties of estimates of the prevalence, as well as those for the sensitivity and specificity of the screening test using the extra information obtained from the quality control procedure, are also presented."], ["A Two-Stage Adaptive Group-Testing Procedure for Estimating Small Proportions", null], ["Some Tests for Comparing Cumulative Incidence Functions and Cause-Specific Hazard Rates", "We consider the competing risks problem with the available data in the form of times and causes of failure. In many practical situations (e.g., in reliability testing) it is important to know whether two risks are equal or whether one is \u201cmore serious\u201d than the other. We propose some distribution-free tests for comparing cumulative incidence functions and cause-specific hazard rates against ordered alternatives without making any assumptions on the nature of dependence between the risks. Both the censored and the uncensored cases are studied. The performance of the proposed tests is assessed in a simulation study. As an illustration, we compare the risks of two types of cancer mortality (thymic lymphoma and reticulum cell carcinoma) in a strain of laboratory mice."], ["Data-Driven Version of Neyman's Smooth Test of Fit", null], ["Uniformly More Powerful Tests in a One-Sided Multivariate Problem", null], [null, null], ["Inference for Shift Functions in the Two-Sample Problem With Right-Censored Data: With Applications", null], ["Measuring Change in Latent Subgroups Using Dichotomous Data: Unconditional, Conditional, and Semiparametric Maximum Likelihood Estimation", "Changes in dichotomous data caused by treatments can be analyzed by means of the so-called linear logistic model with relaxed assumptions (LLRA). In contrast to models of the Rasch type, the LLRA allows incidental multidimensional parameters describing the response behavior of the subjects so that each observable criterion used for assessing the changes is governed by its own subject parameter. Because generalizability of the treatment effects over subjects and criteria is scientifically desirable, statistical tests for this desideratum play an important role in practical situations. Whereas generalizability over the criteria can easily be tested, generalizability over subjects can be tested only by stratifying the sample according to some further observable subject variables. However, one cannot be sure to detect nongeneralizability over the subjects by this method with certainty. Therefore, the mixture LLRA is proposed that allows directly unobservable types of subjects reacting differently on the treatments. Three maximum likelihood (ML) methods for estimating the parameters of this mixture model are investigated (unconditional ML, partial- and full-information conditional ML, and semiparametric ML) with respect to some properties of their parameter estimates and their power against falsely specified number of latent change-types using simulated data sets of varying heterogeneity of the subject parameters. Because the unconditional ML method gives estimates that become the more biased the more the subjects are heterogeneous, and the conditional ML estimates always seem to be slightly biased, the semiparametric ML method, even if somewhat harder to apply, is favored. For all three methods, the usual goodness-of-fit tests contrasting the respective model to the unconstrained multinomial have shown to be insensitive against falsely specified number of change-types for the sample size being equal to 500. Contrary to this, even then the BCVL criterion (i.e., posterior probabilities for the numbers of change-types) resulted in correct decisions for the unconditional and semiparametric ML methods."], ["Use of Estimating Functions for Estimation from Complex Surveys", "We describe a method for point and interval estimation of population parameters from complex surveys using estimating functions. The theory was originally developed for infinite populations and has recently been applied to finite populations. With estimating functions, a unifying framework can be given for point and interval estimation of both finite and infinite population parameters. We discuss test inversion methods to derive confidence intervals for one-dimensional parameters and propose a method for eliminating nuisance parameters in the multidimensional setting. We show that special cases of our proposal result in conditional and orthogonal methods proposed in the literature. We describe a simulation study using real data to compare the coverage probabilities of confidence intervals obtained under various approaches."], ["The Construction of New Bivariate Exponential Distributions from a Bayesian Perspective", "We use an economic approach of Mendel to derive new bivariate exponential lifetime distributions. Features distinguishing this approach from the existing ones are (1) it makes use of the principle of indifference; (2) our parameter of interest is a measurable function of observable quantities; (3) the assessment of the probability measure for random lifetimes is performed by assessing that for random lifetime costs with a change of variables; and (4) characterization properties other than the bivariate loss-of-memory property are used to construct distributions. For the infinite population case, our distributions correspond to mixtures of existing bivariate exponential distributions such as the Freund distribution, the Marshall-Olkin distribution, and the Friday-Patil distribution. Moreover, a family of natural conjugate priors for Bayesian Freund (-type) bivariate exponential distributions is discussed."], ["Distribution Theory of Runs: A Markov Chain Approach", null], ["On Quasi-Independence and Quasi-Dependence in Contingency Tables, with Special Reference to Ordinal Triangular Contingency Tables", null], ["Statistics in Sports: Special Section", null], ["Exploring Baseball Hitting Data: What about those Breakdown Statistics?", "During a broadcast of a baseball game, a fan hears how baseball hitters perform in various situations, such as at home and on the road, on grass and on turf, in clutch situations, and ahead and behind in the count. From this discussion by the media, fans get the misleading impression that much of the variability in players' hitting performance can be explained by one or more of these situational variables. For example, an announcer may state that a particular player struck out because he was behind in the count and was facing a left-handed pitcher. In baseball one can now investigate the effect of various situations, as hitting data is recorded in very fine detail. This article looks at the hitting performance of major league regulars during the 1992 baseball season to see which situational variables are \u201creal\u201d in the sense that they explain a significant amount of the variation in hitting of the group of players. Bayesian hierarchical models are used in measuring the size of a particular situational effect and in identifying players whose hitting performance is very different in a particular situation. Important situational variables are identified together with outstanding players who make the most of a given situation."], ["Rating Skating", "Among judged sports, figure skating uses a unique method of median ranks for determining placement. This system responds positively to increased marks by each judge and follows majority rule when a majority of judges agree on a skater's rank. It is demonstrated that this is the only aggregation system possessing these two properties. Median ranks provide strong safeguards against manipulation by a minority of judges. These positive features do not require the sacrifice of efficiency in controlling measurement error. In a Monte Carlo study, the median rank system consistently outperforms alternatives when judges' marks are significantly skewed toward an upper limit."], ["Estimation with Selected Binomial Information or do you Really Believe that Dave Winfield is Batting .471?", "Often sports announcers, particularly in baseball, provide the listener with exaggerated information concerning a player's performance. For example, we may be told that Dave Winfield, a popular baseball player, has hit safely in 8 of his last 17 chances (a batting average of .471). This is biased, or selected information, as the \u201c17\u201d was chosen to maximize the reported percentage. We model this as observing a maximum success rate of a Bernoulli process and show how to construct the likelihood function for a player's true batting ability. The likelihood function is a high-degree polynomial, but it can be computed exactly. Alternatively, the problem yields to solutions based on either the EM algorithm or Gibbs sampling. Using these techniques, we compute maximum likelihood estimators, Bayes estimators, and associated measures of error. We also show how to approximate the likelihood using a Brownian motion calculation. We find that although constructing good estimators from selected information is difficult, we seem to be able to estimate better than expected, particularly when using prior information. The estimators are illustrated with data from the 1992 Major League Baseball season."], ["Trying Out for the Team: Do Exhibitions Matter? Evidence from the National Football League", null], ["Analysis of Olympic Heptathlon Data", null], ["Logit and Multilevel Logit Modeling of College Graduation for 1984-1985 Freshman Student-Athletes", "This article describes statistical research on the academic performance of student-athletes in college sports programs. We describe several statistical models used in the prediction of academic success defined by college persistence and graduation. Using longitudinal data on the academic performances of about 3,000 student-athletes in NCAA Division I collegiate sports programs, we formulate logit and multilevel logit statistical models for the prediction of graduation rates. These prediction models are based on academic, demographic, and athletic variables, and are used to account for differences in both the students and the colleges. These results show (1) moderate but significant relationships between precollege academic characteristics and college graduation, (2) small but significant differential validity of prediction between major student-athlete groups, (3) notable college-level variance in the average graduation rate, (4) small but significant within-college relationships between precollege academic characteristics and college graduation, and (5) differences between colleges accounted for by institutional graduation rates. We highlight statistical issues about the application of logit and multilevel models and discuss substantive issues about the current implications of these results."], ["Down to Ten: Estimating the Effect of a Red Card in Soccer", "We investigate the effect of the expulsion of a player on the outcome of a soccer match by means of a probability model for the score. We propose estimators of the expulsion effect that are independent of the relative strength of the teams. We use the estimates to illustrate the expulsion effect on the outcome of a match."], ["A Brownian Motion Model for the Progress of Sports Scores", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["W. Edwards Deming 1990\u20131993", null], ["Editors' Report for 1993", null], ["An Approach to Statistical Spatial-Temporal Modeling of Meteorological Fields", "In this article we develop a random field model for the mean temperature over the region in the northern United States covering eastern Montana through the Dakotas and northern Nebraska up to the Canadian border. The readings are temperatures at the stations in the U.S. historical climatological network. The stochastic structure is modeled by a stationary spatial-temporal Gaussian random field. For this region, we find little evidence of temporal dependence while the spatial structure is temporally stable. The approach strives to incorporate the uncertainty in estimating the covariance structure into the predictive distributions and the final inference. As an application of the model, we derive posterior distributions of the areal mean over time. A posterior distribution for the static areal mean is presented as a basis for calibrating temperature shifts by the historical record. For this region and season, the distribution indicates that under the scenario of a gradual increase of 5\u00b0F over 50 years, it will take 30\u201340 winters of data before the change will be discernible from the natural variation in temperatures."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Kriging and Splines: An Empirical Comparison of their Predictive Performance in Some Applications", "In disciplines such as soil science, ecology, meteorology, water resources, mining engineering, and forestry, spatial prediction is of central interest. A sparsely sampled spatial process yields imperfect knowledge of a resource, from which prediction of unobserved parts of the process are to be made. A popular stochastic method that solves this problem is kriging. But the appropriateness of kriging\u2014and, for that matter, of any method based on probabilistic models for spatial data\u2014has been frequently questioned. A number of nonstochastic methods have also been proposed, the leading contender of which appears to be splines. There has been some debate as to which of kriging and splines is better\u2014a debate that has centered largely on operational issues, because the two methods are based on different models for the process. In this article the debate is turned to where it ultimately matters\u2014namely, the precision of prediction based on real data. By dividing data sets into modeling sets and prediction sets, the two methods may be compared. In the cases examined, kriging sometimes outperforms splines by a considerable margin, and it never performs worse than splines. Various configurations of data show that the sampling regime determines when kriging will outperform splines."], ["Comment", null], ["Comments", null], ["Rejoinder", null], ["Statistical Models for Limiting Nutrient Relations in Inland Waters", "The ecological theory of limiting factors holds that the observed level of response in a biological process will be governed by the input factor in least supply\u2014the limiting factor. This theory has formed the basis for numerous attempts by aquatic ecologists to describe the relation between the biological productivity of inland waters and the availability of plant nutrients required for algal growth. Regression analysis has been the primary statistical tool used in the development of such relations, yet any statistical model that represents the limiting effect of some explanatory factor as an expectation contradicts the substantive theory of limiting factors. Limnological data not resulting in an adequate regression of chlorophyll on phosphorus have been viewed as failing to support the limiting effect of this nutrient on algal biomass in lakes. But when represented by a more appropriate model, such data may be seen to provide similar evidence for the relation of chlorophyll to phosphorus as does data resulting in a strong regression. Data from limnological studies often exhibit a scatter of points distributed in the shape of a triangle lying beneath an upper boundary. Appropriate models for such data are introduced to describe the upper boundary or potential limit, the distribution of points falling below the limit, and the degree of random error. An application of the EM algorithm provides marginal maximum likelihood estimates of the parameters in the more complex models considered. Several results are given for the models, including a goodness-of-fit diagnostic and estimation of the large-sample parameter covariance matrix. Application of the models is illustrated by fitting empirical relationships between chlorophyll and the plant nutrient phosphorus in temperate lakes."], ["Screening for the Presence of a Disease by Pooling Sera Samples", "Screening of pooled urine samples was suggested during the Second World War as a method for reducing the cost of detecting syphilis in U.S. soldiers. Recently, pooling has been used in screening for human immunodeficiency virus (HIV) antibody to help curb the further spread of the virus. Pooling reduces the cost but also\u2014and probably more importantly\u2014offers a feasible way to lower the error rates associated with labeling samples when screening low-risk HIV populations. For example, given the limited precision of the presently available test kits, when the screened population has a prevalence of 4 per 1,000 (which is roughly the estimated U.S. prevalence), the probability that a sample labeled positive is antibody-free can be reduced from approximately 90% (if each sample is tested individually) to about 2%. Furthermore, screening pooled sera samples can also be used to reduce the probability that a sample labeled negative in fact has antibodies up to 40-fold in such a population\u2014an important consideration when attempting to preserve the integrity of the blood supply. In this article we generalize some commonly used pooling procedures and introduce some new ones. We discuss and compare these procedures' relative merits and weaknesses under practical situations in the context of HIV screening. The proposed procedures can also be used and generalized to other medical and quality control applications."], ["Empirical Evaluation of Prior Beliefs about Frequencies: Methodology and a Case Study in Congenital Heart Disease", "We consider the problem of critiquing prior beliefs concerning the distribution of a discrete random variable in the light of a sequentially obtained sample. A topical application concerns a probabilistic expert system for the diagnosis of congenital heart disease, which requires specification of a large number of conditional probabilities that are initially imprecisely estimated by a suitable \u201cexpert.\u201d These prior beliefs may be formally updated as data become available, but it would seem essential to contrast the original expert assessments with the data obtained to quickly identify inappropriate subjective inputs. We consider both Bayes factor and significance testing techniques for such a prior/data comparison, both in nonsequential and sequential forms. The common basis as alternative standardizations of the logarithm of the predictive ordinate of the observed data is emphasised, and a Bayesian discrepancy statistic with a variety of interpretations provides a formal means of discounting the expert's judgments in the light of the data. The judgments are found to be of generally high quality, and procedures for automatic monitoring and adaptation are recommended."], ["Analytical Use of Data from Army Training Exercises: A Case Study of Tactical Reconnaissance", "In the early 1980s, the U.S. Army opened the National Training Center (NTC) as its main U.S. training venue for battalions and brigades. The NTC's primary purpose was training, but because the simulated combat was fairly realistic, many hoped it would provide a sort of laboratory for identifying problems and exploring ways to fix them. This article describes a project that studied tactical reconnaissance at the NTC. The project's substantive purpose was to look for problems in the organization, doctrine, equipment, and training for tactical reconnaissance, and to suggest remedies. Its methodological purpose was to learn how to measure reconnaissance activities and how to use those measurements for analytical purposes, neither of which had been done previously. This article gives background on the NTC and on tactical reconnaissance, and then traces the steps of the reconnaissance project. The two main methodological issues are how to gather measurements in venues like the NTC and what kinds of questions such venues can be used to answer. As it turns out, high-tech measurements are generally of little use. Low-tech measurements are useful for diagnosis (description) and hypothesis generation, predictions and causal inferences being fraught with hazard. Modest as these uses are, the NTC data are nonetheless highly valuable."], ["Survival Analysis of the Gamma-Ray Burst Data", "Gamma-ray bursts are short but powerful pulses of electromagnetic radiation coming from space. Specially equipped satellites have observed the arrival of several hundred bursts during the past two decades. The origin of these bursts, whether from near the earth, across the galaxy, or out of the deep cosmos, has become a major question for astronomers. At the heart of this question lies a statistical puzzle relating to familiar issues in survival analysis: Kaplan-Meier-type curves, data truncation, hazard rate analysis, Mantel-Haenszel statistics, and so on. Survival analysis is used here to critically examine two different models that have been proposed for the gamma-ray burst data. The analysis contradicts one of these models."], ["Missing Data, Imputation, and the Bootstrap", "Missing data refers to a class of problems made difficult by the absence of some portions of a familiar data structure. For example, a regression problem might have some missing values in the predictor vectors. This article concerns nonparametric approaches to assessing the accuracy of an estimator in a missing data situation. Three main topics are discussed: bootstrap methods for missing data, these methods' relationship to the theory of multiple imputation, and computationally efficient ways of executing them. The simplest form of nonparametric bootstrap confidence interval turns out to give convenient and accurate answers. There are interesting practical and theoretical differences between bootstrap methods and the multiple imputation approach, as well as some useful similarities."], ["Comment", null], ["Rejoinder", null], ["Approximations to Joint Distributions of Definite Quadratic Forms", null], ["A Graphical Technique for Determining the Number of Components in a Mixture of Normals", "When a population is assumed to be composed of a finite number of subpopulations, a natural model to choose is the finite mixture model. It will often be the case, however, that the number of component distributions is unknown and must be estimated. This problem can be difficult; for instance, the density of two mixed normals is not bimodal unless the means are separated by at least 2 standard deviations. Hence modality of the data per se can be an insensitive approach to component estimation. We demonstrate that a mixture of two normals divided by a normal density having the same mean and variance as the mixed density is always bimodal. This analytic result and other related results form the basis for a diagnostic and a test for the number of components in a mixture of normals. The density is estimated using a kernel density estimator. Under the null hypothesis, the proposed diagnostic can be approximated by a stationary Gaussian process. Under the alternative hypothesis, components in the mixture will express themselves as major modes in the diagnostic plot. A test for mixing is based on the amount of smoothing necessary to suppress these large deviations from a Gaussian process."], ["Nonparametric Estimation of the Moments of a General Statistic Computed from Spatial Data", null], ["Quasi-Likelihood Estimation in Semiparametric Models", null], ["Estimating Functions in Chaotic Systems", "Berliner considered Bayesian and likelihood-based approaches for estimation and prediction in a chaotic system with measurement error. This article proposes the use of estimating functions for this problem. Logistic and exponential maps are analyzed. Estimators are shown to be consistent and asymptotically normal. Small-sample behavior is studied with simulations."], ["Estimating Densities of Functions of Observations", "Density estimates, such as histograms and more sophisticated versions, are important in applied and theoretical statistics. In applied statistics, a density estimate gives the data analyst a graphical overview of the shape of the distribution. This overview allows the data analyst to arrive immediately at a qualitative impression of the location, scale, and various aspects of the extremes of the distribution. In theoretical statistics, the shape of the density allows the researcher to link the data to families of curves, perhaps indexed parametrically. By estimating a density nonparametrically, certain aspects of the data can be viewed without the restriction of a priori imposing limitations of a class of parametric curves. In this article we introduce density estimation for functions of observations. To motivate the study, one type of function used is the interpoint distance between observations arising in spatial statistics from the fields of biometry and regional science. A second type of function considered is the sum of observations as might occur in claims models in insurance. The nonparametric density estimates are introduced, and certain computational issues are discussed. A central limit theorem for the estimator is provided. This asymptotic result is interesting because, under certain mild conditions, the density estimate enjoys a rate of convergence similar to parametric estimates. This rate of convergence is much faster than the usual rate of convergence in nonparametric density estimation."], ["Nonparametric Maximum Likelihood Estimation Based on Ranked Set Samples", null], ["Regular Redescending Rank Estimates", "A study is made of some unusual location estimates first proposed in 1977 by J. S. Maritz, M. Wu, and R. G. Staudte, Jr., who established some strong robustness properties of these estimates, such as redescending influence functions and, in some cases, full efficiency at the Cauchy model. It is further shown here that the estimates, called M-W-S estimates, are derivable from a rank-based scheme founded on convex functions and hence are regular, are unique, and have associated tests and confidence intervals. The tests belong to a family of signed-rank-type tests, and their distributions have nice combinatorial properties. It is not the case, therefore, that a redescending influence function need imply all the computational irregularities possessed by redescending M estimates. In addition, M-W-S estimates are shown to have breakdown point of .5, a very strong robustness property."], ["Bounded Influence and High Breakdown Point Testing Procedures in Linear Models", "Three classes of testing procedures based on one-step high breakdown point bounded influence estimators, for testing subhypotheses in linear models are developed. These are drop-in-dispersion, Wald-type, and score-type tests. The asymptotic distributions of these testing procedures are obtained under the null hypothesis and under contiguous alternatives. Their stability properties are studied in terms of their influence functions and breakdown points. It is shown that the tests have bounded influence functions. For the Wald-type tests, the level and power breakdowns are determined by the breakdown point of the parameter estimate and the associated variance-covariance matrix. The drop-in-dispersion test exhibits high-level breakdown but not high power breakdown point. Similar behavior is exhibited by the score-type tests. But slight modifications can be made in the construction of the test statistics to ensure high breakdown points in terms of both level and power. An example is given to illustrate the usefulness of high-breakdown testing procedures."], [null, null], ["Censored Regression: Local Linear Approximations and their Applications", "Various statistical tools are available for modeling the relationship between response and covariate if the data are fully observable. In the situation of censored data, however, those tools are no longer directly applicable. This article provides an easily implemented methodology for modeling the association, based on censored data. The form of the regression relationship will be completely determined by the data; no assumptions are made about this form. Basic ideas behind the methodology are to transform the observed data in an appropriate simple way and then to apply a locally weighted least squares regression. The proposed estimator involves a variable bandwidth that automatically adapts to the design of the data points. That the methodology is very easy to implement is illustrated by several examples, including simulation studies and an analysis of the Stanford Heart Transplant Data and the Primary Biliary Cirrhosis Data. Several theoretical considerations are reflected in the examples. Finally, some basic asymptotic results are established."], ["Correlation Curves as Local Measures of Variance Explained by Regression", null], ["Comparison of Prediction Methods when Only a Few Components are Relevant", "We consider prediction in a multiple regression model where we also look on the explanatory variables as random. If the number of explanatory variables is large, then the common least squares multiple regression solution may not be the best one. We give a methodology for comparing certain alternative prediction methods by asymptotic calculations and perform such a comparisons for four specific methods. The results indicate that none of these methods dominates the others, and that the difference between the methods typically (but not always) is small when the number of observations is large. In particular, principal component regression does well when the eigenvalues corresponding to components not correlated with the dependent variables (i.e., the irrelevant eigenvalues) are extremely small or extremely large. Partial least squares regression does well for intermediate irrelevant eigenvalues. A maximum likelihood-type method dominates the others asymptotically, at least in the case of one relevant component."], ["Reweighting to Achieve Elliptically Contoured Covariates in Regression", "We investigate a method of constructing weights to induce elliptically contoured covariates in regression analyses. Much recent work in regression has identified various data analytic and model robustness advantages associated with such covariates. In particular, new estimation methods like SIR, SIRII, SAVE, and PHD have been built around the assumption of elliptically contoured covariates. Finite samples of regression covariates may deviate from this ideal in practice, and the method developed here, termed Voronoi weighting, can be used to induce elliptical symmetry in such samples. In a number of examples, we show that reweighting cases by the Voronoi method can substantially enhance various procedures. For covariates that deviate from elliptical symmetry, we show that Voronoi weighting, in conjunction with some trimming via the minimum volume ellipsoid method, can be effective."], ["Nonparametric Spectral Density Estimation Using Penalized Whittle Likelihood", "The penalized likelihood approach is not well developed in time series analysis, even though it has been applied successfully in a number of nonparametric function estimation problems. Chow and Grenander proposed a penalized likelihood-type approach to the nonparametric estimation of the spectral density of Gaussian processes. In this article this estimator is extended to more general stationary processes, its practical implementation is developed in some detail, and some asymptotic rates of convergence are established. Its performance is also compared to more widely used alternatives in the field. A computational algorithm involving an iterative least squares, initialized by the log-periodogram, is first developed. Then, motivated by an asymptotic linearization, an estimator of the integrated squared error between the estimated and true log-spectral densities is proposed. From this, a data-dependent procedure for selection of the amount of smoothing is constructed. The methodology is illustrated with some real and simulated data sets. A simulation study with autoregressive and moving average processes is conducted to provide quantification of the performance characteristics of the approach relative to some well-established methods in the literature, including the smoothed log-periodogram, the logarithm of the smoothed periodogram, and ARMA spectral density estimators. Empirical rates of convergence of the estimator are evaluated and shown to be well predicted by an asymptotic analysis."], ["Estimation, Prediction, and Interpolation for Nonstationary Series with the Kalman Filter", "We show how our definition of the likelihood of an autoregressive integrated moving average (ARIMA) model with missing observations, alternative to that of Kohn and Ansley and based on the usual assumptions made in estimation of and forecasting with ARIMA models, permits a direct and standard state-space representation of the nonstationary (original) data, so that the ordinary Kalman filter and fixed point smoother can be efficiently used for estimation, forecasting, and interpolation. In this way, the problem of estimating missing values in nonstationary series is considerably simplified. The results are extended to regression models with ARIMA errors, and a computer program is available from the authors."], ["Simultaneously Modeling Joint and Marginal Distributions of Multivariate Categorical Responses", "We discuss model-fitting methods for analyzing simultaneously the joint and marginal distributions of multivariate categorical responses. The models are members of a broad class of generalized logit and loglinear models. We fit them by improving a maximum likelihood algorithm that uses Lagrange's method of undetermined multipliers and a Newton-Raphson iterative scheme. We also discuss goodness-of-fit tests and adjusted residuals, and give asymptotic distributions of model parameter estimators. For this class of models, inferences are equivalent for Poisson and multinomial sampling assumptions. Simultaneous models for joint and marginal distributions may be useful in a variety of applications, including studies dealing with longitudinal data, multiple indicators in opinion research, cross-over designs, social mobility, and inter-rater agreement. The models are illustrated for one such application, using data from a recent General Social Survey regarding opinions about various types of government spending."], ["Marginal Modeling of Correlated Ordinal Data Using a Multivariate Plackett Distribution", "An extension of the bivariate model suggested by Dale is proposed for the analysis of dependent ordinal categorical data. The so-called multivariate Dale model is constructed by first generalizing the bivariate Plackett distribution to any dimensions. Because the approach is likelihood based, it satisfies properties that are not fulfilled by other popular methods, such as the generalized estimating equations approach. The proposed method models both the marginal and the association structure in a flexible way. The attractiveness of the multivariate Dale model is illustrated in three key examples, covering areas such as crossover trials, longitudinal studies with patients dropping out from the study, and discriminant analysis applications. The differences and similarities with the generalized estimating approach are highlighted."], ["Conditional Log-Linear Models for Analyzing Categorical Panel Data", "Conditional log-linear models are developed for panel data and used to predict sequences of categorical responses. The class of models considered includes conventional Markov models and independence models as well as distance models in which all previous responses and present and past values of covariates are used to predict the current response. The approach taken in this article has some advantages over the marginal modeling approach that has become popular for longitudinal studies. Quality of prediction is measured by using a logarithmic penalty function. Given a model, conditional probabilities of responses consistent with the model are selected to provide the smallest expected penalty. This minimum expected penalty provides a measure of the predictive power of a model. Models are compared through their predictive power, as measured by the proportional reduction in expected penalty. Ways of incorporating the number of parameters of the competing models are discussed. This emphasis on predictive power contrasts with the conventional emphasis on goodness-of-fit tests. In the case of random sampling, estimates are provided for optimal prediction functions consistent with the model and for measures of predictive power. Large-sample approximations are provided for assessing the accuracy of parameter estimates and of estimated measures of quality of prediction. For measures of quality of prediction, assessments are provided for the bias of estimates. To illustrate techniques, analyses are performed on data from the National Longitudinal Study of Youth on attitudes toward a military career. Because these data are available for the same subject for each of seven years, and because demographic data are available on individual subjects, these data provide a nontrivial application. Because more than 8,000 observations are available, statistical models of practical interest do not fit the data according to conventional criteria, but they still have value in predicting subject responses. Analysis of the data shows that subjects' responses are linked much more closely to their previous responses than to demographic variables. A common Markov model for subject responses is shown to be inferior to other models in terms of predictive power. Methods considered are shown to apply to cases in which censoring or nonresponse problems exist."], ["Testing Goodness of Fit for a Parametric Family of Link Functions", null], [null, null], ["A Generalized Tukey Conjecture for Multiple Comparisons among Mean Vectors", "In this article the Tukey-Kramer procedure for multiple comparisons of pairwise differences of mean vectors in multivariate normal distributions is considered. A multivariate version of the Tukey-Kramer procedure is presented, and a generalized Tukey conjecture of the conservativeness of the simultaneous confidence intervals for all pairwise comparisons by this procedure is affirmatively proved in the case of three correlated mean vectors. Some properties of the multivariate Tukey-Kramer procedure are also presented, and simulation results for some selected parameters are given."], ["Incomplete Repeated Measures Data Analysis in the Presence of Treatment Effects", "This article illustrates an analysis strategy for repeated measures data where some of the measurements are missing from the subjects on one or more occasions, with a special application to data arising from repeated measurement designs when some observations are missing after the first treatment period. Using all available data, \u201calmost\u201d maximum likelihood estimators for the parameters of interest are obtained under the assumption of a multivariate normal distribution of the data. Inference procedures for these parameters in small samples from popular two-treatment designs are discussed."], ["On the Equivalence of Constrained and Compound Optimal Designs", null], ["A Note on Handling Nonresponse in Sample Surveys", "Two distinct types of models are used for handling nonresponse in survey sampling theory. In a response (or quasi-randomization) model, the propensity of survey response is modeled as a random process, an additional phase of sample selection. In a parametric (or superpopulation) model, the survey data are themselves modeled. These two models can be used simultaneously in the estimation of a population mean so that one provides some protection against the potential for failure in the other. Two different estimators are discussed in this article. The first is a regression estimator that is both unbiased under the parametric model and nearly quasi-design unbiased under the response model. The second is a direct expansion estimator with imputed missing values. The imputed values are such that the estimator is both nearly quasi-design unbiased and unbiased under the combination of the parametric model and the original sampling design. The article includes a discussion of variance estimation with the goal of simultaneously estimating quasi-design mean squared error and either parametric model variance or combined (parametric model and original design) variance."], ["Approximate Conditional Inference in Exponential Families via the Gibbs Sampler", "This article presents the Gibbs-Skovgaard algorithm for approximate frequentist inference. The method makes use of the double saddlepoint approximation of Skovgaard to the conditional cumulative distribution function of a sufficient statistic given the remaining sufficient statistics. This approximation is then used in the Gibbs sampler to generate a Markov chain. The equilibrium distribution of this chain approximates the joint distribution of the sufficient statistics associated with the parameters of interest conditional on the observed values of the sufficient statistics associated with the nuisance parameters. This Gibbs-Skovgaard algorithm is applied to the cases of logistic and Poisson regression."], ["Informative Priors for the Bayesian Classification of Satellite Images", "In the Bayesian classification of satellite images, a prior distribution is used that aims to model the belief of spatial homogeneity of the underlying region. We extend this prior distribution to model certain topographical features of the area such as the position of the roads, the slopes, and the aspects. We demonstrate the effectiveness of this prior distribution in a reconstruction algorithm by means of a simulation study in which the quality of the result is assessed by a comparison of estimated and known covertypes. We apply the algorithm to real data with success."], ["Book Reviews", null], ["Publications Received", null], ["Comment on Murry, Stam, and Lastovicka", null], ["Rejoinder to Mackinnon", null], ["Comment on Model", null], ["Comment on Fung", null], ["Correction", null], ["Editorial Board Page", "This article has no abstract"], ["Statistics as a Profession", "In August of 1988 when Bob Hogg was President of the ASA he invited me as a vice-president elect to attended my first meeting of the ASA Board of Directors. I have thus been part of the ASA Board's activities for five full years. It has been a period of many changes for the Association: the new constitution, the newly structured Board, many new Sections, several new publications, a membership campaign, the assorted activities of the ASA Center for Statistical Education, and most recently the modification in the ASA dues structure with its \u201ccafeteria plan\u201d allowing members to select their publications. These many changes are indications that the entire statistics profession, represented at this meeting by the ASA, the IMS, and the Biometric Society, is being challenged not only to meet the needs of its membership but also those of society at large. These are exciting times, much like sailing when the wind comes up. And I still have a year and a half ahead on the ASA Board of Directors. I must remember to ask my favorite actuary Bob Hogg what my chances are of surviving."], ["Nonparametric Estimation for a form of Doubly Censored Data, with Application to Two Problems in AIDS", "In many epidemiologic studies of human immunodeficiency virus (HIV) disease, interest focuses on the distribution of the length of the interval of time between two events. Two such problems are considered here, estimation of the distribution of time or number of sexual contacts between infection of an individual (an index case) and transmission of HIV to their sexual partner, and estimation of the distribution of time between infectiousness as a blood donor and the development of detectable antibody. Data regarding these two problems are available from certain partner studies, and the HIV Lookback Study. In both cases the statistical development is complicated by the fact that the times of both events are interval censored, so that the length of time between the events is never observed exactly. Nonparametric methods for estimation of the interval length distribution are developed by casting the problem in terms of nonparametric estimation of a mixing distribution; particular attention is paid to identifiability issues."], ["Cell lineage analysis: Variance Components Models for Dependent Cell Populations", "Cells grown in culture can be tracked for several generations and measurements taken on size or age at division and other cell characteristics. The observations for the offspring of each cell form a family tree of dependent data. Such cell lineage data are here modeled as repeated measurements on different family trees arising from individual ancestor cells selected at random from a population of cultured cells. The bifurcating autoregression model is embedded in a process that allows for measurement error and variation from tree to tree. Robust methods are presented that accommodate outliers in this time-dependent and branching environment while allowing the statistician to interactively build a variance components model for the process. The methodology is illustrated on a substantial data set of 41 trees of EMT6 cells, with the surprising conclusion that after removing measurement error, sister-cell lifetimes are nearly identical."], ["Comparison of Variance Estimators of the Horvitz-Thompson Estimator for Randomized Variable Probability Systematic Sampling", null], ["Models for Categorical Data with Nonignorable Nonresponse", null], ["Adaptive Principal Surfaces", "We develop a nonlinear generalization of principal components analysis. A principal surface of the data is constructed adaptively, using some ideas from the MARS procedure of Friedman. We explore applications to curve and surface reconstruction and to data summarization."], [null, null], ["Feasible Nonparametric Estimation of Multiargument Monotone Functions", "This article presents a two-stage estimation procedure that uses an ad hoc but very easily implemented isotonization of a kernel estimator. This procedure yields an isotonic estimator with the convergence properties of the kernel estimator. Although the isotonization in the second stage does not satisfy the least squares condition, this hybrid estimator may be considered to be a multidimensional generalization of similar procedures for the one-dimensional case suggested by Friedman and Tibshirani and by Mukarjee. We derive some of the asymptotic properties of our estimator and demonstrate other statistical properties with Monte Carlo studies. We conclude by providing a real data example."], ["Nonparametric Estimation of Mean Functionals with Data Missing at Random", "This article considers a distribution-free estimation procedure for a basic pattern of missing data that often arises from the wellknown double sampling in survey methodology. Without parametric modeling of the missing mechanism or the joint distribution, kernel regression estimators are used to estimate mean functionals through empirical estimation of the missing pattern. A generalization of the method of Cheng and Wei is verified under the assumption of missing at random. Asymptotic distributions are derived for estimating the mean of the incomplete data and for estimating the mean treatment difference in a nonrandomized observational study. The nonparametric method is compared with a naive pairwise deletion method and a linear regression method via the asymptotic relative efficiencies and a simulation study. The comparison shows that the proposed nonparametric estimators attain reliable performances in general."], ["Regression Models with Spatially Correlated Errors", "In this article we consider regression models for two-dimensional spatial data when the errors follow a spatial unilateral first-order autoregressive moving average (ARMA) model studied by Basu and Reinsel. We give details on the convenient computation of the generalized least squares (GLS) estimator of the regression parameters in the presence of spatially correlated errors, and compare the GLS estimator to the ordinary least squares (OLS) estimator in some special cases. We also consider the restricted maximum likelihood estimators of the spatial correlation model parameters, which may be preferred over the maximum likelihood estimators. For the special case of the spatial unilateral first-order AR model, details of the maximum likelihood as well as the restricted maximum likelihood estimation are given. A numerical example is presented to illustrate the methods."], ["Fitting Heteroscedastic Regression Models", null], ["A Neural Net Model for Prediction", "In this article we introduce a neural net designed for nonlinear statistical prediction. The net is based on a stochastic model featuring a multilayer feedforward architecture with random connections between units and noisy response functions. A Bayesian inferential procedure for this model, based on the Kalman filter, is derived. The resulting learning algorithm generalizes the so-called onedimensional Newton method, an updating algorithm currently popular in the neural net literature. A numerical study concerning the prediction of a noisy chaotic time series is presented, and the greater predictive accuracy of the new algorithm with respect to the Newton algorithm is exhibited."], ["An Interpretation of Partial Least Squares", null], ["On the Relationship between Stepwise Decision Procedures and Confidence Sets", null], ["A Note on Variance Estimation for the Regression Estimator in Double Sampling", "Double sampling of a finite population occurs when a sample from the population is itself sampled, with the intent of measuring varites in the subsample not already available in the sample. An important example is the regression estimator for means or totals, which uses values of an auxiliary variable from the full sample to estimate the mean of a variable of interest that is available only on the subsample. This article concerns estimation of the variance of the regression estimator. The estimators of variance recommended by Cochran rely solely on the subsample data, at least to first order. This article note proposes variance estimators that make better use of the entire sample."], ["Determining the Dimensionality in Sliced Inverse Regression", "A general regression problem is one in which a response variable can be expressed as some function of one or more different linear combinations of a set of explanatory variables as well as a random error term. Sliced inverse regression is a method for determining these linear combinations. In this article we address the problem of determining how many linear combinations are involved. Procedures based on conditional means and conditional covariance matrices, as well as a procedure combining the two approaches, are considered. In each case we develop a test that has an asymptotic chi-squared distribution when the vector of explanatory variables is sampled from an elliptically symmetric distribution."], ["Rank-Based Estimates in the Linear Model with High Breakdown Point", null], ["Distribution-Free Two-Sample Tests Based on Rank Spacings", null], ["The Effect of Imperfect Judgment Rankings on Properties of Procedures Based on the Ranked-Set Samples Analog of the Mann-Whitney-Wilcoxon Statistic", null], ["On the Interpretation of Regression Plots", "A framework is developed for the interpretation of regression plots, including plots of the response against selected covariates, residual plots, added-variable plots, and detrended added-variable plots. It is shown that many of the common interpretations associated with these plots can be misleading. The framework also allows for the generalization of standard plots and the development of new plotting paradigms. A paradigm for graphical exploration of regression problems is sketched."], ["Mosaic Displays for Multi-Way Contingency Tables", null], ["A Simple Dynamic Graphical Diagnostic Method for Almost Any Model", null], ["Specification, Estimation, and Evaluation of Smooth Transition Autoregressive Models", "This article considers the application of two families of nonlinear autoregressive models, the logistic (LSTAR) and exponential (ESTAR) autoregressive models. This includes the specification of the model based on simple statistical tests: linearity testing against smooth transition autoregression, determining the delay parameter and choosing between LSTAR and ESTAR models are discussed. Estimation by nonlinear least squares is considered as well as evaluating the properties of the estimated model. The proposed techniques are illustrated by examples using both simulated and real time series."], ["Estimation of Lag in Misregistered, Averaged Images", "In problems where the recorded signal comprises two or more bands (i.e., wavelengths), it is often the case that the bands are not perfectly aligned in the focal plane. Such misregistration may be caused by atmospheric or oceanographic effects, which refract different bands to differing degrees, or it may result from features of the design of the recording equipment. This article develops two methods for estimating the lag, or the amount by which the bands are out of alignment on a pixel grid, in cases where the recorded data are obtained by pixel averaging. Our two techniques are applicable directly to the signal domain and are based on penalized least squares (or errors in variables) and maximum cross-covariance. They use mathematical interpolation to accommodate the effect of pixel averaging. We introduce a concise and tractable asymptotic model for comparing the performance of the two techniques. The model predicts that the techniques should perform similarly. This is borne out by simulation studies and by applications to real data."], ["A Model for Segmentation and Analysis of Noisy Images", "This article proposes a statistical model for image generation that provides automatic segmentation of images into intensity-differentiated regions and facilitates the quantitative assessment of uncertainty associated with identified image features. The model is specified hierarchically within the Bayesian paradigm. At the lowest level in the hierarchy, a Gibbs distribution is used to specify a probability distribution on the space of all possible partitions of the discretized image scene. An important feature of this distribution is that the number of partitioning elements, or image regions, is not specified a priori. At higher levels in the hierarchical specification, random variables representing emission intensities are associated with regions and pixels. Observations are assumed to be generated from exponential family models centered about these values."], ["Approximately Bayesian Inference", "Consider statistical inference about a scalar parameter and suppose that information about that parameter is to be summarized by a system of interval estimates. It is well known that methods of interval estimation that do not correspond to Bayesian inference with respect to some prior distribution have some logical difficulties. This article proposes a measure of how close a system of interval estimates is to being of Bayesian form. Using this measure, several non-Bayesian methods of interval estimation are analyzed; these results are illustrated on several examples."], ["Laplace Approximations for Posterior Expectations When the Mode Occurs at the Boundary of the Parameter Space", "This article gives asymptotic expansions for posterior expectations when the mode is on the boundary of the parameter space. The idea, based on the divergence theorem, is to reduce the high-dimensional integrals over the parameters space to surface integrals over the boundary of the parameter space and then apply the usual interior-mode Laplace method to the latter integrals. It is shown that these approximations have second-order accuracy. The method is illustrated with applications to a two-sample binomial problem and a random-eflects model."], ["An Application of the Laplace Method to Finite Mixture Distributions", "An exact Bayesian analysis of finite mixture distributions is often computationally infeasible, because the number of terms in the posterior density grows exponentially with the sample size. A modification of the Laplace method is presented and applied to estimation of posterior functions in a Bayesian analysis of finite mixture distributions. The procedure, which involves computations similar to those required in maximum likelihood estimation, is shown to have high asymptotic accuracy for finite mixtures of certain exponential-family densities. For these mixture densities, the posterior density is also shown to be asymptotically normal. An approximation of the posterior density of the number of components is presented. The method is applied to Duncan's barley data and to a distribution of lake chemistry data for north-central Wisconsin."], ["Estimating Normal Means with a Dirichlet Process Prior", "In this article, the Dirichlet process prior is used to provide a nonparametric Bayesian estimate of a vector of normal means. In the past there have been computational difficulties with this model. This article solves the computational difficulties by developing a \u201cGibbs sampler\u201d algorithm. The estimator developed in this article is then compared to parametric empirical Bayes estimators (PEB) and nonparametric empirical Bayes estimators (NPEB) in a Monte Carlo study. The Monte Carlo study demonstrates that in some conditions the PEB is better than the NPEB and in other conditions the NPEB is better than the PEB. The Monte Carlo study also shows that the estimator developed in this article produces estimates that are about as good as the PEB when the PEB is better and produces estimates that are as good as the NPEB estimator when that method is better."], ["Sequential Imputations and Bayesian Missing Data Problems", "For missing data problems, Tanner and Wong have described a data augmentation procedure that approximates the actual posterior distribution of the parameter vector by a mixture of complete data posteriors. Their method of constructing the complete data sets is closely related to the Gibbs sampler. Both required iterations, and, similar to the EM algorithm, convergence can be slow. We introduce in this article an alternative procedure that involves imputing the missing data sequentially and computing appropriate importance sampling weights. In many applications this new procedure works very well without the need for iterations. Sensitivity analysis, influence analysis, and updating with new data can be performed cheaply. Bayesian prediction and model selection can also be incorporated. Examples taken from a wide range of applications are used for illustration."], ["Testing the Minimal Repair Assumption in an Imperfect Repair Model", "Models assuming minimal repair specify that on repair, a failed system is returned to the working state, while the effective age of the system is held constant; that is, the distribution of the time until the next failure of the repaired system is the same as for a system of the same age that has not yet failed. These models are common in the literature of operations research and reliability, and many probabilistic results as well as inferential procedures depend on the minimal repair assumption. We propose two nonparametric tests of the assumption that imperfectly repaired systems are minimally repaired in some models. The large sample theory for these tests is derived from the asymptotic joint distribution of a survival function estimator and the ordinary empirical survival function based on the initial failure times of new or perfectly repaired systems. Simulation results are also provided for the null hypothesis case and under other alternatives."], ["Choosing the Resampling Scheme when Bootstrapping: A Case Study in Reliability", null], ["A Predictive Approach to the Analysis of Designed Experiments", null], ["Testing and Selecting for Equivalence with Respect to a Control", "The problem of equivalence assessment of several treatments with a control is considered. The first approach uses a hypothesis testing formulation in which the alternative states global equivalence. With respect to a family of distributions with location parameter, a test based on a two-sided many-one statistic is proposed. Least favorable parameter configuration results are presented as being necessary to evaluate critical values and to determine sample sizes implied by certain power requirements when planning experiments. The second approach concerns a stepwise selection procedure. The goal is to select a subset of treatments containing all those actually equivalent to the control in the absence of global equivalence. The normal case is dealt with in detail for randomized block designs as well as for one-way layouts. Concerning the test problem, optimal allocations of total sample sizes are determined to guarantee specified power requirements for the one-way layout."], ["Maximum Likelihood Variance Components Estimation for Binary Data", "We consider a class of probit-normal models for binary data and describe ML and REML estimation of variance components for that class as well as best prediction for the realized values of the random effects. ML estimates are calculated using an EM algorithm; for complicated models EM includes a Gibbs step. The computations are illustrated through two examples."], ["Fully Nonparametric Hypotheses for Factorial Designs I: Multivariate Repeated Measures Designs", null], [null, "The problem of estimating the number of trials in a Binomial distribution is known to be particularly difficult in certain circumstances. Both the maximum likelihood and method-of-moments estimators can produce very erratic results, being extremely sensitive to minor fluctuations in the data. But existing asymptotic theory for these estimators does not provide any insight into this peculiar behavior, as the theory typically produces asymptotic Normal distributions with respectable means and variances. Likewise, the limiting distributions of other estimators do not convey much information about why those estimators might perform better than their more classical counterparts. In this article we develop an alternative asymptotic theory that clearly and vividly depicts not only the erratic performance of the maximum likelihood and method-of-moments estimators, but also the more regular, robust properties of alternative estimators proposed by Carroll and Lombard. We note that the parameter configurations under which these theoretical results are valid are just those exhibited by well-known, \u201ctroublesome\u201d data sets. In these settings, the maximum likelihood and method-of-moments estimators have limiting distributions with very heavy tails, such as the Cauchy, whereas the Carroll-Lombard estimators have limiting distributions with very light tails."], ["Book Reviews", null], ["Publications Received", null], ["Letters to the Editor: Comment on Geisser", null], ["Letters to the Editor: Reply to Aitkin's Letter", null], ["Editorial Board Page", "This article has no abstract"]]}