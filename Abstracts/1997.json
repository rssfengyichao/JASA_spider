{"1997": [["Shaping Statistics for Success in the 21st Century", null], ["Analysis of Nonrandomly Censored Ordered Categorical Longitudinal Data from Analgesic Trials", "A clinical trial of an analgesic agent compares pain relief scores over time among groups of patients. All subjects experience the same painful procedure, but different subjects are given different randomly assigned doses of active agent or placebo when they first request it. The data are short individual time series of ordered categorical pain relief scores subsequent to dosing. Nonrandom right censoring may be present because patients can elect to remedicate with an active agent if their pain relief is insufficient. The trial is meant to address two questions: (a) Is there proof that the drug relieves pain? If so, (b) What dosage patterns should be investigated further, or recommended for use by a typical patient? Marginal models of human pharmacology are basically empirical models, and although an analysis of a study based on such a model can adequately address the first question, such is not the case for the second question, because this question requires extrapolation to untested dosing patterns. We propose to analyze study data using a hierarchical model so as to address both questions. The analysis uses a semimechanistic subject-specific pain-relief model for the distribution of all (uncensored and censored) observations, conditional on individual random effects, and an empirical model for the censoring outcome, remedication, conditional on observed pain relief and individual random effects. We estimate the parameters of the foregoing (nonlinear mixed effects) model via maximum likelihood, assuming normally distributed random effects. Monte Carlo integration with respect to the random effects is used to compute marginal statistics relevant to the dosing question. Of particular note is that this formulation encourages use of subject matter information in model specification so that the extrapolations required to address the dosing question are credible. An example is given of the application of the analysis to analgesic trial data for the drug ketorolac."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Bayesian Demography: Projecting the Iraqi Kurdish Population, 1977\u20131990", "Projecting populations that have sparse or unreliable data, such as those of many developing countries, presents a challenge to demographers. The assumptions that they make to project data-poor populations frequently fall into the realm of \u201ceducated guesses,\u201d and the resulting projections, often regarded as forecasts, are valid only to the extent that the assumptions on which they are based reasonably represent the past or future, as the case may be. These traditional projection techniques do not incorporate a demographer's assessment of uncertainty in the assumptions. Addressing the challenges of forecasting a data-poor population, we project the Iraqi Kurdish population using a Bayesian approach. This approach incorporates a demographer's uncertainty about past and future characteristics of the population in the form of elicited prior distributions."], ["Improving the Quality of Economic Data: Lessons from the HRS and AHEAD", "Missing data are an increasingly important problem in economic surveys, especially when trying to measure household wealth. However, some relatively simple new survey methods such as follow-up brackets appear to appreciably improve the quality of household economic data. Brackets represent partial responses to asset questions and apparently significantly reduce item nonresponse. Brackets also provide a remedy to deal with nonignorable nonresponse bias, a critical problem with economic survey data."], ["A Bayesian Population Model with Hierarchical Mixture Priors Applied to Blood Count Data", "Population pharmacokinetic and pharmacodynamic studies require analyzing nonlinear growth curves fit to multiple measurements from study subjects. We propose a class of nonlinear population models with nonparametric second-stage priors for analyzing such data. The proposed models apply a flexible class of mixtures to implement the nonparametric second stage. The discussion is based on a pharmacodynamic study involving longitudinal data consisting of hematologic profiles (i.e., blood counts measured over time) of cancer patients undergoing chemotherapy. We describe a full posterior analysis in a Bayesian framework. This includes prediction of future observations (profiles and end points for new patients), estimation of the mean response function for observed individuals, and inference on population characteristics. The mixture model is specified and given a hyperprior distribution by means of a Dirichlet processes prior on the mixing measure. Estimation is implemented by a combination of various Markov chain Monte Carlo schemes, including a novel independence chain scheme for a logistic regression. The discussion is motivated by a pharmacodynamic case study; however, the concepts are more generally applicable to the wider class of population models."], ["Camouflaged Deconvolution with Application to Blood Curve Modeling in FDG PET Studies", null], ["Modeling Repeated Measures with Monotonic Ordinal Responses and Misclassification, with Applications to Studying Maturation", "Many longitudinal studies of children are concerned with the modeling of monotonic responses such as growth and sexual maturation. The National Heart, Lung, and Blood Institute Growth and Health Study (NGHS) is a longitudinal study designed to examine the effect of growth and maturation on the development of obesity and related cardiovascular risk factors among black and white adolescent girls. Sexual maturation is measured with an ordinal outcome and is known to be measured with sizable diagnostic error. Of interest is examining the effects of race and age on the sexual maturation process. Here we propose a class of models for analyzing repeated monotonic ordinal responses with diagnostic misclassification in which we separately model the underlying monotonic response and misclassification processes. We develop an EM algorithm for maximum likelihood estimation that incorporates covariates and randomly missing data. We use the method to analyze the NGHS sexual maturation data."], ["Establishing the Nadir of the Body Mass Index-Mortality Relationship: A Case Study", null], ["The Generalized Estimating Equation Approach When Data are Not Missing Completely at Random", "We propose two methods for handling missing data in generalized estimating equation (GEE) analyses: mean imputation and multiple imputation. Each provides valid GEE estimates when data are missing at random. Missing outcomes are imputed sequentially starting from the outcome nearest in time to the observed outcome. The estimators from the two kinds of imputation are compared with the weighting method of Robins et al. We show that multiple imputation with an infinite number of replications is asymptotically equivalent to mean imputation. The methods are applied to a stroke study in which neurological outcomes are measured over time after stroke but some outcomes are missing due to death or loss to follow up."], ["Group-Sequential Analysis Incorporating Covariate Information", "In this article we survey existing results concerning the joint distribution of the sequence of estimates of the parameter vector when a model is fitted to accumulating data and provide a unified theory that explains the \u201cindependent increments\u201d structure commonly seen in group-sequential test statistics. Our theory covers normal linear models, including the case of correlated observations, and asymptotic results extend to generalized linear models and the proportional hazards regression model for survival data. The asymptotic results are derived using standard methods for the nonsequential case, and they hold as long as these nonsequential techniques are applicable at each individual analysis. In all cases, the joint distribution of the sequence of parameter estimates has the same form, exactly or asymptotically, as that of the sequence of means of an increasing number of independent, identically distributed normal variables. Thus our results provide the formal basis for extending the scope of standard group-sequential methods to a wide range of problems."], ["Semiparametric Efficiency and its Implication on the Design and Analysis of Group-Sequential Studies", "Authors have shown that the time-sequential joint distributions of many statistics used to analyze data arising from group-sequential time-to-event and longitudinal studies are multivariate normal with an independent increments covariance structure. In Theorem 1 of this article, we demonstrate that this limiting distribution arises naturally when one uses an efficient test statistic to test a single parameter in a semiparametric or parametric model. Because we are able to think of many of the statistics in the literature in this fashion, the limiting distribution under investigation is just a special case of Theorem 1. Using this general structure, we then develop an information-based design and monitoring procedure that can be applied to any type of model for any type of group-sequential study provided that there is a unique parameter of interest that can be efficiently tested."], ["A Semiparametric Approach to Hazard Estimation with Randomly Censored Observations", "One method for estimating the hazard function is to use a parametric estimator, provided that the underlying distribution of the data can be assumed to belong to some well known family of distributions depending on an unknown, possibly vector-valued parameter. Another approach is to use nonparametric estimators, such as Cox's proportional hazard models, kernel hazard estimators, and so on. However, nonparametric estimators are less efficient than suitably chosen parametric models. But, regardless of how suitable a parametric model may be, because of errors associated with data collection, it is impossible to determine with certainty whether the observed data are actually generated by the postulated model. Consequently, instead of using exclusively a parametric or a nonparametric estimator, we propose to fit a weighted average of both. The weight is estimated by minimizing the mean square error of the combination. The main point is that we expect the proposed model to assign more weight to the estimator that best fits the data. Indeed, we show that when the parametric model holds, the proposed hazard estimator converges to the true hazard function at the same rate as the parametric hazard estimator; otherwise, it converges at the same rate as the nonparametric estimator."], ["Accurate Bootstrap Confidence Limits for the Cumulative Hazard and Survivor Functions under Random Censoring", null], ["Latent Variable Regression for Multiple Discrete Outcomes", "Quantifying human health and functioning poses significant challenges in many research areas. Commonly in the social and behavioral sciences and increasingly in epidemiologic research, multiple indicators are utilized as responses in lieu of an obvious single measure for an outcome of interest. In this article we study the concomitant latent class model for analyzing such multivariate categorical outcome data. We develop practical theory for reducing and identifying such models. We detail parameter and standard error fitting that parallels standard latent class methodology, thus supplementing the approach proposed by Dayton and Macready. We propose and study diagnostic strategies, exemplifying our methods using physical disability data from an ongoing gerontologic study. Throughout, the focus of our work is on applications for which a primary goal is to study the association between health or functioning and covariates."], ["Efficiency Lost by Analyzing Counts Rather than Event Times in Poisson and Overdispersed Poisson Regression Models", "Inference for point processes is most efficient if the event times for each individual are available. Sometimes, the study design is such that only aggregated data are collected, consisting of the number of events or recurrences for each individual over the observation period. This article discusses the loss in efficiency of an analysis of the aggregated counts versus an analysis of the actual event times. One particular case is exemplified\u2014that in which the purpose of the experiment or trial is to compare the effects of treatments\u2014and the loss in efficiency in the estimator of the treatment effect is computed. The specific point process considered here is the nonhomogeneous Poisson process, with a proportional intensity model for the treatment effects. Random-effects models are also considered, with estimation via a quasi-likelihood approach. The quasi-likelihood analysis proposed here is an extension of such techniques for the homogeneous Poisson process. The resulting estimating equations for the parameters in the random-effects models are simple and intuitive. The results show that for many usual situations, treatment effects are very efficiently estimated using aggregated data, but the underlying intensity function is not."], ["Testing Goodness-of-Fit Based on a Roughness Measure", "A test for the one-sample goodness-of-fit problem is proposed. The test is based on a distance that measures the difference, in terms of roughness, between the underlying density function and the hypothesized density function. One advantage of using a roughness measure is high power in detecting high-frequency alternatives and densities with sharp features. The test statistic that estimates the distance is derived from the viewpoint of kernel density estimation, and a testing procedure is developed based on the asymptotic distribution of the test statistic. The proposed test is compared to the Kolmogorov-Smirnov test."], ["Dynamic Conditional Independence Models and Markov Chain Monte Carlo Methods", null], ["Adaptive Bayesian Wavelet Shrinkage", "When fitting wavelet based models, shrinkage of the empirical wavelet coefficients is an effective tool for denoising the data. This article outlines a Bayesian approach to shrinkage, obtained by placing priors on the wavelet coefficients. The prior for each coefficient consists of a mixture of two normal distributions with different standard deviations. The simple and intuitive form of prior allows us to propose automatic choices of prior parameters. These parameters are chosen adaptively according to the resolution level of the coefficients, typically shrinking high resolution (frequency) coefficients more heavily. Assuming a good estimate of the background noise level, we obtain closed form expressions for the posterior means and variances of the unknown wavelet coefficients. The latter may be used to assess uncertainty in the reconstruction. Several examples are used to illustrate the method, and comparisons are made with other shrinkage methods."], ["Bayesian Prediction of Transformed Gaussian Random Fields", "A model for prediction in some types of non-Gaussian random fields is presented. It extends the work of Handcock and Stein to prediction in transformed Gaussian random fields, where the transformation is known to belong to a parametric family of monotone transformations. The Bayesian transformed Gaussian model (BTG) provides an alternative to trans-Gaussian kriging taking into account the major sources of uncertainty, including uncertainty about the \u201cnormalizing transformation\u201d itself, in the computation of the predictive density function. Unlike trans-Gaussian kriging, this approach mitigates the consequences of a misspecified transformation, giving in this sense a more robust predictive inference. Because the mean of the predictive distribution does not exist for some commonly used families of transformations, the median is used as the optimal predictor. The BTG model is applied in the spatial prediction of weekly rainfall amounts. Cross-validation shows the predicting performance of the BTG model compares favorably with several kriging variants."], ["Classical and Bayesian Inference Robustness in Multivariate Regression Models", null], ["Nonparametric Estimation of a Mixing Density via the Kernel Method", "This article presents a method for estimating the latent distribution of a mixture model. The method is motivated by the standard kernel density estimation, but instead of using an estimate based on the unobserved latent variables, it takes the expectation with respect to their distribution conditional on the data. The resulting estimator is continuous and hence appropriate when there is a strong belief in the continuity of the mixing distribution. An asymptotic justification is presented, and the associated computational problems are discussed. The method is illustrated by an example of fission track analysis in which the density of the age of crystals is estimated."], ["Nonlinearly Smoothed EM Density Estimation with Automated Smoothing Parameter Selection for Nonparametric Deconvolution Problems", "We study a nonparametric deconvolution density estimation problem. The estimator is obtained by an EM algorithm for a smoothed maximum likelihood estimation problem, which has a unique continuous solution. We present an implementation of the procedure incorporating a data-driven discrepancy principle for selecting the smoothing parameter. Simulations illustrate the good properties of the resulting estimator when the unknown distribution is smooth and has regularly varying thin tails. Comparisons with a Fourier kernel deconvolution method are made for the case of normal noise. We show that under mild smoothness conditions, the estimator based on the data-driven smoothing parameter is strongly consistent."], ["Deconvolution of a Distribution Function", "We consider the estimation of a distribution function when observations from this distribution are contaminated by measurement error. The unknown distribution is modeled as a mixture of a finite number of known distributions. Model parameters can be estimated and confidence intervals constructed using well-known likelihood theory. We show that it is also possible to apply this approach to estimation of a unimodal distribution. An application is presented using data from a dietary survey. Simulation results are given to indicate the performance of the estimators and the confidence interval procedures."], ["Approximating the Distribution of the Scan Statistic Using Moments of the Number of Clumps", null], ["A Spatial Scan Statistic for Stochastic Scan Partitions", "This article develops a spatial scan statistic for homogeneity analysis of point processes that utilizes stochastic scan partitions. The derivation of the sampling distribution for the statistic yields an exact test. This test has the potential for improved power over conventional alternatives when the point process is embedded in an underlying continuous random field and is recommended in situations for which the location of subregions of nonhomogeneity in the point process correspond to regions in the underlying field that can be segmented as distinct from their surroundings. The application to the detection of clustered microcalcifications in digital mammography is investigated as a motivating example."], ["Nonparametric Maximum Likelihood Estimation of Features in Spatial Point Processes Using Vorono\u00ef Tessellation", "This article addresses the problem of estimating the support domain of a bounded point process in presence of background noise. This situation occurs, for example, in the detection of a minefield from aerial observations. A maximum likelihood estimator for a mixture of uniform point processes is derived using a natural partition of the space defined by the data themselves: the Voronoi tessellation. The methodology is tested on simulations and compared to a model-based clustering technique."], ["Box-Type Approximations in Nonparametric Factorial Designs", null], ["Robust Designs Based on the Infinitesimal Approach", null], ["An Analysis of Transformations for Additive Nonparametric Regression", "We consider a nonparametric regression model with a parametric family of dependent variable transformations, one of which induces additive covariate effects. We estimate the additive regression effects using the integration method and estimate the transformation parameter from a profiled instrumental variable and pseudolikelihood criterion. The asymptotic distributions of the parameter and regression estimates are given. The practical performance is investigated via an application."], ["A Bayesian Approach to Nonparametric Bivariate Regression", "This article outlines a general Bayesian approach to estimating a bivariate regression function in a nonparametric manner. It models the function using a bivariate regression spline basis with many terms. Binary indicator variables corresponding to these terms are introduced to explicitly model the uncertainty of whether or not the terms provide a significant contribution to the regression. The regression function is estimated using an estimate of its posterior mean, smoothing over the distribution of these binary indicator variables. To make the computations tractable, all estimates are obtained using Markov chain Monte Carlo sampling. Extensive simulated comparisons are provided that demonstrate the competitive performance of this approach against other data-driven bivariate surface estimators prominent in the literature. It is then shown how the procedure can be extended to provide a general approach to nonparametric bivariate surface estimation in two difficult regression settings. The first case allows for outlying values in the dependent variable. The second case considers data collected in time order with the errors potentially autocorrelated. Simulated and real data examples illustrate the effectiveness of the methodology in tackling such difficult problems."], ["Local Polynomial Estimation in Multiparameter Likelihood Models", "The nonparametric regression technique of local polynomial fitting is extended to multiparameter likelihood models. Some well-known appealing features of local polynomial smoothers, such as the behavior at the boundary, are shown to carry over to the multiparameter case. Asymptotic consistency and normality of the resulting estimators are derived under suitable regularity conditions. This work is motivated by the need for a nonparametric alternative to parametric dose-response models for clustered binary data. Probability models for clustered binary response data include a success probability parameter and one or more correlation parameters. The proposed local polynomial estimators can play an important role as a diagnostic tool or to suggest the form of the functional relationships in parametric likelihood models. As an illustration, it is shown how the local likelihood estimation procedure can be implemented for fitting a dose-response curve based on the beta-binomial model. A data example and a small simulation study demonstrate the method's applicability."], ["Additive Splines for Partial Least Squares Regression", "This article introduces a generalization of the partial least squares regression (PLS). Transforming the predictors by means of spline functions is a useful way to extend PLS into nonlinearity and to obtain a multiresponse additive model. We describe both statistical and computational aspects of this new method, termed additive splines partial least squares (ASPLS). The performance of ASPLS compared with other PLS methods is illustrated with chemical and physiological applications."], ["Empirical Bayes Estimation of Finite Population Means from Complex Surveys", "Estimation of finite population means is considered when samples are collected using a stratified sampling design. Finite populations for different strata are assumed to be realizations from different superpopulations. The true means of the observations lie on a regression surface with random intercepts for different strata. The true sampling variances are also different and random for different strata. The strata are connected through two common prior distributions, one for the intercepts and another for the sampling variances for all the strata. The model is appropriate in two important survey situations. First, it can be applied to repeated surveys where the physical characteristics of the sampling units change slowly over time. Second, the model is appropriate in small-area estimation problems where a very few samples are available for any particular area. Empirical Bayes estimators of the finite population means are shown to be asymptotically optimal in the sense of Robbins. The proposed empirical Bayes estimators are also compared to the classical regression estimators in terms of the relative savings loss due to Efron and Morris. A measure of variability of the proposed empirical Bayes estimator is considered based on bootstrap samples. This measure of variability incorporates all sources of variations due to the estimation of various model parameters. A numerical study is conducted to evaluate the performance of the proposed empirical Bayes estimator compared to rival estimators."], ["A Nonparametric Method for Benchmarking Survey Data via Signal Extraction", "This article introduces a nonparametric method to estimate the covariance matrix for the stationary part of the signal (hidden in data), to enable benchmarking via signal extraction. Some discussions and simulations are carried out to compare the proposed benchmarking method to the regression method development by Cholette and Dagum and the signal extraction method developed by Hillmer and Trabelsi suggesting autoregression integrated moving average (ARIMA) models for the signal. The results show that the nonparametric method is feasible, robust, and almost as efficient as the signal extraction method when the true model for the signal is known."], ["The Standardized Influence Matrix and its Applications", "In this article we introduce the standardized influence matrix (SIM) of parameter estimators as a conjugate of the sample covariance matrix of standardized influence functions (SIFs) evaluated at the data points. We propose principal component analysis for the SIM and its complement (SIM analysis) as a diagnostic tool for regression or other statistical inference, and provide theoretical insight to the local influence defined by Cook. We show that SIM analysis reveals the multivariate structure of outlying and/or influential points. Specifically, SIM analysis uncovers hidden structures of influence, such as clustering, that cannot be identified by the lengths of the standardized influences. Finally, examples in linear regression show that a diagnostic method using SIM is more effective if robust parameter estimates are used in calculating the sample SIM."], ["An Approach to Multivariate Rank Tests in Multivariate Analysis of Variance", "A class of multivariate rank-like quantities is defined and used to develop multivariate tests to mimic popular one-dimensional rank tests such as the Mann-Whitney/Wilcoxon two-sample test, the Jonckheere-Terpstra test for trend, and the Kruskal-Wallis one-way analysis of variance test. Tests in one-way analysis of variance are developed based on qualitative orthogonal contrasts, allowing decomposition of an overall statistic into asymptotically independent components based on the contrasts. The class of tests includes the usual normal-theory tests and the componentwise rank tests, but the main focus is on the tests based on a particular definition of multivariate rank. A study of the Pitman efficiency of the latter tests to those based on multivariate medians shows them to be superior at the normal, slightly heavy-tailed, and light-tailed distributions, whereas the median-based tests are superior for heavy tails. These results are analogous to the univariate case."], ["Affine-Invariant Multivariate One-Sample Signed-Rank Tests", null], ["The Simes Method for Multiple Hypothesis Testing with Positively Dependent Test Statistics", "The Simes method for testing intersection of more than two hypotheses is known to control the probability of type I error only when the underlying test statistics are independent. Although this method is more powerful than the classical Bonferroni method, it is not known whether it is conservative when the test statistics are dependent. This article proves that for multivariate distributions exhibiting a type of positive dependence that arise in many multiple-hypothesis testing situations, the Simes method indeed controls the probability of type I error. This extends some results established very recently in the special case of two hypotheses."], ["Fitting the Generalized Pareto Distribution to Data", null], ["Estimation and Prediction for a Class of Dynamic Nonlinear Statistical Models", "A class of nonlinear state-space models, characterized by a single source of randomness, is introduced. A special case, the model underpinning the multiplicative Holt-Winters method of forecasting, is identified. Maximum likelihood estimation based on exponential smoothing instead of a Kalman filter, and with the potential to be applied in contexts involving non-Gaussian disturbances, is considered. A method for computing prediction intervals is proposed and evaluated on both simulated and real data."], ["Exact Initial Kalman Filtering and Smoothing for Nonstationary Time Series Models", "This article presents a new exact solution for the initialization of the Kalman filter for state space models with diffuse initial conditions. For example, the regression model with stochastic trend, seasonal and other nonstationary autoregressive integrated moving average components requires a (partially) diffuse initial state vector. The proposed analytical solution is easy to implement and computationally efficient. The exact solution for smoothing is also given. Missing observations are handled in a straightforward manner. All proofs rely on elementary results."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Correction", "Robert L. Strawderman, George Casella, and Martin T. Wells, \u201cCorrection to Practical Small Sample Asymptotics for Regression Problems,\u201d 91, No. 434 (June 1996), 643\u2013654"], ["1997 Editorial Collaborators", null], ["Editorial Board Page", "This article has no abstract"], ["Statistical Methods for Profiling Providers of Medical Care: Issues and Applications", null], ["Small Area Inference for Binary Variables in the National Health Interview Survey", "We describe the numerical methods needed to obtain the desired posterior moments. Then we compare estimates produced using the exact numerical method with approximations. Finally, we compare the hierarchical Bayes estimates to empirical Bayes estimates and to standard methods, that is, synthetic estimates and estimates obtained from a conventional randomization-based approach. We use a cross-validation exercise to assess the quality of model fit. We also summarize the results of a separate study of the binary indicator of partial work limitation. Because we know the value of this variable for each respondent to the 1990 Census long form, we can compare estimates corresponding to alternative methods and models with very accurate estimates of the true values."], ["Modeling Discrete Choice with Response Error: Food Stamp Participation", null], ["Broken Biological Size Relationships: A Truncated Semiparametric Regression Approach with Measurement Error", "Biological size relationships (i.e., regression of one size variable on another) are typically monotone but often break down at extreme values of the variables. Here we study the way in which a plant's biomass is apportioned to reproductive and other life activities. Working with a theory proposed by Weiner (1988) based an analogy between a biological plant and an industrial plant leads to a truncated regression model formulation. We consider a dataset involving 542 goldenrod plants that has been analyzed in a limited fashion by others. Important extensions that we provide include nonparametric modeling of the size relationship, introduction of covariate information, incorporation of heterogeneity across plant families, and inclusion of measurement error models for both response and explanatory variables. Our approach is through hierarchical models taking advantage of available prior information on the magnitudes of the size variables. Models are fitted using simulation methods enabling a full range of inference. An attractive model choice criterion demonstrates the need to accommodate all of the aforementioned aspects for the given dataset. Our flexible modeling approach can be adapted to investigate other biological size relationships."], ["Analyses of Fish Species Richness with Spatial Covariate", "Legislation passed in 1990 reducing the allowable sulfur dioxide emission levels in the United States is expected to reduce acidity in the Adirondack region of New York State. The number of fish species in a lake (species richness) depends on a number of physical and chemical factors, including the area, elevation, and acidity of the lake. Data on these and other factors are available for 1,166 Adirondack lakes. The data are analyzed with the goal of quantifying the effects of acid deposition on species richness after controlling for other important factors. A plot of the residuals from a standard multiple regression model against spatial location reveals a strong nonlinear relationship between species richness and lake location. With only one realization of the data, it is difficult, if not impossible, to tell whether this dependence should be modeled as deterministic spatial trend or as a spatial covariance structure. Two models are thus considered. The first is a multiple linear regression model with spatially correlated errors. As is common in geostatistics, it is assumed that the correlation between two errors is a function only of the vector separating the corresponding lakes. This is accomplished by modeling the correlation among the errors with a parametric variogram model. The parameters of the variogram are estimated via restricted maximum likelihood, after which the regression parameters are estimated using generalized least squares. The second model considered is a partial linear (or semiparametric) model containing a nonlinear term in spatial location. Locally weighted quadratic regression is used in conjunction with an estimation technique described by Speckman to estimate simultaneously the regression parameters and the \u201csmooth\u201d function of longitude and latitude. Finally, the ultimate goal of the 1990 legislation is a 50% reduction in sulfate deposition. Both models are used to form prediction intervals for species richness under that scenario. Extrapolation is avoided by using the criteria of Weisberg. The results suggest that a 50% reduction in sulfate deposition may result in a substantial increase in the fish species richness of many Adirondack lakes, particularly those with pH values between 4.5 and 6.5."], ["Using Expectations Data to Study Subjective Income Expectations", null], ["Analysis of Subtidal Coastal Sea Level Fluctuations Using Wavelets", "Subtidal coastal sea level fluctuations affect coastal ecosystems and the consequences of destructive events such as tsunamis. We analyze a time series of subtidal fluctuations at Crescent City, California, during 1980\u20131991 using the maximal overlap discrete wavelet transform (MODWT). Our analysis shows that the variability in these fluctuations depends on the season for scales of 32 days and less. We show how the MODWT characterizes nonstationary behavior succinctly and how this characterization can be used to improve forecasts of inundation during tsunamis and storm surges. We provide pseudocode and enough details so that data analysts in other disciplines can readily apply MODWT analysis to other nonstationary time series."], ["Modeling Long-Range Dependence, Nonlinearity, and Periodic Phenomena in Sea Surface Temperatures Using TSMARS", "We analyze a time series of 20 years of daily sea surface temperatures measured off the California coast. The temperatures exhibit quite complicated features, such as effects on many different time scales, nonlinear effects, and long-range dependence. We show how a time series version of the multivariate adaptive regression splines (MARS) algorithm, TSMARS, can be used to obtain univariate adaptive spline threshold autoregressive models that capture many of the physical characteristics of the temperatures and are useful for short- and long-term prediction. We also discuss practical modeling issues, such as handling cycles, long-range dependence, and concurrent predictor time series using TSMARS. Models for the temperatures are evaluated using out-of-sample forecast comparisons, residual diagnostics, model skeletons, and sample functions of simulated series. We show that a categorical seasonal indicator variable can be used to model nonlinear structure in the data that is changing with time of year, but find that none of the models captures all of the cycles apparent in the data."], ["Practical Bayesian Density Estimation Using Mixtures of Normals", "Mixtures of normals provide a flexible model for estimating densities in a Bayesian framework. There are some difficulties with this model, however. First, standard reference priors yield improper posteriors. Second, the posterior for the number of components in the mixture is not well defined (if the reference prior is used). Third, posterior simulation does not provide a direct estimate of the posterior for the number of components. We present some practical methods for coping with these problems. Finally, we give some results on the consistency of the method when the maximum number of components is allowed to grow with the sample size."], ["Computing Bayes Factors by Combining Simulation and Asymptotic Approximations", "The Bayes factor is a ratio of two posterior normalizing constants, which may be difficult to compute. We compare several methods of estimating Bayes factors when it is possible to simulate observations from the posterior distributions, via Markov chain Monte Carlo or other techniques. The methods that we study are all easily applied without consideration of special features of the problem, provided that each posterior distribution is well behaved in the sense of having a single dominant mode. We consider a simulated version of Laplace's method, a simulated version of Bartlett correction, importance sampling, and a reciprocal importance sampling technique. We also introduce local volume corrections for each of these. In addition, we apply the bridge sampling method of Meng and Wong. We find that a simulated version of Laplace's method, with local volume correction, furnishes an accurate approximation that is especially useful when likelihood function evaluations are costly. A simple bridge sampling technique in conjunction with Laplace's method often achieves an order of magnitude improvement in accuracy."], ["Bayesian Tests and Model Diagnostics in Conditionally Independent Hierarchical Models", null], ["Hierarchical Selection Models with Applications in Meta-Analysis", "Hierarchical selection models are introduced and shown to be useful in meta-analysis. These models combine the use of hierarchical models, allowing investigation of variability both within and between studies, and weight functions, allowing modeling of nonrandomly selected studies. Markov chain Monte Carlo (MCMC) methods are used to estimate the hierarchical selection model. This is first illustrated for known weight functions, and then extended to allow for estimation of unknown weight functions. To investigate sensitivity of results to unobserved studies directly, which is shown to be different from modeling bias in the selection of observed studies, the hierarchical selection model is used in conjunction with data augmentation. Again, MCMC methods may be used to estimate the model. This is illustrated for an unknown weight function."], ["Nonparametric Maximum Likelihood Estimation from Samples with Irrelevant Data and Verification Bias", null], ["A Hybrid Algorithm for Computation of the Nonparametric Maximum Likelihood Estimator from Censored Data", "We present a hybrid algorithm for nonparametric maximum likelihood estimation from censored data when the log-likelihood is concave. The hybrid algorithm uses a composite algorithmic mapping combining the expectation-maximization (EM) algorithm and the (modified) iterative convex minorant (ICM) algorithm. Global convergence of the hybrid algorithm is proven; the iterates generated by the hybrid algorithm are shown to converge to the nonparametric maximum likelihood estimator (NPMLE) unambiguously. Numerical simulations demonstrate that the hybrid algorithm converges more rapidly than either of the EM or the naive ICM algorithm for doubly censored data. The speed of the hybrid algorithm makes it possible to accompany the NPMLE with bootstrap confidence bands."], ["Sieve Estimation for the Proportional-Odds Failure-Time Regression Model with Interval Censoring", null], ["Maximum Likelihood Estimation in the Proportional Odds Model", "We consider maximum likelihood estimation of the parameters in the proportional odds model with right-censored data. The estimator of the regression coefficient is shown to be asymptotically normal with efficient variance. The maximum likelihood estimator of the unknown monotonic transformation of the survival time converges uniformly at a parametric rate to the true transformation. Estimates for the standard errors of the estimated regression coefficients are obtained by differentiation of the profile likelihood and are shown to be consistent. A likelihood ratio test for the regression coefficient is also considered."], ["Extended Weighted Log-Rank Estimating Functions in Censored Regression", "In the censored regression model, some regression estimators are introduced using weighted integrals of the log-rank estimating functions. Their limiting covariance matrices do not involve the error density and can be reliably estimated. Inference can then be easily obtained. Some of these estimators have high asymptotic efficiency at some important submodels. Some lack-of-fit tests are derived that require little extra computing time. These tests are asymptotically normal under the model and consistent against certain monotone or convex model misspecifications. Numerical studies show that for some weight functions, the estimators and tests perform well. Implementation of the proposed procedures is discussed and illustrated in a real data example."], ["A Marginal Likelihood Approach to Estimation in Frailty Models", "A marginal likelihood approach is proposed for estimating the parameters in a frailty model using clustered survival data. To overcome the analytic intractability of the marginal likelihood function, we propose a Monte Carlo approximation using the technique of importance sampling. Implementation is by means of simulations from the uniform distribution. The suggested method can cope with censoring and unequal cluster sizes and can be applied to any frailty distribution with explicit Laplace transform. We concentrate on a two-parameter family that includes the gamma, inverse Gaussian, and positive stable distributions as special cases. The method is illustrated using data from an animal carcinogenesis experiment and validated in a simulation study."], ["Modeling Partly Conditional Means with Longitudinal Data", null], ["Influence Diagnostics for Linear Longitudinal Models", "Influence diagnostics are important for analyzing cross-sectional regression studies, because they allow the analyst to understand the impact of individual observations on the estimated regression model. In this article we consider the role of influence diagnostics in subject-specific longitudinal models. Diagnostics are proposed under both fixed and random subject effects. Our approach is based on subject deletion, which in this setting involves deleting a group of correlated observations. We develop partial influence statistics to understand the combined impact of observations from a subject on population parameters. Simple computational formulas make the procedures feasible. Finally, we illustrate the use of our new influence statistics by examining a dataset to model a taxpayer's charitable givings."], ["Testing the Independence Assumption in Linear Models", "We propose using an existing set of statistical tools in a new way that allows one to test the independence assumption in standard normal theory linear models. The set of tools is near-replicate lack-of-fit tests. The classical lack-of-fit test requires a linear model in which some rows of the model matrix are repeated. Near-replicate lack-of-fit tests were developed to mimic the behavior of the classical test by identifying clusters of rows in the design matrix that are similar, though not necessarily exact replications. We argue that meaningful clusters can be formed more generally by constructing rational subgroups of data collected under similar circumstances. As such, observations in the same subgroup may be more highly correlated than observations in different subgroups. We investigate the behavior of these tests when used to identify lack of independence."], ["Robust Linear Model Selection by Cross-Validation", "This article gives a robust technique for model selection in regression models, an important aspect of any data analysis involving regression. There is a danger that outliers will have an undue influence on the model chosen and distort any subsequent analysis. We provide a robust algorithm for model selection using Shao's cross-validation methods for choice of variables as a starting point. Because Shao's techniques are based on least squares, they are sensitive to outliers. We develop our robust procedure using the same ideas of cross-validation as Shao but using estimators that are optimal bounded influence for prediction. We demonstrate the effectiveness of our robust procedure in providing protection against outliers both in a simulation study and in a real example. We contrast the results with those obtained by Shao's method, demonstrating a substantial improvement in choosing the correct model in the presence of outliers with little loss of efficiency at the normal model."], ["On the Roles of Observations in Collinearity in the Linear Model", "It is often found that individual observations play an important role in the conditioning of a linear regression problem. A state of collinearity is sometimes masked by one or two observations. Certain other observations may induce collinearity. In this article we consider a few measures of collinearity that focus on various aspects of the problem and examine the effect of case deletion on these measures. The resulting case-deletion diagnostics have the common property that they are all positive and centered around 1. A value larger than 1 corresponds to a collinearity-reducing observation, whereas a value smaller than 1 indicates a collinearityenhancing effect. The diagnostics can be used to assess the effects of multiple observations as well. Modifications of the results for column-equilibrated data are also provided. Some of the diagnostics involve extensive computation. We provide sharp and easily computable upper and lower bounds for these, complementing some results in the existing literature. We report the accuracy of the bounds for a classical dataset and show how the proposed diagnostics can contribute to data analysis. Finally, we provide a set of similar measures and inequalities for analyzing the impact of additional observations on collinearity. These results help in the choice of new design points to reduce collinearity optimally."], ["An Efficient Estimator for the Generalized Semilinear Model", null], ["Subseries Methods in Regression", null], ["Empirical-Bias Bandwidths for Local Polynomial Nonparametric Regression and Density Estimation", "A data-based local bandwidth selector is proposed for nonparametric regression by local fitting of polynomials. The estimator, called the empirical-bias bandwidth selector (EBBS), is rather simple and easily allows multivariate predictor variables and estimation of any order derivative of the regression function. EBBS minimizes an estimate of mean squared error consisting of a squared bias term plus a variance term. The variance term used is exact, not asymptotic, though it involves the conditional variance of the response given the predictors that must be estimated. The bias term is estimated empirically, not from an asymptotic expression. Thus EBBS is similar to the \u201cdouble smoothing\u201d approach of H\u00e4rdle, Hall, and Marron and a local bandwidth selector of Schucany, but is developed here for a far wider class of estimation problems than what those authors considered. EBBS is tested on simulated data, and its performance seems quite satisfactory. Local polynomial smoothing of a histogram is a highly effective technique for density estimation, and several of the examples involve density estimation by EBBS applied to binned data."], ["A Comparison of Higher-Order Bias Kernel Density Estimators", null], ["Covariate-Matched One-Sided Tests for the Difference between Functional Means", "When testing hypotheses about the effects of different treatments, variation among covariates can become confounded with that between treatments unless the treatments are applied using paired covariates. In the context of unpaired covariates, we propose implicit covariate-matching methods for testing the hypothesis that one treatment effect is greater than another. The methods are founded on the assumption that the mean treatment effect, conditional on the covariate, is a smooth function of the covariate. They are implemented using new interpolation techniques for nonparametric curve estimation. Bootstrap arguments are used to construct critical points. We show that even when the covariate distributions are identical for both treatments, covariate matching of the type that we propose produces tests of greater power than methods that do not attempt matching. Our techniques have application to two-sided hypothesis testing."], ["Testing Goodness of Fit with Multinomial Data", "Several new test procedures are proposed for assessing the goodness of fit of a postulated multinomial distribution. The new tests are Neyman smooth-type tests with orders selected adaptively from the data. They are shown, through Fourier and large-sample analyses, to provide potential improvements over classical methods in terms of their ability to detect certain types of alternatives. Simulation results and a real example illustrate the finite-sample validity of the large-sample theory and the practical utility of the proposed methods."], ["Data-Driven Smooth Tests When the Hypothesis is Composite", "In recent years several authors have recommended smooth tests for testing goodness of fit. However, the number of components in the smooth test statistic should be chosen well; otherwise, considerable loss of power may occur. Schwarz's selection rule provides one such good choice. Earlier results on simple null hypotheses are extended here to composite hypotheses, which tend to be of more practical interest. For general composite hypotheses, consistency of the data-driven smooth tests holds at essentially any alternative. Monte Carlo experiments on testing exponentiality and normality show that the data-driven version of Neyman's test compares well to other, even specialized, tests over a wide range of alternatives."], ["A Double-Scan Statistic for Clusters of Two Types of Events", "We develop a scan-type statistic to measure the unusualness of the clustering of two types of events over time. The statistic allows for a lagged effect between the two types of events. We derive the expected number of nonoverlapping clumps of clusters under retrospective and prospective chance models of no association. Results are derived and approaches are given to handle both uniform and more general distributions of events over time. We investigate the power of the statistic against an alternative where the observed data is a mixture of linked and unassociated clusters. The statistic is applied to data on homicide/suicide clusters over a 7-year period for several counties and several sex/race combinations."], ["Univariate Random Cut-Points Theory for the Analysis of Ordered Categorical Data", null], ["Asymmetry Models for Contingency Tables", null], ["Modeling and Identifying Optimum Designs for Fitting Dose-Response Curves Based on Raw Optical Density Data", null], ["Multiple Use Confidence Regions in Multivariate Calibration", "The problem of multivariate calibration is considered in the setup where a normally distributed response variable is related to an explanatory variable through a multivariate linear model. The variance covariance matrix of the response variable is assumed to be a multiple of the identity matrix. The calibration data, that is, data obtained on the response variable corresponding to known values of the explanatory variable, are to be used for the construction of confidence regions for unknown values of the explanatory variable. The calibration problem addressed in this article deals with the construction of multiple use confidence regions; that is, the calibration data will be used repeatedly in order to construct a sequence of confidence regions for a sequence of unknown values of the explanatory variable. Such a procedure is characterized using two coverage probabilities, say 1 \u2014 \u03b1 and 1 \u2014 \u03b2. Given that the confidence regions are constructed using the same calibration data, the proportion of confidence regions that include the true values of the corresponding parameters is to be at least 1 \u2014 \u03b2. The probability that the calibration data will provide 100(1 \u2014\u03b2)% coverage is to be at least 1 \u2014 \u03b1 A multiple use confidence region is constructed using a pivot statistic that is a natural choice. The procedure is then generalized to linear models where the explanatory variable enters the model nonlinearly, such as in a polynomial regression model. The computational aspects and the practical implementation of our confidence region are illustrated in detail using two examples."], ["Bayesian Discrimination between Two Multivariate Normal Populations with Equal Covariance Matrices", null], ["An Approximate Likelihood Ratio Test for Comparing Several Treatments to a Control", "Some parametric tests for comparing several treatments to a control are examined. Those tests developed earlier tend to rest on a single principle: Simplicity or power; those developed later attempt to fill the gap. We propose a new test that represents another such attempt. The basic idea is to find a simple statistic that is a good approximation to the likelihood ratio statistic. This leads to a power function similar to that of the likelihood ratio test, which is optimal among the available competitors. Like the orthogonal contrast test, the new test is built on an orthogonal relationship, resulting in a relatively simple null distribution that depends on the sample sizes only through their total. Thus critical values can be easily computed and a detailed listing results in only a moderately large table. Computing the statistic is elementary when the sample sizes of the new treatments are equal; otherwise, some matrix operations are needed. A simple procedure is given to estimate the sample size required to achieve a given power level. To compete with Dunnett's test in terms of pairwise comparison, the new test (or any of the other tests) can be applied according to the closed testing procedure. In this setting the new test is compared to Dunnett's test by simulation."], ["Optimal Unbiased Tests for Equivalence in Intrasubject Variability", "The equivalence in average bioavailability between formulations may not be sufficient for assessment of bioequivalence. The difference in intrasubject variability between formulations should also be considered. This article presents an unbiased test procedure for equivalence in intrasubject variability of bioavailability that is uniformly more powerful than the two one-sided tests procedure proposed by Liu and Chow. Under a stronger condition, a uniformly most powerful invariant test for this problem is proposed. Some numerical comparisons and an example are also presented."], ["Bounds on Treatment Effects from Studies with Imperfect Compliance", "This article establishes nonparametric formulas that can be used to bound the average treatment effect in experimental studies in which treatment assignment is random but subject compliance is imperfect. The bounds provided are the tightest possible, given the distribution of assignments, treatments, and responses. The formulas show that even with high rates of noncompliance, experimental data can yield useful and sometimes accurate information on the average effect of a treatment on the population."], ["A Martingale Approach to the Changepoint Problem", "The changepoint problem for a binary sequence is considered. A test statistic based on recursive residuals is compared to the test statistic suggested by Pettitt. The new test statistic has more interesting properties for use in sequential testing. However, neither of the two test statistics dominates the other. Sequential versions of the martingale-based test, forward and reverse, are given and compared to other tests by means of a simulation study. The reverse martingale tests detect a shift earlier, if it is detected. The price to be paid is a slightly higher probability of not detecting a shift."], ["On Fractionally Integrated Autoregressive Moving-Average Time Series Models with Conditional Heteroscedasticity", null], ["Semiparametric Bayesian Analysis of Survival Data", "This review article investigates the potential of Bayes methods for the analysis of survival data using semiparametric models based on either the hazard or the intensity function. The nonparametric part of every model is assumed to be a realization of a stochastic process. The parametric part, which may include a regression parameter or a parameter quantifying the heterogeneity of a population, is assumed to have a prior distribution with possibly unknown hyperparameters. Careful applications of some recently popular computational tools, including sampling-based algorithms, are used to find posterior estimates of several quantities of interest even when dealing with complex models and unusual data structures. The methodologies developed herein are motivated and aimed at analyzing some common types of survival data from different medical studies; here we focus on univariate survival data in the presence of fixed and time-dependent covariates, multiple event-time data for repeated nonfatal events, and multivariate survival data (subjects are related; e.g., families or litters), each patient with interval-censored infection time and interval-censored disease occurrence time in tandem [e.g., patients with acquired immunodeficiency syndrome (AIDS) and other infectious diseases with long incubation times]. Bayesian exploratory data analysis (EDA) methods and diagnostics for model selection and model assessment are considered for each case. Special attention is given to tests of the parametric modeling assumptions and to censoring."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Editors' Report for 1996", null], ["Ozone Exposure and Population Density in Harris County, Texas", "We address the following question: What is the pattern of human exposure to ozone in Harris County (Houston) since 1980? While there has been considerable research on characterizing ozone measured at fixed monitoring stations, little is known about ozone away from the monitoring stations, and whether areas of higher ozone correspond to areas of high population density. To address this question, we build a spatial-temporal model for hourly ozone levels that predicts ozone at any location in Harris County at any time between 1980 and 1993. Along with building the model, we develop a fast model-fitting method that can cope with the massive amounts of available data and takes into account the substantial number of missing observations. Having built the model, we combine it with census tract information, focusing on young children. We conclude that the highest ozone levels occur at locations with relatively small populations of young children. Using various measures of exposure, we estimate that exposure of young children to ozone decreased by approximately 20% from 1980 to 1993. An examination of the distribution of population exposure has several policy implications. In particular, we conclude that the current siting of monitors is not ideal if one is concerned with population exposure assessment. Monitors appear to be well sited in the downtown Houston and close-in southeast portions of the county. However, the area of peak population is southwest of the urban center, coincident with a rapidly growing residential area. Currently, only one monitor measures air quality in this area. The far north-central and northwest parts of the county are also experiencing rapid population growth, and our model predicts relatively high levels of population exposure in these areas. Again, only one monitor is sited to assess exposure over this large area. The model we developed for the ozone prediction consists of first using a square root transformation and then decomposing the transformed data into a trend part and an irregular part, the latter modeled as a Gaussian random field with both time and space correlations. Due to the large number of observations and high-dimensional optimization problem, we developed a fast method to estimate the parameters of the model. The model and estimation method are general and can be used in many problems with space-time observations."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Efficient Matrix Sampling Instruments for Correlated Latent Traits: Examples from the National Assessment of Educational Progress", "We study the efficiency of administering different subsets of a large collection of items in a sample survey to different people to estimate the distribution of several correlated traits. The designs are motivated by examples from the National Assessment of Educational Progress, an ongoing survey of U.S. students in the fourth, eighth, and twelfth grades. In this survey the traits represent proficiency in different subjects. For example, the Mathematics assessment estimates proficiency with Numbers and Operations, Measurement, and several other traits. Time constraints and concerns about student motivation limit the number of items that can be administered to a small subset of the items defining each trait. We find that effective matrix designs assign some items to measure each trait, even if the resulting number of items assigned to each trait must be small. Efficient allocations of items are ones that produce measurement error variances for a given trait that are similar for each sampled student. These allocations can be substantially better than designs that split the sample and measure traits on different subsets of students. Our results are consistent with recent research on efficient survey instrument design, but lead to substantially different recommendations due to the errors in the measurement of the traits."], ["A Nested Frailty Model for Survival Data, with an Application to the Study of Child Survival in Northeast Brazil", "This article presents a multivariate hazard model for survival data that are clustered at two hierarchical levels. The model provides corrected parameter estimates and standard errors, as well as estimates of the intragroup correlation at both levels. The model is estimated using the expectation-maximization (EM) algorithm. We apply the model to an analysis of the covariates of child survival using survey data from northeast Brazil collected via a hierarchically clustered sampling scheme. We find that family and community frailty effects are fairly small in magnitude but are of importance because they alter the results in a systematic pattern."], ["A Two-Step Approach to Measurement Error in Time-Dependent Covariates in Nonlinear Mixed-Effects Models, with Application to IGF-I Pharmacokinetics", "The usual approach to the analysis of population pharmacokinetic studies is to represent the concentration-time data by a nonlinear mixed-effects model. Primary objectives are to characterize the pattern of drug disposition in the population and to identify individual-specific covariates associated with pharmacokinetic behavior. We consider data from a study of insulin-like growth factor I (IGF-I) administered by intravenous infusion to patients with severe head trauma. Failure to maintain steady-state levels of IGF-I was thought to be related to the temporal pattern of several covariates measured in the study, and an analysis investigating this issue was of interest. Observations on these potentially relevant covariates for each subject were made at time points different from those at which IGF-I concentrations were determined; moreover, the covariates themselves were likely subject to measurement error. The usual approach to time-dependent covariates in population analysis is to invoke a simple interpolation scheme, such as carrying forward the most recent covariate value, ignoring measurement error; however, for these data, the complicated observed covariate pattern makes this approach suspect. A nonlinear mixed-effects model incorporating a model for time-dependent covariates measured with error is used to describe the IGF-I data, and fitting is accomplished by a two-step strategy implemented using standard software. The performance of the method is evaluated via simulation."], ["No-Observed-Adverse-Effect Levels in Severity Data", "Toxicity data are often categorized by severity of response and dose level with the assumption that there is a tolerated dose below which there is no toxicity. For data from a controlled experiment, the largest observed dose at or below the tolerated dose is called the no-observed-adverse-effect level (NOAEL). The problem of identifying the NOAEL can be viewed statistically as estimating the maximal observed dose for which there is no increased severity or frequency of toxic response. We previously proposed a method based on the Akaike information criterion (AIC) for the case with only two response levels (presence or absence of a toxic endpoint). We show here that repeated applications of that method to suitably defined subsets of data provide the maximum penalized likelihood estimate of the NOAEL when there are multiple severity levels, under a slight modification of the continuation-ratio logit model. Three sets of data on controlled exposure of rodents are used to illustrate the method."], ["A Panel Analysis of Liquidity Constraints and Firm Investment", "The issue of financial constraints on company investment is revisited using the U.S. panel data of 561 firms from 1971\u20131992. A number of economically meaningful factors are discovered to partition firms into relatively homogeneous groups. A mixed fixed-and random-coefficients framework is then used to capture unobserved heterogeneity within groups. The prediction criterion is used to select the final specification and evaluate the importance of financial constraints on firm's investment decisions."], ["Interpolation Methods for Adapting to Sparse Design in Nonparametric Regression", "We suggest interpolation methods for overcoming the problem of sparse design in local linear smoothing. These methods are based on simple rules, determined by the kernel and bandwidth, for deciding when and where pseudo-design points should be added to augment the original design sequence. New ordinates for the added design points are computed by simple interpolation, then local linear smoothing is applied directly to the expanded dataset. The method is competitive with alternatives (e.g., those involving ridge regression), in terms of both simplicity and performance."], ["Comment", null], ["Rejoinder", null], ["Generalized Partially Linear Single-Index Models", null], ["Graphics for Assessing the Adequacy of Regression Models", null], ["Smoothing Parameter Selection for Power Optimality in Testing of Regression Curves", "We consider selection of smoothing parameters to obtain optimal power in tests of regression curves. We examine three tests and propose empirical smoothing parameters to maximize the power in each test. We also show that the data-based smoothing parameters converge to the optimal smoothing parameters as sample sizes get larger. We conduct a simulation study for various classes of alternative showing the effectiveness of the proposed procedures."], ["Weighted Semiparametric Estimation in Regression Analysis with Missing Covariate Data", "This article investigates estimation of the regression coefficients in an assumed mean function when covariates on some subjects are missing. We examine the performance of a Horvitz and Thompson (1952)-type weighted estimator by using different estimates of the selection probabilities, which may be treated as nuisance parameters (or a nuisance function). In particular, we investigate the properties of the estimate of the regression parameters when the selection probabilities are estimated by kernel smoothers. We present large sample theory for the new estimator and conduct simulation studies comparing the proposed estimator to the maximum likelihood estimator and multiple imputation under various model assumptions and different missingness mechanisms. In addition, we provide two real examples that motivate this investigation."], ["Density Estimation for the Case of Supersmooth Measurement Error", "The problem is to estimate the probability density of a random variable contaminated by an independent measurement error. I explore one of the worst-case scenario when the characteristic function of this measurement error decreases exponentially and thus optimal estimators converge only with logarithmic rate. The particular example of such measurement error is any random variable contaminated by normal, Cauchy, or another stable random variable. For this setting and circular data, I suggest an asymptotically efficient data-driven estimator that is adaptive to both smoothness of estimated density and distribution of measurement error. Moreover, this estimator is universal in sense that its derivatives and integral are sharp estimators of the corresponding derivatives and the cumulative distribution function, and these estimators are sharp both globally and pointwise. For the case of small sample sizes, I suggest a modified estimator that mimics an optimal linear pseudoestimator. I explore this estimator theoretically, via intensive Monte Carlo simulations and practical examples."], [null, null], ["Improvements on Cross-Validation: The 632+ Bootstrap Method", "A training set of data has been used to construct a rule for predicting future responses. What is the error rate of this rule? This is an important question both for comparing models and for assessing a final selected model. The traditional answer to this question is given by cross-validation. The cross-validation estimate of prediction error is nearly unbiased but can be highly variable. Here we discuss bootstrap estimates of prediction error, which can be thought of as smoothed versions of cross-validation. We show that a particular bootstrap method, the .632+ rule, substantially outperforms cross-validation in a catalog of 24 simulation experiments. Besides providing point estimates, we also consider estimating the variability of an error rate estimate. All of the results here are nonparametric and apply to any possible prediction rule; however, we study only classification problems with 0\u20131 loss in detail. Our simulations include \u201csmooth\u201d prediction rules like Fisher's linear discriminant function and unsmooth ones like nearest neighbors."], ["A Nonparametric Test of Independence between Two Vectors", null], ["Nonparametric Methods for Factorial Designs with Censored Data", null], ["A Multidimensional Goodness-of-Fit Test Based on Interpoint Distances", null], ["Hierarchical Mixture Models in Neurological Transmission Analysis", "Hierarchically structured mixture models are studied in the context of data analysis and inference on neural synaptic transmission characteristics in mammalian, and other, central nervous systems. Mixture structures arise due to uncertainties about the stochastic mechanisms governing the responses to electrochemical stimulation of individual neurotransmitter release sites at nerve junctions. Models attempt to capture such scientific features as the sensitivity of individual synaptic transmission sites to electrochemical stimuli and the extent of their electrochemical responses when stimulated. This is done via suitably structured classes of prior distributions for parameters describing these features. Such priors may be structured to permit assessment of currently topical scientific hypotheses about fundamental neural function. Posterior analysis is implemented via stochastic simulation. Several data analyses are described to illustrate the approach, with resulting neurophysiological insights in some recently generated experimental contexts. Further developments and open questions, both neurophysiological and statistical, are noted."], ["Hierarchical Spatio-Temporal Mapping of Disease Rates", "Maps of regional morbidity and mortality rates are useful tools in determining spatial patterns of disease. Combined with sociodemographic census information, they also permit assessment of environmental justice; that is, whether certain subgroups suffer disproportionately from certain diseases or other adverse effects of harmful environmental exposures. Bayes and empirical Bayes methods have proven useful in smoothing crude maps of disease risk, eliminating the instability of estimates in low-population areas while maintaining geographic resolution. In this article we extend existing hierarchical spatial models to account for temporal effects and spatio-temporal interactions. Fitting the resulting highly parameterized models requires careful implementation of Markov chain Monte Carlo (MCMC) methods, as well as novel techniques for model evaluation and selection. We illustrate our approach using a dataset of county-specific lung cancer rates in the state of Ohio during the period 1968\u20131988."], ["Hierarchical Poisson Regression Modeling", "The Poisson model and analyses here feature nonexchangeable gamma distributions (although exchangeable following a scale transformation) for individual parameters, with standard deviations proportional to means. A relatively uninformative prior distribution for the shrinkage values eliminates the ill behavior of maximum likelihood estimators of the variance components. When tested in simulation studies, the resulting procedure provides better coverage probabilities and smaller risk than several other published rules, and thus works well from Bayesian and frequentist perspectives alike. The computations provide fast, accurate density approximations to individual parameters and to structural regression coefficients. The computer program is publicly available through Statlib."], ["Dirichlet Process Mixed Generalized Linear Models", "Although generalized linear models (GLM's) are an attractive and widely used class of models, they are limited in the range of density shapes that they can provide. For instance, they are unimodal exponential families of densities in the response variable with tail behavior determined by the implicit mean-variance relationship. Dirichlet process (DP) mixing adds considerable flexibility to these models. Using such mixing, we develop models that we call DPMGLM's, which still retain the GLM character with regard to the mean. Overdispersed GLM's (OGLM's) provide an alternative class of models to cope with extra variability in samples. We show that how OGLM's may be DP mixed, leading to what we call DPMOGLM's. These models are extremely rich. Moreover, recent computational advances enable them to be fitted straightforwardly. We illustrate this with both simulated and real datasets. We also address the question of choosing between the GLM, OGLM, DPMGLM, and DPMOGLM. Finally, we consider extensions, by DP mixing, of hierarchical or multistage GLM's."], ["Bayesian Forecasting of Multinomial Time Series through Conditionally Gaussian Dynamic Models", "We consider inference in the class of conditionally Gaussian dynamic models for nonnormal multivariate time series. In such models, data are represented as drawn from nonnormal sampling distributions whose parameters are related both through time and hierarchically across several multivariate series. A key example\u2014the main focus here\u2014is time series of multinomial observations, a common occurrence in sociological and demographic studies involving categorical count data. However, we present this development in a more general setting, as the resulting methods apply beyond the multinomial context. We discuss inference in the proposed model class via a posterior simulation scheme based on appropriate modifications of existing Markov chain Monte Carlo algorithms for normal dynamic linear models and including Metropolis-Hastings components. We develop an analysis of time series of flows of students in the Italian secondary education system as an illustration of the models and methods."], ["Estimating Bayes Factors via Posterior Simulation with the Laplace\u2014Metropolis Estimator", "The key quantity needed for Bayesian hypothesis testing and model selection is the integrated, or marginal, likelihood of a model. We describe a way to use posterior simulation output to estimate integrated likelihoods. We describe the basic Laplace\u2014Metropolis estimator for models without random effects. For models with random effects, we introduce the compound Laplace-Metropolis estimator. We apply this estimator to data from the World Fertility Survey and show it to give accurate results. Batching of simulation output is used to assess the uncertainty involved in using the compound Laplace-Metropolis estimator. The method allows us to test for the effects of independent variables in a random-effects model and also to test for the presence of the random effects."], ["Approximate Bayes Factors When a Mode Occurs on the Boundary", "Bayes factors, measuring the strength of evidence in favor of the null, are often used in the Bayesian approach to testing hypotheses. Laplace approximations to Bayes factors are convenient and quite accurate in many contexts. However, one usual assumption\u2014the existence of an interior mode\u2014does not always hold. The posterior mode can occur at the boundary of the parameter space. This article discusses the boundary mode Laplace's method for boundary modes, uses it to approximate the Bayes factor, and presents a modification to the Schwarz criterion. The sensitivity of Bayes factor to the choice of prior on the nuisance parameter in testing nested models is also investigated. Results are illustrated for the case of testing extrabinomial variability."], ["Spherical-Radial Integration Rules for Bayesian Computation", null], [null, null], ["Bayesian Testing and Estimation of Association in a Two-Way Contingency Table", "In a two-way contingency table, one is interested in checking the goodness of fit of simple models such as independence, quasi-independence, symmetry, and constant association, and estimating parameters that describe the association structure of the table. In a large table, one may be interested in detecting a few outlying cells that deviate from the main association pattern in the table. Bayesian tests of these hypotheses are described using a prior defined on the set of interaction terms of the log-linear model. These tests and associated estimation procedures have several advantages over classical fitting/estimation procedures. First, the tests can give measures of evidence in support of simple hypotheses. Second, the Bayes factors can be used to give estimates of association parameters of the table that allow for uncertainty that the hypothesized model is true. These methods are illustrated for a number of tables."], ["Hypothesis Estimates and Acceptability Profiles for 2 \u00d7 2 Contingency Tables", null], ["Improving Tests for Superior Treatment in Contingency Tables", "When comparing two treatments on the basis of ordinal data, a natural alternative is stochastic order, as it implies that more favorable outcomes receive greater probability. Results of Eaton can be used to provide a complete class of tests. We find that many tests in current use are not in this class and hence are inadmissible. More important, we present methods of improving such tests. Often, the increase in power is substantial although the size remains the same."], ["Summarizing DNA Evidence When Relatives are Possible Suspects", "Summaries of deoxyribonucleic acid (DNA) evidence in criminal proceedings have been controversial in part because calculations of the probability of a \u201cmatch\u201d between crime and suspect samples assuming that the pairing of these samples occurred by chance typically have depended on independence or conditional independence assumptions that are subject to dispute. This article describes a new methodology that summarizes DNA evidence by addressing the possibility that a relative of the accused individual is the source of a crime sample, which can be viewed as complementary to assessing the probability of a \u201cmatch\u201d assuming that individuals in a broader population are paired by chance. The new technique uses a statistical criterion based on distances between bands from autoradiograph images to distinguish individuals without relying on any conditional independence assumptions. We apply the method to data from a paternity testing laboratory to illustrate its ability to distinguish between the DNA profiles of related individuals. In addition, a simulation study suggests that to distinguish reliably between family members, more genetic loci should be probed than the three to six loci typically investigated in current practice."], ["A Restricted Test of Circadian Rhythm", "In medical applications there is interest in whether deaths or other events occur randomly throughout the day, as opposed to in a definite circadian pattern. At times, knowledge of the circadian phenomenon will suggest that a single cluster of events should occur during a specific period during the day, such as mid-morning. We propose a test that incorporates such knowledge. The test can be viewed as the likelihood ratio test for a restricted alternative for data from the von Mises (i.e., circular normal) distribution. The test's power is compared to a commonly used test of randomness that ignores when a peak is likely to occur. Methods for dealing with interval censored data are discussed, and an example is given. A Bayesian alternative is also explored."], ["Tests for Seasonal Moving Average Unit Root in ARIMA Models", null], ["Testing and Locating Variance Changepoints with Application to Stock Prices", "This article explores testing and locating multiple variance changepoints in a sequence of independent Gaussian random variables (assuming known and common mean). This type of problem is very common in applied economics and finance. A binary procedure combined with the Schwarz information criterion (SIC) is used to search all of the possible variance changepoints existing in the sequence. The simulated power of the proposed procedure is compared to that of the CUSUM procedure used by Incl\u00e1n and Tiao to cope with variance changepoints. The SIC and unbiased SIC for this problem are derived. To obtain the percentage points of the SIC criterion, the asymptotic null distribution of a function of the SIC is obtained, and then the approximate percentage points of the SIC are tabulated. Finally, the results are applied to the weekly stock prices. The unknown but common mean case is also outlined at the end."], ["Prediction Intervals for Artificial Neural Networks", "The artificial neural network (ANN) is becoming a very popular model for engineering and scientific applications. Inspired by brain architecture, artificial neural networks represent a class of nonlinear models capable of learning from data. Neural networks have been applied in many areas, including pattern matching, classification, prediction, and process control. This article focuses on the construction of prediction intervals. Previous statistical theory for constructing confidence intervals for the parameters (or the weights in an ANN), is inappropriate, because the parameters are unidentifiable. We show in this article that the problem disappears in prediction. We then construct asymptotically valid prediction intervals and also show how to use the prediction intervals to choose the number of nodes in the network. We then apply the theory to an example for predicting the electrical load."], ["The Discrimination Subspace Model", null], ["Mean and Covariance Structure Analysis: Theoretical and Practical Improvements", "The most widely used multivariate statistical models in the social and behavioral sciences involve linear structural relations among observed and latent variables. In practice, these variables are generally nonnormally distributed; hence classical multivariate analysis, based on multinomial error-free variables having no simultaneous interrelations, is not adequate to deal with such data. A promising alternative, based on asymptotically distribution-free (ADF) covariance structure analysis, has been found to be virtually useless in practical model evaluation at finite sample sizes with nonnormal data. We take a new look at the basic statistical theory of structural models under arbitrary distributions, using the methodology of nonlinear regression and generalized least squares estimation. For example, we adopt the use of residual weight matrices from regression theory. We develop a series of estimators and tests based on arbitrary distribution theory. We obtain a type of probabilistic Bartlett correction for various test statistics that can be simply applied in practice. A small simulation study replicates the extremely inadequate performance of one of our own and the original ADF model tests. In contrast, our corrected statistics have approximately correct means at all sample sizes, though their variances tend to be too low at the smallest sample sizes, leading to some \u201coveracceptance\u201d of the true model."], ["A Random-Effects Model for Multiple Characteristics with Possibly Missing Data", "The use of random-effects models for the analysis of longitudinal data with missing responses has been discussed by several authors. This article extends the random-effects model for a single characteristic to the case of multiple characteristics, allowing for arbitrary patterns of observed data. Two different structures for the covariance matrix of measurement error are considered: uncorrelated error between responses and correlation of error terms at the same measurement times. Parameters for this model are estimated via the EM algorithm. The set of equations for this estimation procedure is derived; these equations are appropriately modified to deal with missing data. The methodology is illustrated with an example from clinical trials."], ["Variance Estimation for the Regression Estimator in Two-Phase Sampling", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Correction", null], ["Editorial Board Page", "This article has no abstract"], ["A Voyage of Discovery", null], ["Latent Variable Models for Teratogenesis Using Multiple Binary Outcomes", "Multiple outcomes are commonly measured in the study of birth defects. The reason is that most teratogens do not cause a single, uniquely defined defect, but rather result in a range of effects, including major malformations, minor anomalies, and deficiencies in birth weight, length and head circumference. The spectrum of effects associated with a particular teratogen is sometimes described as a \u201csyndrome.\u201d In this article we develop a latent variable model to characterize exposure effects on multiple binary outcomes. Not only does the method allow comparisons of control and exposed infants with respect to multiple outcomes, but it also provides a measure of the \u201cseverity\u201d of each child's condition. Data from a study of the teratogenic effects of anticonvulsants illustrate our results."], ["A Regression Method for Spatial Disease Rates: An Estimating Function Approach", "Epidemiologists commonly study the geographical variation of disease rates to generate and refine testable hypotheses regarding etiology. Poisson regression is often used to estimate the parameters that characterize the effects of risk factors on disease. Hierarchical models have been proposed to utilize spatial locations and neighbors as surrogates for unknown or unmeasured risk factors in the analysis of disease rates. Although hierarchical models are useful in modeling spatial disease rates in a scientifically meaningful way, the analytic tools for them are generally computationally intensive. We overcome this issue by applying a conditional spatial modeling technique using unbiased estimating functions. The resulting estimator of regression coefficients is consistent and asymptotically normally distributed under mild conditions. A simulation study compares the performance of the proposed scheme with the approximate inference methods for hierarchical models. We apply the proposed scheme to Scottish lip cancer incidence data to illustrate its use in practice."], ["Order-Restricted Bayesian Estimation of the Age Composition of a Population of Atlantic Cod", null], ["Determining the Interdependence of Historical Astronomical Tables", null], ["Fully Bayesian Reconstructions from Single-Photon Emission Computed Tomography Data", "With suitably chosen priors, Bayesian models are useful in image reconstruction. I consider reconstructions from single-photon emission computerized tomography data using a Gibbs pairwise difference prior. A fully Bayesian approach is presented where the prior parameters are considered drawn from hyperpriors. The approach is problematical, because the normalization constant in the prior is an intractable function of its parameters. Here the constant is estimated off-line by reverse logistic regression. Markov chain Monte Carlo methods are used on simulated and real data to gain estimates of the posterior mean and pixelwise credibility bounds. The resulting reconstructions are compared to those obtained by filtered back-projection, maximum likelihood/EM, and the one-step-late solution to the fixed parameter posterior mode."], ["Spatial Smoothing of Geographically Aggregated Data, with Application to the Construction of Incidence Maps", "We address the commonly encountered situation in spatial statistics where data such as counts of incidences of a certain disease are available only in geographically aggregated form. We develop fairly general models and propose a modified version of the locally weighted least squares method to recover the unknown smooth spatial function that is assumed to generate the observations. In the special case of count data, the target function is the intensity function, conditional on the total number of observations. Our method avoids the arbitrariness of selecting a point within each geographic area at which the measurement for the whole area is supposed to be located. We derive basic asymptotic properties, and apply our methods to acquired immune deficiency syndrome (AIDS) incidence data in San Francisco for 1980\u20131992, where counts are available aggregated over zip code areas."], ["An Accelerated-Time Model for Response Curves", "We propose a nonparametric method of data analysis for situations where the data consist of a sample of curves and a covariate is present. We assume the observed curves to be time-accelerated versions of a basic underlying stochastic process and assume the time acceleration factor for each observed process, determining the \u201ceigenzeit,\u201d to be a smooth function of the covariate. We discuss an iterative procedure to estimate the components of the model nonparametrically, using cross-validation to determine smoothing parameters using a leave-one-curve-out technique. Our example concerns time courses of mortality for a sample of cohorts of fruit flies."], ["On the Estimation of a Convex Set from Noisy Data on its Support Function", "In problems of medical imaging and robotic vision, measurements of a convex set are sometimes obtained via the set's support function. The standard way in which the convex set is recovered from such data is to suppose that it is polygonal, model the errors as Normal random variables, and apply constrained maximum likelihood methods. Typically, the number of sides assumed of the polygon is equal to or a little less than the number of data points. However, from a statistical viewpoint, the number of sides should really be interpreted as a smoothing parameter and chosen to optimize some measure of performance. Additionally, if the true set is not a polygon, then a polygonal estimate can be aesthetically unsatisfactory. In this article we suggest periodic smoothing methods for estimating the convex set."], ["A Region-Based Segmentation Method for Multichannel Image Data", "Segmentation has become a widely used tool in modern image analysis. Typical methods are highly nonlinear and often ad hoc, and none so far have proved accessible to detailed statistical analysis even by asymptotic approximations. Can such methods be any good? This article describes a segmentation procedure for multichannel image data and attempts to develop an understanding of its statistical performance characteristics in terms of nonparametric regression. The procedure is based on a recursive merging algorithm defined via a nested sequence of discretizations of the image domain. A cross-validation rule with a near-neighbor block replacement strategy is proposed for selecting the final segmentation model. Idealized numerical simulation experiments are used to evaluate the rate of convergence of the mean square error of estimation and also to study the efficiency of the cross-validation technique. Interestingly, the results show that the rate of convergence tends to decrease as the degree of smoothness of the underlying image increases. This complements the more familiar estimation characteristic associated with conventional nonparametric regression algorithms. The cross-validation is found to be effective at choosing a segmentation that minimizes the mean squared deviation between the segmented image and the underlying truth. Furthermore, by manipulating the block size in the replacement scheme, it is possible to maintain some robustness to artifacts caused by blurring. Physical phantom datasets taken from positron emission tomography (PET) are used to evaluate the segmentation procedure in a setting of practical interest. Some data from a human brain imaging study with PET are used for further illustration."], ["Hybrid Adaptive Splines", "An adaptive spline method for smoothing is proposed that combines features from both regression spline and smoothing spline approaches. One of its advantages is the ability to vary the amount of smoothing in response to the inhomogeneous \u201ccurvature\u201d of true functions at different locations. This method can be applied to many multivariate function estimation problems, which is illustrated by an application to smoothing temperature data on the globe. The method's performance in a simulation study is found to be comparable to the wavelet shrinkage methods proposed by Donoho and Johnstone. The problem of how to count the degrees of freedom for an adaptively chosen set of basis functions is addressed. This issue arises also in the MARS procedure proposed by Friedman and other adaptive regression spline procedures."], ["Polychotomous Regression", "An automatic procedure that uses linear splines and their tensor products is proposed for fitting a regression model to data involving a polychotomous response variable and one or more predictors. The fitted model can be used for multiple classification. The automatic fitting procedure involves maximum likelihood estimation, stepwise addition, stepwise deletion, and model selection by the Akaike information criterion, cross-validation, or an independent test set. A modified version of the algorithm has been constructed that is applicable to large datasets, and it is illustrated using a phoneme recognition dataset with 250,000 cases, 45 classes, and 63 predictors."], ["The Reduced Monotonic Regression Method", "Medical researchers often desire to categorize patients into monotonic response groups based on the relationship between continuous variables. Isotonic regression fits consist of level sets of increasing value, for which the estimated response is constant. However, the number of level sets obtained is often large, preventing simple description. This article introduces two new nonparametric methods called reduced isotonic regression and reduced monotonic regression, the latter being a two-sided extension of the former for use when the direction of the trend is unknown. Using a backward elimination algorithm, the new procedures reduce the number of level sets by combining those whose values do not differ greatly. For the statistical relations examined here, the reduced monotonic method averaged at most 30% of the number of level sets obtained for isotonic regression. The method is illustrated with an example that examines the relationship between risk factors for survival among children with leukemia. In simulation studies, the reduced monotonic method fits the data as closely as alternative methods that combine isotonicity and smoothing, while improving greatly on isotonic regression. The method is also related to changepoint models of normally distributed sequences."], ["High-Breakdown Linear Discriminant Analysis", "The classification rules of linear discriminant analysis are defined by the true mean vectors and the common covariance matrix of the populations from which the data come. Because these true parameters are generally unknown, they are commonly estimated by the sample mean vector and covariance matrix of the data in a training sample randomly drawn from each population. However, these sample statistics are notoriously susceptible to contamination by outliers, a problem compounded by the fact that the outliers may be invisible to conventional diagnostics. High-breakdown estimation is a procedure designed to remove this cause for concern by producing estimates that are immune to serious distortion by a minority of outliers, regardless of their severity. In this article we motivate and develop a high-breakdown criterion for linear discriminant analysis and give an algorithm for its implementation. The procedure is intended to supplement rather than replace the usual sample-moment methodology of discriminant analysis either by providing indications that the dataset is not seriously affected by outliers (supporting the usual analysis) or by identifying apparently aberrant points and giving resistant estimators that are not affected by them."], ["Monotone Discriminant Functions and Their Applications in Rheumatology", "Some applications of discriminant analysis (e.g., in rheumatology) naturally require that the discriminator satisfies certain monotonicity constraints in terms of the measurements on which the classification is based. This article presents a dynamic programming approach to the problem of finding the monotone function that minimizes the total misclassification cost incurred when classifying two types of cases on the basis of two variables measured on each case. Questions of uniqueness and convexity are explored, and the way in which the solution varies with choice of misclassification costs is investigated. The use of the bootstrap to estimate the accuracy of summary statistics of interest is discussed. The methodology is illustrated using data on rheumatology patients. Some comparisons with linear discriminant functions and classification tree methods are made."], ["Bounded Influence Estimation in the Mixed Linear Model", null], ["Maximum Likelihood Algorithms for Generalized Linear Mixed Models", "Maximum likelihood algorithms are described for generalized linear mixed models. I show how to construct a Monte Carlo version of the EM algorithm, propose a Monte Carlo Newton-Raphson algorithm, and evaluate and improve the use of importance sampling ideas. Calculation of the maximum likelihood estimates is feasible for a wide variety of problems where they were not previously. I also use the Newton-Raphson algorithm as a framework to compare maximum likelihood to the \u201cjoint-maximization\u201d or penalized quasi-likelihood methods and explain why the latter can perform poorly."], ["Inferences for the Linear Errors-in-Variables with Changepoint Models", null], ["Bayesian Model Averaging for Linear Regression Models", null], ["Product Partition Models for Normal Means", "I consider probability models for the estimation of normal means that allow for some of the means to be equal. These probability models, called product partition models, specify prior probabilities for a random partition. The posterior probability of the partition given the observations has the same form. The resulting estimate of the means\u2014the product estimate\u2014is obtained by conditioning on the partition and summing over all possible partitions. The large number of computations involved leads to the use of Markov sampling to compute the product estimate. I compare the product estimate to other estimates of normal means both in a simulation study and in the prediction of baseball batting averages."], ["On the Variability of Case-Deletion Importance Sampling Weights in the Bayesian Linear Model", "I consider a standard specification of the Bayesian linear model and derive necessary and sufficient conditions for the variance of the case-deletion importance sampling weights to be finite. The conditions have an intuitive interpretation in terms of familiar frequentist measures of leverage and influence and are easy to verify. I present two real data examples in which the necessary conditions fail to hold for some observations and the corresponding importance sampling estimates are highly unreliable."], ["Bayesian Analysis of Stochastically Ordered Distributions of Categorical Variables", "This article considers a finite set of discrete distributions all having the same finite support. The problem of interest is to assess the strength of evidence produced by sampled data for a hypothesis of a specified stochastic ordering among the underlying distributions and to estimate these distributions subject to the ordering. We present a Bayesian approach that is an alternative to using the posterior probability of the hypothesis and the Bayes factor in favor of the hypothesis. We develop computational methods for the implementation of Bayesian analyses. We analyze examples to illustrate inferential and computational developments. The methodology used for testing a hypothesis is seen to apply to a wide class of problems in Bayesian inference and has some distinct advantages."], ["Likelihood Ratio-Based Confidence Bands for Survival Functions", "Thomas and Grunkemeier introduced a nonparametric likelihood ratio approach to confidence interval estimation of survival probabilities based on right-censored data. We construct simultaneous confidence bands using this approach. The boundaries of the bands are contained within [0, 1]. A procedure essentially equivalent to a bias correction is developed. The resulting increase in coverage accuracy is illustrated by an example and a simulation study. We look at various versions of log-likelihood ratio-based confidence bands and compare them to the Hall-Wellner band and Nair's equal precision band. We also construct likelihood ratio-based bands for cumulative hazard functions."], ["Predicting Survival Probabilities with Semiparametric Transformation Models", "Prediction of survival probabilities for future patients is one of the main goals of fitting survival data with regression models. In this article we consider a large class of semiparametric transformation models, which includes the well-known proportional hazards and proportional odds models, for the analysis of failure time data. Specifically, we propose pointwise and simultaneous confidence interval procedures for the survival probability of future patients with specific covariates. These procedures can be easily implemented through simulation and are illustrated with the data from two well-known clinical studies."], ["Semiparametric Likelihood Ratio-Based Inferences for Truncated Data", "In astronomic, demographic, epidemiologic, and other studies, the variable of interest, say the survival time, is often truncated by an associated variable. In many situations, the distribution of the truncation variable can be described by a parametric form. Unlike in the standard right-censorship model in which the censoring distribution is noninformative, knowledge of the truncation distribution can be used to improve estimation of the survival distribution. This article derives likelihood ratio-based confidence intervals for survival probabilities and for the truncation proportion under the two models in which the truncation distribution is assumed either to be known or to belong to a parametric family. Our proposed methods enable one to incorporate both the information contained in the data and the available information on the truncation distribution and thus are expected to have better performance than fully nonparametric methods. Our approach also has applications to some biased sampling problems. A simulation study is done to assess the small-sample performance of the proposed methods and to compare it with some existing nonparametric methods. An illustration is also given using a transfusion-related acquired immune deficiency syndrome (AIDS) data."], ["Stochastic Network Models for Survival Analysis", "We present methodology giving highly accurate approximations for Bayesian predictive densities and distribution functions of first passage times between states of a semi-Markov process with a finite number of states. When the states describe a degenerative disorder with an absorbing end state, such predictive distributions are the survival distributions of a patient. We illustrate these methods with a variety of examples, including data from the San Francisco AIDS study. We achieve our approximations using a three-step sequence. First, we introduce advanced concepts of flowgraph theory, which allow us to compute the moment generating function of the first passage time given the model parameters. Next, we use saddlepoint approximations to convert this into a density or distribution function conditional on the model parameter. Finally, we use Monte Carlo methods to remove dependence on the model parameter. These methods apply quite generally to all finite-state semi-Markov models in discrete or continuous time. Currently, there are no competing alternative methods that can achieve the saddlepoint accuracy of these computations."], ["Nonparametric Hypotheses and Rank Statistics for Unbalanced Factorial Designs", "Factorial designs are studied with independent observations, fixed number of levels, and possibly unequal number of observations per factor level combination. In this context, the nonparametric null hypotheses introduced by Akritas and Arnold are considered. New rank statistics are derived for testing the nonparametric hypotheses of no main effects, no interaction, and no factor effects in unbalanced crossed classifications. The formulation of all results includes tied observations. Extensions of these procedures to higher-way layouts are given, and the efficacies of the test statistics against nonparametric alternatives are derived. A modification of the test statistics and approximations to their finite-sample distributions are also given. The small-sample performance of the procedures for two factors is examined in a simulation study. As an illustration, a real dataset with ordinal data is analyzed."], [null, null], ["Bootstrap-Adjusted Calibration Confidence Intervals for Immunoassay", "In immunoassay, a nonlinear heteroscedastic regression model is used to characterize assay concentration-response, and the model fitted to data from standard samples is used to calibrate unknown test samples. Usual large-sample methods to construct individual confidence intervals for calibrated concentrations have been observed in empirical studies to be seriously inaccurate in terms of achieving the nominal level of coverage. We show theoretically that this inaccuracy is due largely to estimation of parameters characterizing assay response variance. By exploiting the theory, we propose a bootstrap procedure to adjust the usual intervals to achieve a higher degree of accuracy. We provide both theoretical results and simulation evidence to show that the proposed method attains the nominal level. A practical advantage of the procedure is that it may be implemented reliably using far fewer bootstrap samples than are needed in other resampling schemes."], ["Optimum Partition Procedures for Separating Good and Bad Treatments", null], ["Multiple Testing of General Contrasts Using Logical Constraints and Correlations", null], ["Confidence Subset Containing the Unknown Peaks of an Umbrella Ordering", "Umbrella ordering has received considerable attention in the statistical literature during the past 15 years. However, there is still no solution to the important problem of estimating the unknown peaks of an umbrella ordering. This article discusses confidence estimation of the unknown peaks. A random subset of all of the treatments that contains the unknown peaks of an umbrella ordering with any prespecified confidence level is considered. The proposed order-restricted confidence subset theory explicitly utilizes the order information and has several advantages for umbrella orderings over the usual subset selection theory. The utility and general applicability of the proposed method is shown by its applications in various experimental designs, assuming normal or other location-scale distributions with known or unknown variances. Examples based on real data are provided."], ["A Model for Repeated Measurements of a Multivariate Binary Response", "This article presents a logit model for a vector of binary variables observed for each subject under multiple conditions. The model contains a vector of random effects to account for correlations among the repeated measurements. A nonparametric treatment of the random effects implies a multivariate log-linear model having quasi-symmetric structure for the cross-classification of responses at the various conditions. The fit yields estimates of within-subject effects comparing the conditions for each variable. The estimates are identical to conditional maximum likelihood estimates for a fixed-effects version of the logit model. Extensions incorporate independent groups or allow variables to have multiple response categories."], ["A New Approach in Modeling a Categorical Response. I. Binary Case", "A new approach in modeling categorical response variables is presented. This approach is based on characterizing symmetrical and unimodal distributions with respect to the biological mechanisms that they describe. This characterization is achieved through the notion of the intrinsic odds ratio. Two nonparametric classes of distributions are introduced, and discrimination among symmetric unimodal distributions is demonstrated. Two extended models for binary response variables\u2014increasing and decreasing odds-ratio models\u2014are presented."], ["Robust Estimation for Grouped Data", "Here we investigate the robustness properties of the class of minimum power divergence estimators for grouped data. This class contains the classical maximum likelihood estimators for grouped data. We find that the bias of these estimators due to deviations from the assumed underlying model can be large. Therefore, we propose a more general class of estimators that allows us to construct robust procedures. By analogy with Hampel's theorem, we define optimal bounded influence function estimators, and by a simulation study, we show that under small model contaminations, these estimators are more stable than the classical estimators for grouped data. Finally, we apply our results to a particular real example."], ["Another Look at the Salamander Mating Data: A Modified Laplace Approximation Approach", "The salamander mating dataset of McCullagh and Nelder has drawn the attention of statisticians because of its challenging structure: the design is crossed rather than nested. The standard Laplace approximation used to evaluate marginal likelihood functions fails in this case because the dimension of the integral is the square root of the sample size. A modification has been proposed by Shun and McCullagh. This article reanalyzes the salamander data using this modified Laplace approximation and focuses on the asymptotic order and computation of the score function."], ["Inference of Vector Autoregressive Models with Cointegration and Scalar Components", "For the partially nonstationary vector autoregressive model of Ann and Reinsel, I further assume that the first differenced series has scalar components of lower order and study estimation of these models along with asymptotic properties of the estimators. It is shown that Gaussian reduced rank estimation can be easily carried out by simple modification of the Ahn and Reinsel's method. The asymptotic distribution for the estimator of the nonstationary parameter is a locally asymptotically mixed normal, and for that of the stationary parameter is asymptotically a normal. Testing hypothesis of the assumed structure of scalar components, including serial correlation common feature, is briefly discussed. A numerical example is provided to illustrate the methods."], ["Impulse Response Functions Based on a Causal Approach to Residual Orthogonalization in Vector Autoregressions", null], ["Aligning Estimates for Common Variables in Two or More Sample Surveys", "In practice, for many sample surveys the estimates of several population totals are based on one set of weights, which reproduces the known population totals of auxiliary variables. Such a set can always be obtained by using the general regression estimator. If some variables, not necessarily with known population totals, are jointly collected in two sample surveys, then it may be desirable that the weights of both surveys produce the same estimates for the unknown population totals of the common variables. In this article we propose adjusting the general regression estimator to meet this consistency requirement by considering the common variables as additional auxiliary variables. It turns out that the adjusted general regression estimator generalizes Zieschang's method."], ["On the Determination and Construction of Optimal Block Designs in the Presence of Linear Trends", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"]]}