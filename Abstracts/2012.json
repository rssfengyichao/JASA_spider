{"2012": [["A Bayesian Model of NMR Spectra for the Deconvolution and Quantification of Metabolites in Complex Biological Mixtures", "Nuclear magnetic resonance (NMR) spectra are widely used in metabolomics to obtain profiles of metabolites dissolved in biofluids such as cell supernatants. Methods for estimating metabolite concentrations from these spectra are presently confined to manual peak fitting and to binning procedures for integrating resonance peaks. Extensive information on the patterns of spectral resonance generated by human metabolites is now available in online databases. By incorporating this information into a Bayesian model, we can deconvolve resonance peaks from a spectrum and obtain explicit concentration estimates for the corresponding metabolites. Spectral resonances that cannot be deconvolved in this way may also be of scientific interest; so, we model them jointly using wavelets. We describe a Markov chain Monte Carlo algorithm that allows us to sample from the joint posterior distribution of the model parameters, using specifically designed block updates to improve mixing. The strong prior on resonance patterns allows the algorithm to identify peaks corresponding to particular metabolites automatically, eliminating the need for manual peak assignment. We assess our method for peak alignment and concentration estimation. Except in cases when the target resonance signal is very weak, alignment is unbiased and precise. We compare the Bayesian concentration estimates with those obtained from a conventional numerical integration method and find that our point estimates have six-fold lower mean squared error. Finally, we apply our method to a spectral dataset taken from an investigation of the metabolic response of yeast to recombinant protein expression. We estimate the concentrations of 26 metabolites and compare with manual quantification by five expert spectroscopists. We discuss the reason for discrepancies and the robustness of our method's concentration estimates. This article has supplementary materials online."], ["A Nonparametric Regression Model With Tree-Structured Response", "Developments in science and technology over the last two decades has motivated the study of complex data objects. In this article, we consider the topological properties of a population of tree-structured objects. Our interest centers on modeling the relationship between a tree-structured response and other covariates. For tree-structured objects, this poses serious challenges since most regression methods rely on linear operations in Euclidean space. We generalize the notion of nonparametric regression to the case of a tree-structured response variable. In addition, we develop a fast algorithm and give its theoretical justification. We implement the proposed method to analyze a dataset of human brain artery trees. An important lesson is that smoothing in the full tree space can reveal much deeper scientific insights than the simple smoothing of summary statistics. This article has supplementary materials online."], ["Direct Simultaneous Inference in Additive Models and Its Application to Model Undernutrition", null], ["Functional Causal Mediation Analysis With an\u00a0Application to Brain Connectivity", "Mediation analysis is often used in the behavioral sciences to investigate the role of intermediate variables that lie on the causal path between a randomized treatment and an outcome variable. Typically, mediation is assessed using structural equation models (SEMs), with model coefficients interpreted as causal effects. In this article, we present an extension of SEMs to the functional data analysis (FDA) setting that allows the mediating variable to be a continuous function rather than a single scalar measure, thus providing the opportunity to study the functional effects of the mediator on the outcome. We provide sufficient conditions for identifying the average causal effects of the functional mediators using the extended SEM, as well as weaker conditions under which an instrumental variable estimand may be interpreted as an effect. The method is applied to data from a functional magnetic resonance imaging (fMRI) study of thermal pain that sought to determine whether activation in certain brain regions mediated the effect of applied temperature on self-reported pain. Our approach provides valuable information about the timing of the mediating effect that is not readily available when using the standard nonfunctional approach. To the best of our knowledge, this work provides the first application of causal inference to the FDA framework."], ["Measurement Error Case Series Models With Application to Infection-Cardiovascular Risk in Older Patients on Dialysis", "Infection and cardiovascular disease are leading causes of hospitalization and death in older patients on dialysis. Our recent work found an increase in the relative incidence of cardiovascular outcomes during the \u223c 30 days after infection-related hospitalizations using the case series model, which adjusts for measured and unmeasured baseline confounders. However, a major challenge in modeling/assessing the infection-cardiovascular risk hypothesis is that the exact time of infection, or more generally \u201cexposure,\u201d onsets cannot be ascertained based on hospitalization data. Only imprecise markers of the timing of infection onsets are available. Although there is a large literature on measurement error in the predictors in regression modeling, to date, there is no work on measurement error on the timing of a time-varying exposure to our knowledge. Thus, we propose a new method, the measurement error case series (MECS) models, to account for measurement error in time-varying exposure onsets. We characterized the general nature of bias resulting from estimation that ignores measurement error and proposed a bias-corrected estimation for the MECS models. We examined in detail the accuracy of the proposed method to estimate the relative incidence of cardiovascular events. Hospitalization data from the United States Renal Data System, which captures nearly all (>99%) patients with end-stage renal disease in the United States over time, are used to illustrate the proposed method. The results suggest that the estimate of the relative incidence of cardiovascular events during the 30\u00a0days after infections, a period where acute effects of infection on vascular endothelium may be most pronounced, is substantially attenuated in the presence of infection onset measurement error."], ["Nonparametric Estimation for Censored Mixture Data With Application to the Cooperative Huntington\u2019s Observational Research Trial", "This work presents methods for estimating genotype-specific outcome distributions from genetic epidemiology studies where the event times are subject to right censoring, the genotypes are not directly observed, and the data arise from a mixture of scientifically meaningful subpopulations. Examples of such studies include kin-cohort studies and quantitative trait locus (QTL) studies. Current methods for analyzing censored mixture data include two types of nonparametric maximum likelihood estimators (NPMLEs; Type I and Type II) that do not make parametric assumptions on the genotype-specific density functions. Although both NPMLEs are commonly used, we show that one is inefficient and the other inconsistent. To overcome these deficiencies, we propose three classes of consistent nonparametric estimators that do not assume parametric density models and are easy to implement. They are based on inverse probability weighting (IPW), augmented IPW (AIPW), and nonparametric imputation (IMP). AIPW achieves the efficiency bound without additional modeling assumptions. Extensive simulation experiments demonstrate satisfactory performance of these estimators even when the data are heavily censored. We apply these estimators to the Cooperative Huntington\u2019s Observational Research Trial (COHORT), and provide age-specific estimates of the effect of mutation in the Huntington gene on mortality using a sample of family members. The close approximation of the estimated noncarrier survival rates to that of the U.S. population indicates small ascertainment bias in the COHORT family sample. Our analyses underscore an elevated risk of death in Huntington gene mutation carriers compared with that in noncarriers for a wide age range, and suggest that the mutation equally affects survival rates in both genders. The estimated survival rates are useful in genetic counseling for providing guidelines on interpreting the risk of death associated with a positive genetic test, and in helping future subjects at risk to make informed decisions on whether to undergo genetic mutation testing. Technical details and additional numerical results are provided in the online supplementary materials."], ["A Stochastic Model for Calibrating the Survival Benefit of Screen-Detected Cancers", "Comparison of the survival of clinically detected and screen-detected cancer cases from either population-based service screening programs or opportunistic screening is often distorted by both lead-time and length biases. Both are correlated with each other and are also affected by measurement errors and tumor attributes such as regional lymph node spread. We propose a general stochastic approach to calibrate the survival benefit of screen-detected cancers related to both biases, measurement errors, and tumor attributes. We apply our proposed method to breast cancer screening data from one arm of the Swedish Two-County trial in the trial period together with the subsequent service screening for the same cohort. When there is no calibration, the results\u2014assuming a constant (exponentially distributed) post-lead-time hazard rate (i.e., a homogeneous stochastic process)\u2014show a 57% reduction in breast cancer death over 25\u00a0years. After correction, the reduction was 30%, with approximately 12% of the overestimation being due to lead-time bias and 15% due to length bias. The additional impacts of measurement errors (sensitivity and specificity) depend on the type of the proposed model and follow-up time. The corresponding analysis when the Weibull distribution was applied\u2014relaxing the assumption of a constant hazard rate\u2014yielded similar findings and lacked statistical significance compared with the exponential model. The proposed calibration approach allows the benefit of a service cancer screening program to be fairly evaluated. This article has supplementary materials online."], ["Using Mixed Integer Programming for Matching in an Observational Study of Kidney Failure After Surgery", null], ["Modeling Protein Expression and Protein Signaling Pathways", "High-throughput functional proteomic technologies provide a way to quantify the expression of proteins of interest. Statistical inference centers on identifying the activation state of proteins and their patterns of molecular interaction formalized as dependence structure. Inference on dependence structure is particularly important when proteins are selected because they are part of a common molecular pathway. In that case, inference on dependence structure reveals properties of the underlying pathway. We propose a probability model that represents molecular interactions at the level of hidden binary latent variables that can be interpreted as indicators for active versus inactive states of the proteins. The proposed approach exploits available expert knowledge about the target pathway to define an informative prior on the hidden conditional dependence structure. An important feature of this prior is that it provides an instrument to explicitly anchor the model space to a set of interactions of interest, favoring a local search approach to model determination. We apply our model to reverse-phase protein array data from a study on acute myeloid leukemia. Our inference identifies relevant subpathways in relation to the unfolding of the biological process under study."], ["Estimating Identification Disclosure Risk Using Mixed Membership Models", "Statistical agencies and other organizations that disseminate data are obligated to protect data subjects\u2019 confidentiality. For example, ill-intentioned individuals might link data subjects to records in other databases by matching on common characteristics (keys). Successful links are particularly problematic for data subjects with combinations of keys that are unique in the population. Hence, as part of their assessments of disclosure risks, many data stewards estimate the probabilities that sample uniques on sets of discrete keys are also population uniques on those keys. This is typically done using log-linear modeling on the keys. However, log-linear models can yield biased estimates of cell probabilities for sparse contingency tables with many zero counts, which often occurs in databases with many keys. This bias can result in unreliable estimates of probabilities of uniqueness and, hence, misrepresentations of disclosure risks. We propose an alternative to log-linear models for datasets with sparse keys based on a Bayesian version of grade of membership (GoM) models. We present a Bayesian GoM model for multinomial variables and offer a Markov chain Monte Carlo algorithm for fitting the model. We evaluate the approach by treating data from a recent U.S. Census Bureau public use microdata sample as a population, taking simple random samples from that population, and benchmarking estimated probabilities of uniqueness against population values. Compared to log-linear models, GoM models provide more accurate estimates of the total number of uniques in the samples. Additionally, they offer record-level predictions of uniqueness that dominate those based on log-linear models. This article has online supplementary materials."], ["Minkowski\u2013Weyl Priors for Models With Parameter Constraints: An Analysis of the BioCycle Study", null], ["Tracking Epidemics With Google Flu Trends Data and a State-Space SEIR Model", "In this article, we use Google Flu Trends data together with a sequential surveillance model based on state-space methodology to track the evolution of an epidemic process over time. We embed a classical mathematical epidemiology model [a susceptible-exposed-infected-recovered (SEIR) model] within the state-space framework, thereby extending the SEIR dynamics to allow changes through time. The implementation of this model is based on a particle filtering algorithm, which learns about the epidemic process sequentially through time and provides updated estimated odds of a pandemic with each new surveillance data point. We show how our approach, in combination with sequential Bayes factors, can serve as an online diagnostic tool for influenza pandemic. We take a close look at the Google Flu Trends data describing the spread of flu in the United States during 2003\u20132009 and in nine separate U.S. states chosen to represent a wide range of health care and emergency system strengths and weaknesses. This article has online supplementary materials."], ["Modeling Criminal Careers as Departures From a Unimodal Population Age\u2013Crime Curve: The Case of Marijuana Use", "A major aim of longitudinal analyses of life-course data is to describe the within- and between-individual variability in a behavioral outcome, such as crime. Statistical analyses of such data typically draw on mixture and mixed-effects growth models. In this work, we present a functional analytic point of view and develop an alternative method that models individual crime trajectories as departures from a population age\u2013crime curve. Drawing on empirical and theoretical claims in criminology, we assume a unimodal population age\u2013crime curve and allow individual expected crime trajectories to differ by their levels of offending and patterns of temporal misalignment. We extend Bayesian hierarchical curve registration methods to accommodate count data and to incorporate influence of baseline covariates on individual behavioral trajectories. Analyzing self-reported counts of yearly marijuana use from the Denver Youth Survey, we examine the influence of race and gender categories on differences in levels and timing of marijuana smoking. We find that our approach offers a flexible model for longitudinal crime trajectories and allows for a rich array of inferences of interest to criminologists and drug abuse researchers. This article has supplementary materials online."], ["Testing for Gene\u2013Environment and Gene\u2013Gene Interactions Under Monotonicity Constraints", null], ["Estimation of High Conditional Quantiles for Heavy-Tailed Distributions", "Estimation of conditional quantiles at very high or low tails is of interest in numerous applications. Quantile regression provides a convenient and natural way of quantifying the impact of covariates at different quantiles of a response distribution. However, high tails are often associated with data sparsity, so quantile regression estimation can suffer from high variability at tails especially for heavy-tailed distributions. In this article, we develop new estimation methods for high conditional quantiles by first estimating the intermediate conditional quantiles in a conventional quantile regression framework and then extrapolating these estimates to the high tails based on reasonable assumptions on tail behaviors. We establish the asymptotic properties of the proposed estimators and demonstrate through simulation studies that the proposed methods enjoy higher accuracy than the conventional quantile regression estimates. In a real application involving statistical downscaling of daily precipitation in the Chicago area, the proposed methods provide more stable results quantifying the chance of heavy precipitation in the area. Supplementary materials for this article are available online."], ["SURE Estimates for a Heteroscedastic Hierarchical Model", null], [null, null], ["Landmark Prediction of Long-Term Survival Incorporating Short-Term Event Time Information", "In recent years, a wide range of markers have become available as potential tools to predict risk or progression of disease. In addition to such biological and genetic markers, short-term outcome information may be useful in predicting long-term disease outcomes. When such information is available, it would be desirable to combine this along with predictive markers to improve the prediction of long-term survival. Most existing methods for incorporating censored short-term event information in predicting long-term survival focus on modeling the disease process and are derived under restrictive parametric models in a multistate survival setting. When such model assumptions fail to hold, the resulting prediction of long-term outcomes may be invalid or inaccurate. When there is only a single discrete baseline covariate, a fully nonparametric estimation procedure to incorporate short-term event time information has been previously proposed. However, such an approach is not feasible for settings with one or more continuous covariates due to the curse of dimensionality. In this article, we propose to incorporate short-term event time information along with multiple covariates collected up to a landmark point via a flexible varying-coefficient model. To evaluate and compare the prediction performance of the resulting landmark prediction rule, we use robust nonparametric procedures that do not require the correct specification of the proposed varying-coefficient model. Simulation studies suggest that the proposed procedures perform well in finite samples. We illustrate them here using a dataset of postdialysis patients with end-stage renal disease."], ["Interactions in the Analysis of Variance", null], ["Correct Ordering in the Zipf\u2013Poisson Ensemble", null], ["Spike-and-Slab Priors for Function Selection in Structured Additive Regression Models", "Structured additive regression (STAR) provides a general framework for complex Gaussian and non-Gaussian regression models, with predictors comprising arbitrary combinations of nonlinear functions and surfaces, spatial effects, varying coefficients, random effects, and further regression terms. The large flexibility of STAR makes function selection a challenging and important task, aiming at (1) selecting the relevant covariates, (2) choosing an appropriate and parsimonious representation of the impact of covariates on the predictor, and (3) determining the required interactions. We propose a spike-and-slab prior structure for function selection that allows to include or exclude single coefficients as well as blocks of coefficients representing specific model terms. A novel multiplicative parameter expansion is required to obtain good mixing and convergence properties in a Markov chain Monte Carlo simulation approach and is shown to induce desirable shrinkage properties. In simulation studies and with (real) benchmark classification data, we investigate sensitivity to hyperparameter settings and compare performance to competitors. The flexibility and applicability of our approach are demonstrated in an additive piecewise exponential model with time-varying effects for right-censored survival times of intensive care patients with sepsis. Geoadditive and additive mixed logit model applications are discussed in an extensive online supplement."], ["Sparse Reduced-Rank Regression for Simultaneous Dimension Reduction and Variable Selection", "The reduced-rank regression is an effective method in predicting multiple response variables from the same set of predictor variables. It reduces the number of model parameters and takes advantage of interrelations between the response variables and hence improves predictive accuracy. We propose to select relevant variables for reduced-rank regression by using a sparsity-inducing penalty. We apply a group-lasso type penalty that treats each row of the matrix of the regression coefficients as a group and show that this penalty satisfies certain desirable invariance properties. We develop two numerical algorithms to solve the penalized regression problem and establish the asymptotic consistency of the proposed method. In particular, the manifold structure of the reduced-rank regression coefficient matrix is considered and studied in our theoretical analysis. In our simulation study and real data analysis, the new method is compared with several existing variable selection methods for multivariate regression and exhibits competitive performance in prediction and variable selection."], ["Higher-Order Infinitesimal Robustness", null], ["A Multiresolution Method for Parameter Estimation of Diffusion Processes", "Diffusion process models are widely used in science, engineering, and finance. Most diffusion processes are described by stochastic differential equations in continuous time. In practice, however, data are typically observed only at discrete time points. Except for a few very special cases, no analytic form exists for the likelihood of such discretely observed data. For this reason, parametric inference is often achieved by using discrete-time approximations, with accuracy controlled through the introduction of missing data. We present a new multiresolution Bayesian framework to address the inference difficulty. The methodology relies on the use of multiple approximations and extrapolation and is significantly faster and more accurate than known strategies based on Gibbs sampling. We apply the multiresolution approach to three data-driven inference problems, one of which features a multivariate diffusion model with an entirely unobserved component."], ["AdaptSPEC: Adaptive Spectral Estimation for Nonstationary Time Series", "We propose a method for analyzing possibly nonstationary time series by adaptively dividing the time series into an unknown but finite number of segments and estimating the corresponding local spectra by smoothing splines. The model is formulated in a Bayesian framework, and the estimation relies on reversible jump Markov chain Monte Carlo (RJMCMC) methods. For a given segmentation of the time series, the likelihood function is approximated via a product of local Whittle likelihoods. Thus, no parametric assumption is made about the process underlying the time series. The number and lengths of the segments are assumed unknown and may change from one MCMC iteration to another. The frequentist properties of the method are investigated by simulation, and applications to electroencephalogram and the El Ni\u00f1o Southern Oscillation phenomenon are described in detail."], ["Optimal Detection of Changepoints With a Linear Computational Cost", "In this article, we consider the problem of detecting multiple changepoints in large datasets. Our focus is on applications where the number of changepoints will increase as we collect more data: for example, in genetics as we analyze larger regions of the genome, or in finance as we observe time series over longer periods. We consider the common approach of detecting changepoints through minimizing a cost function over possible numbers and locations of changepoints. This includes several established procedures for detecting changing points, such as penalized likelihood and minimum description length. We introduce a new method for finding the minimum of such cost functions and hence the optimal number and location of changepoints that has a computational cost, which, under mild conditions, is linear in the number of observations. This compares favorably with existing methods for the same problem whose computational cost can be quadratic or even cubic. In simulation studies, we show that our new method can be orders of magnitude faster than these alternative exact methods. We also compare with the binary segmentation algorithm for identifying changepoints, showing that the exactness of our approach can lead to substantial improvements in the accuracy of the inferred segmentation of the data. This article has supplementary materials available online."], ["Modeling Repeated Functional Observations", "We introduce a new methodological framework for repeatedly observed and thus dependent functional data, aiming at situations where curves are recorded repeatedly for each subject in a sample. Our methodology covers the case where the recordings of the curves are scheduled on a regular and dense grid and also situations more typical for longitudinal studies, where the timing of recordings is often sparse and random. The proposed models lead to an interpretable and straightforward decomposition of the inherent variation in repeatedly observed functional data and are implemented through a straightforward two-step functional principal component analysis. We provide consistency results and asymptotic convergence rates for the estimated model components. We compare the proposed model with an alternative approach via a two-dimensional Karhunen-Lo\u00e8ve expansion and illustrate it through the analysis of longitudinal mortality data from period lifetables that are repeatedly observed for a sample of countries over many years, and also through simulation studies. This article has online supplementary materials."], ["Consistent High-Dimensional Bayesian Variable Selection via Penalized Credible Regions", "For high-dimensional data, particularly when the number of predictors greatly exceeds the sample size, selection of relevant predictors for regression is a challenging problem. Methods such as sure screening, forward selection, or penalized regressions are commonly used. Bayesian variable selection methods place prior distributions on the parameters along with a prior over model space, or equivalently, a mixture prior on the parameters having mass at zero. Since exhaustive enumeration is not feasible, posterior model probabilities are often obtained via long Markov chain Monte Carlo (MCMC) runs. The chosen model can depend heavily on various choices for priors and also posterior thresholds. Alternatively, we propose a conjugate prior only on the full model parameters and use sparse solutions within posterior credible regions to perform selection. These posterior credible regions often have closed-form representations, and it is shown that these sparse solutions can be computed via existing algorithms. The approach is shown to outperform common methods in the high-dimensional setting, particularly under correlation. By searching for a sparse solution within a joint credible region, consistent model selection is established. Furthermore, it is shown that, under certain conditions, the use of marginal credible intervals can give consistent selection up to the case where the dimension grows exponentially in the sample size. The proposed approach successfully accomplishes variable selection in the high-dimensional setting, while avoiding pitfalls that plague typical Bayesian variable selection methods."], ["A Semiparametric Change-Point Regression Model for Longitudinal Observations", "Many longitudinal studies involve relating an outcome process to a set of possibly time-varying covariates, giving rise to the usual regression models for longitudinal data. When the purpose of the study is to investigate the covariate effects when experimental environment undergoes abrupt changes or to locate the periods with different levels of covariate effects, a simple and easy-to-interpret approach is to introduce change-points in regression coefficients. In this connection, we propose a semiparametric change-point regression model, in which the error process (stochastic component) is nonparametric and the baseline mean function (functional part) is completely unspecified, the observation times are allowed to be subject specific, and the number, locations, and magnitudes of change-points are unknown and need to be estimated. We further develop an estimation procedure that combines the recent advance in semiparametric analysis based on counting process argument and multiple change-points inference and discuss its large sample properties, including consistency and asymptotic normality, under suitable regularity conditions. Simulation results show that the proposed methods work well under a variety of scenarios. An application to a real dataset is also given."], ["Instrumental Variable Estimators for Binary Outcomes", "Instrumental variables (IVs) can be used to construct estimators of exposure effects on the outcomes of studies affected by nonignorable selection of the exposure. Estimators that fail to adjust for the effects of nonignorable selection will be biased and inconsistent. Such situations commonly arise in observational studies, but are also a problem for randomized experiments affected by nonignorable noncompliance. In this article, we review IV estimators for studies in which the outcome is binary, and consider the links between different approaches developed in the statistics and econometrics literatures. The implicit assumptions made by each method are highlighted and compared within our framework. We illustrate our findings through the reanalysis of a randomized placebo-controlled trial, and highlight important directions for future work in this area."], ["Book Reviews", null], ["Erratum", null], ["Editorial Collaborators", null], ["Editorial Board EOV", null], ["Bayesian Spatio-Dynamic Modeling in Cell Motility Studies: Learning Nonlinear Taxic Fields Guiding the Immune Response", "We develop and analyze models of the spatio-temporal organization of lymphocytes in the lymph nodes and spleen. The spatial dynamics of these immune system white blood cells are influenced by biochemical fields and represent key components of the overall immune response to vaccines and infections. A primary goal is to learn about the structure of these fields that fundamentally shape the immune response. We define dynamic models of single-cell motion involving nonparametric representations of scalar potential fields underlying the directional biochemical fields that guide cellular motion. Bayesian hierarchical extensions define multicellular models for aggregating models and data on colonies of cells. Analysis via customized Markov chain Monte Carlo methods leads to Bayesian inference on cell-specific and population parameters together with the underlying spatial fields. Our case study explores data from multiphoton intravital microscopy in lymph nodes of mice, and we use a number of visualization tools to summarize and compare posterior inferences on the three-dimensional taxic fields."], ["Comment: Cell Motility Models and Inference for Dynamic Systems", null], ["Comment", null], ["Comment on the Mechanistic Modeling and Inference for Cell Motility by Manolopoulou et al.", null], ["Rejoinder", null], ["Multilevel Bayesian Models for Survival Times and Longitudinal Patient-Reported Outcomes With Many Zeros", "Regulatory approval of new therapies often depends on demonstrating prolonged survival. Particularly when these survival benefits are modest, consideration of therapeutic benefits to patient-reported outcomes (PROs) may add value to the traditional biomedical clinical trial endpoints. We extend a popular class of joint models for longitudinal and survival data to accommodate the excessive zeros common in PROs, building hierarchical Bayesian models that combine information from longitudinal PRO measurements and survival outcomes. The model development is motivated by a clinical trial for malignant pleural mesothelioma, a rapidly fatal form of pulmonary cancer usually associated with asbestos exposure. By separately modeling the presence and severity of PROs, using our zero-augmented beta (ZAB) likelihood, we are able to model PROs on their original scale and learn about individual-level parameters from both presence and severity of symptoms. Correlations among an individual's PROs and survival are modeled using latent random variables, adjusting the fitted trajectories to better accommodate the observed data for each individual. This work contributes to understanding the impact of treatment on two aspects of mesothelioma: patients\u2019 subjective experience of the disease process and their progression-free survival times. We uncover important differences between outcome types that are associated with therapy (periodic, worse in both treatment groups after therapy initiation) and those that are responsive to treatment (aperiodic, gradually widening gap between treatment groups). Finally, our work raises questions for future investigation into multivariate modeling, choice of link functions, and the relative contributions of multiple data sources in joint modeling contexts."], ["Structural Nested Cumulative Failure Time Models to Estimate the Effects of Interventions", null], ["Contrasting Evidence Within and Between Institutions That Provide Treatment in an Observational Study of Alternate Forms of Anesthesia", "In a randomized trial, subjects are assigned to treatment or control by the flip of a fair coin. In many nonrandomized or observational studies, subjects find their way to treatment or control in two steps, either or both of which may lead to biased comparisons. By a vague process, perhaps affected by proximity or sociodemographic issues, subjects find their way to institutions that provide treatment. Once at such an institution, a second process, perhaps thoughtful and deliberate, assigns individuals to treatment or control. In the current article, the institutions are hospitals, and the treatment under study is the use of general anesthesia alone versus some use of regional anesthesia during surgery. For a specific operation, the use of regional anesthesia may be typical in one hospital and atypical in another. A new matched design is proposed for studies of this sort, one that creates two types of nonoverlapping matched pairs. Using a new extension of optimal matching with fine balance, pairs of the first type exactly balance treatment assignment across institutions, so each institution appears in the treated group with the same frequency that it appears in the control group; hence, differences between institutions that affect everyone in the same way cannot bias this comparison. Pairs of the second type compare institutions that assign most subjects to treatment and other institutions that assign most subjects to control, so each institution is represented in the treated group if it typically assigns subjects to treatment or, alternatively, in the control group if it typically assigns subjects to control, and no institution appears in both groups. By and large, in the second type of matched pair, subjects became treated subjects or controls by choosing an institution, not by a thoughtful and deliberate process of selecting subjects for treatment within institutions. The design provides two evidence factors, that is, two tests of the null hypothesis of no treatment effect that are independent when the null hypothesis is true, where each factor is largely unaffected by certain unmeasured biases that could readily invalidate the other factor. The two factors permit separate and combined sensitivity analyses, where the magnitude of bias affecting the two factors may differ. The case of knee surgery in the study of regional versus general anesthesia is considered in detail."], ["A Multiscale Community Blockmodel for Network Exploration", "Real-world networks exhibit a complex set of phenomena such as underlying hierarchical organization, multiscale interaction, and varying topologies of communities. Most existing methods do not adequately capture the intrinsic interplay among such phenomena. We propose a nonparametric multiscale community blockmodel (MSCB) to model the generation of hierarchies in social communities, selective membership of actors to subsets of these communities, and the resultant networks due to within- and cross-community interactions. By using the nested Chinese restaurant process, our model automatically infers the hierarchy structure from the data. We develop a collapsed Gibbs sampling algorithm for posterior inference, conduct extensive validation using synthetic networks, and demonstrate the utility of our model in real-world datasets, such as predator\u2013prey networks and citation networks."], ["Searching for Alternative Splicing With a Joint Model on Probe Measurability and Expression Intensities", null], ["Composite Partial Likelihood Estimation Under Length-Biased Sampling, With Application to a Prevalent Cohort Study of Dementia", null], ["Identifying the Effects of SNAP (Food Stamps) on Child Health Outcomes When Participation Is Endogenous and Misreported", "The literature assessing the efficacy of the Supplemental Nutrition Assistance Program (SNAP), formerly known as the Food Stamp Program, has long puzzled over positive associations between SNAP receipt and various undesirable health outcomes such as food insecurity. Assessing the causal impacts of SNAP, however, is hampered by two key identification problems: endogenous selection into participation and extensive systematic underreporting of participation status. Using data from the National Health and Nutrition Examination Survey (NHANES), we extend partial identification bounding methods to account for these two identification problems in a single unifying framework. Specifically, we derive informative bounds on the average treatment effect (ATE) of SNAP on child food insecurity, poor general health, obesity, and anemia across a range of different assumptions used to address the selection and classification error problems. In particular, to address the selection problem, we apply relatively weak nonparametric assumptions on the latent outcomes, selected treatments, and observed covariates. To address the classification error problem, we formalize a new approach that uses auxiliary administrative data on the size of the SNAP caseload to restrict the magnitudes and patterns of SNAP reporting errors. Layering successively stronger assumptions, an objective of our analysis is to make transparent how the strength of the conclusions varies with the strength of the identifying assumptions. Under the weakest restrictions, there is substantial ambiguity; we cannot rule out the possibility that SNAP increases or decreases poor health. Under stronger but plausible assumptions used to address the selection and classification error problems, we find that commonly cited relationships between SNAP and poor health outcomes provide a misleading picture about the true impacts of the program. Our tightest bounds identify favorable impacts of SNAP on child health."], ["Modeling of Multivariate Monotone Disease Processes in the Presence of Misclassification", "Motivated by a longitudinal oral health study, the Signal\u2013Tandmobiel\u00ae study, we propose a multivariate binary inhomogeneous Markov model in which unobserved correlated response variables are subject to an unconstrained misclassification process and have a monotone behavior. The multivariate baseline distributions and Markov transition matrices of the unobserved processes are defined as a function of covariates through the specification of compatible full conditional distributions. Distinct misclassification models are discussed. In all cases, the possibility that different examiners were involved in the scoring of the responses of a given subject across time is taken into account. A full Bayesian implementation of the model is described and its performance is evaluated using simulated data. We provide theoretical and empirical evidence that the parameters can be estimated without any external information about the misclassification parameters. Finally, the analyses of the motivating study are presented. Appendices 1\u20137 are available in the online supplementary materials."], ["A Class of Discrete Transformation Survival Models With Application to Default Probability Prediction", "Corporate bankruptcy prediction plays a central role in academic finance research, business practice, and government regulation. Consequently, accurate default probability prediction is extremely important. We propose to apply a discrete transformation family of survival models to corporate default risk predictions. A class of Box-Cox transformations and logarithmic transformations is naturally adopted. The proposed transformation model family is shown to include the popular Shumway model and the grouped relative risk model. We show that a transformation parameter different from those two models is needed for default prediction using a bankruptcy dataset. In addition, we show using out-of-sample validation statistics that our model improves performance. We use the estimated default probability to examine a popular asset pricing question and determine whether default risk has carried a premium. Due to some distinct features of the bankruptcy application, the proposed class of discrete transformation survival models with time-varying covariates is different from the continuous survival models in the survival analysis literature. Their similarities and differences are discussed."], ["Spatial Statistical Data Fusion for Remote Sensing Applications", "In this article, we predict the true aerosol process from two noisy and possibly biased datasets, and we also estimate the uncertainties of these estimates. Our data-fusion methodology scales linearly and bears some resemblance to Fixed Rank Kriging (FRK), a variant of kriging that is designed for spatial interpolation of a single, massive dataset. Our spatial statistical approach does not require assumptions of stationarity or isotropy and, crucially, allows for change of spatial support. We compare our methodology to FRK and Bayesian melding, and we show that ours has superior prediction standard errors compared to FRK and much faster computational speed compared to Bayesian melding."], ["Estimating False Discovery Proportion Under Arbitrary Covariance Dependence", "Multiple hypothesis testing is a fundamental problem in high-dimensional inference, with wide applications in many scientific fields. In genome-wide association studies, tens of thousands of tests are performed simultaneously to find if any single-nucleotide polymorphisms (SNPs) are associated with some traits and those tests are correlated. When test statistics are correlated, false discovery control becomes very challenging under arbitrary dependence. In this article, we propose a novel method\u2014based on principal factor approximation\u2014that successfully subtracts the common dependence and weakens significantly the correlation structure, to deal with an arbitrary dependence structure. We derive an approximate expression for false discovery proportion (FDP) in large-scale multiple testing when a common threshold is used and provide a consistent estimate of realized FDP. This result has important applications in controlling false discovery rate and FDP. Our estimate of realized FDP compares favorably with Efron's approach, as demonstrated in the simulated examples. Our approach is further illustrated by some real data applications. We also propose a dependence-adjusted procedure that is more powerful than the fixed-threshold procedure. Supplementary material for this article is available online."], ["Comment", null], ["Comment: Robustness to Assumption of Normally Distributed Errors", null], ["Comment: FDP vs FDR and the Effect of Conditioning", null], ["Comment", null], ["Rejoinder", null], ["Likelihood-Based EWMA Charts for Monitoring Poisson Count Data With Time-Varying Sample Sizes", "Many applications involve monitoring incidence rates of the Poisson distribution when the sample size varies over time. Recently, a couple of cumulative sum and exponentially weighted moving average (EWMA) control charts have been proposed to tackle this problem by taking the varying sample size into consideration. However, we argue that some of these charts, which perform quite well in terms of average run length (ARL), may not be appealing in practice because they have rather unsatisfactory run length distributions. With some charts, the specified in-control (IC) ARL is attained with elevated probabilities of very short and very long runs, as compared with a geometric distribution. This is reflected in a larger run length standard deviation than that of a geometric distribution and an elevated probability of false alarms with short runs, which, in turn, hurt an operator's confidence in valid alarms. Furthermore, with many charts, the IC ARL exhibits considerable variations with different patterns of sample sizes. Under the framework of weighted likelihood ratio test, this article suggests a new EWMA control chart which automatically integrates the varying sample sizes with the EWMA scheme. It is fast to compute, easy to construct, and quite efficient in detecting changes of Poisson rates. Two important features of the proposed method are that the IC run length distribution is similar to that of a geometric distribution and the IC ARL is robust to various patterns of sample size variation. Our simulation results show that the proposed chart is generally more effective and robust compared with existing EWMA charts. A health surveillance example based on mortality data from New Mexico is used to illustrate the implementation of the proposed method. This article has online supplementary materials."], ["Pair Copula Constructions for Multivariate Discrete Data", null], ["The Hybrid Wild Bootstrap for Time Series", "We introduce a new and simple bootstrap procedure for general linear processes, called the hybrid wild bootstrap. The hybrid wild bootstrap generates frequency domain replicates of the periodogram that imitate asymptotically correct the first- and second-order properties of the ordinary periodogram including its weak dependence structure at different frequencies. As a consequence, the hybrid wild bootstrapped periodogram succeeds in approximating consistently the distribution of statistics that can be expressed as functionals of the periodogram, including the important class of spectral means for which all so far existing frequency domain bootstrap methods generally fail. Moreover, by inverting the hybrid wild bootstrapped discrete Fourier transform, pseudo-observations in the time domain are obtained. The generated time domain pseudo-observations can be used to approximate correctly the random behavior of statistics, the distribution of which depends on the first-, second-, and, to some extent, on the fourth-order structure of the underlying linear process. Thus, the proposed hybrid wild bootstrap procedure applied to general time series overcomes several of the limitations of standard linear time domain bootstrap methods."], ["Nonparametric Construction of Multivariate Kernels", null], ["Inference on the Order of a Normal Mixture", null], ["Estimating Individualized Treatment Rules Using Outcome Weighted Learning", "There is increasing interest in discovering individualized treatment rules (ITRs) for patients who have heterogeneous responses to treatment. In particular, one aims to find an optimal ITR that is a deterministic function of patient-specific characteristics maximizing expected clinical outcome. In this article, we first show that estimating such an optimal treatment rule is equivalent to a classification problem where each subject is weighted proportional to his or her clinical outcome. We then propose an outcome weighted learning approach based on the support vector machine framework. We show that the resulting estimator of the treatment rule is consistent. We further obtain a finite sample bound for the difference between the expected outcome using the estimated ITR and that of the optimal treatment rule. The performance of the proposed approach is demonstrated via simulation studies and an analysis of chronic depression data."], ["A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs", "We present a method to estimate block membership of nodes in a random graph generated by a stochastic blockmodel. We use an embedding procedure motivated by the random dot product graph model, a particular example of the latent position model. The embedding associates each node with a vector; these vectors are clustered via minimization of a square error criterion. We prove that this method is consistent for assigning nodes to blocks, as only a negligible number of nodes will be misassigned. We prove consistency of the method for directed and undirected graphs. The consistent block assignment makes possible consistent parameter estimation for a stochastic blockmodel. We extend the result in the setting where the number of blocks grows slowly with the number of nodes. Our method is also computationally feasible even for very large graphs. We compare our method with Laplacian spectral clustering through analysis of simulated data and a graph derived from Wikipedia documents."], ["Feature Screening via Distance Correlation Learning", "This article is concerned with screening features in ultrahigh-dimensional data analysis, which has become increasingly important in diverse scientific fields. We develop a sure independence screening procedure based on the distance correlation (DC-SIS). The DC-SIS can be implemented as easily as the sure independence screening (SIS) procedure based on the Pearson correlation proposed by Fan and Lv. However, the DC-SIS can significantly improve the SIS. Fan and Lv established the sure screening property for the SIS based on linear models, but the sure screening property is valid for the DC-SIS under more general settings, including linear models. Furthermore, the implementation of the DC-SIS does not require model specification (e.g., linear model or generalized linear model) for responses or predictors. This is a very appealing property in ultrahigh-dimensional data analysis. Moreover, the DC-SIS can be used directly to screen grouped predictor variables and multivariate response variables. We establish the sure screening property for the DC-SIS, and conduct simulations to examine its finite sample performance. A numerical comparison indicates that the DC-SIS performs much better than the SIS in various models. We also illustrate the DC-SIS through a real-data example."], ["Optimal Designs for Quantile Regression Models", null], ["Statistical Modeling of Curves Using Shapes and Related Features", null], ["Deconvolution When Classifying Noisy Data Involving Transformations", "In the present study, we consider the problem of classifying spatial data distorted by a linear transformation or convolution and contaminated by additive random noise. In this setting, we show that classifier performance can be improved if we carefully invert the data before the classifier is applied. However, the inverse transformation is not constructed so as to recover the original signal, and in fact, we show that taking the latter approach is generally inadvisable. We introduce a fully data-driven procedure based on cross-validation, and use several classifiers to illustrate numerical properties of our approach. Theoretical arguments are given in support of our claims. Our procedure is applied to data generated by light detection and ranging (Lidar) technology, where we improve on earlier approaches to classifying aerosols. This article has supplementary materials online."], ["Robust Estimation of Multivariate Location and Scatter in the Presence of Missing Data", null], ["Sparse Matrix Graphical Models", "Matrix-variate observations are frequently encountered in many contemporary statistical problems due to a rising need to organize and analyze data with structured information. In this article, we propose a novel sparse matrix graphical model for these types of statistical problems. By penalizing, respectively, two precision matrices corresponding to the rows and columns, our method yields a sparse matrix graphical model that synthetically characterizes the underlying conditional independence structure. Our model is more parsimonious and is practically more interpretable than the conventional sparse vector-variate graphical models. Asymptotic analysis shows that our penalized likelihood estimates enjoy better convergent rates than that of the vector-variate graphical model. The finite sample performance of the proposed method is illustrated via extensive simulation studies and several real datasets analysis."], ["Minimax and Adaptive Prediction for Functional Linear Regression", "This article considers minimax and adaptive prediction with functional predictors in the framework of functional linear model and reproducing kernel Hilbert space. Minimax rate of convergence for the excess prediction risk is established. It is shown that the optimal rate is determined jointly by the reproducing kernel and the covariance kernel. In particular, the alignment of these two kernels can significantly affect the difficulty of the prediction problem. In contrast, the existing literature has so far focused only on the setting where the two kernels are nearly perfectly aligned. This motivates us to propose an easily implementable data-driven roughness regularization predictor that is shown to attain the optimal rate of convergence adaptively without the need of knowing the covariance kernel. Simulation studies are carried out to illustrate the merits of the adaptive predictor and to demonstrate the theoretical results."], ["Mode Identification of Volatility in Time-Varying Autoregression", "In many applications, time series exhibit nonstationary behavior that might reasonably be modeled as a time-varying autoregressive (AR) process. In the context of such a model, we discuss the problem of testing for modality of the variance function. We propose a test of modality that is local and, when used iteratively, can be used to identify the total number of modes in a given series. This problem is closely related to peak detection and identification, which has applications in many fields. We propose a test that, under appropriate assumptions, is asymptotically distribution free under the null hypothesis, even though nonparametric estimation of the AR parameter functions is involved. Simulation studies and applications to real datasets illustrate the behavior of the test."], ["Mixed Effects Designs: The Symmetry Assumption and Missing Data", null], ["Generalized Measures of Correlation for Asymmetry, Nonlinearity, and Beyond", "Applicability of Pearson's correlation as a measure of explained variance is by now well understood. One of its limitations is that it does not account for asymmetry in explained variance. Aiming to develop broad applicable correlation measures, we study a pair of generalized measures of correlation (GMC) that deals with asymmetries in explained variances, and linear or nonlinear relations between random variables. We present examples under which the paired measures are identical, and they become a symmetric correlation measure that is the same as the squared Pearson's correlation coefficient. As a result, Pearson's correlation is a special case of GMC. Theoretical properties of GMC show that GMC can be applicable in numerous applications and can lead to more meaningful conclusions and improved decision making. In statistical inference, the joint asymptotics of the kernel-based estimators for GMC are derived and are used to test whether or not two random variables are symmetric in explaining variances. The testing results give important guidance in practical model selection problems. The efficiency of the test statistics is illustrated in simulation examples. In real-data analysis, we present an important application of GMC in explained variances and market movements among three important economic and financial monetary indicators. This article has online supplementary materials."], ["Book Reviews", null], ["Correction", null], ["Erratum", null], ["Nonparametric Bayesian Multiple Imputation for Missing Data Due to Mid-Study Switching of Measurement Methods", null], ["Evaluating the Effect of Training on Wages in the Presence of Noncompliance, Nonemployment, and Missing Outcome Data", "The effects of a job training program, Job Corps, on both employment and wages are evaluated using data from a randomized study. Principal stratification is used to address, simultaneously, the complications of noncompliance, wages that are only partially defined because of nonemployment, and unintended missing outcomes. The first two complications are of substantive interest, whereas the third is a nuisance. The objective is to find a parsimonious model that can be used to inform public policy. We conduct a likelihood-based analysis using finite mixture models estimated by the expectation-maximization (EM) algorithm. We maintain an exclusion restriction assumption for the effect of assignment on employment and wages for noncompliers, but not on missingness. We provide estimates under the \u201cmissing at random\u201d assumption, and assess the robustness of our results to deviations from it. The plausibility of meaningful restrictions is investigated by means of scaled log-likelihood ratio statistics. Substantive conclusions include the following. For compliers, the effect on employment is negative in the short term; it becomes positive in the long term, but these effects are small at best. For always employed compliers, that is, compliers who are employed whether trained or not trained, positive effects on wages are found at all time periods. Our analysis reveals that background characteristics of individuals differ markedly across the principal strata. We found evidence that the program should have been better targeted, in the sense of being designed differently for different groups of people, and specific suggestions are offered. Previous analyses of this dataset, which did not address all complications in a principled manner, led to less nuanced conclusions about Job Corps."], ["Application of Branching Models in the Study of Invasive Species", null], ["Topological Analysis of Variance and the Maxillary Complex", "It is common to reduce the dimensionality of data before applying classical multivariate analysis techniques in statistics. Persistent homology, a recent development in computational topology, has been shown to be useful for analyzing high-dimensional (nonlinear) data. In this article, we connect computational topology with the traditional analysis of variance and demonstrate the value of combining these approaches on a three-dimensional orthodontic landmark dataset derived from the maxillary complex. Indeed, combining appropriate techniques of both persistent homology and analysis of variance results in a better understanding of the data\u2019s nonlinear features over and above what could have been achieved by classical means. Supplementary material for this article is available online."], ["Evaluation of Viable Dynamic Treatment Regimes in a Sequentially Randomized Trial of Advanced Prostate Cancer", "We present new statistical analyses of data arising from a clinical trial designed to compare two-stage dynamic treatment regimes (DTRs) for advanced prostate cancer. The trial protocol mandated that patients be initially randomized among four chemotherapies, and that those who responded poorly be re-randomized to one of the remaining candidate therapies. The primary aim was to compare the DTRs\u2019 overall success rates, with success defined by the occurrence of successful responses in each of two consecutive courses of the patient\u2019s therapy. Of the 150 study participants, 47 did not complete their therapy as per the algorithm. However, 35 of them did so for reasons that precluded further chemotherapy, that is, toxicity and/or progressive disease. Consequently, rather than comparing the overall success rates of the DTRs in the unrealistic event that these patients had remained on their assigned chemotherapies, we conducted an analysis that compared viable switch rules defined by the per-protocol rules but with the additional provision that patients who developed toxicity or progressive disease switch to a non-prespecified therapeutic or palliative strategy. This modification involved consideration of bivariate per-course outcomes encoding both efficacy and toxicity. We used numerical scores elicited from the trial\u2019s principal investigator to quantify the clinical desirability of each bivariate per-course outcome, and defined one endpoint as their average over all courses of treatment. Two other simpler sets of scores as well as log survival time were also used as endpoints. Estimation of each DTR-specific mean score was conducted using inverse probability weighted methods that assumed that missingness in the 12 remaining dropouts was informative but explainable in that it only depended on past recorded data. We conducted additional worst- and best-case analyses to evaluate sensitivity of our findings to extreme departures from the explainable dropout assumption."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Does Marriage Boost Men\u2019s Wages?: Identification of Treatment Effects in Fixed Effects Regression Models for Panel Data", "Social scientists have generated a large and inconclusive literature on the effect(s) of marriage on men\u2019s wages. Researchers have hypothesized that the wage premium enjoyed by married men may reflect both a tendency for more productive men to marry and an effect of marriage on productivity. To sort out these explanations, researchers have used fixed effects regression models for panel data to adjust for selection on unobserved time-invariant confounders, interpreting coefficients for the time-varying marriage variables as effects. However, they did not define these effects or give conditions under which the regression coefficients would warrant a causal interpretation. Consequently, they failed to appropriately adjust for important time-varying confounders and misinterpreted their results. Regression models for panel data with unobserved time-invariant confounders are also widely used in many other policy-relevant contexts and the same problems arise there. This article draws on recent statistical work on causal inference with longitudinal data to clarify these problems and help researchers use appropriate methods to model their data. A basic set of treatment effects is defined and used to define derived effects. Causal models for panel data with unobserved time-invariant confounders are defined and the treatment effects are reexpressed in terms of these models. Ignorability conditions under which the parameters of the causal models are identified from the regression models are given. Even when these hold, a number of interesting and important treatment effects are typically not identified."], ["Inference With Interference Between Units in an fMRI Experiment of Motor Inhibition", "An experimental unit is an opportunity to randomly apply or withhold a treatment. There is interference between units if the application of the treatment to one unit may also affect other units. In cognitive neuroscience, a common form of experiment presents a sequence of stimuli or requests for cognitive activity at random to each experimental subject and measures biological aspects of brain activity that follow these requests. Each subject is then many experimental units, and interference between units within an experimental subject is, likely, in part because the stimuli follow one another quickly and in part because human subjects learn or become experienced or primed or bored as the experiment proceeds. We use a recent functional magnetic resonance imaging (fMRI) experiment concerned with the inhibition of motor activity to illustrate and further develop recently proposed methodology for inference in the presence of interference. A simulation evaluates the power of competing procedures."], ["Evaluating the Effect of Early Versus Late ARV Regimen Change if Failure on an Initial Regimen: Results From the AIDS Clinical Trials Group Study A5095", "In causal inference, a large class of doubly robust estimators are derived through semiparametric theory with applications to missing data problems. This class of estimators is motivated through geometric arguments and relies on large samples for good performance. By now, several authors have noted that a doubly robust estimator may be suboptimal when the outcome model is misspecified even if it is semiparametric efficient when the outcome regression model is correctly specified. Through auxiliary variables, two-stage designs, and within the contextual backdrop of our scientific problem and clinical study, we propose improved doubly robust, locally efficient estimators of a population mean and average causal effect for early versus delayed switching to second-line ARV treatment regimens. Our analysis of the ACTG A5095 data further demonstrates how methods that use auxiliary variables can improve over methods that ignore them. Using the methods developed here, we conclude that patients who switch within 8 weeks of virologic failure have better clinical outcomes, on average, than patients who delay switching to a new second-line ARV regimen after failing on the initial regimen. Ordinary statistical methods fail to find such differences. This article has online supplementary material."], ["Meta-Analysis of Rare Binary Adverse Event Data", "We examine the use of fixed-effects and random-effects moment-based meta-analytic methods for analysis of binary adverse-event data. Special attention is paid to the case of rare adverse events that are commonly encountered in routine practice. We study estimation of model parameters and between-study heterogeneity. In addition, we examine traditional approaches to hypothesis testing of the average treatment effect and detection of the heterogeneity of treatment effect across studies. We derive three new methods, a simple (unweighted) average treatment effect estimator, a new heterogeneity estimator, and a parametric bootstrapping test for heterogeneity. We then study the statistical properties of both the traditional and the new methods via simulation. We find that in general, moment-based estimators of combined treatment effects and heterogeneity are biased and the degree of bias is proportional to the rarity of the event under study. The new methods eliminate much, but not all, of this bias. The various estimators and hypothesis testing methods are then compared and contrasted using an example dataset on treatment of stable coronary artery disease."], ["Spatio-Spectral Mixed-Effects Model for Functional Magnetic Resonance Imaging Data", null], ["Clustering, Spatial Correlations, and Randomization Inference", "It is a standard practice in regression analyses to allow for clustering in the error covariance matrix if the explanatory variable of interest varies at a more aggregate level (e.g., the state level) than the units of observation (e.g., individuals). Often, however, the structure of the error covariance matrix is more complex, with correlations not vanishing for units in different clusters. Here, we explore the implications of such correlations for the actual and estimated precision of least squares estimators. Our main theoretical result is that with equal-sized clusters, if the covariate of interest is randomly assigned at the cluster level, only accounting for nonzero covariances at the cluster level, and ignoring correlations between clusters as well as differences in within-cluster correlations, leads to valid confidence intervals. However, in the absence of random assignment of the covariates, ignoring general correlation structures may lead to biases in standard errors. We illustrate our findings using the 5% public-use census data. Based on these results, we recommend that researchers, as a matter of routine, explore the extent of spatial correlations in explanatory variables beyond state-level clustering."], ["Vast Portfolio Selection With Gross-Exposure Constraints", "This article introduces the large portfolio selection using gross-exposure constraints. It shows that with gross-exposure constraints, the empirically selected optimal portfolios based on estimated covariance matrices have similar performance to the theoretical optimal ones and there is no error accumulation effect from estimation of vast covariance matrices. This gives theoretical justification to the empirical results by Jagannathan and Ma. It also shows that the no-short-sale portfolio can be improved by allowing some short positions. The applications to portfolio selection, tracking, and improvements are also addressed. The utility of our new approach is illustrated by simulation and empirical studies on the 100 Fama\u2013French industrial portfolios and the 600 stocks randomly selected from Russell 3000."], ["The Variance Profile", null], ["Inverse Realized Laplace Transforms for Nonparametric Volatility Density Estimation in Jump-Diffusions", "This article develops a nonparametric estimator of the stochastic volatility density of a discretely observed It\u00f4 semimartingale in the setting of an increasing time span and finer mesh of the observation grid. There are two basic steps involved. The first step is aggregating the high-frequency increments into the realized Laplace transform, which is a robust nonparametric estimate of the underlying volatility Laplace transform. The second step is using a regularized kernel to invert the realized Laplace transform. These two steps are relatively quick and easy to compute, so the nonparametric estimator is practicable. The article also derives bounds for the mean squared error of the estimator. The regularity conditions are sufficiently general to cover empirically important cases such as level jumps and possible dependencies between volatility moves and either diffusive or jump moves in the semimartingale. The Monte Carlo analysis in this study indicates that the nonparametric estimator is reliable and reasonably accurate in realistic estimation contexts. An empirical application to 5-min data for three large-cap stocks, 1997\u20132010, reveals the importance of big short-term volatility spikes in generating high levels of stock price variability over and above those induced by price jumps. The application also shows how to trace out the dynamic response of the volatility density to both positive and negative jumps in the stock price."], ["Objective Priors for Discrete Parameter Spaces", null], ["Bayesian Model Selection in High-Dimensional Settings", null], ["Moderate-Deviation-Based Inference for Random Degeneration in Paired Rank Lists", null], ["Multiple Testing of Composite Null Hypotheses in Heteroscedastic Models", "In large-scale studies, the true effect sizes often range continuously from zero to small to large, and are observed with heteroscedastic errors. In practical situations where the failure to reject small deviations from the null is inconsequential, specifying an indifference region (or forming composite null hypotheses) can greatly reduce the number of unimportant discoveries in multiple testing. The heteroscedasticity issue poses new challenges for multiple testing with composite nulls. In particular, the conventional framework in multiple testing, which involves rescaling or standardization, is likely to distort the scientific question. We propose the concept of a composite null distribution for heteroscedastic models and develop an optimal testing procedure that minimizes the false nondiscovery rate, subject to a constraint on the false discovery rate. The proposed approach is different from conventional methods in that the effect size, statistical significance, and multiplicity issues are addressed integrally. The external information of heteroscedastic errors is incorporated for optimal simultaneous inference. The new features and advantages of our approach are demonstrated using both simulated and real data. The numerical studies demonstrate that our new procedure enjoys superior performance with greater accuracy and better interpretability of results."], ["Joint Analysis of Longitudinal Data With Informative Observation Times and a Dependent Terminal Event", "In many longitudinal studies, repeated measures are often correlated with observation times. Also, there may exist a dependent terminal event such as death that stops the follow-up. In this article, we propose a new joint model for the analysis of longitudinal data in the presence of both informative observation times and a dependent terminal event via latent variables. Estimating equation approaches are developed for parameter estimation, and the resulting estimators are shown to be consistent and asymptotically normal. In addition, some graphical and numerical procedures are presented for model checking. Simulation studies demonstrate that the proposed method performs well for practical settings. An application to a medical cost study of chronic heart failure patients from the University of Virginia Health System is provided."], ["Informative Estimation and Selection of Correlation Structure for Longitudinal Data", "Identifying an informative correlation structure is important in improving estimation efficiency for longitudinal data. We approximate the empirical estimator of the correlation matrix by groups of known basis matrices that represent different correlation structures, and transform the correlation structure selection problem to a covariate selection problem. To address both the complexity and the informativeness of the correlation matrix, we minimize an objective function that consists of two parts: the difference between the empirical information and a model approximation of the correlation matrix, and a penalty that penalizes models with too many basis matrices. The unique feature of the proposed estimation and selection of correlation structure is that it does not require the specification of the likelihood function, and therefore it is applicable for discrete longitudinal data. We carry out the proposed method through a groupwise penalty strategy, which is able to identify more complex structures. The proposed method possesses the oracle property and selects the true correlation structure consistently. In addition, the estimator of the correlation parameters follows a normal distribution asymptotically. Simulation studies and a data example confirm that the proposed method works effectively in estimating and selecting the true structure in finite samples, and it enables improvement in estimation efficiency by selecting the true structures."], ["Mixture of Regression Models With Varying Mixing Proportions: A Semiparametric Approach", "In this article, we study a class of semiparametric mixtures of regression models, in which the regression functions are linear functions of the predictors, but the mixing proportions are smoothing functions of a covariate. We propose a one-step backfitting estimation procedure to achieve the optimal convergence rates for both regression parameters and the nonparametric functions of mixing proportions. We derive the asymptotic bias and variance of the one-step estimate, and further establish its asymptotic normality. A modified expectation-maximization-type (EM-type) estimation procedure is investigated. We show that the modified EM algorithms preserve the asymptotic ascent property. Numerical simulations are conducted to examine the finite sample performance of the estimation procedures. The proposed methodology is further illustrated via an analysis of a real dataset."], ["Conditional Inference Functions for Mixed-Effects Models With Unspecified Random-Effects Distribution", "In longitudinal studies, mixed-effects models are important for addressing subject-specific effects. However, most existing approaches assume a normal distribution for the random effects, and this could affect the bias and efficiency of the fixed-effects estimator. Even in cases where the estimation of the fixed effects is robust with a misspecified distribution of the random effects, the estimation of the random effects could be invalid. We propose a new approach to estimate fixed and random effects using conditional quadratic inference functions (QIFs). The new approach does not require the specification of likelihood functions or a normality assumption for random effects. It can also accommodate serial correlation between observations within the same cluster, in addition to mixed-effects modeling. Other advantages include not requiring the estimation of the unknown variance components associated with the random effects, or the nuisance parameters associated with the working correlations. We establish asymptotic results for the fixed-effect parameter estimators that do not rely on the consistency of the random-effect estimators. Real data examples and simulations are used to compare the new approach with the penalized quasi-likelihood (PQL) approach, and SAS GLIMMIX and nonlinear mixed-effects model (NLMIXED) procedures. Supplemental materials including technical details are available online."], [null, null], ["A Studentized Permutation Test for the Comparison of Spatial Point Patterns", null], ["Quantile Periodograms", "Two periodogram-like functions, called quantile periodograms, are introduced for spectral analysis of time series. The quantile periodograms are constructed from trigonometric quantile regression and motivated by different interpretations of the ordinary periodogram. Analytical and numerical results demonstrate the capability of the quantile periodograms for detecting hidden periodicity in the quantiles and for providing an additional view of time-series data. A connection between the quantile periodograms and the so-called level-crossing spectrum is established through an asymptotic analysis."], ["New Weighted Portmanteau Statistics for Time Series Goodness of Fit Testing", null], ["The Geometry of Nonparametric Filament Estimation", null], ["Smooth Blockwise Iterative Thresholding: A Smooth Fixed Point Estimator Based on the Likelihood\u2019s Block Gradient", null], ["Structured, Sparse Aggregation", "This article introduces a method for aggregating many least-squares estimators so that the resulting estimate has two properties: sparsity and structure. That is, only a few candidate covariates are used in the resulting model, and the selected covariates follow some structure over the candidate covariates that is assumed to be known a priori. Although sparsity is well studied in many settings, including aggregation, structured sparse methods are still emerging. We demonstrate a general framework for structured sparse aggregation that allows for a wide variety of structures, including overlapping grouped structures and general structural penalties defined as set functions on the set of covariates. We show that such estimators satisfy structured sparse oracle inequalities\u2014their finite sample risk adapts to the structured sparsity of the target. These inequalities reveal that under suitable settings, the structured sparse estimator performs at least as well as, and potentially much better than, a sparse aggregation estimator. We empirically establish the effectiveness of the method using simulation and an application to HIV drug resistance."], ["Probabilistic Foundation of Confirmatory Adaptive Designs", "Adaptive designs allow the investigator of a confirmatory trial to react to unforeseen developments by changing the design. This broad flexibility comes at the price of a complex statistical model where important components, such as the adaptation rule, remain unspecified. It has thus been doubted whether Type I error control can be guaranteed in general adaptive designs. This criticism is fully justified as long as the probabilistic framework on which an adaptive design is based remains vague and implicit. Therefore, an indispensable step lies in the clarification of the probabilistic fundamentals of adaptive testing. We demonstrate that the two main principles of adaptive designs, namely the conditional Type I error rate and the conditional invariance principle, will provide Type I error rate control, if the conditional distribution of the second-stage data, given the first-stage data, can be described in terms of a regression model. A similar assumption is required for regression analysis where the distribution of the covariates is a nuisance parameter and the model needs to be identifiable independently from the covariate distribution. We further show that under the assumption of a regression model, the events of an arbitrary adaptive design can be embedded into a formal probability space without the need of posing any restrictions on the adaptation rule. As a consequence of our results, artificial constraints that had to be imposed on the investigator only for mathematical tractability of the model are no longer necessary."], ["A Martingale Representation for Matching Estimators", "Matching estimators are widely used in statistical data analysis. However, the large sample distribution of matching estimators has been derived only for particular cases. This article establishes a martingale representation for matching estimators. This representation allows the use of martingale limit theorems to derive the large sample distribution of matching estimators. As an illustration of the applicability of the theory, we derive the asymptotic distribution of a matching estimator when matching is carried out without replacement, a result previously unavailable in the literature. In addition, we apply the techniques proposed in this article to derive a correction to the standard error of a sample mean when missing data are imputed using the \u201chot deck,\u201d a matching imputation method widely used in the Current Population Survey (CPS) and other large surveys in the social sciences. We demonstrate the empirical relevance of our methods using two Monte Carlo designs based on actual datasets. In these Monte Carlo exercises, the large sample distribution of matching estimators derived in this article provides an accurate approximation to the small sample behavior of these estimators. In addition, our simulations show that standard errors that do not take into account hot-deck imputation of missing data may be severely downward biased, while standard errors that incorporate the correction for hot-deck imputation perform extremely well. This article has online supplementary materials."], ["Testing for Trend in the Presence of Autoregressive Error: A Comment", null], ["Book Reviews", null], ["Nonparametric Covariate-Adjusted Association Tests Based on the Generalized Kendall's Tau", "Identifying the risk factors for comorbidity is important in psychiatric research. Empirically, studies have shown that testing multiple correlated traits simultaneously is more powerful than testing a single trait at a time in association analysis. Furthermore, for complex diseases, especially mental illnesses and behavioral disorders, the traits are often recorded in different scales, such as dichotomous, ordinal, and quantitative. In the absence of covariates, nonparametric association tests have been developed for multiple complex traits to study comorbidity. However, genetic studies generally contain measurements of some covariates that may affect the relationship between the risk factors of major interest (such as genes) and the outcomes. While it is relatively easy to adjust for these covariates in a parametric model for quantitative traits, it is challenging to adjust for covariates when there are multiple complex traits with possibly different scales. In this article, we propose a nonparametric test for multiple complex traits that can adjust for covariate effects. The test aims to achieve an optimal scheme of adjustment by using a maximum statistic calculated from multiple adjusted test statistics. We derive the asymptotic null distribution of the maximum test statistic and also propose a resampling approach, both of which can be used to assess the significance of our test. Simulations are conducted to compare the Type I error and power of the nonparametric adjusted test to the unadjusted test and other existing adjusted tests. The empirical results suggest that our proposed test increases the power through adjustment for covariates when there exist environmental effects and is more robust to model misspecifications than some existing parametric adjusted tests. We further demonstrate the advantage of our test by analyzing a dataset on genetics of alcoholism."], ["Intrinsic Regression Models for Medial Representation of Subcortical Structures", "The aim of this article is to develop a semiparametric model to describe the variability of the medial representation of subcortical structures, which belongs to a Riemannian manifold, and establish its association with covariates of interest, such as diagnostic status, age, and gender. We develop a two-stage estimation procedure to calculate the parameter estimates. The first stage is to calculate an intrinsic least squares estimator of the parameter vector using the annealing evolutionary stochastic approximation Monte Carlo algorithm, and then the second stage is to construct a set of estimating equations to obtain a more efficient estimate with the intrinsic least squares estimate as the starting point. We use Wald statistics to test linear hypotheses of unknown parameters and establish their limiting distributions. Simulation studies are used to evaluate the accuracy of our parameter estimates and the finite sample performance of the Wald statistics. We apply our methods to the detection of the difference in the morphological changes of the left and right hippocampi between schizophrenia patients and healthy controls using a medial shape description. This article has online supplementary material."], ["Modeling Waves of Extreme Temperature: The Changing Tails of Four Cities", "Heat waves are a serious threat to society, the environment, and the economy. Estimates of the recurrence probabilities of heat waves may be obtained following the successful modeling of daily maximum temperature, but working with the latter is difficult as we have to recognize, and allow for, not only a time trend but also seasonality in the mean and in the variability, as well as serial correlation. Furthermore, as the extreme values of daily maximum temperature have a different form of nonstationarity from the body, additional modeling is required to completely capture the realities. We present a time series model for the daily maximum temperature and use an exceedance over high thresholds approach to model the upper tail of the distribution of its scaled residuals. We show how a change-point analysis can be used\u00a0to identify seasons of constant crossing rates and how a time-dependent shape parameter can then be introduced to capture a change in the distribution of the exceedances. Daily maximum temperature series for Des Moines, New York, Portland, and Tucson are analyzed. In-sample and out-of-sample goodness-of-fit measures show that the proposed model is an excellent fit to the data. The fitted model is then used to estimate the recurrence probabilities of runs over seasonally high temperatures, and we show that the probability of long and intense heat waves has increased considerably over 50 years. We also find that the increases vary by city and by time of year."], ["Adjustment for Missing Confounders Using External Validation Data and Propensity Scores", "Reducing bias from missing confounders is a challenging problem in the analysis of observational data. Information about missing variables is sometimes available from external validation data, such as surveys or secondary samples drawn from the same source population. In principle, the validation data permit us to recover information about the missing data, but the difficulty is in eliciting a valid model for the nuisance distribution of the missing confounders. Motivated by a British study of the effects of trihalomethane exposure on risk of full-term low birthweight, we describe a flexible Bayesian procedure for adjusting for a vector of missing confounders using external validation data. We summarize the missing confounders with a scalar summary score using the propensity score methodology of Rosenbaum and Rubin. The score has the property that it induces conditional independence between the exposure and the missing confounders, given the measured confounders. It balances the unmeasured confounders across exposure groups, within levels of measured covariates. To adjust for bias, we need only model and adjust for the summary score during Markov chain Monte Carlo computation. Simulation results illustrate that the proposed method reduces bias from several missing confounders over a range of different sample sizes for the validation data. Appendices A\u2013C are available as online supplementary material."], ["Partially Hidden Markov Model for Time-Varying Principal Stratification in HIV Prevention Trials", "It is frequently of interest to estimate the intervention effect that adjusts for post-randomization variables in clinical trials. In the recently completed HPTN 035 trial, there is differential condom use between the three microbicide gel arms and the no-gel control arm, so intention-to-treat (ITT) analyses only assess the net treatment effect that includes the indirect treatment effect mediated through differential condom use. Various statistical methods in causal inference have been developed to adjust for post-randomization variables. We extend the principal stratification framework to time-varying behavioral variables in HIV prevention trials with a time-to-event endpoint, using a partially hidden Markov model (pHMM). We formulate the causal estimand of interest, establish assumptions that enable identifiability of the causal parameters, and develop maximum likelihood methods for estimation. Application of our model on the HPTN 035 trial reveals an interesting pattern of prevention effectiveness among different condom-use principal strata."], ["Using Conditional Kernel Density Estimation for Wind Power Density Forecasting", null], ["Bayesian Inference for Dynamic Treatment Regimes: Mobility, Equity, and Efficiency in Student Tracking", null], ["Bayesian Estimation and Prediction for Inhomogeneous Spatiotemporal Log-Gaussian Cox Processes Using Low-Rank Models, With Application to Criminal Surveillance", "In this article, we propose a method for conducting likelihood-based inference for a class of nonstationary spatiotemporal log-Gaussian Cox processes. The method uses convolution-based models to capture spatiotemporal correlation structure, is computationally feasible even for large datasets, and does not require knowledge of the underlying spatial intensity of the process. We describe an application to a surveillance system for detecting emergent spatiotemporal clusters of homicides in Belo Horizonte, Brazil, and discuss the advantages and drawbacks of our model-based approach by comparison with other spatiotemporal surveillance methods that have been proposed in the literature."], ["MRI Tissue Classification Using High-Resolution Bayesian Hidden Markov Normal Mixture Models", "Magnetic resonance imaging (MRI) is used to identify the major tissues within a subject's brain. Classification is usually based on a single image providing one measurement for each volume element, or voxel, in a discretization of the brain. A simple model views each voxel as homogeneous, belonging entirely to one of the three major tissue types: gray matter, white matter, or cerebrospinal fluid. The measurements are normally distributed, with means and variances depending on the tissue types of their voxels. Because nearby voxels tend to be of the same tissue type, a Markov random field model can be used to capture the spatial similarity of voxels. A more realistic model takes into account the fact that some voxels are not homogeneous and contain tissues of more than one type. Our approach to this problem is to construct a higher-resolution image in which each voxel is divided into subvoxels and subvoxels are in turn assumed to be homogeneous and to follow a Markov random field model. In the present work we used a Bayesian hierarchical model to perform MRI tissue classification. Conditional independence was exploited to improve the speed of sampling. The subvoxel approach provides more accurate tissue classification and also allows more effective estimation of the proportion of major tissue types present in each voxel for both simulated and real datasets."], ["Bias-Corrected Hierarchical Bayesian Classification With a Selected Subset of High-Dimensional Features", null], ["Cross-Dimensional Inference of Dependent High-Dimensional Data", "A growing number of modern scientific problems in areas such as genomics, neurobiology, and spatial epidemiology involve the measurement and analysis of thousands of related features that may be stochastically dependent at arbitrarily strong levels. In this work, we consider the scenario where the features follow a multivariate Normal distribution. We demonstrate that dependence is manifested as random variation shared among features, and that standard methods may yield highly unstable inference due to dependence, even when the dependence is fully parameterized and utilized in the procedure. We propose a \u201ccross-dimensional inference\u201d framework that alleviates the problems due to dependence by modeling and removing the variation shared among features, while also properly regularizing estimation across features. We demonstrate the framework on both simultaneous point estimation and multiple hypothesis testing in scenarios derived from the scientific applications of interest."], ["Sparse Estimation of Conditional Graphical Models With Application to Gene Networks", null], ["A Semiparametric Approach to Dimension Reduction", "We provide a novel and completely different approach to dimension-reduction problems from the existing literature. We cast the dimension-reduction problem in a semiparametric estimation framework and derive estimating equations. Viewing this problem from the new angle allows us to derive a rich class of estimators, and obtain the classical dimension reduction techniques as special cases in this class. The semiparametric approach also reveals that in the inverse regression context while keeping the estimation structure intact, the common assumption of linearity and/or constant variance on the covariates can be removed at the cost of performing additional nonparametric regression. The semiparametric estimators without these common assumptions are illustrated through simulation studies and a real data example. This article has online supplementary material."], ["A Valid Mat\u00e9rn Class of Cross-Covariance Functions for Multivariate Random Fields With Any Number of Components", "We introduce a valid parametric family of cross-covariance functions for multivariate spatial random fields where each component has a covariance function from a well-celebrated Mat\u00e9rn class. Unlike previous attempts, our model indeed allows for various smoothnesses and rates of correlation decay for any number of vector components. We present the conditions on the parameter space that result in valid models with varying degrees of complexity. We discuss practical implementations, including reparameterizations to reflect the conditions on the parameter space and an iterative algorithm to increase the computational efficiency. We perform various Monte Carlo simulation experiments to explore the performances of our approach in terms of estimation and cokriging. The application of the proposed multivariate Mat\u00e9rn model is illustrated on two meteorological datasets: temperature/pressure over the Pacific Northwest (bivariate) and wind/temperature/pressure in Oklahoma (trivariate). In the latter case, our flexible trivariate Mat\u00e9rn model is valid and yields better predictive scores compared with a parsimonious model with common scale parameters."], [null, null], ["Information Ratio Test for Model Misspecification in Quasi-Likelihood Inference", "In this article, we focus on the circumstances in quasi-likelihood inference that the estimation accuracy of mean structure parameters is guaranteed by correct specification of the first moment, but the estimation efficiency could be diminished due to misspecification of the second moment. We propose an information ratio (IR) statistic to test for model misspecification of the variance/covariance structure through a comparison between two forms of information matrix: the negative sensitivity matrix and the variability matrix. We establish asymptotic distributions of the proposed IR test statistics. We also suggest an approximation to the asymptotic distribution of the IR statistic via a perturbation resampling method. Moreover, we propose a selection criterion based on the IR test to select the best fitting variance/covariance structure from a class of candidates. Through simulation studies, it is shown that the IR statistic provides a powerful statistical tool to detect different scenarios of misspecification of the variance/covariance structures. In addition, the IR test as well as the proposed model selection procedure shows substantial improvement over some of the existing statistical methods. The IR-based model selection procedure is illustrated by analyzing the Madras Longitudinal Schizophrenia data. Appendices are included in the supplemental materials, which are available online."], ["Quantile Regression for Analyzing Heterogeneity in Ultra-High Dimension", "Ultra-high dimensional data often display heterogeneity due to either heteroscedastic variance or other forms of non-location-scale covariate effects. To accommodate heterogeneity, we advocate a more general interpretation of sparsity, which assumes that only a small number of covariates influence the conditional distribution of the response variable, given all candidate covariates; however, the sets of relevant covariates may differ when we consider different segments of the conditional distribution. In this framework, we investigate the methodology and theory of nonconvex, penalized quantile regression in ultra-high dimension. The proposed approach has two distinctive features: (1) It enables us to explore the entire conditional distribution of the response variable, given the ultra-high-dimensional covariates, and provides a more realistic picture of the sparsity pattern; (2) it requires substantially weaker conditions compared with alternative methods in the literature; thus, it greatly alleviates the difficulty of model checking in the ultra-high dimension. In theoretic development, it is challenging to deal with both the nonsmooth loss function and the nonconvex penalty function in ultra-high-dimensional parameter space. We introduce a novel, sufficient optimality condition that relies on a convex differencing representation of the penalized loss function and the subdifferential calculus. Exploring this optimality condition enables us to establish the oracle property for sparse quantile regression in the ultra-high dimension under relaxed conditions. The proposed method greatly enhances existing tools for ultra-high-dimensional data analysis. Monte Carlo simulations demonstrate the usefulness of the proposed procedure. The real data example we analyzed demonstrates that the new approach reveals substantially more information as compared with alternative methods. This article has online supplementary material."], ["Likelihood-Based Selection and Sharp Parameter Estimation", null], ["Block Bootstraps for Time Series With Fixed Regressors", "This article examines block bootstrap methods in linear regression models with weakly dependent error variables and nonstochastic regressors. Contrary to intuition, the tapered block bootstrap (TBB) with a smooth taper not only loses its superior bias properties but may also fail to be consistent in the regression problem. A similar problem, albeit at a smaller scale, is shown to exist for the moving and the circular block bootstrap (MBB and CBB, respectively). As a remedy, an additional block randomization step is introduced that balances out the effects of nonuniform regression weights, and restores the superiority of the (modified) TBB. The randomization step also improves the MBB or CBB. Interestingly, the stationary bootstrap (SB) automatically balances out regression weights through its probabilistic blocking mechanism, without requiring any modification, and enjoys a kind of robustness. Optimal block sizes are explicitly determined for block bootstrap variance estimators under regression. Finite sample performance and practical uses of the methods are illustrated through a simulation study and two data examples, respectively. Supplementary materials are available online."], ["Semiparametric Double Balancing Score Estimation for Incomplete Data With Ignorable Missingness", null], ["One-Sided and Two-Sided Tolerance Intervals in General Mixed and Random Effects Models Using Small-Sample Asymptotics", "The computation of tolerance intervals in mixed and random effects models has not been satisfactorily addressed in a general setting when the data are unbalanced and/or when covariates are present. This article derives satisfactory one-sided and two-sided tolerance intervals in such a general scenario, by applying small-sample asymptotic procedures. In the case of one-sided tolerance limits, the problem reduces to the interval estimation of a percentile, and accurate confidence limits are derived using small-sample asymptotics. In the case of a two-sided tolerance interval, the problem does not reduce to an interval estimation problem; however, it is possible to derive an approximate margin of error statistic that is an upper confidence limit for a linear combination of the variance components. For the latter problem, small-sample asymptotic procedures can once again be used to arrive at an accurate upper confidence limit. In the article, balanced and unbalanced data situations are treated separately, and computational issues are addressed in detail. Extensive numerical results show that the tolerance intervals derived based on small-sample asymptotics exhibit satisfactory performance regardless of the sample size. The results are illustrated using some examples. Some technical derivations, additional simulation results, and R codes are available online as supplementary materials."], ["Estimating Space and Space-Time Covariance Functions for Large Data Sets: A Weighted Composite Likelihood Approach", "In this article, we propose two methods for estimating space and space-time covariance functions from a Gaussian random field, based on the composite likelihood idea. The first method relies on the maximization of a weighted version of the composite likelihood function, while the second one is based on the solution of a weighted composite score equation. This last scheme is quite general and could be applied to any kind of composite likelihood. An information criterion for model selection based on the first estimation method is also introduced. The methods are useful for practitioners looking for a good balance between computational complexity and statistical efficiency. The effectiveness of the methods is illustrated through examples, simulation experiments, and by analyzing a dataset on ozone measurements."], ["Modeling Nonstationary Processes Through Dimension Expansion", "In this article, we propose a novel approach to modeling nonstationary spatial fields. The proposed method works by expanding the geographic plane over which these processes evolve into higher-dimensional spaces, transforming and clarifying complex patterns in the physical plane. By combining aspects of multidimensional scaling, group lasso, and latent variable models, a dimensionally sparse projection is found in which the originally nonstationary field exhibits stationarity. Following a comparison with existing methods in a simulated environment, dimension expansion is studied on a classic test-bed dataset historically used to study nonstationary models. Following this, we explore the use of dimension expansion in modeling air pollution in the United Kingdom, a process known to be strongly influenced by rural/urban effects, amongst others, which gives rise to a nonstationary field."], ["Estimation of Copula Models With Discrete Margins via Bayesian Data Augmentation", "Estimation of copula models with discrete margins can be difficult beyond the bivariate case. We show how this can be achieved by augmenting the likelihood with continuous latent variables, and computing inference using the resulting augmented posterior. To evaluate this, we propose two efficient Markov chain Monte Carlo sampling schemes. One generates the latent variables as a block using a Metropolis\u2013Hastings step with a proposal that is close to its target distribution, the other generates them one at a time. Our method applies to all parametric copulas where the conditional copula functions can be evaluated, not just elliptical copulas as in much previous work. Moreover, the copula parameters can be estimated joint with any marginal parameters, and Bayesian selection ideas can be employed. We establish the effectiveness of the estimation method by modeling consumer behavior in online retail using Archimedean and Gaussian copulas. The example shows that elliptical copulas can be poor at modeling dependence in discrete data, just as they can be in the continuous case. To demonstrate the potential in higher dimensions, we estimate 16-dimensional D-vine copulas for a longitudinal model of usage of a bicycle path in the city of Melbourne, Australia. The estimates reveal an interesting serial dependence structure that can be represented in a parsimonious fashion using Bayesian selection of independence pair-copula components. Finally, we extend our results and method to the case where some margins are discrete and others continuous. Supplemental materials for the article are also available online."], [null, null], ["Estimating Regression Parameters in an Extended Proportional Odds Model", "The proportional odds model may serve as a useful alternative to the Cox proportional hazards model to study association between covariates and their survival functions in medical studies. In this article, we study an extended proportional odds model that incorporates the so-called \u201cexternal\u201d time-varying covariates. In the extended model, regression parameters have a direct interpretation of comparing survival functions, without specifying the baseline survival odds function. Semiparametric and maximum likelihood estimation procedures are proposed to estimate the extended model. Our methods are demonstrated by Monte Carlo simulations, and applied to a landmark randomized clinical trial of a short-course nevirapine (NVP) for mother-to-child transmission (MTCT) of human immunodeficiency virus type-1 (HIV-1). Additional application includes an analysis of the well-known Veterans Administration (VA) lung cancer trial."], ["Recursively Imputed Survival Trees", "We propose recursively imputed survival tree (RIST) regression for right-censored data. This new nonparametric regression procedure uses a novel recursive imputation approach combined with extremely randomized trees that allows significantly better use of censored data than previous tree-based methods, yielding improved model fit and reduced prediction error. The proposed method can also be viewed as a type of Monte Carlo EM algorithm, which generates extra diversity in the tree-based fitting process. Simulation studies and data analyses demonstrate the superior performance of RIST compared with previous methods."], ["Interim Design Modifications in Time-to-Event Studies", "We propose a flexible method for interim design modifications in time-to-event studies. With this method, it is possible to inspect the data at any time during the course of the study, without the need for prespecification of a learning phase, and to make certain types of design modifications depending on the interim data without compromising the Type I error risk. The method can be applied to studies designed with a conventional statistical test, fixed sample, or group sequential, even when no adaptive interim analysis and no specific method for design adaptations (such as combination tests) had been foreseen in the protocol. Currently, the method supports design changes such as an extension of the recruitment or follow-up period, as well as certain modifications of the number and the schedule of interim analyses as well as changes of inclusion criteria. In contrast to existing methods offering the same flexibility, our approach allows us to make use of the full interim information collected until the time of the adaptive data inspection. This includes time-to-event data from patients who have already experienced an event at the time of the data inspection, and preliminary information from patients still alive, even if this information is predictive for survival, such as early treatment response in a cancer clinical trial. Our method is an extension of the so-called conditional rejection probability (CRP) principle. It is based on the conditional distribution of the test statistic given the final value of the same test statistic from a subsample, namely the learning sample. It is developed in detail for the example of the logrank statistic, for which we derive this conditional distribution using martingale techniques."], ["On Fractile Transformation of Covariates in Regression", "The need for comparing two regression functions arises frequently in statistical applications. Comparison of the usual regression functions is not very meaningful in situations where the distributions and the ranges of the covariates are different for the populations. For instance, in econometric studies, the prices of commodities and people's incomes observed at different time points may not be on comparable scales due to inflation and other economic factors. In this article, we describe a method of standardizing the covariates and estimating the transformed regression function, which then become comparable. We develop smooth estimates of the fractile regression function and study its statistical properties analytically as well as numerically. We also provide a few real examples that illustrate the difficulty in comparing the usual regression functions and motivate the need for the fractile transformation. Our analysis of the real examples leads to new and useful statistical conclusions that are missed by comparison of the usual regression functions."], ["Simplex Factor Models for Multivariate Unordered Categorical Data", "Gaussian latent factor models are routinely used for modeling of dependence in continuous, binary, and ordered categorical data. For unordered categorical variables, Gaussian latent factor models lead to challenging computation and complex modeling structures. As an alternative, we propose a novel class of simplex factor models. In the single-factor case, the model treats the different categorical outcomes as independent with unknown marginals. The model can characterize flexible dependence structures parsimoniously with few factors, and as factors are added, any multivariate categorical data distribution can be accurately approximated. Using a Bayesian approach for computation and inferences, a Markov chain Monte Carlo (MCMC) algorithm is proposed that scales well with increasing dimension, with the number of factors treated as unknown. We develop an efficient proposal for updating the base probability vector in hierarchical Dirichlet models. Theoretical properties are described, and we evaluate the approach through simulation examples. Applications are described for modeling dependence in nucleotide sequences and prediction from high-dimensional categorical features."], ["Bootstrapping for Significance of Compact Clusters in Multidimensional Datasets", null], ["Sliced Latin Hypercube Designs", "This article proposes a method for constructing a new type of space-filling design, called a sliced Latin hypercube design, intended for running computer experiments. Such a design is a special Latin hypercube design that can be partitioned into slices of smaller Latin hypercube designs. It is desirable to use the constructed designs for collective evaluations of computer models and ensembles of multiple computer models. The proposed construction method is easy to implement, capable of accommodating any number of factors, and flexible in run size. Examples are given to illustrate the method. Sampling properties of the constructed designs are examined. Numerical illustration is provided to corroborate the derived theoretical results."], ["Optimal Designs for Rational Function Regression", null], ["Vast Volatility Matrix Estimation Using High-Frequency Data for Portfolio Selection", "Portfolio allocation with gross-exposure constraint is an effective method to increase the efficiency and stability of portfolios selection among a vast pool of assets, as demonstrated by Fan, Zhang, and Yu. The required high-dimensional volatility matrix can be estimated by using high-frequency financial data. This enables us to better adapt to the local volatilities and local correlations among a vast number of assets and to increase significantly the sample size for estimating the volatility matrix. This article studies the volatility matrix estimation using high-dimensional, high-frequency data from the perspective of portfolio selection. Specifically, we propose the use of \u201cpairwise-refresh time\u201d and \u201call-refresh time\u201d methods based on the concept of \u201crefresh time\u201d proposed by Barndorff-Nielsen, Hansen, Lunde, and Shephard for the estimation of vast covariance matrix and compare their merits in the portfolio selection. We establish the concentration inequalities of the estimates, which guarantee desirable properties of the estimated volatility matrix in vast asset allocation with gross-exposure constraints. Extensive numerical studies are made via carefully designed simulations. Comparing with the methods based on low-frequency daily data, our methods can capture the most recent trend of the time varying volatility and correlation, hence provide more accurate guidance for the portfolio allocation in the next time period. The advantage of using high-frequency data is significant in our simulation and empirical studies, which consist of 50 simulated assets and 30 constituent stocks of Dow Jones Industrial Average index."], ["Book Reviews", null]]}