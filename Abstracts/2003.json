{"2003": [["Statistical Models and Bioterrorism", " A general likelihood function for a multicommon source outbreak is developed where the dates of exposure to the source (e.g., anthrax spores) may or may not be known. Estimates of the incubation period distribution are derived from an outbreak in Sverdlovsk, Russia. The methods are applied to the 2001 U.S. outbreak. The sensitivity of the estimates to the assumed incubation period is investigated. Properties of the estimators, particularly when the outbreak sizes are small, are evaluated by simulation. In the absence of antibiotic prophylaxis, the outbreak could have been about twice as large but unlikely would have been more than 50 cases. The results underscore the importance of early detection of outbreaks together with targeted and effective public health control measures."], ["Multidimensional Residual Analysis of Point Process Models for Earthquake Occurrences", "Residual analysis methods for examining the fit of multidimensional point process models are applied to point process models for the space\u2013time\u2013magnitude distribution of earthquake occurrences, using, in particular, the multidimensional version of Ogata's epidemic-type aftershock sequence (ETAS) model and a 30-year catalog of 580 earthquakes occurring in Bear Valley, California. One method involves rescaled residuals, obtained by transforming points along one coordinate to form a homogeneous Poisson process inside a random, irregular boundary. Another method involves thinning the point process according to the conditional intensity to form a homogeneous Poisson process on the original, untransformed space. The thinned residuals suggest that the fit of the model may be significantly improved by using an anisotropic spatial distance function in the estimation of the spatially varying background rate. Using rescaled residuals, it is shown that the temporal\u2013magnitude distribution of aftershock activity is not separable, and that, in particular, in contrast to the ETAS model, the triggering density of earthquakes appears to depend on the magnitude of the secondary events in question. The residual analysis highlights that the fit of the space\u2013time ETAS model may be improved by allowing the parameters governing the triggering density to vary for earthquakes of different magnitudes. Such modifications may be important because the ETAS model is widely used in seismology for hazard analysis."], ["Infilling Sparse Records of Spatial Fields", "Historical records of weather, such as monthly precipitation and temperatures from the last century, are an invaluable database to use in studying changes and variability in climate. These data also provide the starting point for understanding and modeling the relationship among climate, ecological processes, and human activities. However, these data are observed irregularly over space and time. The basic statistical problem is to create a complete data record that is consistent with the observed data and is useful for other scientific disciplines. We modify the Gaussian-inverted Wishart spatial field model to accommodate irregular data patterns and to facilitate computations. Novel features of our implementation include the use of cross-validation to determine the relative prior weight given to the regression and geostatistical components and the use of a space-filling subset to reduce the computations for some parameters. We feel that the overall approach has merit, treading a line between computational feasibility and statistical validity. Furthermore, we are able to produce reliable measures of uncertainty for the estimates."], ["Multiple Edit/Multiple Imputation for Multivariate Continuous Data", null], ["Assessing Variability Due to Race Bridging", "In 1997, the Office of Management and Budget revised the standards for classification of Federal data on race and ethnicity. A key provision of the revised standards is that each respondent in a Federal data collection is now allowed to select more than one race to describe the person in question. The prior standards, published in 1977, specified that each respondent be instructed to select only one race. In the 2000 census, data on race were collected under the new standards. To make the 2000 census data compatible with other data systems that have not yet implemented the new standards as well as with historical data collected under the prior standards, the National Center for Health Statistics of the Centers for Disease Control and Prevention, with assistance from the Bureau of the Census, has produced bridged census counts, that is, estimates of the counts by race that would have been obtained had the prior standards been in effect. This article presents techniques for assessing the variability due to race bridging. The methods developed by Schafer and Schenker for inference with imputed conditional means, which can be considered a first-order approximation to a multiple-imputation analysis with an infinite number of imputations, are adapted to the bridging problem and applied to bridged 2000 census counts as well as to selected vital rates for 2000 computed using bridged census counts as denominators. The relative standard errors of estimated census counts by race under the 1977 standards tend to be higher for finer geographic levels and lower for coarser geographic levels. For each state (or the District of Columbia), the relative standard error of the count for a given race is no greater than .05. For birth and death rates by age group and 1977 race at the national level, on an absolute basis, bridging of the census counts in the denominators does not add substantially to the relative standard errors."], ["Forecast Uncertainties in Macroeconomic Modeling", "Weargue that probability forecasts convey information on the uncertainties that surround macroeconomic forecasts in a straightforward manner that is preferable to other alternatives, including the use of confidence intervals. Probability forecasts obtained using a small benchmark macroeconometric model and a number of other alternatives are presented and evaluated using recursive forecasts generated over the period 1999q1\u20132001q1. Out-of-sample probability forecasts of inflation and output growth are also provided over the period 2001q2\u20132003q1, and their implications are discussed in relation to the Bank of England's inflation target and the need to avoid recessions, both as separate events and jointly. The robustness of the results to parameter and model uncertainties is also investigated using Bayesian model-averaging techniques."], ["Bayesian Modeling and Forecasting of Intraday Electricity Load", "The advent of wholesale electricity markets has brought renewed focus on intraday electricity load forecasting. This article proposes a multi-equation regression model with a diagonal first-order stationary vector autoregresson (VAR) for modeling and forecasting intraday electricity load. The correlation structure of the disturbances to the VAR and the appropriate subset of regressors are explored using Bayesian model selection methodology. The full spectrum of finite-sample inference is obtained using a Bayesian Markov chain Monte Carlo sampling scheme. This includes the predictive distribution of load and the distribution of the time and level of daily peak load, something that is difficult to obtain with other methods of inference. The method is applied to several multiequation models of half-hourly total system load in New South Wales, Australia. A detailed model based on 3 years of data reveals trend, seasonal, bivariate temperature/humidity, and serial correlation components that all vary intraday, justifying the assumption of a multiequation approach. Short-term forecasts from simple models highlight the gains that can be made if accurate temperature predictions are exploited. Bayesian predictive means for half-hourly load compare favorably with point forecasts obtained using iterated generalized least squares estimation of the same models."], ["Profiling Placebo Responders by Self-Consistent Partitioning of Functional Data", "Identification of placebo responders among subjects treated with active drug has significant clinical and research implications. In clinical practice, when a patient treated with medication improves, this improvement may be attributed to the chemical component of the drug itself, a \u201cplacebo effect,\u201d or some combination of these. Determining the proper subsequent treatment and maintenance of the patient may be greatly aided by understanding the mechanism of patient improvement. In a research context, classification of patient response has bearing on how efficacy and effectiveness clinical trials are designed and conducted. This article presents a framework for studying placebo response in diverse areas of medicine. To identify placebo responders among drug-treated patients, a profile of the clinical status over time (outcome profile) is estimated for each subject. Self-consistent partitioning techniques are used to group subjects based on the amount of curvature in the profile as well as the overall trend in the profile. The resulting partitions determine representative profiles for subjects in the drug group that subsequently can be used to classify patients. The proposed method is applied to data from a clinical trial for treatment of depression involving placebo and the active drug phenelzine. Data from the placebo arm of the study is used to help validate the procedure, because the drug-treated and placebo-treated subjects should share common profiles."], ["Characterizing the Progression of Viral Mutations Over Time", "Development and spread of resistance of human immunodeficiency virus type 1 to antiretroviral therapies is a serious medical and public health concern. A wide variety of mutations have been identified that either singly or in combination reduce the susceptibility of the virus to available therapies. This paper describes methods for understanding the genetic pathways that lead to high-level drug resistance under selective drug pressure, as well as for estimating the rates at which viral populations progress along these pathways. These methods can be used to determine whether the presence of certain mutations among drug-sensitive viruses predispose a patient under a particular treatment to develop patterns of mutations that confer high-level drug resistance. Our approach assumes that viral genotypes can be characterized as belonging to discrete states, defined by patterns of viral mutations, and considers two approaches to modeling the rates of transition between these states. The first approach treats the state at a given time point as known, whereas the second treats this as a latent variable. We apply our methods to genetic sequences of viruses cloned from the plasma of 170 patients who participated in three phase II clinical studies of efavirenz combination therapy (DMP 266-003, -004, -005). Multiple viral clones are available from each plasma sample at each time of measurement, allowing for consideration of the effect of minority species on the evolution of the viral populations infecting patients; the availability of such information motivates the second analytic approach. The sequences can be found in the Stanford HIV RT and Protease Sequence Database."], ["A Method for Normalizing Microarrays Using Genes That Are Not Differentially Expressed", "One of the more challenging, yet easily overlooked, aspects of the analysis of microarrays is how to normalize arrays so that comparisons can be made across arrays. Most studies that utilize microarrays to detect differential gene expression between samples find the data only enable one to conclude that a handful of genes are differentially expressed. The basic idea here is to use the genes that are not differentially expressed to conduct the normalization. Of course, because one cannot determine which genes are differentially expressed until the normalization is conducted, this is a nontrivial problem. Here a general framework and computational method (using the Gibbs sampler) is devised to allow for such normalization. We apply the method to a gene expression experiment aimed at furthering our understanding of Porcine reproductive and respiratory syndrome virus, a major source of economic loss in the swine industry."], ["Frequentist Model Average Estimators", "The traditional use of model selection methods in practice is to proceed as if the final selected model had been chosen in advance, without acknowledging the additional uncertainty introduced by model selection. This often means underreporting of variability and too optimistic confidence intervals. We build a general large-sample likelihood apparatus in which limiting distributions and risk properties of estimators post-selection as well as of model average estimators are precisely described, also explicitly taking modeling bias into account. This allows a drastic reduction in complexity, as competing model averaging schemes may be developed, discussed, and compared inside a statistical prototype experiment where only a few crucial quantities matter. In particular, we offer a frequentist view on Bayesian model averaging methods and give a link to generalized ridge estimators. Our work also leads to new model selection criteria. The methods are illustrated with real data applications."], ["The Focused Information Criterion", "A variety of model selection criteria have been developed, of general and specific types. Most of these aim at selecting a single model with good overall properties, for example, formulated via average prediction quality or shortest estimated overall distance to the true model. The Akaike, the Bayesian, and the deviance information criteria, along with many suitable variations, are examples of such methods. These methods are not concerned, however, with the actual use of the selected model, which varies with context and application. The present article takes the view that the model selector should instead focus on the parameter singled out for interest; in particular, a model that gives good precision for one estimand may be worse when used for inference for another estimand. We develop a method that, for a given focus parameter, estimates the precision of any submodel-based estimator. The framework is that of large-sample likelihood inference. Using an unbiased estimate of limiting risk, we propose a focused information criterion for model selection. We investigate and discuss properties of the method, establish some connections to Akaike's information criterion, and illustrate its use in a variety of situations."], ["Discussion", null], ["Discussion", null], ["Discussion", null], ["Discussion", null], ["Discussion", null], ["Discussion", null], ["Rejoinder", null], ["Directional Rates of Change Under Spatial Process Models", "Spatial process models are now widely used for inference in many areas of application. In such contexts interest is often in the rate of change of a spatial surface at a given location in a given direction. Examples include temperature or rainfall gradients in meteorology, pollution gradients for environmental data, and surface roughness assessment for digital elevation models. Because the spatial surface is viewed as a random realization, all such rates of change are random as well. We formalize the notions of directional finite difference processes and directional derivative processes building upon the concept of mean square differentiability as developed by Stein and Banerjee and Gelfand. We obtain complete distribution theory results under the assumptions of a stationary Gaussian process model either for the data or for spatial random effects. We present inference under a Bayesian framework which, in this setting, presents several advantages. Finally, we illustrate our methodology with a simulated dataset and also with a real estate dataset consisting of selling prices of individual homes."], ["Adaptive Estimators and Tests of Stationary and Nonstationary Short- and Long-Memory ARFIMA\u2013GARCH Models", null], ["Using the Bootstrap to Select One of a New Class of Dimension Reduction Methods", null], ["More Efficient Local Polynomial Estimation in Nonparametric Regression With Autocorrelated Errors", "We propose a modification of local polynomial time series regression estimators that improves efficiency when the innovation process is autocorrelated. The procedure is based on a pre-whitening transformation of the dependent variable that must be estimated from the data. We establish the asymptotic distribution of our estimator under weak dependence conditions. We show that the proposed estimation procedure is more efficient than the conventional local polynomial method. We also provide simulation evidence to suggest that gains can be achieved in moderate-sized samples."], ["Mobius-Like Mappings and Their Use in Kernel Density Estimation", "It is well known that the manipulation of sample data by means of a parametric function can improve the performance of kernel density estimation. This article proposes a two-parameter Mobius-like function to map sample data drawn from a semi-infinite space into [\u22121,1). A standard kernel method is then used to estimate the density. The proposed method is shown to yield effective estimates of density and is computationally more efficient than other well-known transformation methods. The efficacy of the technique is demonstrated in a practical setting by application to two datasets."], ["Censored Regression Quantiles", "Using quantile regression to analyze survival times offers an valuable complement to traditional Cox proportional hazards modelling. Unfortunately, this approach has been hampered by the lack of a conditional quantile estimator for censored data that is directly analogous to the Kaplan\u2013Meier estimator and applies under standard assumptions for censored regression models. Here a recursively reweighted estimator of the regression quantile process is developed as a direct generalization of the Kaplan\u2013Meier estimator. Specifically, the asymptotic behavior is directly analogous to that of the Kaplan\u2013Meier estimator, and computation is essentially equivalent to current simplex methods for the quantile process in the uncensored case. Some preliminary examples suggest the strong potential of these methods as a complement to the use of Cox models."], ["A Lack-of-Fit Test for Quantile Regression", "We propose an omnibus lack-of-fit test for linear or nonlinear quantile regression based on a cusum process of the gradient vector. The test does not involve nonparametric smoothing but is consistent for all nonparametric alternatives without any moment conditions on the regression error. In addition, the test is suitable for detecting the local alternatives of any order arbitrarily close to n\u22121/2 from the null hypothesis. The limiting distribution of the proposed test statistic is non-Gaussian but can be characterized by a Gaussian process. We propose a simple sequential resampling scheme to carry out the test whose nominal levels are well approximated in our empirical study for"], ["A Latent Variable Model of Segregation Analysis for Ordinal Traits", "Many health conditions, including cancer and psychiatric disorders, are believed to have a complex genetic basis, and genes and environmental factors are likely to interact in the presence and severity of these conditions. Assessing familial aggregation and inheritability of disease is a classic topic of genetic epidemiology, commonly referred to as segregation analysis. Although today it is routine to conduct such analyses for quantitative and dichotomous traits, methods and software that accommodate ordinal traits do not exist. Tothis end, we propose a latent variable model by extending the work of Zhang and Merikangas, who examined binary traits. The advantage of this latent variable model lies in its flexibility to include environmental factors (usually represented by covariates) and its potential to allow gene\u2013environment interactions. The model building uses the EM algorithm for maximization and a peeling algorithm for computational efficiency. We provide asymptotic theory for statistical inference, and conduct simulation studies to confirm that the asymptotic theory is adequate in practical applications. We also apply our model to examine the familial transmission of alcoholism, which is categorized into three ordinal levels: normal control, alcohol abuse, and alcohol dependence. Our analysis not only confirms that alcoholism is familial, but also suggests that the transmission may have a major gene component not revealed by previous analyses using dichotomous traits."], ["Determining Inheritance Distributions via Stochastic Penetrances", null], ["Information Recovery in a Study With Surrogate Endpoints", null], ["Estimating Cure Rates From Survival Data", "This article considers the utility of the bounded cumulative hazard model in cure rate estimation, which is an appealing alternative to the widely used two-component mixture model. This approach has the following distinct advantages: (1) It allows for a natural way to extend the proportional hazards regression model, leading to a wide class of extended hazard regression models. (2) In some settings the model can be interpreted in terms of biologically meaningful parameters. (3) The model structure is particularly suitable for semiparametric and Bayesian methods of statistical inference. Notwithstanding the fact that the model has been around for less than a decade, a large body of theoretical results and applications has been reported to date. This review article is intended to give a big picture of these modeling techniques and associated statistical problems. These issues are discussed in the context of survival data in cancer."], ["The Politics of Large Numbers: a History of Statistical Reasoning", null], ["Annotated Readings in the History of Statistics", null], ["Contemporary Statistical Models for the Plant and Soil Sciences. Oliver Schabenberger and Francis J. Pierce", null], ["Multivariate Dispersion, Central Regions and Depth: the Lift Zonoid Approach. Karl Mosler", null], ["Principal Component Analysis", null], ["A Distribution-Free Theory of Nonparametric Regression. Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, and Harro Walk", null], ["The Statistical Analysis of Failure Time Data", null], ["Elements of Applied Stochastic Processes", null], ["An Introduction to Generalized Linear Models", null], ["Debugging SAS Programs: a Handbook of Tools and Techniques", null], ["Telegraphic Reviews", null], ["Transform Estimation of Parameters for Stage-Frequency Data", "We consider multistage development models that occur in the maturation of biological organisms, disease progressions, and industrial processes. The situations that we address are distinguished by the essentially destructive sampling required to assess the stage reached by each individual. We develop robust estimators of stage-dependent maturation rates and overall death rates using method of moments based on Laplace transforms, for which we develop variance estimates under different sampling schemes. We apply these methods to studies of cattle parasite and grasshopper life cycles and show that the Laplace methods compare well with maximum likelihood methods."], ["The Intrinsic Distribution and Selection Bias of Long-Period Cometary Orbits", "A question that arises in the study of cometary orbits is whether or not the directed normals to the orbits are uniformly distributed on the celestial sphere. Previous studies by statisticians have not taken selection effects into account and have tended to reject uniformity. Here a plausible selection mechanism is proposed that gives rise to a one-parameter family of distributions on the sphere. Data on long-period comets are analyzed using this one-parameter family. A nonzero selection effect is detected, and its size is estimated. Subject to this selection effect, uniformity of the directed normals can no longer be ruled out."], ["Estimating the Large-Scale Structure of the Universe Using Quasi-Stellar Object Carbon IV Absorbers", null], ["How Much Does the Far Future Matter? A Hierarchical Bayesian Analysis of the Public's Willingness to Mitigate Ecological Impacts of Climate Change", "How much does the far future matter? This question lies at the heart of many important environmental policy issues, such as global climate change, biodiversity loss, and the disposal of radioactive waste. Although philosophers, experts, and others offer their viewpoints on this deep question, the solution to many environmental problems lies in the willingness of the public to bear significant costs now to make the far future a better place. Short of national plebiscites, the only way to assess the public's willingness to mitigate impacts in the far future is to ask them. Using a unique set of survey data in which respondents were provided with sets of scenarios describing different amounts of forest loss due to climate change, along with associated mitigation methods and costs, we can infer the respondents' willingness to bear additional costs to mitigate future ecological impacts of climate change. The survey also varied the timing of the impacts, which allows us to assess how the willingness to mitigate depends on the timing of the impacts. The responses to the survey questions are a consequence of latent utilities with complex ordinal structures that result in nonrectangular probabilities. Whereas the nonrectangular probabilities complicate standard maximum likelihood\u2013based approaches, we show how the nonrectangular probabilities fit neatly into a hierarchical Bayesian model. We show how to fit these models using the Gibbs sampler, overcoming problems in parameter identification to improve mixing of the induced Markov chain. The results indicate that the public's willingness to incur additional costs to mitigate ecological impacts of climate change is an increasing nonlinear function of the magnitude of the impact, and that they discount future impacts at somewhat less than 1%% per year."], ["Measurement of Higher Education in the Census and Current Population Survey", "We examine measurement error in the reporting of higher education in the 1990 Decennial Census and the post-1991 Current Population Survey (CPS). We document that measurement error in the reporting of higher education is prevalent in Census data. Further, these errors violate models of classical measurement error in important ways. The level of education is consistently reported as higher than it is (errors are not mean 0), errors in the reporting of education are correlated with covariates that appear in earnings regressions, and errors in the reporting of education appear correlated with the error term in a model of earnings determination. Thus, neither well-known results on classical measurement error nor recent models of nonclassical measurement error are likely valid when using Census and CPS data. We find some evidence that the measurement error is lower in the CPS than in the Census, presumably because first interviews are generally conducted in person."], ["Dynamic Latent Trait Models for Multidimensional Longitudinal Data", "This article presents a new approach for analysis of multidimensional longitudinal data, motivated by studies using an item response battery to measure traits of an individual repeatedly over time. A general modeling framework is proposed that allows mixtures of count, categorical, and continuous response variables. Each response is related to age-specific latent traits through a generalized linear model that accommodates item-specific measurement errors. A transition model allows the latent traits at a given age to depend on observed predictors and on previous latent traits for that individual. Following a Bayesian approach to inference, a Markov chain Monte Carlo algorithm is proposed for posterior computation. The methods are applied to data from a neurotoxicity study of the pesticide methoxychlor, and evidence of a dose-dependent increase in motor activity is presented."], ["A Model of the Joint Distribution of Purchase Quantity and Timing", null], ["Wavelet-Based Nonparametric Modeling of Hierarchical Functions in Colon Carcinogenesis", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Order-Preserving Nonparametric Regression, With Applications to Conditional Distribution and Quantile Function Estimation", null], ["Calibrating the Degrees of Freedom for Automatic Data Smoothing and Effective Curve Checking", "Curve fitting and curve checking based on the local polynomial regression technique are commonly used data-analytic methods in statistics. This article examines, in nonparametric settings, both the asymptotic expressions and empirical formulas for degrees of freedom (DF), a notion introduced by Hastie and Tibshirani, of linear smoothers. The asymptotic results give useful insights into the nonparametric modeling complexity. Meanwhile, by substituting the exact DFs by the empirical formula, an empirical version of the generalized cross-validation (EGCV) is obtained. An automatic bandwidth selection method based on minimizing EGCV is proposed for conducting local smoothing. This procedure preserves full benefits of the ordinary and generalized cross-validation, but offers a substantial reduction in computational burden. Furthermore, the EGCV-minimizing bandwidth can be extended in a very simple manner to fit multivariate models, such as the varying-coefficient models. Applications of calibrating DFs to important inferential issues, such as assessing the validity of useful model assumptions and measuring the significance of predictor variables based on the generalized likelihood ratio statistics are also discussed. Simulation studies are presented to illustrate the performance of the proposed procedures in a range of statistical problems."], ["Semiparametric Estimation of Multivariate Fractional Cointegration", null], ["Smoothing Spline ANOVA for Time-Dependent Spectral Analysis", "In this article we propose a smoothing spline ANOVA model (SS-ANOVA) to estimate and to make inference on the time-varying log-spectrum of a locally stationary process. The time-varying spectrum is assumed to be smooth in both time and frequency. This assumption essentially turns a time-frequency spectral estimation problem into a 2-dimensional surface estimation problem. A smooth localized complex exponential (SLEX) basis is used to calculate the initial periodograms, and a SS-ANOVA is fitted to the log-periodograms. This approach allows the time and frequency domains to be modeled in a unified approach and jointly estimated. Inference procedures, such as confidence intervals, and hypothesis tests proposed for the SS-ANOVA can be adopted for the time-varying spectrum. Because of the smoothness assumption of the underlying spectrum, once we have the estimates on a time-frequency grid, we can calculate the estimate at any given time and frequency. This leads to a high computational efficiency, because for large datasets we need only estimate the initial raw periodograms at a much coarser grid. We study a penalized least squares estimator and a penalized Whittle likelihood estimator. The penalized Whittle likelihood estimator has smaller mean squared errors, whereas inference based on the penalized least squares method can adopt existing results. We present simulation results and apply our method to electroencephalogram data recorded during an epileptic seizure."], ["Evolutionary Similarity Among Genes", null], ["Frequency of Recurrent Events at Failure Time", "Recurrent events arise in many longitudinal medical studies where time to a terminal event or failure is the primary endpoint. With incomplete follow-up data, the analysis of recurrent events is a challenge owing to their association with the failure. One specific quantity of interest rarely addressed in the statistical literature is the recurrence frequency at the failure time; an example is hospitalization frequency, which is often used as a rough measure of lifetime medical cost. In this article we show that a marginal model (e.g., the log-linear model) of the recurrence frequency, although desirable, is typically not identifiable. For this reason, we advocate modeling the recurrent events and the failure time jointly, and propose an approach to forming semiparametric joint models from prespecified marginal ones. We suggest two conceptually simple and nested regression models aiming at the recurrence frequency as a mark of the failure and at the process of recurrent events. We formulate monotone estimating functions and propose novel interval-estimation procedures to accommodate non-smooth estimating functions. The resulting estimators are consistent and asymptotically normal. Simulation studies and the application to an AIDS clinical trial exhibit that these proposals are easy to implement and reliable for practical use. Finally, we generalize our proposals to marked recurrent events, and also devise a global inference procedure for recurrent events of multiple types."], ["Optimality, Variability, Power", null], ["Robust and Efficient Designs for the Michaelis\u2013Menten Model", null], ["Characterizing Additively Closed Discrete Models by a Property of Their Maximum Likelihood Estimators, With an Application to Generalized Hermite Distributions", "This article reports on two-parameter count distributions (satisfying very general conditions) that are closed under addition so that their maximum likelihood estimator (MLE) of the population mean is the sample mean. The most important of these in practice, the generalized Hermite distribution, is analyzed, and a necessary and sufficient condition is given to ensure that the MLE is the solution of likelihood equations. Score test to contrast the Poisson assumption is studied, and two examples of applications are given."], ["On the Comparison of Reliability Experiments Based on the Convolution Order", null], ["Multiple Imputation for Incomplete Data With Semicontinuous Variables", "We consider the application of multiple imputation to data containing not only partially missing categorical and continuous variables, but also partially missing 'semicontinuous' variables (variables that take on a single discrete value with positive probability but are otherwise continuously distributed). As an imputation model for data sets of this type, we introduce an extension of the standard general location model proposed by Olkin and Tate; our extension, the blocked general location model, provides a robust and general strategy for handling partially observed semicontinuous variables. In particular, we incorporate a two-level model for the semicontinuous variables into the general location model. The first level models the probability that the semicontinuous variable takes on its point mass value, and the second level models the distribution of the variable given that it is not at its point mass. In addition, we introduce EM and data augmentation algorithms for the blocked general location model with missing data; these can be used to generate imputations under the proposed model and have been implemented in publicly available software. We illustrate our model and computational methods via a simulation study and an analysis of a survey of Massachusetts Megabucks Lottery winners."], ["The Mean Squared Error of Small Area Predictors Constructed With Estimated Area Variances", "In the small area estimation literature, the sampling error variances are customarily assumed to be known or to depend on a finite number of parameters. We consider the empirical best linear unbiased predictor (EBLUP) obtained by using the individual directly estimated variance for each small area. An approximation for the mean squared error (MSE) of the EBLUP that recognizes the impact on the predictors of estimation of the variance components is derived. Simulation studies show that the theoretical expressions are good approximations for the MSE of the predictors unless the between-area variance component is very small (relative to the within-area variance). An improved estimator of the MSE is developed that has smaller overestimation than the original estimator when the between-area variance component is small. The robustness of the MSE estimator is studied and predictors for nonnormal sampling errors are proposed. An example from the National Resources Inventory that motivated the development of the theory is described."], ["On \u03c8-Learning", "The concept of large margins have been recognized as an important principle in analyzing learning methodologies, including boosting, neural networks, and support vector machines (SVMs). However, this concept alone is not adequate for learning in nonseparable cases. We propose a learning methodology, called \u03c8-learning, that is derived from a direct consideration of generalization errors. We provide a theory for \u03c8-learning and show that it essentially attains the optimal rates of convergence in two learning examples. Finally, results from simulation studies and from breast cancer classification confirm the ability of \u03c8-learning to outperform SVM in generalization."], ["A Bayesian Solution for a Statistical Auditing Problem", null], ["Likelihood-Based Inference in Some Continuous Exponential Families With Unknown Threshold Parameters", "We study likelihood-based inference in some continuous exponential families with unknown threshold parameters. The introduction of threshold parameters necessitates modification of the standard asymptotic arguments, and some possibly unexpected limiting distributions result."], ["Finding the Number of Clusters in a Dataset", null], ["Testing for Normality", null], ["Geographically Weighted Regression", null], ["A First Course in Linear Model Theory", null], ["Hierarchical Linear Models: Applications and Data Analysis Methods", null], ["Predictions in Time Series Using Regression Models", null], ["Martingales and Markov Chains: Solved Exercises and Elements of Theory", null], ["Panel Data Econometrics: Method-of-Moments and Limited Dependent Variables", null], ["Statistical Analysis of Designed Experiments", null], ["Algorithms for Worst-Case Design and Applications to Risk Management", null], ["Subjective Probability Models for Lifetimes", null], ["Analysis of Failure and Survival Data", null], ["Elements of Computational Statistics", null], ["Longitudinal Data and SAS: A Programmer's Guide", null], ["Analyzing Medical Data Using S-PLUS", null], ["Clinical Trials", null], ["Introduction to Nonparametric Item Response Theory", null], ["Logit and Probit: Ordered and Multinomial Models", null], ["Multivariate Statistical Modelling Based on Generalized Linear Models", null], ["Telegraphic Reviews", null], ["Corrections", null], ["Mixed-Model Functional ANOVA for Studying Human Tactile Perception", "Human tactile perception is studied through digitized images of hand-drawn curves generated by subjects treated with various facial preparations. The drawings represent their subjective assessment of a small brush moving across the face. In this study, exploratory data analysis is carried out through a functional data analog of principal components analysis and curve decomposition based on Fourier representations. In addition, high-dimensional analysis of variance is adapted for mixed-model functional data and applied to test whether preparation effects are significant."], ["Cross-Calibration of Stroke Disability Measures", "It is common to assess disability of stroke patients using standardized scales, such as the Rankin Stroke Outcome Scale (RS) and the Barthel Index (BI). The RS, which was designed for applications to stroke, is based on assessing directly the global conditions of a patient. The BI, which was designed for more general applications, is based on a series of questions about the patient's ability to carry out 10 basic activities of daily living. Because both scales are commonly used, but few studies use both, translating between scales is important in gaining an overall understanding of the efficacy of alternative treatments, and in developing prognostic models that combine several datasets. The objective of our analysis is to provide a tool for translating between BI and RS. Specifically, we estimate the conditional probability distributions of each given the other. Subjects consisted of 459 individuals who sustained a stroke and who were recruited for the Kansas City Stroke Study from 1995 to 1998. We assessed patients with BI and RS measures 1, 3, and 6 months after stroke. In addition, we included data from the Framingham study, in the form of a table cross-classifying patients by RS and coarsely aggregated BI. Our statistical estimation approach is motivated by several goals: (a) overcoming the difficulty presented by the fact that our two sources report data at different resolutions; (b) smoothing the empirical counts to provide estimates of probabilities in regions of the table that are sparsely populated; (c) avoiding estimates that would conflict with medical knowledge about the relationship between the two measures; and (d) estimating the relationship between RS and BI at three months after the stroke, while borrowing strength from measurements made at 1 month and 6 months. We address these issues via a Bayesian analysis combining data augmentation and constrained semiparametric inference. Our results provide the basis for comparing and integrating the results of clinical trials using different disability measures, and integrating clinical trials results into a comprehensive decision model for the assessment of long-term implications and cost-effectiveness of stroke prevention and acute treatment interventions. In addition, our results indicate that the degree of agreement between the two measures is less strong than commonly reported, and emphasize the importance of trial designs that include multiple assessments of outcome."], ["Hierarchical Models for Permutations", "The popularity of the sport of auto racing is increasing rapidly, but its fans remain less interested in statistics than the fans of other sports. In this article, we propose a new class of models for permutations that closely resembles the behavior of auto racing results. We pose the model in a Bayesian hierarchical framework, which permits hierarchical specification and fully hierarchical estimation of interaction terms. We demonstrate the methodology using several rich datasets that consist of repeated rankings for a collection of drivers. Our models can potentially identify individuals racing in \u201cminor league\u201d divisions who have higher potential for competitive performance at higher levels. We also present evidence that one of the sport's more controversial figures, Jeff Gordon, is a statistically dominant figure."], ["Recounts From Undervotes", "The vote recount in the 2000 Presidential election (Broward, Miami-Dade and Palm Beach Counties, Florida) is examined for evidence of bias. A precinct-level dataset is constructed, incorporating the machine-vote tally, the recount vote tally, voter registration demographics, and the ballot review by media sources. A new multivariate beta-logit model is introduced that allows joint modeling of multivariate unobserved latent probabilities. A simple two-step estimator is proposed that approximates the joint maximum likelihood estimator. The estimates are consistent with a strong hypothesis: that the recount vote tally was unbiased. Specifically, it is found that the precinct-level machine-vote probability for a candidate is an unbiased predictor for the hand-recount undervote probability. There is no evidence of bias in the recount."], ["Principal Stratification Approach to Broken Randomized Experiments", "The precarious state of the educational system in the inner cities of the United States, as well as its potential causes and solutions, have been popular topics of debate in recent years. Part of the difficulty in resolving this debate is the lack of solid empirical evidence regarding the true impact of educational initiatives. The efficacy of so-called \u201cschool choice\u201d programs has been a particularly contentious issue. A current multimillion dollar program, the School Choice Scholarship Foundation Program in New York, randomized the distribution of vouchers in an attempt to shed some light on this issue. This is an important time for school choice, because on June 27, 2002 the U.S. Supreme Court upheld the constitutionality of a voucher program in Cleveland that provides scholarships both to secular and religious private schools. Although this study benefits immensely from a randomized design, it suffers from complications common to such research with human subjects: noncompliance with assigned \u201ctreatments\u201d and missing data. Recent work has revealed threats to valid estimates of experimental effects that exist in the presence of noncompliance and missing data, even when the goal is to estimate simple intention-to-treat effects. Our goal was to create a better solution when faced with both noncompliance and missing data. This article presents a model that accommodates these complications that is based on the general framework of \u201cprincipal stratification\u201d and thus relies on more plausible assumptions than standard methodology. Our analyses revealed positive effects on math scores for children who applied to the program from certain types of schools\u2014those with average test scores below the citywide median. Among these children, the effects are stronger for children who applied in the first grade and for African-American children."], [null, null], ["A Model-Free Test for Reduced Rank in Multivariate Regression", "We propose a test of dimension in multivariate regression. This test is in the spirit of tests on the rank of the coefficient matrix in a multivariate linear model, but it does not require a prespecified model. The test may be particularly useful at the outset of an analysis before a multivariate model is posited, because it can lead to low-dimensional summary plots that are inferred to contain all of the sample information on the multivariate mean function."], ["Generalized Nonlinear Modeling With Multivariate Free-Knot Regression Splines", "A Bayesian method is presented for the nonparametric modeling of univariate and multivariate non-Gaussian response data. Data-adaptive multivariate regression splines are used where the number and location of the knot points are treated as random. The posterior model space is explored using a reversible-jump Markov chain Monte Carlo sampler. Computational difficulties are partly alleviated by introducing a random residual effect in the model that leaves many of the posterior conditional distributions of the model parameters in standard form. The use of the latent residual effect provides a convenient vehicle for modeling correlation in multivariate response data, and as such our method can be seen to generalize the seemingly unrelated regression model to non-Gaussian data. We illustrate the method on a number of examples, including two previously unpublished datasets relating to the spatial smoothing of multivariate accident data in Texas and the modeling of credit card use across multiple retail sectors."], ["Identification of Linear Directions in Multivariate Adaptive Spline Models", "Identifying linear directions in multivariate regression has been a statistical challenge and has attracted particular attention since projection pursuit was developed. To this end, we propose and investigate the use of linear discriminant analysis and projection Hessian directions in conjunction with multivariate adaptive regression splines. Simulation studies in a variety of settings demonstrate the usefulness of our approach in revealing both the functional forms and the linear substructures based on the observed data. Mathematical results are also provided to support our approach. Comparisons are made between our approach and existing approaches, and the improvements are evident. Depending on the circumstance, the extent of improvement can be substantial."], ["Nonlinear State-Space Models With State-Dependent Variances", "Nonlinear state-space models with state-dependent variances (SDVs) are commonly used in financial time series. Important examples include stochastic volatility (SV) and affine term structure models. We propose a methodology for state smoothing in this class of models. Our smoothing technique is simulation based and uses an auxiliary mixture model. Key features of the auxiliary mixture model are the use of state-dependent weights and efficient block sampling algorithms to jointly update all unobserved states given latent mixture indicators. Conditional on latent indicator variables, the auxiliary mixture model reduces to a normal dynamic linear model. We illustrate our methodology with two time series applications. First, we show how to construct the auxiliary model for a logarithmic SV model and compare the performance of our methodology with the current literature. Next, we implement a square-root SV model with jumps for short-term interest rates in Hong Kong."], ["Spatial Modeling With Spatially Varying Coefficient Processes", "In many applications, the objective is to build regression models to explain a response variable over a region of interest under the assumption that the responses are spatially correlated. In nearly all of this work, the regression coefficients are assumed to be constant over the region. However, in some applications, coefficients are expected to vary at the local or subregional level. Here we focus on the local case. Although parametric modeling of the spatial surface for the coefficient is possible, here we argue that it is more natural and flexible to view the surface as a realization from a spatial process. We show how such modeling can be formalized in the context of Gaussian responses providing attractive interpretation in terms of both random effects and explaining residuals. We also offer extensions to generalized linear models and to spatio-temporal setting. We illustrate both static and dynamic modeling with a dataset that attempts to explain (log) selling price of single-family houses."], ["Clustering for Sparsely Sampled Functional Data", "We develop a flexible model-based procedure for clustering functional data. The technique can be applied to all types of curve data but is particularly useful when individuals are observed at a sparse set of time points. In addition to producing final cluster assignments, the procedure generates predictions and confidence intervals for missing portions of curves. Our approach also provides many useful tools for evaluating the resulting models. Clustering can be assessed visually via low-dimensional representations of the curves, and the regions of greatest separation between clusters can be determined using a discriminant function. Finally, we extend the model to handle multiple functional and finite-dimensional covariates and show how it can be applied to standard finite-dimensional clustering problems involving missing data."], ["Semiparametric Regression for the Area Under the Receiver Operating Characteristic Curve", "Medical advances continue to provide new and potentially better means for detecting disease. Such is true in cancer, for example, where biomarkers are sought for early detection and where improvements in imaging methods may pick up the initial functional and molecular changes associated with cancer development. In other binary classification tasks, computational algorithms such as neural networks, support vector machines, and evolutionary algorithms have been applied to areas as diverse as credit scoring, object recognition, and peptide-binding prediction. Before a classifier becomes an accepted technology, it must undergo rigorous evaluation to determine its ability to discriminate between states. Characterization of factors influencing classifier performance is an important step in this process. Analysis of covariates may reveal subpopulations in which classifier performance is greatest or identify features of the classifier that improve accuracy. We develop regression methods for the nonparametric area under the receiver operating characteristic curve, a well-accepted summary measure of classifier accuracy. The estimating function generalizes standard approaches and, interestingly, is related to the two-sample Mann\u2013Whitney U statistic. Implementation is straightforward, because it is an adaptation of binary regression methods. Asymptotic theory is nonstandard, because the regressor variables are cross-correlated. Nevertheless, simulation studies show that the method produces estimates with small bias and reasonable coverage probability. Application of the method to evaluate the covariate effects on a new device for diagnosing hearing impairment reveals that the device performs better in more severely impaired subjects and that certain test parameters, which are adjustable by the device operator, are key to test performance."], ["Sample Size Reestimation for Clinical Trials With Censored Survival Data", "A flexible design with updating of sample size in clinical trials is proposed for censored survival data. Patients enter trials serially and are subject to random loss to follow-up. The statistical inference is based on a weighted average of the linear rank statistics, where the weight function at each look depends on prior observed data. A stopping rule is devised to allow early termination and acceptance of the null hypothesis when the experimental treatment offers no advantage or is inferior to the control. The null hypothesis that two survival distributions are equal may be rejected only at the last step when the weight function is used up. The overall type I error rate is preserved. Independent increments for the sequential linear rank statistic is key to deriving asymptotic properties of the test statistic. Some extensive simulations have been carried out to compare the operating characteristics of the method under different scenarios, and for a comparison with the usual log-rank test with fixed sample design. The methodology is also illustrated with a colon cancer clinical trial."], ["Inferring Spatial Phylogenetic Variation Along Nucleotide Sequences", null], ["Detecting Differentially Expressed Genes in Microarrays Using Bayesian Model Selection", "DNA microarrays open up a broad new horizon for investigators interested in studying the genetic determinants of disease. The high throughput nature of these arrays, where differential expression for thousands of genes can be measured simultaneously, creates an enormous wealth of information, but also poses a challenge for data analysis because of the large multiple testing problem involved. The solution has generally been to focus on optimizing false-discovery rates while sacrificing power. The drawback of this approach is that more subtle expression differences will be missed that might give investigators more insight into the genetic environment necessary for a disease process to take hold. We introduce a new method for detecting differentially expressed genes based on a high-dimensional model selection technique, Bayesian ANOVA for microarrays (BAM), which strikes a balance between false rejections and false nonrejections. The basis of the new approach involves a weighted average of generalized ridge regression estimates that provides the benefits of using shrinkage estimation combined with model averaging. A simple graphical tool based on the amount of shrinkage is developed to visualize the trade-off between low false-discovery rates and finding more genes. Simulations are used to illustrate BAM's performance, and the method is applied to a large database of colon cancer gene expression data. Our working hypothesis in the colon cancer analysis is that large differential expressions may not be the only ones contributing to metastasis\u2014in fact, moderate changes in expression of genes may be involved in modifying the genetic environment to a sufficient extent for metastasis to occur. A functional biological analysis of gene effects found by BAM, but not other false-discovery-based approaches, lends support to this hypothesis."], ["Surveillance of a Simple Linear Regression", null], ["From the Statistics of Data to the Statistics of Knowledge", "Increasingly, datasets are so large they must be summarized in some fashion so that the resulting summary dataset is of a more manageable size, while still retaining as much knowledge inherent to the entire dataset as possible. One consequence of this situation is that the data may no longer be formatted as single values such as is the case for classical data, but rather may be represented by lists, intervals, distributions, and the like. These summarized data are examples of symbolic data. This article looks at the concept of symbolic data in general, and then attempts to review the methods currently available to analyze such data. It quickly becomes clear that the range of methodologies available draws analogies with developments before 1900 that formed a foundation for the inferential statistics of the 1900s, methods largely limited to small (by comparison) datasets and classical data formats. The scarcity of available methodologies for symbolic data also becomes clear and so draws attention to an enormous need for the development of a vast catalog (so to speak) of new symbolic methodologies along with rigorous mathematical and statistical foundational work for these methods."], ["Learning With Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", null], ["Learning Kernel Classifiers", null], ["Statistical Methods in Bioinformatics: An Introduction", null], ["Statistics in the 21st Century.", null], ["Interpreting Quantitative Data", null], ["Statistical Decision Theory", null], ["Binomial Distribution Handbook for Scientists and Engineers", null], ["Maximum Penalized Likelihood Estimation, Vol. I: Density Estimation", null], ["A User's Guide to Measure-Theoretic Probability", null], ["Risk Management: Value at Risk and Beyond", null], ["Matrix Algebra and Its Applications to Statistics and Econometrics", null], ["Matrix Variate Distributions", null], ["Multilevel Analysis: Techniques and Applications", null], ["Sequential Monte Carlo Methods in Practice", null], ["Numerical Methods of Statistics", null], ["Telegraphic Reviews", null], ["Corrections", null], ["Statistics", "New technologies benefit our lives in many ways, but they can also bring increased risks, disruptions to our society, and even the malevolence of war. Statisticians can play a critical role in influencing the paths along which technology will take our society. But taking on this role requires changing our discipline, our profession, and our ASA. Change, however, is not an option; if we do not change, then technology may force change on us in detrimental ways. Understanding how to change requires a broader view of statistics as a human activity with a human purpose that must evolve within a social, or human, system. Considered as a technology, statistics is a fundamental and invaluable part of the infrastructure of other sciences. Statistics advances discoveries in other sciences. Universities and foundations must encourage interdisciplinary research as a primary contribution to our field. Our curricula must reflect modern approaches that other sciences require, and students with quantitative talents must be attracted from other disciplines. Statistics should permeate the mathematics curricula at all elementary and secondary levels, and all children should understand variability and uncertainty, how to make sense from data, and the elements involved in making decisions. In industry, we must capitalize on the importance of quality improvement programs to further contributions of statistics. From government, we need timely and accurate statistics that are relevant to the world of the future as well as to today. In particular, we must commit ourselves now to changing our 40-year-old poverty measure and improve access to government statistics with different and stronger ways to protect confidentiality. We must work toward a comprehensive, integrated network of knowledge and information systems for research on individual, social, and organizational change and for decision making by individuals, organizations, and public policy makers at all levels\u2014local, regional, and national. In all of these directions, the ASA must foster and encourage change and, in many cases, lead the way. With the commitment and perseverance of our next generation, statistics can become a leader in the advancement of science and technology to promote human welfare."], ["Selection of Related Multivariate Means", "Quantitative performance monitoring of health care providers has become an increasingly prominent activity in health care delivery. As a consequence, multidimensional \u201creport cards\u201d comparing the quality of services provided by diverse health care organizations have begun to appear. We present a case study examining the performance of 22 geographically defined health care systems, known as Veterans Integrated Service Networks; involving the treatment of more than 37,000 veterans discharged from general psychiatry program beds in fiscal year 1998. We describe multilevel multidimensional latent variable models and fit these to a set of related mixed discrete and continuous outcomes to identify networks providing inadequate care to its patients. We use validation sample to assess model fit by comparing observed statistics to the distribution of the statistics across replicated datasets. We propose using posterior tail probabilities associated with functions of the latent variables to select networks with poor outcomes, and finally compare our results with those from current methods."], ["The Selection or Seeding of College Basketball or Football Teams for Postseason Competition", "Systems for ranking college basketball or football teams take many forms, ranging from polls of selected coaches or members of the media to so-called computer-ranking systems. Some of these are used in ways that have considerable impact on the teams. The committee responsible for the selection and seeding of teams for the postseason National Collegiate Athletic Association (NCAA) Division I men's basketball tournament is influenced by various rankings, including ones based on the ratings percentage index (RPI). The Bowl Championship Series (BCS) rankings of NCAA Division I-A football teams determine which two teams compete in a postseason national championship game and determine eligibility for other prestigious postseason games. There are certain attributes that seem desirable in any ranking system to be used in selecting or seeding teams for postseason competition or that may have some other tangible or intangible effect on the teams. These attributes include accuracy, appropriateness, impartiality, unobtrusiveness, nondisruptiveness, verifiability, and comprehensibility. The polls, the RPI, and the BCS rankings are notably deficient in several of these attributes. A system having all of the attributes, except for unobtrusiveness, can be achieved by applying least squares to a statistical model in which the expected difference in score in each game is modeled as a difference in team effects plus or minus a home court/field advantage. The potential obtrusiveness of this approach can be circumvented by introducing modifications to reward winning per se and to eliminate any incentive for \u201crunning up the score\u201d or for deliberately surrendering a lead so as to extend a game into overtime. The modified least squares system was applied to the 1999\u20132000 basketball and 1999\u20132001 football seasons. Its accuracy in predicting the outcomes of 73 postseason football games and 93 postseason basketball games was undiminished by the modifications and was comparable to that of the betting line."], ["Bayesian Modeling of Markers of Day-Specific Fertility", "Cervical mucus hydration increases during the fertile interval before ovulation. Because sperm can only penetrate mucus having a high water content, cervical secretions provide a reliable marker of the fertile days of the menstrual cycle. This article develops a Bayesian approach for modeling of daily observations of cervical mucus and applies the approach to assess heterogeneity among women and cycles from a given woman with respect to the increase in mucus hydration during the fertile interval. The proposed model relates the mucus observations to an underlying normal mucus hydration score, which varies relative to a peak hydration day. Uncertainty in the timing of the peak is accounted for, and a novel weighted mixture model is used to characterize heterogeneity in distinct features of the underlying mean function. Prior information on the mucus hydration trajectory is incorporated, and a Markov chain Monte Carlo approach is developed. Based on data from a study of daily fecundability, there appears to be substantial heterogeneity among women in detected preovulatory increases in mucus hydration, but only minimal differences among cycles from a given woman."], ["Estimating Vaccine Efficacy From Secondary Attack Rates", "Epidemiologists have used secondary attack rates (SARs) to estimate the protective effects of vaccination since the 1930s. SARs can also be used to estimate the effect of vaccination on reducing infectiousness in breakthrough cases. The conventional SAR approach has been to pool the denominators and numerators across transmission units, then to use a confidence interval for a simple relative risk. We demonstrate appropriate model-based methods to estimate vaccine efficacy (VE) from SARs using generalized estimating equations taking correlation within transmission units into account. The model-based procedures require transformation of the parameter estimates to the SAR scale to obtain vaccine efficacy estimates. Appropriate confidence intervals are then based on the bootstrap, with resampling done by transmission unit. We show that the usual confidence intervals are too narrow. We estimated the effect of pertussis vaccination on person-to-person transmission. The results show that pertussis vaccination reduces the ability of a breakthrough clinical case to produce other clinical cases. The methods can be used in evaluating VE for susceptibility and infectiousness from SARs in other infectious diseases."], ["The Impact of Prior Distributions for Uncontrolled Confounding and Response Bias", "This article examines the potential for misleading inferences from conventional analyses and sensitivity analyses of observational data, and describes some proposed solutions based on specifying prior distributions for uncontrolled sources of bias. The issues are illustrated in a sensitivity analysis of confounding in a study of residential wire code and childhood leukemia and in a pooled analysis of 12 studies of magnetic-field measurements and childhood leukemia. Both analyses have been interpreted as evidence in favor of a causal effect of magnetic fields on leukemia risk. This interpretation is contrasted with results from analyses based on prior distributions for the unidentified bias parameters used in the original sensitivity-analysis model. These analyses indicate that accounting for uncontrolled confounding and response bias under a reasonable prior can substantially alter inferences about the existence of a magnetic-field effect. More generally, analyses with informative priors for unidentified bias parameters can help avoid misinterpretation of conventional results and ordinary sensitivity analyses."], ["Discovery of Conserved Sequence Patterns Using a Stochastic Dictionary Model", null], ["Robust Indirect Inference", null], ["Edge Detection, Spatial Smoothing, and Image Reconstruction With Partially Observed Multivariate Data", "Situations with incomplete multivariate spatial data on a lattice are considered. The goal is to impute the missing data in the presence of edges or boundaries and recover the image. Two methods based on Bayesian hierarchical models that iterate between edge detection and spatial smoothing to impute the missing data within identified homogeneous regions are examined. Their performance is compared with another method that imputes the missing values using edge-preserving spatial smoothers with locally varying weights. The performances of the three methods are compared on artificial and real datasets. It is seen that information from the multivariate data is critical in recovering the images. An application with color images where only one of three primary colors (red, green, or blue) is observed at each pixel is used to illustrate the results."], ["Estimates of Regression Coefficients Based on Lift Rank Covariance Matrix", null], ["Dimension Reduction for Multivariate Response Data", "This article concerns the analysis of multivariate response data with multivariate regressors. Methods for reducing the dimensionality of response variables are developed, with the goal of preserving as much regression information as possible. We note parallels between this goal and the goal of sliced inverse regression, which intends to reduce the regressor dimension in a univariate regression while preserving as much regression information as possible. A detailed discussion is given for the case where the response is a curve measured at fixed points. The problem in this setting is to select basis functions for fitting an aggregate of curves. We propose that instead of focusing on goodness of fit, attention should be shifted to the problem of explaining the variation of the curves in terms of the regressor variables. A data-adaptive basis searching method based on dimension reduction theory is proposed. Simulation results and an application to a climatology problem are given."], ["Alternating Subspace-Spanning Resampling to Accelerate Markov Chain Monte Carlo Simulation", "This article provides a simple method to accelerate Markov chain Monte Carlo sampling algorithms, such as the data augmentation algorithm and the Gibbs sampler, via alternating subspace-spanning resampling (ASSR). The ASSR algorithm often shares the simplicity of its parent sampler but has dramatically improved efficiency. The methodology is illustrated with Bayesian estimation for analysis of censored data from fractionated experiments. The relationships between ASSR and existing methods are also discussed."], ["A Reexamination of Diffusion Estimators With Applications to Financial Model Validation", "Time-homogeneous diffusion models have been widely used for describing the stochastic dynamics of the underlying economic variables. Recently, Stanton proposed drift and diffusion estimators based on a higher-order approximation scheme and kernel regression method. He claimed that \u201chigher order approximations must outperform lower order approximations\u201d and concluded nonlinearity in the instantaneous return function of short-term interest rates. To examine the impact of higher-order approximations, we develop general and explicit formulas for the asymptotic behavior of both drift and diffusion estimators. We show that these estimators will reduce the numerical approximation errors in asymptotic biases, but their asymptotic variances escalate nearly exponentially with the order of approximation. Simulation studies also confirm our asymptotic results. This variance inflation problem arises not only from nonparametric fitting, but also from parametric fitting. Stanton's work also postulates the interesting question of whether the short-term rate drift is nonlinear. Based on empirical simulation studies, Chapman and Pearson suggested that the nonlinearity might be spurious, due partially to the boundary effect of kernel regression. This prompts us to use the local linear fit based on the first-order approximation, proposed by Fan and Yao, to ameliorate the boundary effect and to construct formal tests of parametric financial models against the nonparametric alternatives. Our simulation results show that the local linear method indeed outperforms the kernel approach. Furthermore, our nonparametric \u201cgeneralized likelihood ratio tests\u201d are indeed versatile and powerful in detecting nonparametric alternatives. Using this formal testing procedure, we show that the evidence against the linear drift of the short-term interest rates is weak, whereas evidence against a family of popular models for the volatility function is very strong. Application to Standard & Poor 500 data is also illustrated."], ["On Additive Conditional Quantiles With High-Dimensional Covariates", "We investigate the estimation of the conditional quantile when many covariates are involved. In particular, we model the conditional quantile of a response as a nonlinear additive function of relevant covariates. For this setup, we propose a nonparametric smoother to estimate the unknown functions. The estimator provides direct computation of the nonlinear functions. Because it does not require any iteration, the estimator allows fast and routine data analysis. On the theoretical front, we also show asymptotic properties of the estimator, including mean squared error and limiting distribution. The theory confirms that for moderate dimension of the covariates, the estimator escapes the \u201ccurse of dimensionality\u201d problem. Both simulated and real data examples are provided to illustrate the methodology."], ["Distribution of Rankings for Groups Exhibiting Heteroscedasticity and Correlation", "In one-way analysis of variance, a main interest is in differences among the groups that comprise the population. For a given parameter, such as a mean value, data yield parameter estimates for each group, as well as group rankings based on these statistics. Here ranking probabilities are studied under the assumption that parameter estimates are well approximated by a normal distribution, either in finite samples or asymptotically, with possible intergroup heteroscedasticity and correlation. Particular interest lies in the ranking distribution as a descriptor of experiments on some future dataset. Examples include contract bidding, global economic competition, and rival contests in mating. Ranking distributions are analytically complicated, yet some interesting properties can be derived for them via the symmetry and elliptical geometry of the normal distribution. Some relationships between ranking probabilities and group parameters are described, with attention given to the role of between-group heterogeneity and correlation. For the ranking probabilities, estimators and asymptotically valid standard errors formulas and hypothesis tests are proposed. Simulation is used to describe the sample size needed for accurate asymptotic approximation, and the methods are illustrated with an economic example."], ["A Pseudoscore Estimator for Regression Problems With Two-Phase Sampling", "Two-phase stratified sampling designs yield efficient estimates of population parameters in regression models while minimizing the costs of data collection. In measurement error problems, for example, error-free covariates are ascertained only for units selected in a validation sample. Estimators proposed heretofore for such designs require all units to have positive probability of being selected. We describe a new semiparametric estimator that relaxes this assumption and that is applicable to, for example, case-only or control-only validation sampling for binary regression problems. It uses a weighted empirical covariate distribution, with weights determined by the regression model, to estimate the score equations. Implementation is relatively easy for both discrete and continuous outcome data. For designs that are amenable to alternative methods, simulation studies show that the new estimator outperforms the currently available weighted and pseudolikelihood methods and often achieves efficiency comparable to that of semiparametric maximum likelihood. The simulations also demonstrate the vulnerability of the case-only or control-only designs to model misspecification. These results are illustrated by the analysis of data from a population-based case-control study of leprosy."], ["Asymptotic Variance and Convergence Rates of Nearly-Periodic Markov Chain Monte Carlo Algorithms", "This article considers nearly-periodic Markov chains that may have excellent functional estimation properties but poor distributional convergence rate. It shows how simple modifications of the chain (involving using a random number of iterations) can greatly improve the distributional convergence of the chain. Various theoretical results about convergence rates of the modified chains are proven. A number of examples, including a transdimensional Markov chain Monte Carlo example, a card-shuffling example, and several antithetic Metropolis algorithms, are considered."], ["Doubly Smoothed EM Algorithm for Statistical Inverse Problems", null], ["Functional Inference in Frailty Measurement Error Models for Clustered Survival Data Using the SIMEX Approach", "We consider frailty models for clustered survival data in the presence of measurement errors in covariates. We first show that when the measurement error is accounted for in a full likelihood analysis but the distribution of the unobserved covariate is misspecified, the maximum likelihood estimators are asymptotically biased, especially for the variance component, whose bias can be substantial. We then discuss making inference using functional estimation via the SIMEX method where no distribution of the unobserved error-prone covariate is assumed. The SIMEX method is easy to implement by repeatedly fitting standard frailty models. We study the asymptotic properties of the SIMEX estimates and show that they are consistent and asymptotically normal. In simulation studies, we compare the SIMEX method and the likelihood method in terms of efficiency and robustness. We also propose a SIMEX score test for the variance component to test for the within-cluster correlation and evaluate its performance through simulation studies. The SIMEX variance component score test does not require specifying distributions for the random effect and the unobserved error-prone covariate, and is easy to implement by repeatedly fitting standard Cox models. The proposed methods are illustrated using the Kenya parasitemia data."], ["On Optimality Properties of the Power Prior", null], ["Generalized Autoregressive Moving Average Models", "A class of generalized autoregressive moving average (GARMA) models is developed that extends the univariate Gaussian ARMA time series model to a flexible observation-driven model for non-Gaussian time series data. The dependent variable is assumed to have a conditional exponential family distribution given the past history of the process. The model estimation is carried out using an iteratively reweighted least squares algorithm. Properties of the model, including stationarity and marginal moments, are either derived explicitly or investigated using Monte Carlo simulation. The relationship of the GARMA model to other models is shown, including the autoregressive models of Zeger and Qaqish, the moving average models of Li, and the reparameterized generalized autoregressive conditional heteroscedastic GARCH model (providing the formula for its fourth marginal moment not previously derived). The model is demonstrated by the application of the GARMA model with a negative binomial conditional distribution to a well-known time series dataset of poliomyelitis counts."], ["Marginal Likelihood and Bayes Factors for Dirichlet Process Mixture Models", "We present a method for comparing semiparametric Bayesian models, constructed under the Dirichlet process mixture (DPM) framework, with alternative semiparameteric or parameteric Bayesian models. A distinctive feature of the method is that it can be applied to semiparametric models containing covariates and hierarchical prior structures, and is apparently the first method of its kind. Formally, the method is based on the marginal likelihood estimation approach of Chib (1995) and requires estimation of the likelihood and posterior ordinates of the DPM model at a single high-density point. An interesting computation is involved in the estimation of the likelihood ordinate, which is devised via collapsed sequential importance sampling. Extensive experiments with synthetic and real data involving semiparametric binary data regression models and hierarchical longitudinal mixed-effects models are used to illustrate the implementation, performance, and applicability of the method."], ["Outlier Detection and False Discovery Rates for Whole-Genome DNA Matching", null], ["Statistics of the Galaxy Distribution", null], ["Statistical Methods for the Analysis of Repeated Measurements", null], ["Concise Handbook of Experimental Methods for the Behavioral and Biological Sciences", null], ["Topics in Optimal Design", null], ["Applied Statistics in the Pharmaceutical Industry With Case Studies Using S-PLUS", null], ["Design and Analysis of Bioavailability and Bioequivalence Studies", null], ["Biostatistical Methods in Epidemiology", null], ["Nonlinear Models in Medical Statistics", null], ["Principles of Data Mining", null], ["Graphical Models: Methods for Data Analysis and Mining", null], ["Conducting Meta Analysis Using SAS", null], ["From Elementary Probability to Stochastic Differential Equations With MAPLE", null], ["Time Series Analysis by State-Space Methods", null], ["Bayesian Statistical Modelling", null], ["Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis", null], ["Advanced Linear Modeling", null], ["Permutation Methods: A Distance Function Approach", null], ["Statistical Inference: An Integrated Approach", null], ["Telegraphic Reviews", null], ["Letter to the Editor", null]]}