{"2008": [["Statistics: Harnessing the Power of Information", " Statisticians are harnessing the power of information buried in today\u2019s massive and complex data. High-dimensional data sets, often the result of combining multiple databases, have led to important challenges in mathematical statistics. The increasing quantity of high-dimensional complex data and the ability of statisticians to uncover the information in it have led to ever greater demand for their expertise. For example, many businesses seek to build analytic capabilities, and statisticians are essential to this effort. To meet the increased demand, the academic community has been called on to provide more statistics courses, faculty, and graduates. Although statisticians who can meet these demands of modern data are highly sought after, the employment prospects for statisticians are not easy to track, because the term \u201cstatistician\u201d often is not used for relevant positions. However, the trend is unmistakable for statisticians who can unlock the power of these new data sets. Statistics graduate students are uncovering the value of information in data for local government and community nonprofit organizations through Statistics in the Community (STATCOM), an American Statistical Association student volunteer organization for pro bono statistics. STATCOM provides statistics students with real-world experiences while providing invaluable community assistance. "], ["Can Nonrandomized Experiments Yield Accurate Answers? A Randomized Experiment Comparing Random and Nonrandom Assignments", " Please see the online supplements for a Letter to the Editor. "], ["Comment", null], ["Comment", null], ["Comment: The Design and Analysis of Gold Standard Randomized Experiments", null], ["Rejoinder", null], ["Branching Processes as Models of Progenitor Cell Populations and Estimation of the Offspring Distributions", " We consider two new models of reducible age-dependent branching processes with emigration in conjunction with estimation problems arising in cell biology. Methods of statistical inference are developed using the relevant embedded discrete branching structure. Based on observations of the branching process with emigration, estimators of the offspring probabilities are proposed for the hidden unobservable process without emigration, which is of prime interest to investigators. The problem under consideration is motivated by experimental data generated by time-lapse video recording of cultured cells, which provides abundant information on their individual evolutions and thus on the basic parameters of their life cycle in tissue culture. Some parameters, such as the mean and variance of the mitotic cycle time, can be estimated nonparametrically without resorting to any mathematical model of cell population kinetics. For other parameters, such as the offspring distribution, a model-based inference is needed. Age-dependent branching processes have proven to be useful models for that purpose. A special feature of the data generated by time-lapse experiments is the presence of censoring effects due to the migration of cells out of the field of observation. For the time-to-event observations, such as the mitotic cycle time, the effects of data censoring can be accounted for by standard methods of survival analysis. No methods are available to accommodate such effects in the statistical inference on the offspring distribution. Within the framework of branching processes, the loss of cells to follow-up can be modeled as a process of emigration. Incorporating the emigration process into a pertinent branching model of cell evolution provides the basis for the proposed estimation techniques. Statistical inference on the offspring distribution is illustrated with an application to the development of oligodendrocytes in cell culture. "], ["Comparing Stochastic Optimization Methods for Variable Selection in Binary Outcome Prediction, With Application to Health Policy", null], ["Records in Athletics Through Extreme-Value Theory", null], ["Does Finasteride Affect the Severity of Prostate Cancer? A Causal Sensitivity Analysis", " In 2003 Thompson and colleagues reported that daily use of finasteride reduced the prevalence of prostate cancer by 25% compared to placebo. These results were based on the double-blind randomized Prostate Cancer Prevention Trial (PCPT), which followed 18,882 men with no prior or current indications of prostate cancer annually for 7 years. Enthusiasm for the risk reduction afforded by the chemopreventative agent and adoption of its use in clinical practice, however, was severely dampened by the additional finding in the trial of an increased absolute number of high-grade (Gleason score \u2265 7) cancers on the finasteride arm. The question arose as to whether this finding truly implied that finasteride increased the risk of more severe prostate cancer or was a study artifact due to a series of possible postrandomization selection biases, including differences among treatment arms in patient characteristics of cancer cases, differences in biopsy verification of cancer status due to increased sensitivity of prostate-specific antigen under finasteride, differential grading by biopsy due to prostate volume reduction by finasteride, and nonignorable dropout. Via a causal inference approach implementing inverse probability weighted estimating equations, this analysis addresses the question of whether finasteride caused more severe prostate cancer by estimating the mean treatment difference in prostate cancer severity between finasteride and placebo for the principal stratum of participants who would have developed prostate cancer regardless of treatment assignment. We perform sensitivity analyses that sequentially adjust for the numerous potential postrandomization biases conjectured in the PCPT. "], ["Domain-Level Covariance Analysis for Multilevel Survey Data With Structured Nonresponse", " Health care quality surveys in the United States are administered to individual respondents (i.e., hospital patients, health plan members) to evaluate the performance of health care organizations (i.e., hospitals, health plans), which thus constitute estimation domains. For better understanding and more parsimonious reporting of dimensions of quality, we analyze relationships among quality measures at the domain level. Rather than specifying a full parametric model for the observed responses and the nonresponse patterns at the lower (patient) level, we first fit generalized variance\u2013covariance functions that take into account nonresponse patterns in the survey responses, then specify a likelihood function for the domain mean responses using these generalized variance\u2013covariance functions. This allows us to model directly the relationships among domain means for different items. Because the response scales are bounded, we assume that these means follow a truncated multivariate normal distribution. We calculate maximum likelihood (ML) estimates using the EM algorithm and sample under Bayesian models using Markov chain Monte Carlo. Finally, we perform factor analysis on the estimated or sampled between-domain covariance matrixes. Using posterior draws, we assess posterior distributions of the number of selected factors and the assignment of items to groups under conventional rules. We compare ML estimates of this factor structure with those from several Bayesian models with different prior distributions for the between-domain covariance. We present analyses of data from the Consumer Assessment of Healthcare Providers and Systems (CAHPS) survey of Medicare Advantage health plans. "], ["The Dynamics of Economic Functions: Modeling and Forecasting the Yield Curve", null], ["High-Dimensional Sparse Factor Modeling: Applications in Gene Expression Genomics", " We describe studies in molecular profiling and biological pathway analysis that use sparse latent factor and regression models for microarray gene expression data. We discuss breast cancer applications and key aspects of the modeling and computational methodology. Our case studies aim to investigate and characterize heterogeneity of structure related to specific oncogenic pathways, as well as links between aggregate patterns in gene expression profiles and clinical biomarkers. Based on the metaphor of statistically derived \u201cfactors\u201d as representing biological \u201csubpathway\u201d structure, we explore the decomposition of fitted sparse factor models into pathway subcomponents and investigate how these components overlay multiple aspects of known biological activity. Our methodology is based on sparsity modeling of multivariate regression, ANOVA, and latent factor models, as well as a class of models that combines all components. Hierarchical sparsity priors address questions of dimension reduction and multiple comparisons, as well as scalability of the methodology. The models include practically relevant non-Gaussian/nonparametric components for latent structure, underlying often quite complex non-Gaussianity in multivariate expression patterns. Model search and fitting are addressed through stochastic simulation and evolutionary stochastic search methods that are exemplified in the oncogenic pathway studies. Supplementary supporting material provides more details of the applications, as well as examples of the use of freely available software tools for implementing the methodology. "], ["Hierarchical Insurance Claims Modeling", null], ["A Case Study in Pharmacologic Colon Imaging Using Principal Curves in Single-Photon Emission Computed Tomography", " In this article we are concerned with functional imaging of the colon to assess the kinetics of microbicide lubricants. The overarching goal is to understand the distribution of the lubricants in the colon. Such information is crucial for understanding the potential impact of microbicides on human immunodeficiency virus transmission. The experiment was conducted by imaging a radiolabeled lubricant distributed in the subject\u2019s colon. The tracer imaging was conducted via single-photon emission computed tomography, a noninvasive, in vivo functional imaging technique. We have developed a novel principal curve algorithm to construct a three-dimensional curve through the colon images. The algorithm was tested and debugged on several difficult two-dimensional images of familiar curves where the original principal curve algorithm does not apply. The final curve fit to the colon data is compared with experimental sigmoidoscope collection. "], ["Multiple Inference and Gender Differences in the Effects of Early Intervention: A Reevaluation of the Abecedarian, Perry Preschool, and Early Training Projects", null], ["Assessing the Effect of Selection at the Amino Acid Level in Malaria Antigen Sequences Through Bayesian Generalized Linear Models", null], ["Bayesian Inference on Changes in Response Densities Over Predictor Clusters", " In epidemiology, it often is of interest to assess how individuals with different trajectories over time in an environmental exposure or biomarker differ with respect to a continuous response. For ease in interpretation and presentation of results, epidemiologists typically categorize predictors before analysis. To extend this approach to time-varying predictors, individuals can be clustered by their predictor trajectory, with the cluster index included as a predictor in a regression model for the response. This article develops a semiparametric Bayes approach that avoids assuming a prespecified number of clusters and allows the response to vary nonparametrically over predictor clusters. This methodology is motivated by interest in relating trajectories in weight gain during pregnancy to the distribution of birth weight adjusted for gestational age at delivery. In this setting, the proposed approach allows the tails of the birth weight density to vary flexibly over weight gain clusters. "], ["A Few Remarks on \u201cA Capture\u2013Recapture Approach for Screening Using Two Diagnostic Tests With Availability of Disease Status for the Test Positives Only\u201d by B\u00f6hning and Patilea", null], ["Semiparametric Estimation of Covariance Matrixes for Longitudinal Data", " Estimation of longitudinal data covariance structure poses significant challenges because the data usually are collected at irregular time points. A viable semiparametric model for covariance matrixes has been proposed that allows one to estimate the variance function nonparametrically and to estimate the correlation function parametrically by aggregating information from irregular and sparse data points within each subject. But the asymptotic properties of the quasi-maximum likelihood estimator (QMLE) of parameters in the covariance model are largely unknown. We address this problem in the context of more general models for the conditional mean function, including parametric, nonparametric, or semiparametric. We also consider the possibility of rough mean regression function and introduce the difference-based method to reduce biases in the context of varying-coefficient partially linear mean regression models. This provides a more robust estimator of the covariance function under a wider range of situations. Under some technical conditions, consistency and asymptotic normality are obtained for the QMLE of the parameters in the correlation function. Simulation studies and a real data example are used to illustrate the proposed approach. "], ["Functional Additive Models", " In commonly used functional regression models, the regression of a scalar or functional response on the functional predictor is assumed to be linear. This means that the response is a linear function of the functional principal component scores of the predictor process. We relax the linearity assumption and propose to replace it by an additive structure, leading to a more widely applicable and much more flexible framework for functional regression models. The proposed functional additive regression models are suitable for both scalar and functional responses. The regularization needed for effective estimation of the regression parameter function is implemented through a projection on the eigenbasis of the covariance operator of the functional components in the model. The use of functional principal components in an additive rather than linear way leads to substantial broadening of the scope of functional regression models and emerges as a natural approach, because the uncorrelatedness of the functional principal components is shown to lead to a straightforward implementation of the functional additive model, based solely on a sequence of one-dimensional smoothing steps and without the need for backfitting. This facilitates the theoretical analysis, and we establish the asymptotic consistency of the estimates of the components of the functional additive model. We illustrate the empirical performance of the proposed modeling framework and estimation methods through simulation studies and in applications to gene expression time course data. "], ["Covariance Tapering for Likelihood-Based Estimation in Large Spatial Data Sets", null], ["Variable Selection in Nonparametric Varying-Coefficient Models for Analysis of Repeated Measurements", " Nonparametric varying-coefficient models are commonly used for analyzing data measured repeatedly over time, including longitudinal and functional response data. Although many procedures have been developed for estimating varying coefficients, the problem of variable selection for such models has not been addressed to date. In this article we present a regularized estimation procedure for variable selection that combines basis function approximations and the smoothly clipped absolute deviation penalty. The proposed procedure simultaneously selects significant variables with time-varying effects and estimates the nonzero smooth coefficient functions. Under suitable conditions, we establish the theoretical properties of our procedure, including consistency in variable selection and the oracle property in estimation. Here the oracle property means that the asymptotic distribution of an estimated coefficient function is the same as that when it is known a priori which variables are in the model. The method is illustrated with simulations and two real data examples, one for identifying risk factors in the study of AIDS and one using microarray time-course gene expression data to identify the transcription factors related to the yeast cell-cycle process. "], ["Parameter Estimation for Differential Equation Models Using a Framework of Measurement Error in Regression Models", " Differential equation (DE) models are widely used in many scientific fields, including engineering, physics, and biomedical sciences. The so-called \u201cforward problem,\u201d the problem of simulations and predictions of state variables for given parameter values in the DE models, has been extensively studied by mathematicians, physicists, engineers, and other scientists. However, the \u201cinverse problem,\u201d the problem of parameter estimation based on the measurements of output variables, has not been well explored using modern statistical methods, although some least squares\u2013based approaches have been proposed and studied. In this article we propose parameter estimation methods for ordinary differential equation (ODE) models based on the local smoothing approach and a pseudo\u2013least squares (PsLS) principle under a framework of measurement error in regression models. The asymptotic properties of the proposed PsLS estimator are established. We also compare the PsLS method to the corresponding simulation-extrapolation (SIMEX) method and evaluate their finite-sample performances via simulation studies. We illustrate the proposed approach using an application example from an HIV dynamic study. "], ["Bandwidth Selection in Nonparametric Kernel Testing", " We propose a sound approach to bandwidth selection in nonparametric kernel testing. The main idea is to find an Edgeworth expansion of the asymptotic distribution of the test concerned. Due to the involvement of a kernel bandwidth in the leading term of the Edgeworth expansion, we are able to establish closed-form expressions to explicitly represent the leading terms of both the size and power functions and then determine how the bandwidth should be chosen according to certain requirements for both the size and power functions. For example, when a significance level is given, we can choose the bandwidth such that the power function is maximized while the size function is controlled by the significance level. Both asymptotic theory and methodology are established. In addition, we develop an easy implementation procedure for the practical realization of the established methodology and illustrate this on two simulated examples and a real data example. "], ["Nonparametric Quantile Estimations for Dynamic Smooth Coefficient Models", null], ["Nonparametric Variable Selection: The EARTH Algorithm", null], ["On Adaptive Extensions of Group Sequential Trials for Clinical Investigations", " In group sequential trials, it is important to obtain adequate data to assess overall benefits and risks of an experimental treatment for patients. To achieve this goal, we provide a general, formal framework for adaptively extending a group sequential trial to stop at any interim analysis time, often after a significance boundary for a clinical endpoint is crossed. For statistical inference, we propose to order the sample space by a class of well-ordered group sequential tests. On this basis, we develop a unified sequential statistical inference approach that is applicable to both interim monitoring and final analysis. We also show that the new ordering provides the foundation for the repeated confidence intervals procedure of Jennison and Turnbull. "], ["A Multiple-Index Model and Dimension Reduction", " Dimension reduction can be used as an initial step in statistical modeling. Further specification of model structure is imminent and important when the reduced dimension is still greater than 1. In this article we investigate one method of specification that involves separating the linear component from the nonlinear components, leading to further dimension reduction in the unknown link function and, thus, better estimation and easier interpretation of the model. The specified model includes the popular econometric multiple-index model and the partially linear single-index model as its special cases. A criterion is developed to validate the model specification. An algorithm is proposed to estimate the model directly. Asymptotic distributions for the estimators of the parameters and the nonparametric link function are derived. Air pollution data in Chicago are used to illustrate the modeling procedure and to demonstrate its advantages over the existing dimension reduction approaches. "], ["Optimal Crossover Designs for Two Treatments in the Presence of Mixed and Self-Carryover Effects", " The main purpose of this article is to identify optimal crossover designs for two treatments under a model that includes mixed and self carryover effects. In addition, results are reported for optimal two-treatment crossover designs under several closely related models, and the performance of various designs for three and four periods is studied under the different models. "], ["Model Selection Criteria for Missing-Data Problems Using the EM Algorithm", null], ["Properties and Implementation of Jeffreys\u2019s Prior in Binomial Regression Models", null], ["Smoothly Clipped Absolute Deviation on High Dimensions", " The smoothly clipped absolute deviation (SCAD) estimator, proposed by Fan and Li, has many desirable properties, including continuity, sparsity, and unbiasedness. The SCAD estimator also has the (asymptotically) oracle property when the dimension of covariates is fixed or diverges more slowly than the sample size. In this article we study the SCAD estimator in high-dimensional settings where the dimension of covariates can be much larger than the sample size. First, we develop an efficient optimization algorithm that is fast and always converges to a local minimum. Second, we prove that the SCAD estimator still has the oracle property on high-dimensional problems. We perform numerical studies to compare the SCAD estimator with the LASSO and SIS\u2013SCAD estimators in terms of prediction accuracy and variable selectivity when the true model is sparse. Through the simulation, we show that the variance estimator of Fan and Li still works well for some limited high-dimensional cases where the true nonzero coefficients are not too small and the sample size is moderately large. We apply the proposed algorithm to analyze a high-dimensional microarray data set. "], ["Order Selection in Finite Mixture Models With a Nonsmooth Penalty", " Order selection is a fundamental and challenging problem in the application of finite mixture models. We develop a new penalized likelihood approach that we call MSCAD. MSCAD deviates from information-based methods, such as Akaike information criterion and the Bayes information criterion, by introducing two penalty functions that depend on the mixing proportions and the component parameters. It is consistent in estimating both the order of the mixture model and the mixing distribution. Simulations show that MSCAD performs much better than some existing methods. Two real-data examples are examined to illustrate its performance. "], ["Correlation-Based Functional Clustering via Subspace Projection", " A correlation-based functional clustering method is proposed for grouping curves with similar shapes. A correlation between two random functions defined through the functional inner product is used as a similarity measure. Curves with similar shapes are embedded in the cluster subspace spanned by a mean shape function and eigenfunctions of the covariance kernel. The cluster membership prediction for each curve attempts to maximize the functional correlation between the observed and predicted curves via shape standardization and subspace projection among all possible clusters. The proposed method accounts for shape differentials through the functional multiplicative random-effects shape function model for each cluster, which regards random scales and intercept shifts as a nuisance. A consistent estimate is proposed for the random scale effect, whose sample variance estimate is also consistent. The derived identifiability conditions for the clustering procedure unravel the predictability of cluster memberships. Simulation studies and a real data example illustrate the proposed method. "], ["Multiply Robust Inference for Statistical Interactions", " A primary focus of an increasing number of scientific studies is to determine whether two exposures interact in the effect that they produce on an outcome of interest. Interaction is commonly assessed by fitting regression models in which the linear predictor includes the product between those exposures. When the main interest lies in the interaction, this approach is not entirely satisfactory, because it is prone to (possibly severe) bias when the main exposure effects or the associations between outcome and extraneous factors are misspecified. In this article we consider conditional mean models with identity or log link that postulate the statistical interaction in terms of a finite-dimensional parameter but are otherwise unspecified. We show that estimation of the interaction parameter often is not feasible in this model, because it requires nonparametric estimation of auxiliary conditional expectations given high-dimensional variables. We thus consider multiply robust estimation under a union model that assumes that at least one of several working submodels holds. Our approach is novel in that it makes use of information on the joint distribution of the exposures conditional on the extraneous factors in making inferences about the interaction parameter of interest. In the special case of a randomized trial or a family-based genetic study in which the joint exposure distribution is known by design or by Mendelian inheritance, the resulting multiply robust procedure leads to asymptotically distribution-free tests of the null hypothesis of no interaction on an additive scale. We illustrate the methods through simulation and analysis of a randomized follow-up study. "], ["Book Reviews", null], ["Corrections", null], ["2008 Editorial Collaborators", null], ["Index to Volume 103 (2008)", null], ["Multiple Model Evaluation Absent the Gold Standard Through Model Combination", null], ["Nonparametric Risk Management With Generalized Hyperbolic Distributions", null], ["War and Wages", "An instrument manipulates a treatment that it does not entirely control, but the instrument affects the outcome only indirectly through its manipulation of the treatment. The idealized prototype is the randomized encouragement design, in which subjects are randomly assigned to receive either encouragement to accept the treatment or no such encouragement, but not all subjects comply by doing what they are encouraged to do, and the situation is such that only the treatment itself, not disregarded encouragement alone, can affect the outcome. An instrument is weak if it has only a slight impact on acceptance of the treatment, that is, if most people disregard encouragement to accept the treatment. Typical applications of instrumental variables are not ideal; encouragement is not randomized, although it may be assigned in a far less biased manner than the treatment itself. Using the concept of design sensitivity, we study the sensitivity of instrumental variable analyses to departures from the ideal of random assignment of encouragement, with particular reference to the strength of the instrument. With these issues in mind, we reanalyze a clever study by Angrist and Krueger concerning the effects of military service during World War II on subsequent earnings, in which birth cohorts of very similar but not identical age were differently \u201cencouraged\u201d to serve in the war. A striking feature of this example is that those who served earned more, but the effect of service on earnings appears to be negative; that is, the instrumental variables analysis reverses the sign of the naive comparison. For expository purposes, this example has the convenient feature of enabling, by selecting different birth cohorts, the creation of instruments of varied strength, from extremely weak to fairly strong, although separated by the same time interval and thus perhaps similarly biased. No matter how large the sample size becomes, even if the effect under study is quite large, studies with weak instruments are extremely sensitive to tiny biases, whereas studies with stronger instruments can be insensitive to moderate biases."], ["Spatial Analysis to Quantify Numerical Model Bias and Dependence", "A limited number of complex numerical models that simulate the Earth's atmosphere, ocean, and land processes are the primary tool to study how climate may change over the next century due to anthropogenic emissions of greenhouse gases. A standard assumption is that these climate models are random samples from a distribution of possible models centered around the true climate. This implies that agreement with observations and the predictive skill of climate models will improve as more models are added to an average of the models. In this article we present a statistical methodology to quantify whether climate models are indeed unbiased and whether and where model biases are correlated across models. We consider the simulated mean state and the simulated trend over the period 1970\u20131999 for Northern Hemisphere summer and winter temperature. The key to the statistical analysis is a spatial model for the bias of each climate model and the use of kernel smoothing to estimate the correlations of biases across different climate models. The spatial model is particularly important to determine statistical significance of the estimated correlations under the hypothesis of independent climate models. Our results suggest that most of the climate model bias patterns are indeed correlated. In particular, climate models developed by the same institution have highly correlated biases. Also, somewhat surprisingly, we find evidence that the model skills for simulating the mean climate and simulating the warming trends are not strongly related."], ["A Bayesian Capture\u2013Recapture Population Model With Simultaneous Estimation of Heterogeneity", null], ["Stochastic Networks in Nanoscale Biophysics", "Advances in nanotechnology enable scientists for the first time to study biological processes on a nanoscale molecule-by-molecule basis. A surprising discovery from recent nanoscale single-molecule biophysics experiments is that biological reactions involving enzymes behave fundamentally differently from what classical theory predicts. In this article we introduce a stochastic network model to explain the experimental puzzles (by modeling enzymatic reactions as a stochastic network connected by different enzyme conformations). Detailed analyses of the model, including analyses of the first-passage-time distributions and goodness of fit, show that the stochastic network model is capable of explaining the experimental surprises. The model is analytically tractable and closely fits experimental data. The biological/chemical meaning of the model is discussed."], ["Breast Cancer Relative Hazard Estimates From Case\u2013Control and Cohort Designs With Missing Data on Mammographic Density", "We analyzed data from the Breast Cancer Detection Demonstration Project (BCDDP) to obtain multivariate relative hazard models for breast cancer that included mammographic density (MD) in addition to standard risk factors. Data from the BCDDP were collected from a stratified case\u2013control study in the screening phase (1973\u20131980) and from follow-up of three subcohorts in the follow-up phase (1980\u20131995). For both phases, MD measurements were only available for about half the women who developed breast cancer (cases) and a small fraction of noncases. We used a logistic regression model for the stratified case\u2013control study and developed a general pseudo-likelihood approach to accommodate missing covariate data (MD) by adapting the method of Scott and Wild and Breslow and Holubkov. We showed that this method was substantially more efficient than a previously proposed weighted-likelihood method. We assumed piecewise exponential models for the analysis of each subcohort, with the missing covariate (MD) distribution conditional on the observed information modeled with polytomous logistic regression. We developed an EM algorithm for estimation, which allowed for time-varying covariates, incomplete follow-up, and left truncation. We analyzed the three follow-up subcohorts separately and then combined the relative hazard models from the case\u2013control and cohort data. The final model included main effects for MD, weight, age at first live birth, number of previous breast biopsies, and number of sisters or mother with breast cancer and was more discriminating (higher concordance) than the original model of Gail et al., which included standard risk factors but not MD. In a separate work, we combined this relative hazard model with other data to project absolute breast cancer risk."], ["Assessing Identification Risk in Survey Microdata Using Log-Linear Models", "This article considers the assessment of the risk of identification of respondents in survey microdata, in the context of applications at the United Kingdom (UK) Office for National Statistics (ONS). The threat comes from the matching of categorical \u201ckey\u201c variables between microdata records and external data sources and from the use of log-linear models to facilitate matching. While the potential use of such statistical models is well established in the literature, little consideration has been given to model specification or to the sensitivity of risk assessment to this specification. In numerical work not reported here, we have found that standard techniques for selecting log-linear models, such as chi-squared goodness-of-fit tests, provide little guidance regarding the accuracy of risk estimation for the very sparse tables generated by typical applications at ONS, for example, tables with millions of cells formed by cross-classifying six key variables, with sample sizes of 10 or 100,000. In this article we develop new criteria for assessing the specification of a log-linear model in relation to the accuracy of risk estimates. We find that, within a class of \u201creasonable\u201c models, risk estimates tend to decrease as the complexity of the model increases. We develop criteria that detect \u201cunderfitting\u201c (associated with overestimation of the risk). The criteria may also reveal \u201coverfitting\u201c (associated with underestimation) although not so clearly, so we suggest employing a forward model selection approach. Our criteria turn out to be related to established methods of testing for overdispersion in Poisson log-linear models. We show how our approach may be used for both file-level and record-level measures of risk. We evaluate the proposed procedures using samples drawn from the 2001 UK Census where the true risks can be determined and show that a forward selection approach leads to good risk estimates. There are several \u201cgood\u201c models between which our approach provides little discrimination. The risk estimates are found to be stable across these models, implying a form of robustness. We also apply our approach to a large survey dataset. There is no indication that increasing the sample size necessarily leads to the selection of a more complex model. The risk estimates for this application display more variation but suggest a suitable upper bound."], ["Analysis of Smoking Cessation Patterns Using a Stochastic Mixed-Effects Model With a Latent Cured State", "We apply our methodology to the Alpha-Tocopherol, Beta-Carotene Cancer Prevention (ATBC) study, a large (29,133 participants) longitudinal cohort study. While ATBC was designed as a cancer prevention study, it contains unique information about the smoking status of each participant during every 4-month period of the study. These data are used to model smoking cessation patterns using a discrete-time stochastic mixed-effects model with three states: smoking, transient cessation, and permanent cessation (absorbent state). Random participant-specific transition probabilities among these states are used to account for participant-to-participant heterogeneity. Another important innovation in our article is to design computationally practical methods for dealing with the size of the dataset and complexity of the models. This is achieved using the marginal likelihood obtained by integrating over the Beta distribution of random effects."], ["Exploring Voting Blocs Within the Irish Electorate", "The methodology is used to explore data from two Irish elections. Data from eight opinion polls taken during the six weeks prior to the 1997 Irish presidential election are analyzed. These data reveal the evolution of the structure of the electorate during the election campaign. In addition, data that record the votes from the Dublin West constituency of the 2002 Irish general election are analyzed to reveal distinct voting blocs within the electorate; these blocs are characterized by party politics, candidate profile, and political ideology."], ["Imputing Risk Tolerance From Survey Responses", "Economic theory assigns a central role to risk preferences. This article develops a measure of relative risk tolerance using responses to hypothetical income gambles in the Health and Retirement Study. In contrast to most survey measures that produce an ordinal metric, this article shows how to construct a cardinal proxy for the risk tolerance of each survey respondent. The article also shows how to account for measurement error in estimating this proxy and how to obtain consistent regression estimates despite the measurement error. The risk tolerance proxy is shown to explain differences in asset allocation across households."], ["A Test for Partial Differential Expression", null], ["Election Forecasts Using Spatiotemporal Models", "There exists a large literature on the problem of forecasting election results. But none of the published methods take spatial information into account, although there is clear evidence of geographic trends. To fill this gap, we use geostatistical procedures to build a spatial model of voting patterns. We test the model in three close elections and find that it outperforms rival methods in current use. We apply kriging (a spatial model) and cokriging (in a spatiotemporal model version) to improve the accuracy of election night forecasts. We compare the results with actual outcomes and also to predictions made using models that use only historical data from polling stations in previous elections. Despite the apparent volatility leading up to the three elections in our study, the use of spatial information strongly improves the accuracy of the prediction. Compared with forecasts using historical data alone, the spatiotemporal models are better whenever the proportion of counted votes in the election night tally exceeds 5%%."], ["A Directional Model for the Estimation of the Rotation Axes of the Ankle Joint", "This article is motivated by the estimation of the directions of the two rotation axes of the ankle. These axes carry information on individual ankles; they are useful in the construction of biomechanical models and the treatment of orthopedic problems. In biomechanics, the rotation axes of the ankle often are estimated using optimization techniques. This work investigates a statistical model for carrying out the estimation. The data set for analysis is a time-ordered sequence of 3\u00d73 rotation matrixes giving the ankle's orientations as the foot moves with respect to the lower leg. These rotation matrixes are assumed to follow Fisher\u2013von Mises distributions. The predicted values for the observed rotations feature four angles for the orientation of two rotation axes and two series of time-varying rotation angles about the two axes. Maximum likelihood estimators of the parameters are derived. Approximations to their sampling distributions are obtained when the errors are clustered around the identity matrix. Sandwich variance estimators, accounting for autocorrelation in the errors, are proposed for the estimators of the two axes. An extension of the model that uses the translational motion in the estimation of the two axes is presented. Compared with the traditional biomechanical techniques for estimating the anatomic axes of the ankle, the proposed model has two advantages: It allows the calculation of standard errors for the estimates, and can distinguish the parameters whose estimation is affected by a limited range of motion about the two axes. This is illustrated by the estimation of the rotation axes of the ankles of two subjects."], ["Model-Independent Estimates of Dark Matter Distributions", null], ["Prediction of Protein Interdomain Linker Regions by a Nonstationary Hidden Markov Model", "Proteins have two types of regions: linker regions that connect active domains and nonlinker regions, which include domains and terminal residues. It is important to try to distinguish these regions using information in the sequence of amino acids that, when folded, form the protein. In this article we develop a nonstationary hidden Markov model to infer region boundaries. The approach is based on a latent variable defined on the sequence; this variable is a continuous index of the region type and affects the probabilities that specific amino acids will appear. We develop an efficient Bayesian estimate of the model using Markov chain Monte Carlo methods, particularly Gibbs sampling, to simulate the parameters from the posteriors. We apply our method to protein sequences with domains and interdomain linkers delineated using the Pfam-A database. The prediction results are superior to simpler methods. Importantly, our method estimates the probability that each amino acid belongs to a linker region, giving insight into the properties of interdomain linkers."], ["Modeling Price Dynamics in eBay Auctions Using Differential Equations", "Empirical research of online auctions has grown dramatically in recent years. Studies using publicly available bid data from such websites as eBay.com have found many divergences of bidding behavior and auction outcomes compared with ordinary offline auctions and auction theory. Among the main differences between online and offline auctions are the former's longer duration, anonymity of bidders and sellers, and low barriers of entry. All of these factors lead to dynamics in the bid arrival and price process that change throughout the auction. In this work we examine the price process in a large and diverse set of eBay auctions, for both low-and high-valued items, in terms of item, auction, bidder, and seller characteristics. We propose a family of differential equation models that captures online auction dynamics. In particular, we show that a second-order linear differential equation well approximates the dynamics that occur in our diverse set of auctions. We also introduce a multiple-comparisons test for comparing dynamic models of auction subpopulations, which we use to compare subpopulations of auctions grouped by characteristics of the auction, item, seller, and bidders. We find that price dynamics change throughout the auction and are influenced mostly by factors that affect the level of uncertainty about the outcome (e.g., seller rating, item condition) and the level of competitiveness (e.g., early bidding, number of bids). We accomplish the modeling tasks within the framework of principal differential analysis and functional data models."], ["Bayesian Treed Gaussian Process Models With an Application to Computer Modeling", "Motivated by a computer experiment for the design of a rocket booster, this article explores nonstationary modeling methodologies that couple stationary Gaussian processes with treed partitioning. Partitioning is a simple but effective method for dealing with nonstationarity. The methodological developments and statistical computing details that make this approach efficient are described in detail. In addition to providing an analysis of the rocket booster simulator, we show that our approach is effective in other arenas as well."], ["The Nested Dirichlet Process", "In multicenter studies, subjects in different centers may have different outcome distributions. This article is motivated by the problem of nonparametric modeling of these distributions, borrowing information across centers while also allowing centers to be clustered. Starting with a stick-breaking representation of the Dirichlet process (DP), we replace the random atoms with random probability measures drawn from a DP. This results in a nested DP prior, which can be placed on the collection of distributions for the different centers, with centers drawn from the same DP component automatically clustered together. Theoretical properties are discussed, and an efficient Markov chain Monte Carlo algorithm is developed for computation. The methods are illustrated using a simulation study and an application to quality of care in U.S. hospitals."], ["Combining Registration and Fitting for Functional Models", "A registration method can be defined as a process of aligning features of a sample of curves by monotone transformations of their domain. The aligned curves exhibit only amplitude variation, and the domain transformations, called warping functions, capture the phase variation in the original curves. In this article we precisely define a new type of registration process, in which the warping functions optimize the fit of a principal components decomposition to the aligned curves. The principal components are effectively the features that this process aligns. We discuss the relationship of registration to closure of a function space under convex operations, and define consistency for registration methods. We define an explicit decomposition of functional variation into amplitude and phase partitions, and develop an algorithm for combining registration with principal components analysis, and apply it to simulated and real data."], ["An Informational Measure of Association and Dimension Reduction for Multiple Sets and Groups With Applications in Morphometric Analysis", "In this article we propose a new general index that measures relationships between multiple sets of random vectors. This index is based on Kullback\u2013Leibler (KL) information, which measures linear or nonlinear dependence between multiple sets using joint and marginal densities of affine transformations of the random vectors. Estimates of the matrixes are obtained by maximizing the KL information and are shown to be consistent. The motivation for introducing such an index comes from morphological integration studies, a topic in biological science. As a special case of this index, we define an overall measure of association and two other measures for dimension reduction. The use of these measures is illustrated through real data analysis in morphometric studies and extensive simulations, and their performance is compared with that of approaches based on canonical correlation analysis. Extensions of the aforementioned measures to multiple groups are also discussed. In contrast to canonical correlation analysis, our general index not only provides an overall measure of association, but also determines nonlinear relationships, thereby making it useful in many other applications."], ["On a Projective Resampling Method for Dimension Reduction With Multivariate Responses", "Consider the dimension reduction problem where both the response and the predictor are vectors. Existing estimators of this problem take one of the following routes: (1) targeting the part of the dimension reduction space that is related to the conditional mean (or moments) of the response vector, (2) pooling the estimates for the marginal dimension reduction spaces, and (3) estimating the whole dimension reduction space directly by multivariate slicing. However, the first two approaches do not fully recover the dimension reduction space, and the third is hampered by the fact that the accuracy of estimators based on multivariate slicing drops sharply as the dimension of response increases\u2014a phenomenon often called the \u201ccurse of dimensionality.\u201d We propose a new method that overcomes both difficulties, in that it involves univariate slicing only and it is guaranteed to fully recover the dimension reduction space under reasonable conditions. The method will be compared with the existing estimators by simulation and applied to a dataset."], ["Estimating Equations Inference With Missing Data", "There is a large and growing body of literature on estimating equation (EE) as an estimation approach. One basic property of EE that has been universally adopted in practice is that of unbiasedness, and there are deep conceptual reasons why unbiasedness is a desirable EE characteristic. This article deals with inference from EEs when data are missing at random. The investigation is motivated by the observation that direct imputation of missing data in EEs generally leads to EEs that are biased and, thus, violates a basic assumption of the EE approach. The main contribution of this article is that it goes beyond existing imputation methods and proposes a procedure whereby one mitigates the effects of missing data through a reformulation of EEs imputed through a kernel regression method. These (modified) EEs then constitute a basis for inference by the generalized method of moments (GMM) and empirical likelihood (EL). Asymptotic properties of the GMM and EL estimators of the unknown parameters are derived and analyzed. Unlike most of the literature, which deals with missingness in either covariate values or response data, our method allows for missingness in both sets of variables. Another important strength of our approach is that it allows auxiliary information to be handled successfully. We illustrate the method using a well-known wormy-fruits dataset and data from a study on Duchenne muscular dystrophy detection and compare our results with several existing methods via a simulation study."], ["Partially Linear Additive Hazards Regression With Varying Coefficients", "To explore the nonlinear interactions between some covariates and an exposure variable, we propose the partially linear additive hazards model for survival data. In a semiparametric setting, we construct a local pseudoscore function to estimate the varying and constant coefficients and establish the asymptotic normality of the proposed estimators. Moreover, we develop the weak convergence property for the local estimator of the baseline cumulative hazard function. We conduct simulation studies to empirically examine the finite-sample performance of the proposed methods and use real data from a breast cancer study for illustration."], ["Power-Transformed Linear Quantile Regression With Censored Data", "We propose a class of power-transformed linear quantile regression models for survival data subject to random censoring. The estimation procedure follows two sequential steps. First, for a given transformation parameter, we can easily obtain the estimates for the regression coefficients by minimizing a well-defined convex objective function. Second, we can estimate the transformation parameter based on a model discrepancy measure by constructing cumulative sum processes. We show that both the regression and transformation parameter estimates are strongly consistent and asymptotically normal. The variance\u2013covariance matrix depends on the unknown density function of the error term, so we estimate the variance by the usual bootstrap approach. We examine the performance of the proposed method for finite sample sizes through simulation studies and illustrate it with a real data example."], ["Optimal Designs for Dose-Finding Studies", "Understanding and properly characterizing the dose\u2013response relationship is a fundamental step in the investigation of a new compound, be it a herbicide or fertilizer, a molecular entity, an environmental toxin, or an industrial chemical. In this article we investigate the problem of deriving efficient designs for the estimation of target doses in the context of clinical dose finding. We propose methods to determine the appropriate number and actual levels of the doses to be administered to patients, as well as their relative sample size allocations. More specifically, we derive local optimal designs that minimize the asymptotic variance of the minimum effective dose estimate under a particular dose\u2013response model. We investigate the small-sample properties of these designs, together with their sensitivity to a misspecification of the true parameter values and of the underlying dose\u2013response model, through simulation. Finally, we demonstrate that the designs derived for a fixed model are rather sensitive with respect to this assumption and construct robust optimal designs that take into account a set of potential dose\u2013response profiles within classes of models commonly used in drug development practice."], ["On Consistent Nonparametric Intensity Estimation for Inhomogeneous Spatial Point Processes", "A common nonparametric approach to estimate the intensity function of an inhomogeneous spatial point process is through kernel smoothing. When conducting the smoothing, one typically uses events only in a local set around the point of interest. But the resulting estimator often is inconsistent, because the number of events in a fixed set is of order 1 for spatial point processes. In this article we propose a new covariate-based kernel smoothing method to estimate the intensity function. Our method defines the distance between any two points as the difference between their associated covariate values. Consequently, we determine the kernel weight for a given event of the process as a function of its new distance to the point of interest. Under some suitable conditions on the covariates and the spatial point process, we prove that our new estimator is consistent for the true intensity. To handle the situation with high-dimensional covariates, we also extend sliced inverse regression, a useful dimension-reduction tool in standard regression analysis, to spatial point processes. Simulations and an application to a real data example are used to demonstrate the usefulness of the proposed method."], ["Binary Time Series Modeling With Application to Adhesion Frequency Experiments", "Repeated adhesion frequency assay is the only published method for measuring the kinetic rates of cell adhesion. Cell adhesion plays an important role in many physiological and pathological processes. Traditional analysis of adhesion frequency experiments assumes that the adhesion test cycles are independent Bernoulli trials. This assumption often can be violated in practice. Motivated by the analysis of repeated adhesion tests, a binary time series model incorporating random effects is developed. A goodness-of-fit statistic is introduced to assess the adequacy of distribution assumptions on the dependent binary data with random effects. The asymptotic distribution of the goodness-of-fit statistic is derived, and its finite-sample performance is examined through a simulation study. Application of the proposed methodology to real data from a T-cell experiment reveals some interesting information, including the dependency between repeated adhesion tests."], ["Tests Based on Intrinsic Priors for the Equality of Two Correlated Proportions", null], ["Empirical Likelihood-Based Estimation of the Treatment Effect in a Pretest\u2013Posttest Study", "The pretest\u2013posttest study design is commonly used in medical and social science research to assess the effect of a treatment or an intervention. Recently, interest has been rising in developing inference procedures that improve efficiency while relaxing assumptions used in the pretest\u2013posttest data analysis, especially when the posttest measurement might be missing. In this article we propose a semiparametric estimation procedure based on empirical likelihood (EL) that incorporates the common baseline covariate information to improve efficiency. The proposed method also yields an asymptotically unbiased estimate of the response distribution. Thus functions of the response distribution, such as the median, can be estimated straightforwardly, and the EL method can provide a more appealing estimate of the treatment effect for skewed data. We show that, compared with existing methods, the proposed EL estimator has appealing theoretical properties, especially when the working model for the underlying relationship between the pretest and posttest measurements is misspecified. A series of simulation studies demonstrates that the EL-based estimator outperforms its competitors when the working model is misspecified and the data are missing at random. We illustrate the methods by analyzing data from an AIDS clinical trial (ACTG 175)."], ["Statistical Significance of Clustering for High-Dimension, Low\u2013Sample Size Data", null], ["Selection of Variables for Cluster Analysis and Classification Rules", "In this article we introduce two procedures for variable selection in cluster analysis and classification rules. One is mainly aimed at detecting the \u201cnoisy\u201d noninformative variables, while the other also deals with multicolinearity and general dependence. Both methods are designed to be used after a \u201csatisfactory\u201d grouping procedure has been carried out. A forward\u2013backward algorithm is proposed to make such procedures feasible in large datasets. A small simulation is performed and some real data examples are analyzed."], ["Variable Inclusion and Shrinkage Algorithms", "The Lasso is a popular and computationally efficient procedure for automatically performing both variable selection and coefficient shrinkage on linear regression models. One limitation of the Lasso is that the same tuning parameter is used for both variable selection and shrinkage. As a result, it typically ends up selecting a model with too many variables to prevent overshrinkage of the regression coefficients. We suggest an improved class of methods called variable inclusion and shrinkage algorithms (VISA). Our approach is capable of selecting sparse models while avoiding overshrinkage problems and uses a path algorithm, and so also is computationally efficient. We show through extensive simulations that VISA significantly outperforms the Lasso and also provides improvements over more recent procedures, such as the Dantzig selector, relaxed Lasso, and adaptive Lasso. In addition, we provide theoretical justification for VISA in terms of nonasymptotic bounds on the estimation error that suggest it should exhibit good performance even for large numbers of predictors. Finally, we extend the VISA methodology, path algorithm, and theoretical bounds to the generalized linear models framework."], ["Analysing Ecological Data", null], ["Applied Chemometrics for Scientists", null], ["The Basel II Risk Parameters: Estimation, Validation, and Stress Testing", null], ["Bayes Linear Statistics: Theory and Methods", null], ["Bioequivalence Studies in Drug Development: Methods and Applications", null], ["The Construction of Optimal Stated Choice Experiments: Theory and Methods", null], ["Information and Complexity in Statistical Modeling", null], ["Introduction to Applied Bayesian Statistics and Estimation for Social Scientists", null], ["An Introduction to Categorical Data Analysis", null], ["Introduction to Stochastic Integration", null], ["Introduction to Variance Estimation", null], ["Markov Chains: Models, Algorithms and Applications", null], ["Model-Based Geostatistics", null], ["Models for Ecological Data: An Introduction", null], ["Nonparametric Statistics, With Applications to Science and Engineering", null], ["Optimum Experimental Designs, With SAS", null], ["Telegraphic Reviews", null], ["Estimating Incumbency Advantage and Its Variation, as an Example of a Before\u2013After Study", "Incumbency advantage is one of the most widely studied features in American legislative elections. In this article we construct and implement an estimate that allows incumbency advantage to vary between individual incumbents. This model predicts that open-seat elections will be less variable than those with incumbents running, an observed empirical pattern that is not explained by previous models. We apply our method to the U.S. House of Representatives in the twentieth century. Our estimate of the overall pattern of incumbency advantage over time is similar to previous estimates (although slightly lower), and we also find a pattern of increasing variation. More generally, our multilevel model represents a new method for estimating effects in before\u2013after studies."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Eye-Movement Analysis of Search Effectiveness", "Advances in eye-tracking technology have promoted its widespread use to understand and improve target searches in psychology, industrial engineering, human factors, medical diagnostics, and marketing. Eye movements are the realization of a complex, unobserved spatiotemporal attention process with many sources of variation. Eye-tracking data often have been aggregated and/or summarized descriptively, because few adequate statistical models are available for their analysis. This article proposes a model that may serve to uncover the latent attention processes of people searching for targets in complex scenes. It recognizes the spatial nature of eye movements and represents two latent attention states, a localization state and an identification state, between which people may switch over time according to a Markov process. A saliency map, based on low-level perceptual features and the scene's organization, guide target searches in the localization state. In the identification state, people verify whether a selected candidate object is the target. The model is applied to analyze commercial eye-tracking data from more than 100 people engaged in a target search task on a computer-simulated retail shelf display. Rapid switching between attention states over time is revealed. Estimates of the feature and saliency maps are provided and found to be related to search performance. The results facilitate the evaluation of the effectiveness of alternative visual search strategies."], ["Longitudinal Nested Compliance Class Model in the Presence of Time-Varying Noncompliance", "This article discusses a nested latent class model for analyzing longitudinal randomized trials when subjects do not always adhere to the treatment to which they are randomized. In the Prevention of Suicide in Primary Care Elderly: Collaborative Trial, subjects were randomized to either the control treatment, where they received standard care, or to the intervention, where they received standard care in addition to meeting with depression health specialists. The health specialists educate patients, their families, and physicians about depression and monitor their treatment. Those randomized to the control treatment have no access to the health specialists; however, those randomized to the intervention could choose not to meet with the health specialists, hence receiving only the standard care. Subjects participated in the study for two years where depression severity and adherence to meeting with health specialists were measured at each follow-up. The outcome of interest is the effect of meeting with the health specialists on depression severity. Traditional intention-to-treat and as-treated analyses may produce biased causal effect estimates in the presence of subject noncompliance. Utilizing a nested latent class model that uses subject-specific and time-invariant \u201csuperclasses\u201d allows us to summarize longitudinal trends of compliance patterns and to estimate the effect of the intervention using intention-to-treat contrasts within principal strata that correspond to longitudinal compliance behavior patterns. Analyses show that subjects with more severe depression are more likely to adhere to treatment randomization, and those that are compliant and meet with health specialists benefit from the meetings and show improvement in depression. Simulation results show that our estimation procedure produces reasonable parameter estimates under correct model assumptions."], ["Causal Inference in Hybrid Intervention Trials Involving Treatment Choice", " Please see the supplemental material for a correction to this article. "], ["Bayesian Hidden Markov Modeling of Array CGH Data", "We adopt a Bayesian approach, relying on the hidden Markov model to account for the inherent dependence in the intensity ratios. Posterior inferences are made about gains and losses in copy number. Localized amplifications (associated with oncogene mutations) and deletions (associated with mutations of tumor suppressors) are identified using posterior probabilities. Global trends such as extended regions of altered copy number are detected. Because the posterior distribution is analytically intractable, we implement a Metropolis-within-Gibbs algorithm for efficient simulation-based inference. Publicly available data on pancreatic adenocarcinoma, glioblastoma multiforme, and breast cancer are analyzed, and comparisons are made with some widely used algorithms to illustrate the reliability and success of the technique."], ["Analysis of Episodic Data With Application to Recurrent Pulmonary Exacerbations in Cystic Fibrosis Patients", "We consider a special type of recurrent event data, termed \u201crecurrent episode\u201d data, arising in episodic illness studies. When an event occurs, it lasts for a random length of time. A naive recurrent-event analysis disregards the length of the episodes, which may contain important information about the severity of the disease, the associated medical costs, and quality of life. Bivariate gap time models have been suggested in which length of episodes and time between episodes are modeled jointly. These models are useful but may obscure the overall effects of treatment and other prognostic factors. The analysis can be further complicated if covariate effects change over time, as may occur when the effects vary across episodes. This article reviews the existing methods applied to recurrent episode data and approaches the problem using the recently developed temporal process regression. Novel endpoints are constructed that summarize both episode frequency and the length of episodes and time between episodes. Time-varying coefficient models, with inferences based on functional estimating equations, are proposed. Both existing and new methods are applied to a clinical trial to assess the efficacy of a treatment for patients with cystic fibrosis, many of whom experienced multiple episodes of pulmonary exacerbations."], ["How Useful Is Bagging in Forecasting Economic Time Series? A Case Study of U.S. Consumer Price Inflation", "This article focuses on the widely studied question of whether the inclusion of indicators of real economic activity lowers the prediction mean squared error of forecasting models of U.S. consumer price inflation. We propose three variants of the bagging algorithm specifically designed for this type of forecasting problem and evaluate their empirical performance. Although bagging predictors in our application are clearly more accurate than equally weighted forecasts, median forecasts, ARM forecasts, AFTER forecasts, or Bayesian forecast averages based on one extra predictor at a time, they are generally about as accurate as the Bayesian shrinkage predictor, the ridge regression predictor, the iterated LASSO predictor, or the Bayesian model average predictor based on random subsets of extra predictors. Our results show that bagging can achieve large reductions in prediction mean-squared errors even in such challenging applications as inflation forecasting; however, bagging is not the only method capable of achieving such gains."], ["Bayesian Accelerated Failure Time Model With Multivariate Doubly Interval-Censored Data and Flexible Distributional Assumptions", "In this article we consider the relationship of covariates to the time to caries of permanent first molars. This involves an analysis of multivariate doubly interval-censored data. To describe this relationship, we suggest an accelerated failure time model with random effects, taking into account that the observations are clustered. Indeed, up to four permanent molars per child enter into the analysis, implying up to four caries times for each child. Each distributional part of the model is specified in a flexible way as a penalized Gaussian mixture with an overspecified number of mixture components. A Bayesian approach with the Markov chain Monte Carlo methodology is used to estimate the model parameters, and a software package in the R language has been written that implements it."], ["Bayesian Selection and Clustering of Polymorphisms in Functionally Related Genes", "In epidemiologic studies, there is often interest in assessing the relationship between polymorphisms in functionally related genes and a health outcome. For each candidate gene, single nucleotide polymorphism (SNP) data are collected at a number of locations, resulting in a large number of possible genotypes. Because instabilities can result in analyses that include all the SNPs, dimensionality is typically reduced by conducting single SNP analyses or attempting to identify haplotypes. This article proposes an alternative Bayesian approach for reducing dimensionality. A multilevel Dirichlet process prior is used for the distribution of the SNP-specific regression coefficients within genes, incorporating a variable selection-type mixture structure to allow SNPs with no effect. This structure allows simultaneous selection of important SNPs and soft clustering of SNPs having similar impact on the health outcome. The methods are illustrated using data from a study of pro- and anti-inflammatory cytokine polymorphisms and spontaneous preterm birth."], ["Statistical Treatment Choice", "Choosing among a number of treatments the most suitable for a particular client is an issue of everyday concern. In this article two methodological advances for statistically assisted treatment choice are developed: First, it permits one to combine a dataset on previously treated clients with a dataset on new clients when the regressors available in these two datasets do not coincide. It thereby incorporates additional regressors on previously treated clients that are not available for the current clients. Such a situation often arises because of cost considerations, data confidentiality reasons, or time delays in data availability. Second, statistical inference on the recommended treatment choice is analyzed and conveyed to the agent or caseworker in a comprehensible and transparent way. The implementation of this methodology in a pilot study in Switzerland for choosing among active labor market programs for unemployed job seekers is described."], ["Two-Sided Estimation of Mate Preferences for Similarities in Age, Education, and Religion", "We propose a two-sided method to simultaneously estimate men's and women's preferences for relative age, education, and religious characteristics of potential mates using cross-sectional data on married couples and single individuals, in conjunction with a behavioral model developed in game theory and discrete choice estimation methods developed for simpler, one-sided choice situations. We use fixed effects to control for characteristics that are observed by the opposite sex but are missing from our data. Estimated mean preference coefficients determine the average degree to which measured characteristics of individuals affect others' evaluations of them as marital partners, whereas the model also accounts for variation of preferences around the means and for limitations in men's and women's information about members of the opposite sex. By assuming that each individual chooses freely from the set of potential partners that he or she finds available, we estimate preferences without having to observe these sets or specify any details of the matching process. This makes our method robust to unknown features of the process. Application of the method to data from the first wave of the National Survey of Families and Households indicates roughly symmetric or complementary preferences of men and women for age, education, and religious affliation characteristics of potential mates and a much stronger preference for religious homogamy among conservative Protestants relative to mainline Protestants than was suggested by an earlier, retrospective study of religious differences in the temporal stability of marriages. Our method should be useful in many situations in which voluntary pairings have arisen through some complex process, the details of which have not been recorded. Besides marriage and cohabitation, data on employment, college attendance, and the coresidence of elderly parents with adult children often have this character, as do some biological data on nonhuman mating."], ["Computer Model Calibration Using High-Dimensional Output", "This work focuses on combining observations from field experiments with detailed computer simulations of a physical process to carry out statistical inference. Of particular interest here is determining uncertainty in resulting predictions. This typically involves calibration of parameters in the computer simulator as well as accounting for inadequate physics in the simulator. The problem is complicated by the fact that simulation code is sufficiently demanding that only a limited number of simulations can be carried out. We consider applications in characterizing material properties for which the field data and the simulator output are highly multivariate. For example, the experimental data and simulation output may be an image or may describe the shape of a physical object. We make use of the basic framework of Kennedy and O'Hagan. However, the size and multivariate nature of the data lead to computational challenges in implementing the framework. To overcome these challenges, we make use of basis representations (e.g., principal components) to reduce the dimensionality of the problem and speed up the computations required for exploring the posterior distribution. This methodology is applied to applications, both ongoing and historical, at Los Alamos National Laboratory."], ["Daytime Arctic Cloud Detection Based on Multi-Angle Satellite Data With Case Studies", "Global climate models predict that the strongest dependences of surface air temperatures on increasing atmospheric carbon dioxide levels will occur in the Arctic. A systematic study of these dependences requires accurate Arctic-wide measurements, especially of cloud coverage. Thus cloud detection in the Arctic is extremely important, but it is also challenging because of the similar remote sensing characteristics of clouds and ice-and snow-covered surfaces. This article proposes two new operational Arctic cloud detection algorithms using Multiangle Imaging SpectroRadiometer (MISR) imagery. The key idea is to identify cloud-free surface pixels in the imagery instead of cloudy pixels as in the existing MISR operational algorithms. Through extensive exploratory data analysis and using domain knowledge, three physically useful features to differentiate surface pixels from cloudy pixels have been identified. The first algorithm, enhanced linear correlation matching (ELCM), thresholds the features with either fixed or data-adaptive cutoff values. Probability labels are obtained by using ELCM labels as training data for Fisher's quadratic discriminant analysis (QDA), leading to the second (ELCM-QDA) algorithm. Both algorithms are automated and computationally efficient for operational processing of the massive MISR data set. Based on 5 million expert-labeled pixels, ELCM results are significantly in terms of both accuracy (92%%) and coverage (100%%) compared with two MISR operational algorithms, one with an accuracy of 80%% and coverage of 27%% and the other with an accuracy of 83%% and a coverage of 70%%. The ELCM-QDA probability prediction is also consistent with the expert labels and is more informative. In conclusion, ELCM and ELCM-QDA provide the best performance to date among all available operational algorithms using MISR data."], ["Statistical Modeling and Analysis for Robust Synthesis of Nanostructures", "Cadmium selenide has been found to exhibit one-dimensional morphologies of nanowires, nanobelts, and nanosaws, often with the three morphologies being intimately intermingled within the as-deposited material. A slight change in growth condition can result in a totally different morphology. To identify the optimal process conditions that maximize the yield of each type of nanostructure and, at the same time, make the synthesis process robust (i.e., less sensitive) to variations of process variables around set values, a large number of trials were conducted with varying process conditions. Here, the response is a vector whose elements correspond to the number of appearances of different types of nanostructures. The fitted statistical models would enable nanomanufacturers to identify the probability of transition from one nanostructure to another when changes, even tiny ones, are made in one or more process variables. Inferential methods associated with the modeling procedure help in judging the relative impact of the process variables and their interactions on the growth of different nanostructures. Owing to the presence of internal noise, that is, variation around the set value, each predictor variable is a random variable. Using Monte Carlo simulations, the mean and variance of transformed probabilities are expressed as functions of the set points of the predictor variables. The mean is then maximized to find the optimum nominal values of the process variables, with the constraint that the variance is under control."], ["Inference for a Proton Accelerator Using Convolution Models", "Proton beams present difficulties in analysis because of the limited data that can be collected. The study of such beams must depend on complex computer simulators that incorporate detailed physical equations. The statistical problem of interest is to infer the initial state of the beam from the limited data collected as the beam passes through a series of focusing magnets. We are thus faced with a classic inverse problem where the computer simulator links the initial state to the observables. We propose a new model for the initial distribution that is derived from the discretized process convolution approach. This model provides a computationally tractable method for this highly challenging problem. Taking a Bayesian perspective allows better estimation of the uncertainty and propagation of this uncertainty."], ["Estimation of Space\u2013Time Branching Process Models in Seismology Using an EM\u2013Type Algorithm", "Maximum likelihood estimation of branching point process models via numerical optimization procedures can be unstable and computationally intensive. We explore an alternative estimation method based on the expectation-maximization algorithm. The method involves viewing the estimation of such branching processes as analogous to incomplete data problems. Using an application from seismology, we show how the epidemic-type aftershock sequence (ETAS) model can, in fact, be estimated this way, and we propose a computationally efficient procedure to maximize the expected complete data log-likelihood function. Using a space\u2013time ETAS model, we demonstrate that this method is extremely robust and accurate and use it to estimate declustered background seismicity rates of geologically distinct regions in Southern California. All regions show similar declustered background intensity estimates except for the one covering the southern section of the San Andreas fault system to the east of San Diego in which a substantially higher intensity is observed."], ["Penalized Clustering of Large-Scale Functional Data With Multiple Covariates", "In this article we propose a penalized clustering method for large-scale data with multiple covariates through a functional data approach. In our proposed method, responses and covariates are linked together through nonparametric multivariate functions (fixed effects), which have great flexibility in modeling various function features, such as jump points, branching, and periodicity. Functional ANOVA is used to further decompose multivariate functions in a reproducing kernel Hilbert space and provide associated notions of main effect and interaction. Parsimonious random effects are used to capture various correlation structures. The mixed-effects models are nested under a general mixture model in which the heterogeneity of functional data is characterized. We propose a penalized Henderson's likelihood approach for model fitting and design a rejection-controlled EM algorithm for the estimation. Our method selects smoothing parameters through generalized cross-validation. Furthermore, Bayesian confidence intervals are used to measure the clustering uncertainty. Simulation studies and real-data examples are presented to investigate the empirical performance of the proposed method. Open-source code is available in the R package MFDA."], ["Survival Analysis With Quantile Regression Models", null], ["Semiparametric Analysis of Heterogeneous Data Using Varying-Scale Generalized Linear Models", "This article describes a class of heteroscedastic generalized linear regression models in which a subset of the regression parameters are rescaled nonparametrically, and develops efficient semiparametric inferences for the parametric components of the models. Such models provide a means to adapt for heterogeneity in the data due to varying exposures, varying levels of aggregation, and so on. The class of models considered includes generalized partially linear models and nonparametrically scaled link function models as special cases. We present an algorithm to estimate the scale function nonparametrically, and obtain asymptotic distribution theory for regression parameter estimates. In particular, we establish that the asymptotic covariance of the semiparametric estimator for the parametric part of the model achieves the semiparametric lower bound. We also describe bootstrap-based goodness-of-scale test. We illustrate the methodology with simulations, published data, and data from collaborative research on ultrasound safety."], ["Variable Selection and Model Averaging in Semiparametric Overdispersed Generalized Linear Models", "We express the mean and variance terms in a double-exponential regression model as additive functions of the predictors and use Bayesian variable selection to determine which predictors enter the model and whether they enter linearly or flexibly. When the variance term is null, we obtain a generalized additive model, which becomes a generalized linear model if the predictors enter the mean linearly. The model is estimated using Markov chain Monte Carlo simulation, and the methodology is illustrated using real and simulated data sets."], ["Penalized Estimating Functions and Variable Selection in Semiparametric Regression Models", "We propose a general strategy for variable selection in semiparametric regression models by penalizing appropriate estimating functions. Important applications include semiparametric linear regression with censored responses and semiparametric regression with missing predictors. Unlike the existing penalized maximum likelihood estimators, the proposed penalized estimating functions may not pertain to the derivatives of any objective functions and may be discrete in the regression coefficients. We establish a general asymptotic theory for penalized estimating functions and present suitable numerical algorithms to implement the proposed estimators. In addition, we develop a resampling technique to estimate the variances of the estimated regression coefficients when the asymptotic variances cannot be evaluated directly. Simulation studies demonstrate that the proposed methods perform well in variable selection and variance estimation. We illustrate our methods using data from the Paul Coverdell Stroke Registry."], ["The Bayesian Lasso", "The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant."], ["Robust and Efficient Adaptive Estimation of Binary-Choice Regression Models", "The binary-choice regression models, such as probit and logit, are used to describe the effect of explanatory variables on a binary response variable. Typically estimated by the maximum likelihood method, estimates are very sensitive to deviations from a model, such as heteroscedasticity and data contamination. At the same time, the traditional robust (high-breakdown point) methods, such as the maximum trimmed likelihood, are not applicable because, by trimming observations, they induce nonidentification of parameter estimates. To provide a robust estimation method for binary-choice regression, we consider a maximum symmetrically trimmed likelihood estimator (MSTLE) and design a parameter-free adaptive procedure for choosing the amount of trimming. The proposed adaptive MSTLE preserves the robust properties of the original MSTLE, significantly improves the finite-sample behavior of MSTLE, and also ensures the asymptotic equivalence of the MSTLE and maximum likelihood estimator under no contamination. The results concerning the trimming identification, robust properties, and asymptotic distribution of the proposed method are accompanied by simulation experiments and an application documenting the finite-sample behavior of some existing and the proposed methods."], ["Trimmed Comparison of Distributions", null], ["A Unified Approach to Nonparametric Comparison of Receiver Operating Characteristic Curves for Longitudinal and Clustered Data", "We present a unified approach to nonparametric comparisons of receiver operating characteristic (ROC) curves for a paired design with clustered data. Treating empirical ROC curves as stochastic processes, their asymptotic joint distribution is derived in the presence of both between-marker and within-subject correlations. A Monte Carlo method is developed to approximate their joint distribution without involving nonparametric density estimation. The developed theory is applied to derive new inferential procedures for comparing weighted areas under the ROC curves, confidence bands for the difference function of ROC curves, confidence intervals for the set of specificities at which one diagnostic test is more sensitive than the other, and multiple comparison procedures for comparing more than two diagnostic markers. Our methods demonstrate satisfactory small-sample performance in simulations. We illustrate our methods using clustered data from a glaucoma study and repeated-measurement data from a startle response study."], ["Wavelet-Based Nonparametric Functional Mapping of Longitudinal Curves", "Functional mapping based on parametric and nonparametric modeling of functional data can estimate the developmental pattern of genetic effects on a complex dynamic or longitudinal process triggered by quantitative trait loci (QTLs). But existing functional mapping models have a limitation for mapping dynamic QTLs with irregular functional data characterized by many local features, such as peaks. We derive a statistical model for QTL mapping of longitudinal curves of any form based on wavelet shrinkage techniques. The fundamental idea of this model is a repeated splitting of an initial sequence into detail coefficients that quantify local fluctuations at a particular scale and smooth coefficients that quantify remaining low-frequency variation in the signal after the high-frequency detail is removed and, subsequently, QTL mapping with the smooth coefficients extracted from noisy longitudinal data. Compared with conventional full-dimensional functional mapping, wavelet-based nonparametric functional mapping provides consistent results, and better results in some circumstances, and is much more computationally efficient. This wavelet-based model is validated by the analysis of a real example for stem diameter growth trajectories in a forest tree, and its statistical properties are examined through extensive simulation studies. Wavelet-based functional mapping broadens the use of functional mapping to studying an arbitrary form of longitudinal curves and will have many implications for investigating the interplay between gene actions/interactions and developmental pathways in various complex biological processes and networks."], ["Density Estimation in the Presence of Heteroscedastic Measurement Error", "We consider density estimation when the variable of interest is subject to heteroscedastic measurement error. The density is assumed to have a smooth but unknown functional form that we model with a penalized mixture of B-splines. We treat the situation in which multiple mismeasured observations of each variable of interest are observed for at least some of the subjects, and the measurement error is assumed to be additive and normal. The measurement error variance function is modeled with a second penalized mixture of B-splines. The article's main contributions are to address the effects of heteroscedastic measurement error effectively, explain the biases caused by ignoring heteroscedasticity, and present an equivalent kernel for a spline-based density estimator. Derivation of the equivalent kernel may be of independent interest. We use small-sigma asymptotics to approximate the biases incurred by assuming that the measurement error is homoscedastic when it actually is heteroscedastic. The biases incurred by misspecifying heteroscedastic measurement error as homoscedastic can be substantial. We fit the model using Bayesian methods and apply it to an example from nutritional epidemiology and a simulation experiment."], ["Covariate Bias Induced by Length-Biased Sampling of Failure Times", "Although many authors have proposed different approaches to the analysis of length-biased survival data, a number of issues have not been fully addressed. The most important among these issues is perhaps that regarding inclusion of covariates into the analysis of length-biased lifetime data collected through cross-sectional sampling of a population. One aspect of this problem, which appears to have been neglected in the literature, concerns the effect of length bias on the sampling distribution of the covariates. In most regression analyses, it is conventional to condition on the observed covariate values; however, certain covariate values could be preferentially selected into the sample, being linked to the long-term survivors, who themselves are favored by the sampling mechanism. This observation raises two questions: (1) Does the conditional analysis of covariates lead to biased estimators of regression coefficients?; and (2) does inference through the joint l likelihood of covariates and failure times yield more efficient estimators of the regression parameters? We present a joint likelihood approach and study the large-sample behavior of the resulting maximum likelihood estimators (MLEs). We find that these MLEs are more efficient than their conditional counterparts even though the two MLEs are asymptotically equal. Our results are illustrated using data on survival with dementia, collected as part of the Canadian Study of Health and Aging."], ["Cure Rate Model With Mismeasured Covariates Under Transformation", "Cure rate models explicitly account for the survival fraction in failure time data. When the covariates are measured with errors, naively treating mismeasured covariates as error-free would cause estimation bias and thus lead to incorrect inference. Under the proportional hazards cure model, we propose a corrected score approach as well as its generalization, and implement a transformation on the mismeasured covariates toward error additivity and/or normality. The corrected score equations can be easily solved through the backfitting procedure, and the biases in the parameter estimates are successfully eliminated. We show that the proposed estimators for the regression coefficients are consistent and asymptotically normal. We conduct simulation studies to examine the finite-sample properties of the new method and apply it to a real data set for illustration."], ["Laplace Periodogram for Time Series Analysis", "A new type of periodogram, called the Laplace periodogram, is derived by replacing least squares with least absolute deviations in the harmonic regression procedure that produces the ordinary periodogram of a time series. An asymptotic analysis reveals a connection between the Laplace periodogram and the zero-crossing spectrum. This relationship provides a theoretical justification for use of the Laplace periodogram as a nonparametric tool for analyzing the serial dependence of time series data. Superiority of the Laplace periodogram in handling heavy-tailed noise and nonlinear distortion is demonstrated by simulations. A real-data example shows its great effectiveness in analyzing heart rate variability in the presence of ectopic events and artifacts."], ["Second-Order Analysis of Inhomogeneous Spatial Point Processes With Proportional Intensity Functions", "We consider pairs of spatial point processes with intensity functions sharing a common multiplicative term. We introduce two novel approaches to estimating the pair correlation function for one of the point processes by treating the other as a baseline so as to account for the unspecified part of the intensity functions. The first approach is based on nonparametric kernel-smoothing, whereas the second approach uses a conditional likelihood estimation approach to fit a parametric model for the pair correlation function. A great advantage of the proposed methods is that they do not require the often difficult estimation of the unspecified part of the intensity functions. We establish the consistency of the resulting estimators and discuss how the parametric estimator can be applied in model diagnostics and inference on regression parameters for the intensity functions. We apply the proposed procedures to two spatial point patterns regarding the spatial distributions of birds in the U.K.'s Peak District in 1990 and 2004."], ["Learning Causal Bayesian Network Structures From Experimental Data", "We propose a method for the computational inference of directed acyclic graphical structures given data from experimental interventions. Order-space Markov chain Monte Carlo, equi-energy sampling, importance weighting, and stream-based computation are combined to create a fast algorithm for learning causal Bayesian network structures."], ["Partially Collapsed Gibbs Samplers", "Ever-increasing computational power, along with ever\u2013more sophisticated statistical computing techniques, is making it possible to fit ever\u2013more complex statistical models. Among the more computationally intensive methods, the Gibbs sampler is popular because of its simplicity and power to effectively generate samples from a high-dimensional probability distribution. Despite its simple implementation and description, however, the Gibbs sampler is criticized for its sometimes slow convergence, especially when it is used to fit highly structured complex models. Here we present partially collapsed Gibbs sampling strategies that improve the convergence by capitalizing on a set of functionally incompatible conditional distributions. Such incompatibility generally is avoided in the construction of a Gibbs sampler, because the resulting convergence properties are not well understood. We introduce three basic tools (marginalization, permutation, and trimming) that allow us to transform a Gibbs sampler into a partially collapsed Gibbs sampler with known stationary distribution and faster convergence."], ["Efficient and Doubly Robust Imputation for Covariate-Dependent Missing Responses", "In this article we study a well-known response missing-data problem. Missing data is an ubiquitous problem in medical and social science studies. Imputation is one of the most popular methods for dealing with missing data. The most commonly used imputation that makes use of covariates is regression imputation, in which the regression model can be parametric, semiparametric, or nonparametric. Parametric regression imputation is efficient but is not robust against misspecification of the regression model. Although nonparametric regression imputation (such as nearest-neighbor imputation and kernel regression imputation) is model-free, it is not efficient, especially if the dimension of covariate vector is high (the well-known problem of curse of dimensionality). Semiparametric regression imputation (such as partially linear regression imputation) can reduce the dimension of the covariate in nonparametric regression fitting but is not robust against misspecification of the linear component in the regression. Assuming that the missing mechanism is covariate-dependent and that the propensity function can be specified correctly, we propose a regression imputation method that has good efficiency and is robust against regression model misspecification. Furthermore, our method is valid as long as either the regression model or the propensity model is correct, a property known as the double-robustness property. We show that asymptotically the sample mean based on our imputation achieves the semiparametric efficient lower bound if both regression and propensity models are specified correctly. Our simulation results demonstrate that the proposed method outperforms many existing methods for handling missing data, especially when the regression model is misspecified. As an illustration, an economic observational data set is analyzed."], ["Sliced Regression for Dimension Reduction", "A new dimension-reduction method involving slicing the region of the response and applying local kernel regression to each slice is proposed. Compared with the traditional inverse regression methods [e.g., sliced inverse regression (SIR)], the new method is free of the linearity condition and has much better estimation accuracy. Compared with the direct estimation methods (e.g., MAVE), the new method is much more robust against extreme values and can capture the entire central subspace (CS) exhaustively. To determine the CS dimension, a consistent cross-validation criterion is developed. Extensive numerical studies, including a real example, confirm our theoretical findings."], ["Sufficient Dimension Reduction With Missing Predictors", "In high-dimensional data analysis, sufficient dimension reduction (SDR) methods are effective in reducing the predictor dimension, while retaining full regression information and imposing no parametric models. However, it is common in high-dimensional data that a subset of predictors may have missing observations. Existing SDR methods resort to the complete-case analysis by removing all the subjects with missingness in any of the predictors under inquiry. Such an approach does not make effective use of the data and is valid only when missingness is independent of both observed and unobserved quantities. In this article, we propose a new class of SDR estimators under a more general missingness mechanism that allows missingness to depend on the observed data. We focus on a widely used SDR method, sliced inverse regression, and propose an augmented inverse probability weighted sliced inverse regression estimator (AIPW\u2013SIR). We show that AIPW\u2013SIR is doubly robust and asymptotically consistent and demonstrate that AIPW\u2013SIR is more effective than the complete-case analysis through both simulations and real data analysis. We also outline the extension of the AIPW strategy to other SDR methods, including sliced average variance estimation and principal Hessian directions."], ["Toward Causal Inference With Interference", "A fundamental assumption usually made in causal inference is that of no interference between individuals (or units); that is, the potential outcomes of one individual are assumed to be unaffected by the treatment assignment of other individuals. However, in many settings, this assumption obviously does not hold. For example, in the dependent happenings of infectious diseases, whether one person becomes infected depends on who else in the population is vaccinated. In this article, we consider a population of groups of individuals where interference is possible between individuals within the same group. We propose estimands for direct, indirect, total, and overall causal effects of treatment strategies in this setting. Relations among the estimands are established; for example, the total causal effect is shown to equal the sum of direct and indirect causal effects. Using an experimental design with a two-stage randomization procedure (first at the group level, then at the individual level within groups), unbiased estimators of the proposed estimands are presented. Variances of the estimators are also developed. The methodology is illustrated in two different settings where interference is likely: assessing causal effects of housing vouchers and of vaccines."], ["Surveillance Strategies for Detecting Changepoint in Incidence Rate Based on Exponentially Weighted Moving Average Methods", "Surveillance is a major issue in the today's world. The need to develop adequate surveillance strategies is genuine in many spheres of human activity. In this article we focus on a specific problem of surveillance and discuss some related statistical issues. This specific problem deals with a possible change in the incidence rate of an event (to a higher value) when a system is studied at discrete time points. We apply the exponentially weighted moving average methods for detecting an increased incidence rate per exposure unit of an event. Different measures of evaluation, suitable in different types of applications, such as the expected delay, out-of-control average run length, and probability of successful detection, are studied. Analytical bounds are provided for those measures of evaluation. Results from intensive simulations indicate that the analytical bounds perform fairly well when the weight parameter is large."], ["Fiducial Intervals for Variance Components in an Unbalanced Two-Component Normal Mixed Linear Model", null], ["Current Methods for Recurrent Events Data With Dependent Termination", "There has been a recent surge of interest in modeling and methods for analyzing recurrent events data with risk of termination dependent on the history of the recurrent events. To aid future users in understanding the implications of modeling assumptions and modeling properties, we review the state-of-the-art statistical methods and present novel theoretical properties, identifiability results, and practical consequences of key modeling assumptions of several fully specified stochastic models. After introducing stochastic models with 2 noninformative termination process, we focus on a class of models that allows both negative and positive association between the risk of termination and the rate of recurrent events through a frailty variable. We also discuss the relationship, as well as the major differences between these models in terms of their motivations and physical interpretations. We discuss associated Bayesian methods based on Markov chain Monte Carlo tools, and novel model diagnostic tools to perform inference based on fully specified models. We demonstrate the usefulness of the current methodology through an analysis of a data set from a clinical trial. Finally, we explore possible future extensions and limitations of the methodology."], ["Applied Linear Statistical Models", null], ["Applying Statistics in the Courtroom: A New Approach for Attorneys and Expert Witnesses; The Expert: The Statistical Analyst in Litigation", null], ["Data Quality and Record Linkage Techniques", null], ["Encyclopedia of Statistics in Behavioral Science", null], ["Extreme Value Theory: An Introduction", null], ["An Introduction to Bayesian Analysis: Theory and Methods", null], ["An Introduction to Random Sets; Theory of Random Sets", null], ["Linear Models: An Integrated Approach", null], ["Nonparametric Econometrics: Theory and Practice", null], ["Pattern Recognition and Machine Learning", null], ["Reliability, Life Testing, and the Prediction of Service Lives for Engineers and Scientists", null], ["Response Surfaces, Mixtures, and Ridge Analyses", null], ["Robust Statistics: Theory and Methods", null], ["Sampling for Natural Resource Monitoring", null], ["Simulation and Monte Carlo: With Applications in Finance and MCMC", null], ["Statistical Analysis of Cost-Effectiveness Data", null], ["Telegraphic Reviews", null], [null, "This article corrects two errors that appear in the study by Banerjee and Bhattacharyya (1976). It shows that the mixture of inverse Gaussian distributions can look like a better model of purchase frequencies than implied by Banerjee and Bhattacharyya's work."], ["Correction", null], ["On Student's 1908 Article \u201cThe Probable Error of a Mean\u201d", "This month marks the 100th anniversary of the appearance of William Sealey Gosset's celebrated article, \u201cThe Probable Error of a Mean\u201d (Student 1908a). Gosset's elegant result represented the first in a series of exact, \u201csmall-sample\u201d results that were developed by Gosset, Fisher, and others to form a central component of the modern theory of statistical inference. This review celebrates the centenary of Gosset's article by discussing both its background and its impact on statistical theory and practice."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], [null, null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Joint Modeling of Time Series Measures and Recurrent Events and Analysis of the Effects of Air Quality on Respiratory Symptoms", null], ["On Estimating Diagnostic Accuracy From Studies With Multiple Raters and Partial Gold Standard Evaluation", "We are often interested in estimating sensitivity and specificity of a group of raters or a set of new diagnostic tests in situations in which gold standard evaluation is expensive or invasive. Numerous authors have proposed latent modeling approaches for estimating diagnostic error without a gold standard. Albert and Dodd showed that, when modeling without a gold standard, estimates of diagnostic error can be biased when the dependence structure between tests is misspecified. In addition, they showed that choosing between different models for this dependence structure is difficult in most practical situations. While these results caution against using these latent class models, the difficulties of obtaining gold standard verification remain a practical reality. We extend two classes of models to provide a compromise that collects gold standard information on a subset of subjects but incorporates information from both the verified and nonverified subjects during estimation. We examine the robustness of diagnostic error estimation with this approach and show that choosing between competing models is easier in this context. In our analytic work and simulations, we consider situations in which verification is completely at random as well as settings in which the probability of verification depends on the actual test results. We apply our methodological work to a study designed to estimate the diagnostic error of digital radiography for gastric cancer."], ["Skill, Luck, and Streaky Play on the PGA Tour", "In this study, we implement a random-effects model that estimates cubic spline-based time-dependent skill functions for 253 active PGA Tour golfers over the 1998\u20132001 period. Our model controls for the first-order autocorrelation of residual 18-hole scores and adjusts for round\u2013course and player\u2013course random effects. Using the model, we are able to estimate time-varying measures of skill and luck for each player in the sample. Estimates of spline-based player-specific skill functions provide strong evidence that the skill levels of PGA Tour players change through time. Using the model, we are able to rank golfers at the end of 2001 on the basis of their estimated mean skill levels and identify the players who experienced the most improvement and deterioration in their skill levels over the 1998\u20132001 period. We find that some luck is required to win PGA Tour events, even for the most highly skilled players. Player\u2013course interactions contribute very little to variability in golfer performance, but the particular course rotations to which players are assigned can have a significant effect on the outcomes of the few tournaments played on more than one course. We also find evidence that a small number of PGA Tour participants experience statistically significant streaky play."], ["Bayesian Gaussian Mixture Models for High-Density Genotyping Arrays", "Affymetrix's SNP (single-nucleotide polymorphism) genotyping chips have increased the scope and decreased the cost of gene-mapping studies. Because each SNP is queried by multiple DNA probes, the chips present interesting challenges in genotype calling. Traditional clustering methods distinguish the three genotypes of an SNP fairly well given a large enough sample of unrelated individuals or a training sample of known genotypes. This article describes our attempt to improve genotype calling by constructing Gaussian mixture models with empirically derived priors. The priors stabilize parameter estimation and borrow information collectively gathered on tens of thousands of SNPs. When data from related family members are available, our models capture the correlations in signals between relatives. With these advantages in mind, we apply the models to Affymetrix probe intensity data on 10,000 SNPs gathered on 63 genotyped individuals spread over eight pedigrees. We integrate the genotype-calling model with pedigree analysis and examine a sequence of symmetry hypotheses involving the correlated probe signals. The symmetry hypotheses raise novel mathematical issues of parameterization. Using the Bayesian information criterion, we select the best combination of symmetry assumptions. Compared to Affymetrix's software, our model leads to a reduction in no-calls with little sacrifice in overall calling accuracy."], ["Principal Stratification for Causal Inference With Extended Partial Compliance", "Many double-blind placebo-controlled randomized experiments with active drugs suffer from complications beyond simple noncompliance. First, the compliance with assigned dose is often partial, with patients taking only part of the assigned dose, whether active or placebo. Second, the blinding may be imperfect in the sense that there may be detectable positive or negative side effects of the active drug, and consequently, simple compliance has to be extended to allow different compliances to active drug and placebo. Efron and Feldman presented an analysis of such a situation and discussed inference for dose\u2013response from the nonrandomized data in the active treatment arm, which stimulated active discussion, including on the role of the intention-to-treat principle in such studies. Here, we formulate the problem within the principal stratification framework of Frangakis and Rubin, which adheres to the intention-to-treat principle, and we present a new analysis of the Efron\u2013Feldman data within this framework. Moreover, we describe precise assumptions under which dose\u2013response can be inferred from such nonrandomized data, which seem debatable in the setting of this example. Although this article only deals in detail with the specific Efron\u2013Feldman data, the same framework can be applied to various circumstances in both natural science and social science."], ["Estimating Flight Departure Delay Distributions\u2014A Statistical Approach With Long-Term Trend and Short-Term Pattern", "In this article we develop a model for estimating flight departure delay distributions required by air traffic congestion prediction models. We identify and study major factors that influence flight departure delays, and develop a strategic departure delay prediction model. This model employs nonparametric methods for daily and seasonal trends. In addition, the model uses a mixture distribution to estimate the residual errors. To overcome problems with local optima in the mixture distribution, we develop a global optimization version of the expectation\u2013maximization algorithm, borrowing ideas from genetic algorithms. The model demonstrates reasonable goodness of fit, robustness to the choice of the model parameters, and good predictive capabilities. We use flight data from United Airlines and Denver International Airport from the years 2000/2001 to train and validate our model."], ["A Test for Anchoring and Yea-Saying in Experimental Consumption Data", "We analyze experimental survey data, with a random split into respondents who get an open-ended question on the amount of total family consumption (with follow-up unfolding brackets of the form \u201cIs consumption $$X or more?\u201d for those who answer \u201cdon't know\u201d or \u201crefuse\u201d) and respondents who are immediately directed to unfolding brackets. In both cases, the entry point of the unfolding bracket sequence is randomized. Allowing for any type of selection into answering the open-ended or bracket questions, a nonparametric test is developed for errors in the answers to the first bracket question that are different from the usual reporting errors that will also affect open-ended answers. Two types of errors are considered explicitly: anchoring and yea-saying. Data are collected in the 1995 wave of the Assets and Health Dynamics survey, which is representative of the population in the United States that is 70 years and older. We reject the joint hypothesis of no anchoring and no yea-saying. Once yea-saying is taken into account, we find no evidence of anchoring at the entry point."], ["Relating Ambient Particulate Matter Concentration Levels to Mortality Using an Exposure Simulator", null], ["A Flexible Method to Measure Synchrony in Neuronal Firing", "Neurons can transmit information about the characteristics of a stimulus through the spike rate of neurons and synchronization of the neurons. Various association measure can be used to describe how \u201csynchronous\u201d two spike trains are. We propose a new measure of synchrony, the conditional synchrony measure, which is the probability of firing together given that at least one of the two neurons is active. Focus is on the specification of a flexible marginal model for multivariate correlated binary data together with a pseudolikelihood estimation approach, to adequately and directly describe the measures of interest. A joint model must allow different time- and covariate-dependent firing rates for each neuron, and also must account for the association between them. The association between neurons might depend on covariates as well."], ["Spatial-Temporal Modeling of Forest Gaps Generated by Colonization From Below- and Above-Ground Bark Beetle Species", "Studies of forest declines are important, because they both reduce timber production and affect successional trajectories of landscapes and ecosystems. Of particular interest is the decline of red pines, which is characterized by expanding areas of dead and chlorotic trees in plantations throughout the Great Lakes region. Here we examine the impact of two bark beetle groups, red turpentine beetles and pine engraver bark beetles, on tree mortality and the subsequent gap formation over time in a plantation in Wisconsin. We construct spatial-temporal statistical models that quantify the relations among red turpentine beetle colonization, pine engraver bark beetle colonization, and mortality of red pine trees while accounting for correlation across space and over time. We extend traditional Markov random-field models to include temporal terms and multiple-response variables aimed at developing a suitable set of statistical models for addressing the scientific questions about the forest ecosystem under study. For statistical inference, we adopt a Bayesian hierarchical modeling approach and devise Markov chain Monte Carlo algorithms for obtaining the posterior distributions of model parameters as well as posterior predictive distributions. In particular, we implement path sampling combined with perfect simulation for autologistic models while formally addressing the posterior propriety under an improper uniform prior. Our data analysis results suggest that red turpentine beetle colonization is associated with a higher likelihood of pine engraver bark beetle colonization and that pine engraver bark beetle colonization is associated with higher likelihood of red pine tree mortality, whereas there is no direct association between red turpentine beetle colonization and red pine tree mortality. There is strong evidence that red turpentine beetle colonization does not kill a red pine tree directly, but rather predisposes the tree to subsequent colonization by pine engraver bark beetles. The evidence is also strong that pine engraver bark beetles are the ultimate mortality agents of red pine trees."], ["Individual Prediction in Prostate Cancer Studies Using a Joint Longitudinal Survival\u2013Cure Model", null], ["Bayesian Clustering of Transcription Factor Binding Motifs", "Genes are often regulated in living cells by proteins called transcription factors that bind directly to short segments of DNA in close proximity to specific genes. These binding sites have a conserved nucleotide appearance, which is called a motif. Several recent studies of transcriptional regulation require the reduction of a large collection of motifs into clusters based on the similarity of their nucleotide composition. We present a principled approach to this clustering problem based on a Bayesian hierarchical model that accounts for both within- and between-motif variability. We use a Dirichlet process prior distribution that allows the number of clusters to vary and we also present a novel generalization that allows the core width of each motif to vary. This clustering model is implemented, using a Gibbs sampling strategy, on several collections of transcription factor motif matrices. Our stochastic implementation allows us to examine the variability of our results in addition to focusing on a set of best clusters. Our clustering results identify several motif clusters that suggest that several transcription factor protein families are actually mixtures of several smaller groups of highly similar motifs, which provide substantially more refined information compared with the full set of motifs in the family. Our clusters provide a means by which to organize transcription factors based on binding motif similarities and can be used to reduce motif redundancy within large databases such as JASPAR and TRANSFAC, which aides the use of these databases for further motif discovery. Finally, our clustering procedure has been used in combination with discovery of evolutionarily conserved motifs to predict co-regulated genes. An alternative to our Dirichlet process prior distribution is presented that differs substantially in terms of a priori clustering characteristics, but shows no substantive difference in the clustering results for our dataset. Despite our specific application to transcription factor binding motifs, our Bayesian clustering model based on the Dirichlet process has several advantages over traditional clustering methods that could make our procedure appropriate and useful for many clustering applications."], ["Application of Multidimensional Selective Item Response Regression Model for Studying Multiple Gene Methylation in SV40 Oncogenic Pathways", "Alteration of gene methylation patterns has been reported to be involved in the early onsets of many human malignancies. Many exogenous risk factors, such as cigarette smoke, dietary additives, chemical exposures, radiation, and biologic agents including viral infection, are involved in the methylation pathways of cancers. We propose a multidimensional selective item response regression model to describe and test how a risk factor may alter molecular pathways involving aberrant methylation of multiple genes in oncogenesis. Our modeling framework is built on an item response model for multivariate dichotomous responses of high dimension, such as aberrant methylation of multiple tumor-suppressor genes, but we allow risk factors such as SV40 viral infection to alter the distribution of the latent factors that subsequently affect the outcome of cancer. We postulate empirical identification conditions under our model formulation. Moreover, we do not prespecify the links between the multiple dichotomous methylation responses and the latent factors, but rather conduct specification searches with a genetic algorithm to discover the links. Parameter estimation through maximum likelihood and specification searches in models with multidimensional latent factors for multivariate binary responses have become practical only recently, due to modern statistical computing development. We illustrate our proposal with the biological finding that simultaneous methylation of multiple tumor-suppressor genes is associated with the presence of SV40 viral sequences and with the cancer status of lymphoma/leukemia. We are able to test whether the data are consistent with the causal hypothesis that SV40 induces aberrant methylation of multiple genes in its oncogenic pathways. At the same time, we are able to evaluate the role of SV40 in the methylation pathway and to determine whether the methylation pathway is responsible for the development of leukemia/lymphoma."], ["A Capture\u2013Recapture Approach for Screening Using Two Diagnostic Tests With Availability of Disease Status for the Test Positives Only", "The article considers screening human populations with two screening tests. If any of the two tests is positive, then full evaluation of the disease status is undertaken; however, if both diagnostic tests are negative, then disease status remains unknown. This procedure leads to a data constellation in which, for each disease status, the 2 \u00d7 2 table associated with the two diagnostic tests used in screening has exactly one empty, unknown cell. To estimate the unobserved cell counts, previous approaches assume independence of the two diagnostic tests and use specific models, including the special mixture model of Walter or unconstrained capture\u2013recapture estimates. Often, as is also demonstrated in this article by means of a simple test, the independence of the two screening tests is not supported by the data. Two new estimators are suggested that allow associations of the screening test, although the form of association must be assumed to be homogeneous over disease status. These estimators are modifications of the simple capture\u2013recapture estimator and easy to construct. The estimators are investigated for several screening studies with fully evaluated disease status in which the superior behavior of the new estimators compared to the previous conventional ones can be shown. Finally, the performance of the new estimators is compared with maximum likelihood estimators, which are more difficult to obtain in these models. The results indicate the loss of efficiency as minor."], ["A Mixture Model With Dependent Observations for the Analysis of CSFE\u2013Labeling Experiments", "Recent advances in flow cytometry have resulted in the development of powerful bioassays to analyze the proliferation of cell populations. The carboxy-fluorescein diacetate succinimidyl ester (CFSE)-labeling experiment is one such assay that is widely used to study cell kinetics and as a standard tool for investigating lymphocyte proliferation. Several mathematical models have been proposed to describe cell proliferation during CFSE experiments. The statistical analysis of CFSE-labeling data has received little attention, but it poses a number of methodological issues. In this article we approach their analysis using a mixture model. The mixing proportions are specified through an age-dependent branching process that models the temporal organization of the cell population. Because the CFSE molecules are partitioned between daughter cells when their mother divides, the observations generated by this assay are dependent. The data structure can be compared with that of a partially observed random-effects model where the clusters cannot be identified. In this context, we propose three estimators. We prove their consistency and asymptotic normality, investigate their finite-sample properties in simulation studies, and contrast their relative merits for the analysis of CFSE-labeling data. We use the proposed methods to analyze the proliferation in vitro of CD4+ and CD8+ T lymphocytes. In particular, we compare their activation and proliferation rates, and also investigate the effect of two stimuli that activate resting lymphocytes: the nonspecific mitogen phytohemaglutanin (PHA) and ligation of both the T-cell receptor CD3 and the costimulatory receptor CD28 cell surface proteins with monoclonal antibodies."], ["Aberrant Effects of Treatment", "Often, the protocol for a randomized controlled trial states that if a patient's health deteriorates substantially in certain specific ways, then the study treatments will no longer be considered adequate medical care, and the patient will be taken off the study treatments and given the needed care. Aberrant responses of this sort present special problems of interpretation. The few such patients may have received only limited exposure to the study treatments, but also may have experienced some of the worst outcomes observed in the trial. The most basic question is whether one of the treatments tends to cause such aberrant responses, quite apart from any other beneficial or harmful effects that treatment may have. Common practice is to reduce aberrant responses to binary events and apply Fisher's exact test for a 2 \u00d7 2 table, but this approach discards the available information about the magnitude of the aberration. We propose a new randomization test for this problem that uses information about both the number of aberrations and their magnitude. In extreme situations, the new test reduces to either Fisher's exact test or Wilcoxon's rank sum test, but in common situations, the new test has features of both methods. Importantly, this exact test reacts only to aberrant effects; it is completely unaffected by treatment effects that do not produce aberrations. When aberrant responses have several aspects, we examine the consequences of selecting, after the fact, the one aspect exhibiting the greatest aberration, and applying the test to that aspect. For a single response variable, a confidence interval is proposed for an additive aberrant effect; the interval is constructed by inverting the test, and a multivariate extension is briefly mentioned. We illustrate the use of the test in the ACE-Inhibitor After Anthracycline (AAA) randomized trial, which was aimed at preserving cardiac function in children treated for cancer. Seven of 135 children were removed from the trial due to substantial cardiac declines; 6 of these were on placebo, 1 of whom subsequently died."], ["Goodness of Fit of Social Network Models", "We present a systematic examination of a real network data set using maximum likelihood estimation for exponential random graph models as well as new procedures to evaluate how well the models fit the observed networks. These procedures compare structural statistics of the observed network with the corresponding statistics on networks simulated from the fitted model. We apply this approach to the study of friendship relations among high school students from the National Longitudinal Study of Adolescent Health (AddHealth). We focus primarily on one particular network of 205 nodes, although we also demonstrate that this method may be applied to the largest network in the AddHealth study, with 2,209 nodes. We argue that several well-studied models in the networks literature do not fit these data well and demonstrate that the fit improves dramatically when the models include the recently developed geometrically weighted edgewise shared partner, geometrically weighted dyadic shared partner, and geometrically weighted degree network statistics. We conclude that these models capture aspects of the social structure of adolescent friendship relations not represented by previous models."], ["Modeling Disease Progression With Longitudinal Markers", "In this article we propose a Bayesian natural history model for disease progression based on the joint modeling of longitudinal biomarker levels, age at clinical detection of disease, and disease status at diagnosis. We establish a link between the longitudinal responses and the natural history of the disease by using an underlying latent disease process that describes the onset of the disease and models the transition to an advanced stage of the disease as dependent on the biomarker levels. We apply our model to data from the Baltimore Longitudinal Study of Aging on prostate-specific antigen to investigate the natural history of prostate cancer."], ["Randomization Inference in a Group\u2013Randomized Trial of Treatments for Depression", "In the Prospect Study, in 10 pairs of two primary-care practices, one practice was picked at random to receive a \u201cdepression care manager\u201d to treat its depressed patients. Randomization inference, properly performed, reflects the assignment of practices, not patients, to treatment or control. Yet, pertinent data describe individual patients: depression outcomes, baseline covariates, compliance with treatment. The methods discussed use only (i) the random assignment of clusters to treatment or control and (ii) the hypothesis about effects being tested or inverted for confidence intervals, so they are randomization inferences in Fisher's strict sense. There is no assumption that the covariance model generated the data, that compliers resemble noncompliers, that dependence is from additive random cluster effects, that individuals in a same cluster do not interfere with one another, or that units are sampled from a population. We contrast methods of covariance adjustment, never assuming the models are \u201ctrue,\u201d obtaining exact randomization inferences. We consider exact inference about effects proportional to doses with noncompliance and effects whose magnitude varies with the degree of improvement that would occur without treatment. A simulation examines power."], ["Using SIMEX for Smoothing-Parameter Choice in Errors-in-Variables Problems", "SIMEX methods are attractive for solving curve estimation problems in errors-in-variables regression, using parametric or semiparametric techniques. However, nonparametric approaches are generally of quite a different type, being based on, for example, kernels, local-linear modeling, ridging, orthogonal series, or splines. All of these techniques involve the challenging (and not well studied) issue of empirical smoothing parameter choice. We show that SIMEX can be used effectively for selecting smoothing parameters when applying nonparametric methods to errors-in-variable regression. In particular, we suggest an approach based on multiple error-inflated (or remeasured) data sets and extrapolation."], ["Sequential Experimental Designs for Generalized Linear Models", null], ["A Uniform Improvement of Bonferroni-Type Tests by Sequential Tests", null], ["Hierarchical False Discovery Rate\u2013Controlling Methodology", null], ["The Matrix Stick-Breaking Process", "In analyzing data from multiple related studies, it often is of interest to borrow information across studies and to cluster similar studies. Although parametric hierarchical models are commonly used, of concern is sensitivity to the form chosen for the random-effects distribution. A Dirichlet process (DP) prior can allow the distribution to be unknown, while clustering studies; however, the DP does not allow local clustering of studies with respect to a subset of the coefficients without making independence assumptions. Motivated by this problem, we propose a matrix stick-breaking process (MSBP) as a prior for a matrix of random probability measures. Properties of the MSBP are considered, and methods are developed for posterior computation using Markov chain Monte Carlo. Using the MSBP as a prior for a matrix of study-specific regression coefficients, we demonstrate advantages over parametric modeling in simulated examples. The methods are further illustrated using a multinational uterotrophic bioassay study."], ["Bayesian Hierarchical Curve Registration", "Functional data often exhibit a common shape, but with variations in amplitude and phase across curves. The analysis often proceeds by synchronization of the data through curve registration. In this article we propose a Bayesian hierarchical model for curve registration. Our hierarchical model provides a formal account of amplitude and phase variability while borrowing strength from the data across curves in the estimation of the model parameters. We discuss extensions of the model by using penalized B-splines in the representation of the shape and time-transformation functions, and by allowing temporal misalignment of the curves. We discuss applications of our model to simulated data, as well as to two data sets. In particular, we use our model in a nonstandard analysis aimed at investigating regulatory network in time course microarray data."], ["Penalized Normal Likelihood and Ridge Regularization of Correlation and Covariance Matrices", "High dimensionality causes problems in various areas of statistics. A particular situation that rarely has been considered is the testing of hypotheses about multivariate regression models in which the dimension of the multivariate response is large. In this article a ridge regularization approach is proposed in which either the covariance or the correlation matrix is regularized to ensure nonsingularity irrespective of the dimensionality of the data. It is shown that the proposed approach can be derived through a penalized likelihood approach, which suggests cross-validation of the likelihood function as a natural approach for estimation of the ridge parameter. Useful properties of this likelihood estimator are derived, discussed, and demonstrated by simulation. For a class of test statistics commonly used in multivariate analysis, the proposed regularization approach is compared with some obvious alternative regularization approaches using generalized inverse and data reduction through principal components analysis. Essentially, the approaches considered differ in how they shrink eigenvalues of sample covariance and correlation matrices. This leads to predictable differences in power properties when comparing the use of different regularization approaches, as demonstrated by simulation. The proposed ridge approach has relatively good power compared with the alternatives considered. In particular, a generalized inverse is shown to perform poorly and cannot be recommended in practice. Finally, the proposed approach is used in analysis of data on macroinvertebrate biomasses that have been classified to species."], ["Point and Interval Estimation of Variogram Models Using Spatial Empirical Likelihood", "We present a spatial blockwise empirical likelihood method for estimating variogram model parameters in the analysis of spatial data on a grid. The method produces point estimators that require no spatial variance estimates to compute, unlike least squares methods for variogram fitting, but are as efficient as the best least squares estimator in large samples. Our approach also produces confidence regions for the variogram, without requiring knowledge of the full joint distribution of the spatial data. In addition, the empirical likelihood formulation extends to spatial regression problems and allows simultaneous inference on both spatial trend and variogram parameters. We examine the asymptotic behavior of the estimator analytically, and investigate its behavior in finite samples through simulation studies."], ["Time-Dependent Predictive Values of Prognostic Biomarkers With Failure Time Outcome", "In a prospective cohort study, information on clinical parameters, tests, and molecular markers often is collected. Such information is useful to predict patient prognosis and to select patients for targeted therapy. We propose a new graphical approach, the positive predictive value (PPV) curve, to quantify the predictive accuracy of prognostic markers measured on a continuous scale with censored failure time outcome. The proposed method highlights the need to consider both predictive values and the marker distribution in the population when evaluating a marker, and it provides a common scale for comparing different markers. We consider both semiparametric- and nonparametric-based estimating procedures. In addition, we provide asymptotic distribution theory and resampling-based procedures for making statistical inference. We illustrate our approach with numerical studies and data sets from the Seattle Heart Failure Study."], ["Efficient Local Estimation for Time-Varying Coefficients in Deterministic Dynamic Models With Applications to HIV-1 Dynamics", "Recently deterministic dynamic models have become very popular in biomedical research and other scientific areas; examples include modeling human immunodeficiency virus (HIV) dynamics, pharmacokinetic/pharmacodynamic analysis, tumor cell kinetics, and genetic network modeling. In this article we propose estimation methods for the time-varying coefficients in deterministic dynamic systems that are usually described by a set of differential equations. Three two-stage local polynomial estimators are proposed, and their asymptotic normality is established. An alternative approach, a discretization method that is widely used in stochastic diffusion models, is also investigated. We show that the discretization method that uses the simple Euler discretization approach for the deterministic dynamic model does not achieve the optimal convergence rate compared with the proposed two-stage estimators. We use Monte Carlo simulations to study the finite-sample performance, and use a real data application to HIV dynamics to illustrate the proposed methods."], ["Equivalence Between Conditional and Random-Effects Likelihoods for Pair-Matched Case-Control Studies", "Two approaches dominate the analysis of highly stratified data sets: use of conditional likelihood and random-effects models. Conditioning is more traditional and has attractive robustness properties. Contemporary random-effects approaches rely more on accurate assumptions but can be applied to a larger class of problems. For pair-matched studies with arbitrary numbers of categorical covariates, we reconcile these two approaches, showing that the conditional approach has an exact interpretation as a specific type of random-effects model. The random-effects models that provide equivalence with conditioning naturally inherit all the attractive properties of that approach, and we argue they are a pragmatic model choice. We also discuss desirable characteristics of the random-effects model that are entirely novel and that have foundational implications. For example, although our model is specified entirely within a full-likelihood framework, its assumed random-effects distribution does not need a parametric form; we make explicit the robustness that this provides. In addition, our model forces a priori exchangeability of case and control status, an attractive property for objective analyses. The equivalence of the two approaches justifies extensions of conditional likelihood methods to new situations. We give a motivating example from a real problem concerning misclassification error in a genotyping process, and discuss refinements specific to genetic applications."], ["An Approach to Multivariate Covariate-Dependent Quantile Contours With Application to Bivariate Conditional Growth Charts", "Multivariate quantile contours are useful in numerous applications and have been studied in different contexts. However, no easy solutions exist when dynamic and conditional quantile contours are needed without strong distributional assumptions. In this article we propose a new form of bivariate quantile contours and a two-stage estimation procedure to take time effect into account. The proposed procedure relies on quantile regression for longitudinal data and is flexible to include potentially important covariates as necessary. In addition, we propose a visual model assessment tool and discuss a practical guideline for model selection. The performance of the proposed methodology is demonstrated by a simulation study, as well as an application to joint height\u2013weight screening of young children in the United States. We construct bivariate growth charts by a nested sequence of age-dependent and covariate-varying quantile contours of height and weight, and use it to locate an individual subject's percentile rank with respect to a reference population. Our work shows that the proposed method is valuable for pediatric growth monitoring and provides more informative readings than the conventional approach based on univariate growth charts."], [null, null], ["Book Reviews", null], ["Essentials of Statistical Inference", null], ["Modern Experimental Design", null], ["Measurement Error in Nonlinear Models: A Modern Perspective", null], ["Linear Mixed Models: A Practical Guide Using Statistical Software", null], ["Sampling Algorithms", null], ["Elements of Information Theory", null], ["Gaussian Processes for Machine Learning", null], ["A Course on Queueing Models", null], ["Stochastic Switching Systems: Analysis and Design", null], ["Statistical Monitoring of Clinical Trials: A Unified Approach", null], ["Adaptive Design Methods in Clinical Trials", null], ["Pharmacometrics: The Science of Quantitative Pharmacology", null], ["Likelihood, Bayesian and MCMC Methods in Quantitative Genetics", null], ["Bayesian Core: A Practical Approach to Computational Bayesian Statistics", null], ["Telegraphic Reviews", null]]}