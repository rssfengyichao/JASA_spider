{"2013": [["The International Year of Statistics: A Celebration and A Call to Action", null], ["Statistical Learning With Time Series Dependence: An Application to Scoring Sleep in Mice", "We develop methodology that combines statistical learning methods with generalized Markov models, thereby enhancing the former to account for time series dependence. Our methodology can accommodate very general and very long-term time dependence structures in an easily estimable and computationally tractable fashion. We apply our methodology to the scoring of sleep behavior in mice. As methods currently used to score sleep in mice are expensive, invasive, and labor intensive, there is considerable interest in developing high-throughput automated systems which would allow many mice to be scored cheaply and quickly. Previous efforts at automation have been able to differentiate sleep from wakefulness, but they are unable to differentiate the rare and important state of rapid eye movement (REM) sleep from non-REM sleep. Key difficulties in detecting REM are that (i) REM is much rarer than non-REM and wakefulness, (ii) REM looks similar to non-REM in terms of the observed covariates, (iii) the data are noisy, and (iv) the data contain strong time dependence structures crucial for differentiating REM from non-REM. Our new approach (i) shows improved differentiation of REM from non-REM sleep and (ii) accurately estimates aggregate quantities of sleep in our application to video-based sleep scoring of mice. Supplementary materials for this article are available online."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Optimal Allocation of Gold Standard Testing Under Constrained Availability: Application to Assessment of HIV Treatment Failure", "The World Health Organization (WHO) guidelines for monitoring the effectiveness of human immunodeficiency virus (HIV) treatment in resource-limited settings are mostly based on clinical and immunological markers (e.g., CD4 cell counts). Recent research indicates that the guidelines are inadequate and can result in high error rates. Viral load (VL) is considered the \u201cgold standard,\u201d yet its widespread use is limited by cost and infrastructure. In this article, we propose a diagnostic algorithm that uses information from routinely collected clinical and immunological markers to guide a selective use of VL testing for diagnosing HIV treatment failure, under the assumption that VL testing is available only at a certain portion of patient visits. Our algorithm identifies the patient subpopulation, such that the use of limited VL testing on them minimizes a predefined risk (e.g., misdiagnosis error rate). Diagnostic properties of our proposed algorithm are assessed by simulations. For illustration, data from the Miriam Hospital Immunology Clinic (Providence, RI) are analyzed."], ["Semiparametric Bayesian Estimation for Marginal Parametric Potential Outcome Modeling: Application to Causal Inference", null], ["Frailty Models for Familial Risk With Application to Breast Cancer", "In evaluating familial risk for disease we have two main statistical tasks: assessing the probability of carrying an inherited genetic mutation conferring higher risk, and predicting the absolute risk of developing diseases over time for those individuals whose mutation status is known. Despite substantial progress, much remains unknown about the role of genetic and environmental risk factors, about the sources of variation in risk among families that carry high-risk mutations, and about the sources of familial aggregation beyond major Mendelian effects. These sources of heterogeneity contribute substantial variation in risk across families. In this article we present simple and efficient methods for accounting for this variation in familial risk assessment. Our methods are based on frailty models. We implemented them in the context of generalizing Mendelian models of cancer risk, and compared our approaches to others that do not consider heterogeneity across families. Our extensive simulation study demonstrates that when predicting the risk of developing a disease over time conditional on carrier status, accounting for heterogeneity results in a substantial improvement in the area under the curve of the receiver operating characteristic. On the other hand, the improvement for carriership probability estimation is more limited. We illustrate the utility of the proposed approach through the analysis of BRCA1 and BRCA2 mutation carriers in the Washington Ashkenazi Kin-Cohort Study of Breast Cancer. Supplementary materials for this article are available online."], ["A Marginal Approach to Reduced-Rank Penalized Spline Smoothing With Application to Multilevel Functional Data", null], ["Bayesian Hierarchical Modeling of the HIV Evolutionary Response to Therapy", "A major challenge for the treatment of human immunodeficiency virus (HIV) infection is the development of therapy-resistant strains. We present a statistical model that quantifies the evolution of HIV populations when exposed to particular therapies. A hierarchical Bayesian approach is used to estimate differences in rates of nucleotide changes between treatment- and control-group sequences. Each group's rates are allowed to vary spatially along the HIV genome. We employ a coalescent structure to address the sequence diversity within the treatment and control HIV populations. We evaluate the model in simulations and estimate HIV evolution in two different applications: a conventional drug therapy and an antisense gene therapy. In both studies, we detect evidence of evolutionary escape response in the HIV population. Supplementary materials for this article are available online."], ["Generalized Jackknife Estimators of Weighted Average Derivatives", "With the aim of improving the quality of asymptotic distributional approximations for nonlinear functionals of nonparametric estimators, this article revisits the large-sample properties of an important member of that class, namely a kernel-based weighted average derivative estimator. Asymptotic linearity of the estimator is established under weak conditions. Indeed, we show that the bandwidth conditions employed are necessary in some cases. A bias-corrected version of the estimator is proposed and shown to be asymptotically linear under yet weaker bandwidth conditions. Implementational details of the estimators are discussed, including bandwidth selection procedures. Consistency of an analog estimator of the asymptotic variance is also established. Numerical results from a simulation study and an empirical illustration are reported. To establish the results, a novel result on uniform convergence rates for kernel estimators is obtained. The online supplemental material to this article includes details on the theoretical proofs and other analytic derivations, and further results from the simulation study."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Classification Using Censored Functional Data", "We consider classification of functional data when the training curves are not observed on the same interval. Different types of classifier are suggested, one of which involves a new curve extension procedure. Our approach enables us to exploit the information contained in the endpoints of these intervals by incorporating it in an explicit but flexible way. We study asymptotic properties of our classifiers, and show that, in a variety of settings, they can even produce asymptotically perfect classification. The performance of our techniques is illustrated in applications to real and simulated data. Supplementary materials for this article are available online."], ["Selecting the Number of Principal Components in Functional Data", "Functional principal component analysis (FPCA) has become the most widely used dimension reduction tool for functional data analysis. We consider functional data measured at random, subject-specific time points, contaminated with measurement error, allowing for both sparse and dense functional data, and propose novel information criteria to select the number of principal component in such data. We propose a Bayesian information criterion based on marginal modeling that can consistently select the number of principal components for both sparse and dense functional data. For dense functional data, we also develop an Akaike information criterion based on the expected Kullback\u2013Leibler information under a Gaussian assumption. In connecting with the time series literature, we also consider a class of information criteria proposed for factor analysis of multivariate time series and show that they are still consistent for dense functional data, if a prescribed undersmoothing scheme is undertaken in the FPCA algorithm. We perform intensive simulation studies and show that the proposed information criteria vastly outperform existing methods for this type of data. Surprisingly, our empirical evidence shows that our information criteria proposed for dense functional data also perform well for sparse functional data. An empirical example using colon carcinogenesis data is also provided to illustrate the results. Supplementary materials for this article are available online."], ["Sampling for Conditional Inference on Network Data", "Random graphs with given vertex degrees have been widely used as a model for many real-world complex networks. However, both statistical inference and analytic study of such networks present great challenges. In this article, we propose a new sequential importance sampling method for sampling networks with a given degree sequence. These samples can be used to approximate closely the null distributions of a number of test statistics involved in such networks and provide an accurate estimate of the total number of networks with given vertex degrees. We study the asymptotic behavior of the proposed algorithm and prove that the importance weight remains bounded as the size of the graph grows. This property guarantees that the proposed sampling algorithm can still work efficiently even for large sparse graphs. We apply our method to a range of examples to demonstrate its efficiency in real problems."], ["Ensemble Subsampling for Imbalanced Multivariate Two-Sample Tests", "Some existing nonparametric two-sample tests for equality of multivariate distributions perform unsatisfactorily when the two sample sizes are unbalanced. In particular, the power of these tests tends to diminish with increasingly unbalanced sample sizes. In this article, we propose a new testing procedure to solve this problem. The proposed test, based on the nearest neighbor method by Schilling, employs a novel ensemble subsampling scheme to remedy this issue. More specifically, the test statistic is a weighted average of a collection of statistics, each associated with a randomly selected subsample of the data. We derive the asymptotic distribution of the test statistic under the null hypothesis and show that the new test is consistent against all alternatives when the ratio of the sample sizes either goes to a finite limit or tends to infinity. Via simulated data examples we demonstrate that the new test has increasing power with increasing sample size ratio when the size of the smaller sample is fixed. The test is applied to a real-data example in the field of corporate finance. Supplementary materials for this article are available online."], ["Bayesian Modeling of Temporal Dependence in Large Sparse Contingency Tables", "It is of interest in many applications to study trends over time in relationships among categorical variables, such as age group, ethnicity, religious affiliation, political party, and preference for particular policies. At each time point, a sample of individuals provides responses to a set of questions, with different individuals sampled at each time. In such settings, there tend to be an abundance of missing data and the variables being measured may change over time. At each time point, we obtained a large sparse contingency table, with the number of cells often much larger than the number of individuals being surveyed. To borrow information across time in modeling large sparse contingency tables, we propose a Bayesian autoregressive tensor factorization approach. The proposed model relies on a probabilistic Parafac factorization of the joint pmf characterizing the categorical data distribution at each time point, with autocorrelation included across times. We develop efficient computational methods that rely on Markov chain Monte Carlo. The methods are evaluated through simulation examples and applied to social survey data. Supplementary materials for this article are available online."], ["Bayesian Inference for Logistic Models Using P\u00f3lya\u2013Gamma Latent Variables", null], ["Two-Stage Importance Sampling With Mixture Proposals", "For importance sampling (IS), multiple proposals can be combined to address different aspects of a target distribution. There are various methods for IS with multiple proposals, including Hesterberg's stratified IS estimator, Owen and Zhou's regression estimator, and Tan's maximum likelihood estimator. For the problem of efficiently allocating samples to different proposals, it is natural to use a pilot sample to select the mixture proportions before the actual sampling and estimation. However, most current discussions are in an empirical sense for such a two-stage procedure. In this article, we establish a theoretical framework of applying the two-stage procedure for various methods, including the asymptotic properties and the choice of the pilot sample size. By our simulation studies, these two-stage estimators can outperform estimators with naive choices of mixture proportions. Furthermore, while Owen and Zhou's and Tan's estimators are designed for estimating normalizing constants, we extend their usage and the two-stage procedure to estimating expectations and show that the improvement is still preserved in this extension."], ["Epistatic Clustering: A Model-Based Approach for Identifying Links Between Clusters", "Most clustering methods assume that the data can be represented by mutually exclusive clusters, although this assumption may not be the case in practice. For example, in gene expression microarray studies, investigators have often found that a gene can play multiple functions in a cell and may, therefore, belong to more than one cluster simultaneously, and that gene clusters can be linked to each other in certain pathways. This article examines the effect of the above assumption on the likelihood of finding latent clusters using theoretical calculations and simulation studies, for which the epistatic structures were known in advance, and on real data analyses. To explore potential links between clusters, we introduce an epistatic mixture model which extends the Gaussian mixture by including epistatic terms. A generalized expectation-maximization (EM) algorithm is developed to compute the related maximum likelihood estimators. The Bayesian information criterion is then used to determine the order of the proposed model. A bootstrap test is proposed for testing whether the epistatic mixture model is a significantly better fit to the data than a standard mixture model in which each data point belongs to one cluster. The asymptotic properties of the proposed estimators are also investigated when the number of analysis units is large. The results demonstrate that the epistatic links between clusters do have a serious effect on the accuracy of clustering and that our epistatic approach can substantially reduce such an effect and improve the fit."], ["Multiple Testing in a Two-Stage Adaptive Design With Combination Tests Controlling FDR", null], ["Multivariate Density Estimation by Bayesian Sequential Partitioning", "Consider a class of densities that are piecewise constant functions over partitions of the sample space defined by sequential coordinate partitioning. We introduce a prior distribution for a density in this function class and derive in closed form the marginal posterior distribution of the corresponding partition. A computationally efficient method, based on sequential importance sampling, is presented for the inference of the partition from this posterior distribution. Compared to traditional approaches such as the kernel method or the histogram, the Bayesian sequential partitioning (BSP) method proposed here is capable of providing much more accurate estimates when the sample space is of moderate to high dimension. We illustrate this by simulated as well as real data examples. The examples also demonstrate how BSP can be used to design new classification methods competitive with the state of the art."], ["On Optimal Designs for Nonlinear Models: A General and Efficient Algorithm", "Finding optimal designs for nonlinear models is challenging in general. Although some recent results allow us to focus on a simple subclass of designs for most problems, deriving a specific optimal design still mainly depends on numerical approaches. There is need for a general and efficient algorithm that is more broadly applicable than the current state-of-the-art methods. We present a new algorithm that can be used to find optimal designs with respect to a broad class of optimality criteria, when the model parameters or functions thereof are of interest, and for both locally optimal and multistage design strategies. We prove convergence to the optimal design, and show in various examples that the new algorithm outperforms the current state-of-the-art algorithms."], ["Local Linear Regression on Manifolds and Its Geometric Interpretation", null], ["Generalized Residuals for General Models for Contingency Tables With Application to Item Response Theory", "Generalized residuals are a tool employed in the analysis of contingency tables to examine possible sources of model error. They have typically been applied to log-linear models and to latent-class models. A general approach to generalized residuals is developed for a very general class of models for contingency tables. To illustrate their use, generalized residuals are applied to models based on item response theory (IRT) models. Such models are commonly applied to analysis of standardized achievement or aptitude tests. To obtain a realistic perspective on application of generalized residuals, actual testing data are employed."], ["Locally Adaptive Bayes Nonparametric Regression via Nested Gaussian Processes", null], ["Case Definition and Design Sensitivity", null], ["Hidden Markov Models With Applications in Cell Adhesion Experiments", "Estimation of the number of hidden states is challenging in hidden Markov models. Motivated by the analysis of a specific type of cell adhesion experiments, a new framework based on a hidden Markov model and double penalized order selection is proposed. The order selection procedure is shown to be consistent in estimating the number of states. A modified expectation\u2013maximization algorithm is introduced to efficiently estimate parameters in the model. Simulations show that the proposed framework outperforms existing methods. Applications of the proposed methodology to real data demonstrate the accuracy of estimating receptor\u2013ligand bond lifetimes and waiting times which are essential in kinetic parameter estimation. Supplementary materials for this article are available online."], ["Discovering Findings That Replicate From a Primary Study of High Dimension to a Follow-Up Study", "We consider the problem of identifying whether findings replicate from one study of high dimension to another, when the primary study guides the selection of hypotheses to be examined in the follow-up study as well as when there is no division of roles into the primary and the follow-up study. We show that existing meta-analysis methods are not appropriate for this problem, and suggest novel methods instead. We prove that our multiple testing procedures control for appropriate error rates. The suggested family-wise error rate controlling procedure is valid for arbitrary dependence among the test statistics within each study. A more powerful procedure is suggested for false discovery rate (FDR) control. We prove that this procedure controls the FDR if the test statistics are independent within the primary study, and independent or have positive dependence in the follow-up study. For arbitrary dependence within the primary study, and either arbitrary dependence or positive dependence in the follow-up study, simple conservative modifications of the procedure control the FDR. We demonstrate the usefulness of these procedures via simulations and real data examples. Supplementary materials for this article are available online."], ["Adaptive Testing of Conditional Association Through Recursive Mixture Modeling", "In many case-control studies, a central goal is to test for association or dependence between the predictors and the response. Relevant covariates must be conditioned on to avoid false positives and loss in power. Conditioning on covariates is easy in parametric frameworks such as the logistic regression\u2014by incorporating the covariates into the model as additional variables. In contrast, nonparametric methods such as the Cochran-Mantel-Haenszel test accomplish conditioning by dividing the data into strata, one for each possible covariate value. In modern applications, this often gives rise to numerous strata, most of which are sparse due to the multidimensionality of the covariate and/or predictor space, while in reality, the covariate space often consists of just a small number of subsets with differential response-predictor dependence. We introduce a Bayesian approach to inferring from the data such an effective stratification and testing for association accordingly. The core of our framework is a recursive mixture model on the retrospective distribution of the predictors, whose mixing distribution is a prior on the partitions on the covariate space. Inference under the model can proceed efficiently in closed form through a sequence of recursions, striking a balance between model flexibility and computational tractability. Simulation studies show that our method substantially outperforms classical tests under various scenarios. Supplementary materials for this article are available online."], ["A Progressive Block Empirical Likelihood Method for Time Series", "This article develops a new blockwise empirical likelihood (BEL) method for stationary, weakly dependent time processes, called the progressive block empirical likelihood (PBEL). In contrast to the standard version of BEL, which uses data blocks of constant length for a given sample size and whose performance can depend crucially on the block length selection, this new approach involves a data-blocking scheme where blocks increase in length by an arithmetic progression. Consequently, no block length selections are required for the PBEL method, which implies a certain type of robustness for this version of BEL. For inference of smooth functions of the process mean, theoretical results establish the chi-squared limit of the log-likelihood ratio based on PBEL, which can be used to calibrate confidence regions. Using the same progressive block scheme, distributional extensions are also provided for other nonparametric likelihoods with time series in the family of Cressie\u2013Read discrepancies. Simulation evidence indicates that the PBEL method can perform comparably to the standard BEL in coverage accuracy (when the latter uses a \u201cgood\u201d block choice) and can exhibit more stability, without the need to select a usual block length. Supplementary materials for this article are available online."], ["Cure Rate Quantile Regression for Censored Data With a Survival Fraction", "Censored quantile regression offers a valuable complement to the traditional Cox proportional hazards model for survival analysis. Survival times tend to be right-skewed, particularly when there exists a substantial fraction of long-term survivors who are either cured or immune to the event of interest. For survival data with a cure possibility, we propose cure rate quantile regression under the common censoring scheme that survival times and censoring times are conditionally independent given the covariates. In a mixture formulation, we apply censored quantile regression to model the survival times of susceptible subjects and logistic regression to model the indicators of whether patients are susceptible. We develop two estimation methods using martingale-based equations: One approach fully uses all regression quantiles by iterating estimation between the cure rate and quantile regression parameters; and the other separates the two via a nonparametric kernel smoothing estimator. We establish the uniform consistency and weak convergence properties for the estimators obtained from both methods. The proposed model is evaluated through extensive simulation studies and illustrated with a bone marrow transplantation data example. Technical proofs of key theorems are given in Appendices A, B, and C, while those of lemmas and additional simulation studies on model misspecification and comparisons with other models are provided in the online Supplementary Materials A and B."], ["Resampling Procedures for Making Inference Under Nested Case\u2013Control Studies", "The nested case\u2013control (NCC) design has been widely adopted as a cost-effective solution in many large cohort studies for risk assessment with expensive markers, such as the emerging biologic and genetic markers. To analyze data from NCC studies, conditional logistic regression and maximum likelihood-based methods have been proposed. However, most of these methods either cannot be easily extended beyond the Cox model or require additional modeling assumptions. More generally applicable approaches based on inverse probability weighting (IPW) have been proposed as useful alternatives. However, due to the complex correlation structure induced by repeated finite risk set sampling, interval estimation for such IPW estimators remain challenging especially when the estimation involves nonsmooth objective functions or when making simultaneous inferences about functions. Standard resampling procedures such as the bootstrap cannot accommodate the correlation and thus are not directly applicable. In this article, we propose a resampling procedure that can provide valid estimates for the distribution of a broad class of IPW estimators. Simulation results suggest that the proposed procedures perform well in settings when analytical variance estimator is infeasible to derive or gives less optimal performance. The new procedures are illustrated with data from the Framingham Offspring Study to characterize individual level cardiovascular risks over time based on the Framingham risk score, C-reactive protein, and a genetic risk score. Supplementary materials for this article are available online."], ["Book Reviews", null], ["Editorial Collaborators", null], ["Editorial Board EOV", null], ["Multinomial Inverse Regression for Text Analysis", null], ["Comment", null], ["Comment", null], ["Rejoinder: Efficiency and Structure in MNIR", null], ["A Nonparametric Bayesian Model for Local Clustering With Application to Proteomics", "We propose a nonparametric Bayesian local clustering (NoB-LoC) approach for heterogeneous data. NoB-LoC implements inference for nested clusters as posterior inference under a Bayesian model. Using protein expression data as an example, the NoB-LoC model defines a protein (column) cluster as a set of proteins that give rise to the same partition of the samples (rows). In other words, the sample partitions are nested within protein clusters. The common clustering of the samples gives meaning to the protein clusters. Any pair of samples might belong to the same cluster for one protein set but to different clusters for another protein set. These local features are different from features obtained by global clustering approaches such as hierarchical clustering, which create only one partition of samples that applies for all the proteins in the dataset. In addition, the NoB-LoC model is different from most other local or nested clustering methods, which define clusters based on common parameters in the sampling model. As an added and important feature, the NoB-LoC method probabilistically excludes sets of irrelevant proteins and samples that do not meaningfully cocluster with other proteins and samples, thus improving the inference on the clustering of the remaining proteins and samples. Inference is guided by a joint probability model for all the random elements. We provide a simulation study and a motivating example to demonstrate the unique features of the NoB-LoC model. Supplementary materials for this article are available online."], ["Sensitivity Analysis of Per-Protocol Time-to-Event Treatment Efficacy in Randomized Clinical Trials", "Assessing per-protocol (PP) treatment efficacy on a time-to-event endpoint is a common objective of randomized clinical trials. The typical analysis uses the same method employed for the intention-to-treat analysis (e.g., standard survival analysis) applied to the subgroup meeting protocol adherence criteria. However, due to potential post-randomization selection bias, this analysis may mislead about treatment efficacy. Moreover, while there is extensive literature on methods for assessing causal treatment effects in compliers, these methods do not apply to a common class of trials where (a) the primary objective compares survival curves, (b) it is inconceivable to assign participants to be adherent and event free before adherence is measured, and (c) the exclusion restriction assumption fails to hold. HIV vaccine efficacy trials including the recent RV144 trial exemplify this class, because many primary endpoints (e.g., HIV infections) occur before adherence is measured, and nonadherent subjects who receive some of the planned immunizations may be partially protected. Therefore, we develop methods for assessing PP treatment efficacy for this problem class, considering three causal estimands of interest. Because these estimands are not identifiable from the observable data, we develop nonparametric bounds and semiparametric sensitivity analysis methods that yield estimated ignorance and uncertainty intervals. The methods are applied to RV144. Supplementary materials for this article are available online."], ["Integrated Modeling of European Migration", "International migration data in Europe are collected by individual countries with separate collection systems and designs. As a result, reported data are inconsistent in availability, definition, and quality. In this article, we propose a Bayesian model to overcome the limitations of the various data sources. The focus is on estimating recent international migration flows among 31 countries in the European Union and European Free Trade Association from 2002 to 2008, using data collated by Eurostat. We also incorporate covariate information and information provided by experts on the effects of undercount, measurement, and accuracy of data collection systems. The methodology is integrated and produces a synthetic database with measures of uncertainty for international migration flows and other model parameters. Supplementary materials for this article are available online."], ["A Nonparametric Spatial Model for Periodontal Data With Nonrandom Missingness", "Periodontal disease (PD) progression is often quantified by clinical attachment level (CAL) defined as the distance down a tooth's root that is detached from the surrounding bone. Measured at six locations per tooth throughout the mouth (excluding the molars), it gives rise to a dependent data setup. These data are often reduced to a one-number summary, such as the whole-mouth average or the number of observations greater than a threshold, to be used as the response in a regression to identify important covariates related to the current state of a subject's periodontal health. Rather than a simple one-number summary, we set forward to analyze all available CAL data for each subject, exploiting the presence of spatial dependence, nonstationarity, and nonnormality. Also, many subjects have a considerable proportion of missing teeth, which cannot be considered missing at random because PD is the leading cause of adult tooth loss. Under a Bayesian paradigm, we propose a nonparametric flexible spatial (joint) model of observed CAL and the location of missing tooth via kernel convolution methods, incorporating the aforementioned features of CAL data under a unified framework. Application of this methodology to a dataset recording the periodontal health of an African-American population, as well as simulation studies reveal the gain in model fit and inference, and provides a new perspective into unraveling covariate\u2013response relationships in the presence of complexities posed by these data."], ["Estimation and Inference Concerning Ordered Means in Analysis of Covariance Models With Interactions", "When interactions are identified in analysis of covariance models, it becomes important to identify values of the covariates for which there are significant differences or, more generally, significant contrasts among the group mean responses. Inferential procedures that incorporate a priori order restrictions among the group mean responses would be expected to be superior to those that ignore this information. In this article, we focus on analysis of covariance models with prespecified order restrictions on the mean response across the levels of a grouping variable when the grouping variable may interact with model covariates. In order for the restrictions to hold in the presence of interactions, it is necessary to impose the requirement that the restrictions hold over all levels of interacting categorical covariates and across prespecified ranges of interacting continuous covariates. The parameter estimation procedure involves solving a quadratic programming minimization problem with a carefully specified constraint matrix. Simultaneous confidence intervals for treatment group contrasts and tests for equality of the ordered group mean responses are determined by exploiting previously unconnected literature. The proposed methods are motivated by a clinical trial of the dopamine agonist pramipexole for the treatment of early-stage Parkinson's disease."], ["Markov-Modulated Nonhomogeneous Poisson Processes for Modeling Detections in Surveys of\u00a0Marine Mammal Abundance", "We consider Markov-modulated nonhomogeneous Poisson processes for modeling sightings of marine mammals in shipboard or aerial surveys. In such surveys, detection of an animal is possible only when it surfaces, and with some species a substantial proportion of animals is missed because they are diving and thus not available for detection. This needs to be adequately accounted for to avoid biased abundance estimates. The tendency of surfacing events of marine mammals to occur in clusters motivates consideration of the flexible class of Markov-modulated Poisson processes in this context. We embed these models in distance sampling models, introducing nonhomogeneity in the process to account for the fact that the observer's probability of detecting an animal decreases with increasing distance to the animal. We derive approximate expressions for the likelihood of Markov-modulated nonhomogeneous Poisson processes that enable us to estimate the model parameters through numerical maximum likelihood. The performance of the approach is investigated in an extensive simulation study, and applications to pilot and beaked whale tag data as well as to minke whale tag and survey data demonstrate its relevance in abundance estimation."], ["Second-Order Exchangeability Analysis for Multimodel Ensembles", "The challenge of understanding complex systems often gives rise to a multiplicity of models. It is natural to consider whether the outputs of these models can be combined to produce a system prediction that is more informative than the output of any one of the models taken in isolation. And, in particular, to consider the relationship between the spread of model outputs and system uncertainty. We describe a statistical framework for such a combination, based on the exchangeability of the models, and their coexchangeability with the system. We demonstrate the simplest implementation of our framework in the context of climate prediction. Throughout we work entirely in means and variances to avoid the necessity of specifying higher-order quantities for which we often lack well-founded judgments."], ["Spatial Shrinkage Estimation of Diffusion Tensors on Diffusion-Weighted Imaging Data", null], ["An Integrative Bayesian Modeling Approach to Imaging Genetics", "In this article we present a Bayesian hierarchical modeling approach for imaging genetics, where the interest lies in linking brain connectivity across multiple individuals to their genetic information. We have available data from a functional magnetic resonance imaging (fMRI) study on schizophrenia. Our goals are to identify brain regions of interest (ROIs) with discriminating activation patterns between schizophrenic patients and healthy controls, and to relate the ROIs\u2019 activations with available genetic information from single nucleotide polymorphisms (SNPs) on the subjects. For this task, we develop a hierarchical mixture model that includes several innovative characteristics: it incorporates the selection of ROIs that discriminate the subjects into separate groups; it allows the mixture components to depend on selected covariates; it includes prior models that capture structural dependencies among the ROIs. Applied to the schizophrenia dataset, the model leads to the simultaneous selection of a set of discriminatory ROIs and the relevant SNPs, together with the reconstruction of the correlation structure of the selected regions. To the best of our knowledge, our work represents the first attempt at a rigorous modeling strategy for imaging genetics data that incorporates all such features."], ["A Phase I Bayesian Adaptive Design to Simultaneously Optimize Dose and Schedule Assignments Both Between and Within Patients", null], ["A Latent Source Model to Detect Multiple Spatial Clusters With Application in a Mobile Sensor Network for Surveillance of Nuclear Materials", "Potential nuclear attacks are among the most devastating terrorist attacks, with severe loss of human lives as well as damage to infrastructure. To deter such threats, it becomes increasingly vital to have sophisticated nuclear surveillance and detection systems deployed in major cities in the United States, such as New York City. In this article, we design a mobile sensor network and develop statistical algorithms and models to provide consistent and pervasive surveillance of nuclear materials in major cities. The network consists of a large number of vehicles on which nuclear sensors and Global Position System (GPS) tracking devices are installed. Real time sensor readings and GPS information are transmitted to and processed at a central surveillance center. Mathematical and statistical analyses are performed, in which we mimic a signal-generating process and develop a latent source modeling framework to detect multiple spatial clusters. A Monte Carlo expectation-maximization algorithm is developed to estimate model parameters, detect significant clusters, and identify their locations and sizes. We also determine the number of clusters using a modified Akaike Information Criterion/Bayesian Information Criterion. Simulation studies to evaluate the effectiveness and detection power of such a network are described."], [null, null], ["Nonparametric Mixture of Regression Models", "Motivated by an analysis of U.S. house price index (HPI) data, we propose nonparametric finite mixture of regression models. We study the identifiability issue of the proposed models, and develop an estimation procedure by employing kernel regression. We further systematically study the sampling properties of the proposed estimators, and establish their asymptotic normality. A modified EM algorithm is proposed to carry out the estimation procedure. We show that our algorithm preserves the ascent property of the EM algorithm in an asymptotic sense. Monte Carlo simulations are conducted to examine the finite sample performance of the proposed estimation procedure. An empirical analysis of the U.S. HPI data is illustrated for the proposed methodology."], ["Validation of Visual Statistical Inference, Applied to Linear Models", "Statistical graphics play a crucial role in exploratory data analysis, model checking, and diagnosis. The lineup protocol enables statistical significance testing of visual findings, bridging the gulf between exploratory and inferential statistics. In this article, inferential methods for statistical graphics are developed further by refining the terminology of visual inference and framing the lineup protocol in a context that allows direct comparison with conventional tests in scenarios when a conventional test exists. This framework is used to compare the performance of the lineup protocol against conventional statistical testing in the scenario of fitting linear models. A human subjects experiment is conducted using simulated data to provide controlled conditions. Results suggest that the lineup protocol performs comparably with the conventional tests, and expectedly outperforms them when data are contaminated, a scenario where assumptions required for performing a conventional test are violated. Surprisingly, visual tests have higher power than the conventional tests when the effect size is large. And, interestingly, there may be some super-visual individuals who yield better performance and power than the conventional test even in the most difficult tasks. Supplementary materials for this article are available online."], ["Latent Supervised Learning", null], ["Nonparametric Estimation of Conditional Distributions and Rank-Tracking Probabilities With Time-Varying Transformation Models in Longitudinal Studies", null], ["Time-Varying Additive Models for Longitudinal Data", "The additive model is an effective dimension-reduction approach that also provides flexibility in modeling the relation between a response variable and key covariates. The literature is largely developed to scalar response and vector covariates. In this article, more complex data are of interest, where both the response and the covariates are functions. We propose a functional additive model together with a new backfitting algorithm to estimate the unknown regression functions, whose components are time-dependent additive functions of the covariates. Such functional data may not be completely observed since measurements may only be collected intermittently at discrete time points. We develop a unified platform and an efficient approach that can cover both dense and sparse functional data and the needed theory for statistical inference. We also establish the oracle properties of the proposed estimators of the component functions."], ["Partial Factor Modeling: Predictor-Dependent Shrinkage for Linear Regression", "We develop a modified Gaussian factor model for the purpose of inducing predictor-dependent shrinkage for linear regression. The new model predicts well across a wide range of covariance structures, on real and simulated data. Furthermore, the new model facilitates variable selection in the case of correlated predictor variables, which often stymies other methods."], ["Parameter Estimation of Partial Differential Equation Models", null], ["Wavelet-Variance-Based Estimation for Composite Stochastic Processes", "This article presents a new estimation method for the parameters of a time series model. We consider here composite Gaussian processes that are the sum of independent Gaussian processes which, in turn, explain an important aspect of the time series, as is the case in engineering and natural sciences. The proposed estimation method offers an alternative to classical estimation based on the likelihood, that is straightforward to implement and often the only feasible estimation method with complex models. The estimator furnishes results as the optimization of a criterion based on a standardized distance between the sample wavelet variances (WV) estimates and the model-based WV. Indeed, the WV provides a decomposition of the variance process through different scales, so that they contain the information about different features of the stochastic model. We derive the asymptotic properties of the proposed estimator for inference and perform a simulation study to compare our estimator to the MLE and the LSE with different models. We also set sufficient conditions on composite models for our estimator to be consistent, that are easy to verify. We use the new estimator to estimate the stochastic error's parameters of the sum of three first order Gauss\u2013Markov processes by means of a sample of over 800, 000 issued from gyroscopes that compose inertial navigation systems. Supplementary materials for this article are available online."], ["Efficiency for Regularization Parameter Selection in\u00a0Penalized Likelihood Estimation of\u00a0Misspecified\u00a0Models", null], ["Asymptotic Equivalence of Regularization Methods in Thresholded Parameter Space", null], ["Estimation of Extreme Conditional Quantiles Through Power Transformation", "The estimation of extreme conditional quantiles is an important issue in numerous disciplines. Quantile regression (QR) provides a natural way to capture the covariate effects at different tails of the response distribution. However, without any distributional assumptions, estimation from conventional QR is often unstable at the tails, especially for heavy-tailed distributions due to data sparsity. In this article, we develop a new three-stage estimation procedure that integrates QR and extreme value theory by estimating intermediate conditional quantiles using QR and extrapolating these estimates to tails based on extreme value theory. Using the power-transformed QR, the proposed method allows more flexibility than existing methods that rely on the linearity of quantiles on the original scale, while extending the applicability of parametric models to borrow information across covariates without resorting to nonparametric smoothing. In addition, we propose a test procedure to assess the commonality of extreme value index, which could be useful for obtaining more efficient estimation by sharing information across covariates. We establish the asymptotic properties of the proposed method and demonstrate its value through simulation study and the analysis of a medical cost data. Supplementary materials for this article are available online."], ["Estimation of Censored Quantile Regression for Panel Data With Fixed Effects", "This article investigates estimation of censored quantile regression (QR) models with fixed effects. Standard available methods are not appropriate for estimation of a censored QR model with a large number of parameters or with covariates correlated with unobserved individual heterogeneity. Motivated by these limitations, the article proposes estimators that are obtained by applying fixed effects QR to subsets of observations selected either parametrically or nonparametrically. We derive the limiting distribution of the new estimators under joint limits, and conduct Monte Carlo simulations to assess their small sample performance. An empirical application of the method to study the impact of the 1964 Civil Rights Act on the black\u2013white earnings gap is considered. Supplementary materials for this article are available online."], ["Semiparametric Efficient and Robust Estimation of an Unknown Symmetric Population Under Arbitrary Sample Selection Bias", "We propose semiparametric methods to estimate the center and shape of a symmetric population when a representative sample of the population is unavailable due to selection bias. We allow an arbitrary sample selection mechanism determined by the data collection procedure, and we do not impose any parametric form on the population distribution. Under this general framework, we construct a family of consistent estimators of the center that is robust to population model misspecification, and we identify the efficient member that reaches the minimum possible estimation variance. The asymptotic properties and finite sample performance of the estimation and inference procedures are illustrated through theoretical analysis and simulations. A data example is also provided to illustrate the usefulness of the methods in practice."], ["From Depth to Local Depth: A Focus on Centrality", null], ["Using Secondary Outcomes to Sharpen Inference in Randomized Experiments With Noncompliance", null], ["Book Reviews", null], ["Correction", null], ["In Praise of Simplicity not Mathematistry! Ten Simple Powerful Ideas for the Statistical Scientist", "Ronald Fisher was by all accounts a first-rate mathematician, but he saw himself as a scientist, not a mathematician, and he railed against what George Box called (in his Fisher lecture) \u201cmathematistry.\u201d Mathematics is the indispensable foundation of statistics, but for me the real excitement and value of our subject lies in its application to other disciplines. We should not view statistics as another branch of mathematics and favor mathematical complexity over clarifying, formulating, and solving real-world problems. Valuing simplicity, I describe 10 simple and powerful ideas that have influenced my thinking about statistics, in my areas of research interest: missing data, causal inference, survey sampling, and statistical modeling in general. The overarching theme is that statistics is a missing data problem and the goal is to predict unknowns with appropriate measures of uncertainty."], ["Partially Ordered Mixed Hidden Markov Model for the Disablement Process of Older Adults", null], ["A Generalized Fellegi\u2013Sunter Framework for Multiple Record Linkage With Application to Homicide Record Systems", null], ["Tracking Multiple Targets Using Binary Decisions From Wireless Sensor Networks", "This article introduces a framework for tracking multiple targets over time using binary decisions collected by a wireless sensor network, and applies the methodology to two case studies\u2014an experiment involving tracking people and a dataset adapted from a project tracking zebras in Kenya. The tracking approach is based on a penalized maximum likelihood framework, and allows for sensor failures, targets appearing and disappearing over time, and complex intersecting target trajectories. We show that binary decisions about the presence/absence of a target in a sensor's neighborhood, corrected locally by a method known as local vote decision fusion, provide the most robust performance in noisy environments and give good tracking results in applications."], ["Spline-Based Emulators for Radiative Shock Experiments With Measurement Error", null], ["A Bayesian Reliability Analysis of Neutron-Induced Errors in High Performance Computing Hardware", "A soft error is an undesired change in an electronic device's state, for example, a bit flip in computer memory, that does not permanently affect its functionality. In microprocessor systems, neutron-induced soft errors can cause crashes and silent data corruption (SDC). SDC occurs when a soft error produces a computational result that is incorrect, without the system issuing a warning or error message. Hence, neutron-induced soft errors are a major concern for high performance computing platforms that perform scientific computation. Through accelerated neutron beam testing of hardware in its field configuration, the frequencies of failures (crashes) and of SDCs in hardware from the Roadrunner platform, the first Petaflop supercomputer, are estimated. The impact of key factors on field performance is investigated and estimates of field reliability are provided. Finally, a novel statistical approach for the analysis of interval-censored survival data with mixed effects and uncertainty in the interval endpoints, key features of the experimental data, is presented. Supplementary materials for this article are available online."], ["Treatment Evaluation With Selective Participation and Ineligibles", "Matching methods for treatment evaluation based on a conditional independence assumption do not balance selective unobserved differences between treated and nontreated. We derive a simple correction term if there is an instrument that shifts the treatment probability to zero in specific cases. Policies with eligibility restrictions, where treatment is impossible if some variable exceeds a certain value, provide a natural application. In an empirical analysis, we exploit the age eligibility restriction in the Swedish Youth Practice subsidized work program for young unemployed, where compliance is imperfect among the young. Adjusting the matching estimator for selectivity changes the results toward making subsidized work detrimental in moving individuals into employment."], ["Straight to the Source: Detecting Aggregate Objects in Astronomical Images With Proper Error Control", null], ["Mediation and Spillover Effects in Group-Randomized Trials: A Case Study of the 4Rs Educational Intervention", "Peer influence and social interactions can give rise to spillover effects in which the exposure of one individual may affect outcomes of other individuals. Even if the intervention under study occurs at the group or cluster level as in group-randomized trials, spillover effects can occur when the mediator of interest is measured at a lower level than the treatment. Evaluators who choose groups rather than individuals as experimental units in a randomized trial often anticipate that the desirable changes in targeted social behaviors will be reinforced through interference among individuals in a group exposed to the same treatment. In an empirical evaluation of the effect of a school-wide intervention on reducing individual students\u2019 depressive symptoms, schools in matched pairs were randomly assigned to the 4Rs intervention or the control condition. Class quality was hypothesized as an important mediator assessed at the classroom level. We reason that the quality of one classroom may affect outcomes of children in another classroom because children interact not simply with their classmates but also with those from other classes in the hallways or on the playground. In investigating the role of class quality as a mediator, failure to account for such spillover effects of one classroom on the outcomes of children in other classrooms can potentially result in bias and problems with interpretation. Using a counterfactual conceptualization of direct, indirect, and spillover effects, we provide a framework that can accommodate issues of mediation and spillover effects in group randomized trials. We show that the total effect can be decomposed into a natural direct effect, a within-classroom mediated effect, and a spillover mediated effect. We give identification conditions for each of the causal effects of interest and provide results on the consequences of ignoring \u201cinterference\u201d or \u201cspillover effects\u201d when they are in fact present. Our modeling approach disentangles these effects. The analysis examines whether the 4Rs intervention has an effect on childrens\u2019 depressive symptoms through changing the quality of other classes as well as through changing the quality of a child's own class. Supplementary materials for this article are available online."], ["A Hierarchical Bayesian Approach for Aerosol Retrieval Using MISR Data", "Atmospheric aerosols can cause serious damage to human health and reduce life expectancy. Using the radiances observed by NASA's Multi-angle Imaging SpectroRadiometer (MISR), the current MISR operational algorithm retrieves aerosol optical depth (AOD) at 17.6\u00a0km resolution. A systematic study of aerosols and their impact on public health, especially in highly populated urban areas, requires finer-resolution estimates of AOD's spatial distribution. We embed MISR's operational weighted least squares criterion and its forward calculations for AOD retrievals in a likelihood framework and further expand into a hierarchical Bayesian model to adapt to finer spatial resolution of 4.4\u00a0km. To take advantage of AOD's spatial smoothness, our method borrows strength from data at neighboring areas by postulating a Gaussian Markov random field prior for AOD. Our model considers AOD and aerosol mixing vectors as continuous variables, whose inference is carried out using Metropolis-within-Gibbs sampling methods. Retrieval uncertainties are quantified by posterior variabilities. We also develop a parallel Markov chain Monte Carlo (MCMC) algorithm to improve computational efficiency. We assess our retrieval performance using ground-based measurements from the AErosol RObotic NETwork (AERONET) and satellite images from Google Earth. Based on case studies in the greater Beijing area, China, we show that 4.4\u00a0km resolution can improve both the accuracy and coverage of remotely sensed aerosol retrievals, as well as our understanding of the spatial and seasonal behaviors of aerosols. This is particularly important during high-AOD events, which often indicate severe air pollution."], ["Bayesian Hierarchical Poisson Regression Models: An Application to a Driving Study With Kinematic Events", "Although there is evidence that teenagers are at a high risk of crashes in the early months after licensure, the driving behavior of these teenagers is not well understood. The Naturalistic Teenage Driving Study (NTDS) is the first U.S. study to document continuous driving performance of newly licensed teenagers during their first 18 months of licensure. Counts of kinematic events such as the number of rapid accelerations are available for each trip, and their incidence rates represent different aspects of driving behavior. We propose a hierarchical Poisson regression model incorporating overdispersion, heterogeneity, and serial correlation as well as a semiparametric mean structure. Analysis of the NTDS data is carried out with a hierarchical Bayesian framework using reversible jump Markov chain Monte Carlo algorithms to accommodate the flexible mean structure. We show that driving with a passenger and night driving decrease kinematic events, while having risky friends increases these events. Further the within-subject variation in these events is comparable to the between-subject variation. This methodology will be useful for other intensively collected longitudinal count data, where event rates are low and interest focuses on estimating the mean and variance structure of the process. Supplementary materials for this article are available online."], ["Partially Identified Treatment Effects Under Imperfect Compliance: The Case of Domestic Violence", "The Minneapolis Domestic Violence Experiment (MDVE) is a randomized social experiment with imperfect compliance that has been extremely influential in how police officers respond to misdemeanor domestic violence. This article reexamines data from the MDVE, using recent literature on partial identification to find recidivism associated with a policy that arrests misdemeanor domestic violence suspects rather than not arresting them. Using partially identified bounds on the average treatment effect, I find that arresting rather than not arresting suspects can potentially reduce recidivism by more than two-and-a-half times the corresponding intent-to-treat estimate and more than two times the corresponding local average treatment effect, even when making minimal assumptions on counterfactuals."], ["A Study of Mexican Free-Tailed Bat Chirp Syllables: Bayesian Functional Mixed Models for Nonstationary Acoustic Time Series", "We describe a new approach to analyze chirp syllables of free-tailed bats from two regions of Texas in which they are predominant: Austin and College Station. Our goal is to characterize any systematic regional differences in the mating chirps and assess whether individual bats have signature chirps. The data are analyzed by modeling spectrograms of the chirps as responses in a Bayesian functional mixed model. Given the variable chirp lengths, we compute the spectrograms on a relative time scale interpretable as the relative chirp position, using a variable window overlap based on chirp length. We use two-dimensional wavelet transforms to capture correlation within the spectrogram in our modeling and obtain adaptive regularization of the estimates and inference for the regions-specific spectrograms. Our model includes random effect spectrograms at the bat level to account for correlation among chirps from the same bat and to assess relative variability in chirp spectrograms within and between bats. The modeling of spectrograms using functional mixed models is a general approach for the analysis of replicated nonstationary time series, such as our acoustical signals, to relate aspects of the signals to various predictors, while accounting for between-signal structure. This can be done on raw spectrograms when all signals are of the same length and can be done using spectrograms defined on a relative time scale for signals of variable length in settings where the idea of defining correspondence across signals based on relative position is sensible. Supplementary materials for this article are available online."], ["Effectively Selecting a Target Population for a Future Comparative Study", "When comparing a new treatment with a control in a randomized clinical study, the treatment effect is generally assessed by evaluating a summary measure over a specific study population. The success of the trial heavily depends on the choice of such a population. In this article, we show a systematic, effective way to identify a promising population, for which the new treatment is expected to have a desired benefit, using the data from a current study involving similar comparator treatments. Specifically, using the existing data, we first create a parametric scoring system as a function of multiple baseline covariates to estimate subject-specific treatment differences. Based on this scoring system, we specify a desired level of treatment difference and obtain a subgroup of patients, defined as those whose estimated scores exceed this threshold. An empirically calibrated threshold-specific treatment difference curve across a range of score values is constructed. The subpopulation of patients satisfying any given level of treatment benefit can then be identified accordingly. To avoid bias due to overoptimism, we use a cross-training-evaluation method for implementing the above two-step procedure. We then show how to select the best scoring system among all competing models. Furthermore, for cases in which only a single prespecified working model is involved, inference procedures are proposed for the average treatment difference over a range of score values using the entire dataset and are justified theoretically and numerically. Finally, the proposals are illustrated with the data from two clinical trials in treating HIV and cardiovascular diseases. Note that if we are not interested in designing a new study for comparing similar treatments, the new procedure can also be quite useful for the management of future patients, so that treatment may be targeted toward those who would receive nontrivial benefits to compensate for the risk or cost of the new treatment. Supplementary materials for this article are available online."], ["Tensor Regression with Applications in Neuroimaging Data Analysis", "Classical regression methods treat covariates as a vector and estimate a corresponding vector of regression coefficients. Modern applications in medical imaging generate covariates of more complex form such as multidimensional arrays (tensors). Traditional statistical and computational methods are proving insufficient for analysis of these high-throughput data due to their ultrahigh dimensionality as well as complex structure. In this article, we propose a new family of tensor regression models that efficiently exploit the special structure of tensor covariates. Under this framework, ultrahigh dimensionality is reduced to a manageable level, resulting in efficient estimation and prediction. A fast and highly scalable estimation algorithm is proposed for maximum likelihood estimation and its associated asymptotic properties are studied. Effectiveness of the new methods is demonstrated on both synthetic and real MRI imaging data. Supplementary materials for this article are available online."], ["Auxiliary Marker-Assisted Classification in the Absence of Class Identifiers", "Constructing classification rules for accurate diagnosis of a disorder is an important goal in medical practice. In many clinical applications, there is no clinically significant anatomical or physiological deviation that exists to identify the gold standard disease status to inform development of classification algorithms. Despite the absence of perfect disease class identifiers, there are usually one or more disease-informative auxiliary markers along with feature variables that comprise known symptoms. Existing statistical learning approaches do not effectively draw information from auxiliary prognostic markers. We propose a large margin classification method, with particular emphasis on the support vector machine, assisted by available informative markers to classify disease without knowing a subject's true disease status. We view this task as statistical learning in the presence of missing data, and introduce a pseudo-Expectation-Maximization (EM) algorithm to the classification. A major difference between a regular EM algorithm and the algorithm proposed here is that we do not model the distribution of missing data given the observed feature variables either parametrically or semiparametrically. We also propose a sparse variable selection method embedded in the pseudo-EM algorithm. Theoretical examination shows that the proposed classification rule is Fisher consistent, and that under a linear rule, the proposed selection has an oracle variable selection property and the estimated coefficients are asymptotically normal. We apply the methods to build decision rules for including subjects in clinical trials of a new psychiatric disorder and present four applications to data available at the University of California, Irvine Machine Learning Repository."], ["Unified Analysis of Secondary Traits in Case\u2013Control Association Studies", "It has been repeatedly shown that in case\u2013control association studies, analysis of a secondary trait that ignores the original sampling scheme can produce highly biased risk estimates. Although a number of approaches have been proposed to properly analyze secondary traits, most approaches fail to reproduce the marginal logistic model assumed for the original case\u2013control trait and/or do not allow for interaction between secondary trait and genotype marker on primary disease risk. In addition, the flexible handling of covariates remains challenging. We present a general retrospective likelihood framework to perform association testing for both binary and continuous secondary traits, which respects marginal models and incorporates the interaction term. We provide a computational algorithm, based on a reparameterized approximate profile likelihood, for obtaining the maximum likelihood (ML) estimate and its standard error for the genetic effect on secondary traits, in the presence of covariates. For completeness, we also present an alternative pseudo-likelihood method for handling covariates. We describe extensive simulations to evaluate the performance of the ML estimator in comparison with the pseudo-likelihood and other competing methods. Supplementary materials for this article are available online."], ["Clustering High-Dimensional Time Series Based on Parallelism", "This article considers the problem of clustering high-dimensional time series based on trend parallelism. The underlying process is modeled as a nonparametric trend function contaminated by locally stationary errors, a special class of nonstationary processes. For each group where the parallelism holds, I semiparametrically estimate its representative trend function and vertical shifts of group members, and establish their central limit theorems. An information criterion, consisting of in-group similarities and number of groups, is then proposed for the purpose of clustering. I prove its theoretical consistency and propose a splitting-coalescence algorithm to reduce the computational burden in practice. The method is illustrated by both simulation and a real-data example."], ["Bayesian Subset Modeling for High-Dimensional Generalized Linear Models", "This article presents a new prior setting for high-dimensional generalized linear models, which leads to a Bayesian subset regression (BSR) with the maximum a posteriori model approximately equivalent to the minimum extended Bayesian information criterion model. The consistency of the resulting posterior is established under mild conditions. Further, a variable screening procedure is proposed based on the marginal inclusion probability, which shares the same properties of sure screening and consistency with the existing sure independence screening (SIS) and iterative sure independence screening (ISIS) procedures. However, since the proposed procedure makes use of joint information from all predictors, it generally outperforms SIS and ISIS in real applications. This article also makes extensive comparisons of BSR with the popular penalized likelihood methods, including Lasso, elastic net, SIS, and ISIS. The numerical results indicate that BSR can generally outperform the penalized likelihood methods. The models selected by BSR tend to be sparser and, more importantly, of higher prediction ability. In addition, the performance of the penalized likelihood methods tends to deteriorate as the number of predictors increases, while this is not significant for BSR. Supplementary materials for this article are available online."], ["Empirical Bayes Confidence Intervals for Selected Parameters in High-Dimensional Data", null], ["Oracally Efficient Two-Step Estimation of Generalized Additive Model", "The generalized additive model (GAM) is a multivariate nonparametric regression tool for non-Gaussian responses including binary and count data. We propose a spline-backfitted kernel (SBK) estimator for the component functions and the constant, which are oracally efficient under weak dependence. The SBK technique is both computationally expedient and theoretically reliable, thus usable for analyzing high-dimensional time series. Inference can be made on component functions based on asymptotic normality. Simulation evidence strongly corroborates the asymptotic theory. The method is applied to estimate insolvent probability and to obtain higher accuracy ratio than a previous study. Supplementary materials for this article are available online."], ["Robust Variable Selection With Exponential Squared Loss", null], ["Efficient Robust Regression via Two-Stage Generalized Empirical Likelihood", "Large- and finite-sample efficiency and resistance to outliers are the key goals of robust statistics. Although often not simultaneously attainable, we develop and study a linear regression estimator that comes close. Efficiency is obtained from the estimator's close connection to generalized empirical likelihood, and its favorable robustness properties are obtained by constraining the associated sum of (weighted) squared residuals. We prove maximum attainable finite-sample replacement breakdown point and full asymptotic efficiency for normal errors. Simulation evidence shows that compared to existing robust regression estimators, the new estimator has relatively high efficiency for small sample sizes and comparable outlier resistance. The estimator is further illustrated and compared to existing methods via application to a real dataset with purported outliers."], ["Bayesian Gaussian Copula Factor Models for Mixed Data", null], ["Nonparametric Identification of Copula Structures", "We propose a unified framework for testing a variety of assumptions commonly made about the structure of copulas, including symmetry, radial symmetry, joint symmetry, associativity and Archimedeanity, and max-stability. Our test is nonparametric and based on the asymptotic distribution of the empirical copula process. We perform simulation experiments to evaluate our test and conclude that our method is reliable and powerful for assessing common assumptions on the structure of copulas, particularly when the sample size is moderately large. We illustrate our testing approach on two datasets."], ["Copula-Based Regression Estimation and Inference", "We investigate a new approach to estimating a regression function based on copulas. The main idea behind this approach is to write the regression function in terms of a copula and marginal distributions. Once the copula and the marginal distributions are estimated, we use the plug-in method to construct our new estimator. Because various methods are available in the literature for estimating both a copula and a distribution, this idea provides a rich and flexible family of regression estimators. We provide some asymptotic results related to this copula-based regression modeling when the copula is estimated via profile likelihood and the marginals are estimated nonparametrically. We also study the finite sample performance of the estimator and illustrate its usefulness by analyzing data from air pollution studies."], ["Simulated Method of Moments Estimation for Copula-Based Multivariate Models", null], ["Linear Transformation Model With Parametric Covariate Transformations", null], ["Simultaneous Grouping Pursuit and Feature Selection Over an Undirected Graph", "In high-dimensional regression, grouping pursuit and feature selection have their own merits while complementing each other in battling the curse of dimensionality. To seek a parsimonious model, we perform simultaneous grouping pursuit and feature selection over an arbitrary undirected graph with each node corresponding to one predictor. When the corresponding nodes are reachable from each other over the graph, regression coefficients can be grouped, whose absolute values are the same or close. This is motivated from gene network analysis, where genes tend to work in groups according to their biological functionalities. Through a nonconvex penalty, we develop a computational strategy and analyze the proposed method. Theoretical analysis indicates that the proposed method reconstructs the oracle estimator, that is, the unbiased least-square estimator given the true grouping, leading to consistent reconstruction of grouping structures and informative features, as well as to optimal parameter estimation. Simulation studies suggest that the method combines the benefit of grouping pursuit with that of feature selection, and compares favorably against its competitors in selection accuracy and predictive performance. An application to eQTL data is used to illustrate the methodology, where a network is incorporated into analysis through an undirected graph."], ["Heteroscedasticity and Autocorrelation Robust Structural Change Detection", "The assumption of (weak) stationarity is crucial for the validity of most of the conventional tests of structure change in time series. Under complicated nonstationary temporal dynamics, we argue that traditional testing procedures result in mixed structural change signals of the first and second order and hence could lead to biased testing results. The article proposes a simple and unified bootstrap testing procedure that provides consistent testing results under general forms of smooth and abrupt changes in the temporal dynamics of the time series. Monte Carlo experiments are performed to compare our testing procedure with various traditional tests. Our robust bootstrap test is applied to testing changes in an environmental and a financial time series and our procedure is shown to provide more reliable results than the conventional tests."], ["The Poisson Compound Decision Problem Revisited", "The compound decision problem for a vector of independent Poisson random variables with possibly different means has a half-century-old solution. However, it appears that the classical solution needs smoothing adjustment. We discuss three such adjustments. We also present another approach that first transforms the problem into the normal compound decision problem. A simulation study shows the effectiveness of the procedures in improving the performance over that of the classical procedure. A real data example is also provided. The procedures depend on a smoothness parameter that can be selected using a nonstandard cross-validation step, which is of independent interest. Finally, we mention some asymptotic results."], ["Book Reviews", null], ["Building the Big Tent for Statistics", null], ["Modeling and Forecasting Daily Electricity Load Curves: A Hybrid Approach", null], ["Circuit Theory and Model-Based Inference for Landscape Connectivity", null], ["A Bayesian Procedure for File Linking to Analyze End-of-Life Medical Costs", null], ["A Nested Dirichlet Process Analysis of Cluster Randomized Trial Data With Application in Geriatric Care Assessment", "In cluster randomized trials, patients seen by the same physician are randomized to the same treatment arm as a group. Besides the natural clustering of patients due to cluster/group randomization, interactions between an individual patient and the attending physician within the group could just as well influence patient care outcomes. Despite the intuitive relevance of these interactions to treatment assessment, few studies have thus far examined their influences. Whether and to what extent these interactions affect assessment of the treatment effect remains unexplored. In fact, few statistical models provide ready accommodation for such interactions. In this research, we propose a general modeling framework based on the nested Dirichlet process (nDP) for assessing treatment effect in cluster randomized trials. The proposed methodology explicitly accounts for physician\u2013patient interactions by assuming that the interactions follow unspecified group-specific distributions from an nDP. In addition to accounting for physician\u2013patient interactions, the model has greatly enhanced the flexibility of traditional mixed effect models by allowing for nonnormally distributed random effects, thus, alleviating concerns about mixed effect misspecification and sidestepping verification of distributional assumptions on random effects. At the same time, the model retains the mixed models\u2019 ability to make inferences on fixed effects. The proposed method is easily extendable to more complicated hierarchical clustering structures. We introduce the method in the context of a real cluster randomized trial. A comprehensive simulation study was conducted to assess the operating characteristics of the proposed nDP model."], ["A Bayesian Graphical Model for ChIP-Seq Data on Histone Modifications", "Histone modifications (HMs) are an important post-translational feature. Different types of HMs are believed to co-exist and co-regulate biological processes such as gene expression and, therefore, are intrinsically dependent on each other. We develop inference for this complex biological network of HMs based on a graphical model using ChIP-Seq data. A critical computational hurdle in the inference for the proposed graphical model is the evaluation of a normalization constant in an autologistic model that builds on the graphical model. We tackle the problem by Monte Carlo evaluation of ratios of normalization constants. We carry out a set of simulations to validate the proposed approach and to compare it with a standard approach using Bayesian networks. We report inference on HM dependence in a case study with ChIP-Seq data from a next generation sequencing experiment. An important feature of our approach is that we can report coherent probabilities and estimates related to any event or parameter of interest, including honest uncertainties. Posterior inference is obtained from a joint probability model on latent indicators for the recorded HMs. We illustrate this in the motivating case study. An R package including an implementation of posterior simulation in C is available from Riten Mitra upon request."], ["Imputation in High-Dimensional Economic Data as Applied to the Agricultural Resource Management Survey", "In this article, we consider imputation in the USDA\u2019s Agricultural Resource Management Survey (ARMS) data, which is a complex, high-dimensional economic dataset. We develop a robust joint model for ARMS data, which requires that variables are transformed using a suitable class of marginal densities (e.g., skew normal family). We assume that the transformed variables may be linked through a Gaussian copula, which enables construction of the joint model via a sequence of conditional linear models. We also discuss the criteria used to select the predictors for each conditional model. For the purpose of developing an imputation method that is conducive to these model assumptions, we propose a regression-based technique that allows for flexibility in the selection of conditional models while providing a valid joint distribution. In this procedure, labeled as iterative sequential regression (ISR), parameter estimates and imputations are obtained using a Markov chain Monte Carlo sampling method. Finally, we apply the proposed method to the full ARMS data, and we present a thorough data analysis that serves to gauge the appropriateness of the resulting imputations. Our results demonstrate the effectiveness of the proposed algorithm and illustrate the specific deficiencies of existing methods. Supplementary materials for this article are available online."], ["Reconstructing Past Populations With Uncertainty From Fragmentary Data", "Current methods for reconstructing human populations of the past by age and sex are deterministic or do not formally account for measurement error. We propose a method for simultaneously estimating age-specific population counts, fertility rates, mortality rates, and net international migration flows from fragmentary data that incorporates measurement error. Inference is based on joint posterior probability distributions that yield fully probabilistic interval estimates. It is designed for the kind of data commonly collected in modern demographic surveys and censuses. Population dynamics over the period of reconstruction are modeled by embedding formal demographic accounting relationships in a Bayesian hierarchical model. Informative priors are specified for vital rates, migration rates, population counts at baseline, and their respective measurement error variances. We investigate calibration of central posterior marginal probability intervals by simulation and demonstrate the method by reconstructing the female population of Burkina Faso from 1960 to 2005. Supplementary materials for this article are available online and the method is implemented in the R package \u201cpopReconstruct.\u201d"], ["Sea Surface Temperature Modeling using Radial Basis Function Networks With a Dynamically Weighted Particle Filter", "The sea surface temperature (SST) is an important factor of the earth climate system. A deep understanding of SST is essential for climate monitoring and prediction. In general, SST follows a nonlinear pattern in both time and location and can be modeled by a dynamic system which changes with time and location. In this article, we propose a radial basis function network-based dynamic model which is able to catch the nonlinearity of the data and propose to use the dynamically weighted particle filter to estimate the parameters of the dynamic model. We analyze the SST observed in the Caribbean Islands area after a hurricane using the proposed dynamic model. Comparing to the traditional grid-based approach that requires a supercomputer due to its high computational demand, our approach requires much less CPU time and makes real-time forecasting of SST doable on a personal computer. Supplementary materials for this article are available online."], ["Dynamic Bayesian Forecasting of Presidential Elections in the States", "I present a dynamic Bayesian forecasting model that enables early and accurate prediction of U.S. presidential election outcomes at the state level. The method systematically combines information from historical forecasting models in real time with results from the large number of state-level opinion surveys that are released publicly during the campaign. The result is a set of forecasts that are initially as good as the historical model, and then gradually increase in accuracy as Election Day nears. I employ a hierarchical specification to overcome the limitation that not every state is polled on every day, allowing the model to borrow strength both across states and, through the use of random-walk priors, across time. The model also filters away day-to-day variation in the polls due to sampling error and national campaign effects, which enables daily tracking of voter preferences toward the presidential candidates at the state and national levels. Simulation techniques are used to estimate the candidates\u2019 probability of winning each state and, consequently, a majority of votes in the Electoral College. I apply the model to preelection polls from the 2008 presidential campaign and demonstrate that the victory of Barack Obama was never realistically in doubt."], ["Effect Modification and Design Sensitivity in Observational Studies", null], ["Estimating Latent Processes on a Network From Indirect Measurements", "Here, we introduce a novel multilevel state-space model (SSM) of aggregate traffic volumes with realistic features. We implement a na\u00efve strategy for estimating unobserved point-to-point traffic volumes from indirect measurements of aggregate traffic, based on particle filtering. We then develop a more efficient two-stage inference strategy that relies on model-based regularization: a simple model is used to calibrate regularization parameters that lead to efficient/scalable inference in the multilevel SSM. We apply our methods to corporate and academic networks, where we show that the proposed inference strategy outperforms existing approaches and scales to larger networks. We also design a simulation study to explore the factors that influence the performance. Our results suggest that model-based regularization may be an efficient strategy for inference in other complex multilevel models. Supplementary materials for this article are available online."], ["Selection Adjusted Confidence Intervals With More Power to Determine the Sign", "When multiple testing procedures are used to control the false discovery rate or other error rates, the resulting threshold for selecting may be data dependent. We show that conditioning the above CIs on the data-dependent threshold still offers false coverage-statement rate (FCR) for many widely used testing procedures. For these reasons, the conditional CIs for the parameters selected this way are an attractive alternative to the available general FCR adjusted intervals. We demonstrate the use of the method in the analysis of some 14,000 correlations between hormone change and brain activity change in response to the subjects being exposed to stressful movie clips. Supplementary materials for this article are available online."], ["Nonparametric Identification and Semiparametric Estimation of Classical Measurement Error Models Without Side Information", "Virtually all methods aimed at correcting for covariate measurement error in regressions rely on some form of additional information (e.g., validation data, known error distributions, repeated measurements, or instruments). In contrast, we establish that the fully nonparametric classical errors-in-variables model is identifiable from data on the regressor and the dependent variable alone, unless the model takes a very specific parametric form. This parametric family includes (but is not limited to) the linear specification with normally distributed variables as a well-known special case. This result relies on standard primitive regularity conditions taking the form of smoothness constraints and nonvanishing characteristic functions\u2019 assumptions. Our approach can handle both monotone and nonmonotone specifications, provided the latter oscillate a finite number of times. Given that the very specific unidentified parametric functional form is arguably the exception rather than the rule, this identification result should have a wide applicability. It leads to a new perspective on handling measurement error in nonlinear and nonparametric models, opening the way to a novel and practical approach to correct for measurement error in datasets where it was previously considered impossible (due to the lack of additional information regarding the measurement error). We suggest an estimator based on non/semiparametric maximum likelihood, derive its asymptotic properties, and illustrate the effectiveness of the method with a simulation study and an application to the relationship between firm investment behavior and market value, the latter being notoriously mismeasured. Supplementary materials for this article are available online."], ["Classification via Bayesian Nonparametric Learning of Affine Subspaces", "It has become common for datasets to contain large numbers of variables in studies conducted in areas such as genetics, machine vision, image analysis, and many others. When analyzing such data, parametric models are often too inflexible while nonparametric procedures tend to be nonrobust because of insufficient data on these high-dimensional spaces. This is particularly true when interest lies in building efficient classifiers in the presence of many predictor variables. When dealing with these types of data, it is often the case that most of the variability tends to lie along a few directions, or more generally along a much smaller dimensional submanifold of the data space. In this article, we propose a class of models that flexibly learn about this submanifold while simultaneously performing dimension reduction in classification. This methodology allows the cell probabilities to vary nonparametrically based on a few coordinates expressed as linear combinations of the predictors. Also, as opposed to many black-box methods for dimensionality reduction, the proposed model is appealing in having clearly interpretable and identifiable parameters that provide insight into which predictors are important in determining accurate classification boundaries. Gibbs sampling methods are developed for posterior computation, and the methods are illustrated using simulated and real data applications."], ["Extending the State-Space Model to Accommodate Missing Values in Responses and Covariates", "This article proposes an extended state-space model for accommodating multivariate panel data. The novel aspect of this contribution is an adjustment to the classical model for multiple subjects that allows missingness in the covariates in addition to the responses. Missing covariate data are handled by a second state-space model nested inside the first to represent unobserved exogenous information. Relevant Kalman filter equations are derived, and explicit expressions are provided for both the E- and M-steps of an expectation-maximization (EM) algorithm, to obtain maximum (Gaussian) likelihood estimates of all model parameters. In the presence of missing data, the resulting EM algorithm becomes computationally intractable, but a simplification of the M-step leads to a new procedure that is shown to be an expectation/conditional maximization (ECM) algorithm under exogeneity of the covariates. Simulation studies reveal that the approach appears to be relatively robust to moderate percentages of missing data, even with fewer subjects and time points, and that estimates are generally consistent with the asymptotics. The methodology is applied to a dataset from a published panel study of elderly patients with impaired respiratory function. Forecasted values thus obtained may serve as an \u201cearly-warning\u201d mechanism for identifying patients whose lung function is nearing critical levels. Supplementary materials for this article are available online."], ["A Unified Approach to Semiparametric Transformation Models Under General Biased Sampling Schemes", "We propose a unified estimation method for semiparametric linear transformation models under general biased sampling schemes. The new estimator is obtained from a set of counting process-based unbiased estimating equations, developed through introducing a general weighting scheme that offsets the sampling bias. The usual asymptotic properties, including consistency and asymptotic normality, are established under suitable regularity conditions. A closed-form formula is derived for the limiting variance and the plug-in estimator is shown to be consistent. We demonstrate the unified approach through the special cases of left truncation, length bias, the case-cohort design, and variants thereof. Simulation studies and applications to real datasets are presented."], ["On a Principal Varying Coefficient Model", null], ["On Partial Sufficient Dimension Reduction With Applications to Partially Linear Multi-Index Models", "Partial dimension reduction is a general method to seek informative convex combinations of predictors of primary interest, which includes dimension reduction as its special case when the predictors in the remaining part are constants. In this article, we propose a novel method to conduct partial dimension reduction estimation for predictors of primary interest without assuming that the remaining predictors are categorical. To this end, we first take the dichotomization step such that any existing approach for partial dimension reduction estimation can be employed. Then we take the expectation step to integrate over all the dichotomic predictors to identify the partial central subspace. As an example, we use the partially linear multi-index model to illustrate its applications for semiparametric modeling. Simulations and real data examples are given to illustrate our methodology."], ["High-Dimensional Sparse Additive Hazards Regression", null], ["Two-Sample Covariance Matrix Testing and Support Recovery in High-Dimensional and Sparse Settings", null], ["Distribution-Free Prediction Sets", "This article introduces a new approach to prediction by bringing together two different nonparametric ideas: distribution-free inference and nonparametric smoothing. Specifically, we consider the problem of constructing nonparametric tolerance/prediction sets. We start from the general conformal prediction approach, and we use a kernel density estimator as a measure of agreement between a sample point and the underlying distribution. The resulting prediction set is shown to be closely related to plug-in density level sets with carefully chosen cutoff values. Under standard smoothness conditions, we get an asymptotic efficiency result that is near optimal for a wide range of function classes. But the coverage is guaranteed whether or not the smoothness conditions hold and regardless of the sample size. The performance of our method is investigated through simulation studies and illustrated in a real data example."], ["Learning Sparse Causal Gaussian Networks With Experimental Intervention: Regularization and Coordinate Descent", null], ["Inferential Models: A Framework for Prior-Free Posterior Probabilistic Inference", "Posterior probabilistic statistical inference without priors is an important but so far elusive goal. Fisher\u2019s fiducial inference, Dempster\u2013Shafer theory of belief functions, and Bayesian inference with default priors are attempts to achieve this goal but, to date, none has given a completely satisfactory picture. This article presents a new framework for probabilistic inference, based on inferential models (IMs), which not only provides data-dependent probabilistic measures of uncertainty about the unknown parameter, but also does so with an automatic long-run frequency-calibration property. The key to this new approach is the identification of an unobservable auxiliary variable associated with observable data and unknown parameter, and the prediction of this auxiliary variable with a random set before conditioning on data. Here we present a three-step IM construction, and prove a frequency-calibration property of the IM\u2019s belief function under mild conditions. A corresponding optimality theory is developed, which helps to resolve the nonuniqueness issue. Several examples are presented to illustrate this new approach."], ["Misspecification Testing in a Class of Conditional Distributional Models", null], ["A Resampling-Based Stochastic Approximation Method for Analysis of Large Geostatistical Data", "The Gaussian geostatistical model has been widely used in modeling of spatial data. However, it is challenging to computationally implement this method because it requires the inversion of a large covariance matrix, particularly when there is a large number of observations. This article proposes a resampling-based stochastic approximation method to address this challenge. At each iteration of the proposed method, a small subsample is drawn from the full dataset, and then the current estimate of the parameters is updated accordingly under the framework of stochastic approximation. Since the proposed method makes use of only a small proportion of the data at each iteration, it avoids inverting large covariance matrices and thus is scalable to large datasets. The proposed method also leads to a general parameter estimation approach, maximum mean log-likelihood estimation, which includes the popular maximum (log)-likelihood estimation (MLE) approach as a special case and is expected to play an important role in analyzing large datasets. Under mild conditions, it is shown that the estimator resulting from the proposed method converges in probability to a set of parameter values of equivalent Gaussian probability measures, and that the estimator is asymptotically normally distributed. To the best of the authors\u2019 knowledge, the present study is the first one on asymptotic normality under infill asymptotics for general covariance functions. The proposed method is illustrated with large datasets, both simulated and real. Supplementary materials for this article are available online."], ["On Sampling Strategies in Bayesian Variable Selection Problems With Large Model Spaces", "One important aspect of Bayesian model selection is how to deal with huge model spaces, since the exhaustive enumeration of all the models entertained is not feasible and inferences have to be based on the very small proportion of models visited. This is the case for the variable selection problem with a moderately large number of possible explanatory variables considered in this article. We review some of the strategies proposed in the literature, from a theoretical point of view using arguments of sampling theory and in practical terms using several examples with a known answer. All our results seem to indicate that sampling methods with frequency-based estimators outperform searching methods with renormalized estimators. Supplementary materials for this article are available online."], ["Book Reviews", null]]}