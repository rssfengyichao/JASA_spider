{"2004": [["A Model-Based Background Adjustment for Oligonucleotide Expression Arrays", null], ["Structural Mean Effects of Noncompliance", "A randomized, placebo-controlled blood pressure (BP) reduction trial has recorded compliance measures by electronic monitoring. Causal questions regarding dose effects are of interest, but one often wants to avoid the assumption that compliance quantiles on both randomized arms are comparable. Structural mean models (SMMs) do avoid this assumption; however, they do not allow for interaction effects between variables observed on different randomized arms. Such an interaction between observed exposure on the treatment arm and latent treatmentfree response (i. e., placebo response) was suggested by the data. Building on structural models, we propose an approach that makes this possible. To allow for a structural interaction effect with potential placebo response, we invoke a compliance selection model. It turns out that SMMs imply identifiable selection models; hence the secondary model is testable. Next, to assess the amount of variation explained by the structural model, we identify separate variance components for different error types. This yields a measure of the variation in treatment effects over individuals. In addition, it allows us to compare the fit of different plausible structural models. On the BP data, we find a significant interaction effect: Higher exposure levels are estimated to have more effect in subgroups with poorer placebo response. Finite-sample properties of the proposed estimators are verified through simulation."], ["A Random Pattern-Mixture Model for Longitudinal Data With Dropouts", "Pattern-mixture models are frequently used for longitudinal data analysis with dropouts because they do not require explicit specification of the dropout mechanism. These models stratify the data according to time to dropout and formulate a model for each stratum. This usually results in underindentifiability, because we need to estimate many pattern-specific parameters even though the eventual interest is usually on the marginal parameters. In this article we extend this framework to a random pattern-mixture model, where the pattern-specific parameters are treated as nuisance parameters and modeled as random instead of fixed. The pattern is defined according to a surrogate for the dropout process. A constraint is then put on the pattern by linking it to the time to dropout using a random-effects survival model. We assume, conditional on the latent pattern effects, that the longitudinal outcome and the dropout process are independent. This model retains the robustness of the traditional pattern-mixture models, while avoiding the overparameterization problem. When we define each subject as a separate stratum, this model reduces to the shared parameter model. Maximum likelihood estimates are obtained using an EM Newton\u2013 Raphson algorithm. We apply the method to the depression data from the Prevention of Suicide in Primary Care Elderly Collaborative Trial (PROSPECT). We show when the dropout information is adjusted for under the proposed model, the treatment seems to reduce depression in the elderly."], ["Improved Semiparametric Time Series Models of Air Pollution and Mortality", "In 2002, methodological issues around time series analyses of air pollution and health attracted the attention of the scientific community, policy makers, the press, and the diverse stakeholders concerned with air pollution. As the U. S. Environmental Protection Agency (EPA) was finalizing its most recent review of epidemiologic evidence on particulate matter air pollution (PM), statisticians and epidemiologists found that the S\u2013PLUS implementation of generalized additive models (GAMs) can overestimate effects of air pollution and understate statistical uncertainty in time series studies of air pollution and health. This discovery delayed completion of the PM Criteria Document prepared as part of the review of the U. S. National Ambient Air Quality Standard, because the time series findings represented a critical component of the evidence. In addition, it raised concerns about the adequacy of current model formulations and their software implementations. In this article we provide improvements in semiparametric regression directly relevant to risk estimation in time series studies of air pollution. First, we introduce a closed-form estimate of the asymptotically exact covariance matrix of the linear component of a GAM. To ease the implementation of these calculations, we develop the S package gam. exact, an extended version of gam. Use of gam. exact allows a more robust assessment of the statistical uncertainty of the estimated pollution coefficients. Second, we develop a bandwidth selection method to reduce confounding bias in the pollution-mortality relationship due to unmeasured time-varying factors, such as season and influenza epidemics. Third, we introduce a conceptual framework to fully explore the sensitivity of the air pollution risk estimates to model choice. We apply our methods to data of the National Mortality Morbidity Air Pollution Study, which includes time series data from the 90 largest U. S. cities for the period 1987\u20131994."], ["Location\u2013Scale Depth", "This article introduces a halfspace depth in the location\u2013scale model that is along the lines of the general theory given by Mizera, based on the idea of Rousseeuw and Hubert, and is complemented by a new likelihood-based principle for designing criterial functions. The most tractable version of the proposed depth\u2014the Student depth\u2014turns out to be nothing but the bivariate halfspace depth interpreted in the Poincar\u00e9 plane model of the Lobachevski geometry. This fact implies many fortuitous theoretical and computational properties, in particular equivariance with respect to the M\u00f6bius group and favorable time complexities of algorithms. It also opens a way to introduce some other depth notions in the location\u2013scale context, for instance, location\u2013scale simplicial depth. A maximum depth estimator of location and scale\u2014the Student median\u2014is introduced. Possible applications of the proposed concepts are investigated on data examples."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Optimal Sample Size for Multiple Testing", "We consider the choice of an optimal sample size for multiple-comparison problems. The motivating application is the choice of the number of microarray experiments to be carried out when learning about differential gene expression. However, the approach is valid in any application that involves multiple comparisons in a large number of hypothesis tests. We discuss two decision problems in the context of this setup: the sample size selection and the decision about the multiple comparisons. We adopt a decision-theoretic approach, using loss functions that combine the competing goals of discovering as many differentially expressed genes as possible, while keeping the number of false discoveries manageable. For consistency, we use the same loss function for both decisions. The decision rule that emerges for the multiple-comparison problem takes the exact form of the rules proposed in the recent literature to control the posterior expected falsediscovery rate. For the sample size selection, we combine the expected utility argument with an additional sensitivity analysis, reporting the conditional expected utilities and conditioning on assumed levels of the true differential expression. We recognize the resulting diagnostic as a form of statistical power facilitating interpretation and communication. As a sampling model for observed gene expression densities across genes and arrays, we use a variation of a hierarchical gamma/gamma model. But the discussion of the decision problem is independent of the chosen probability model. The approach is valid for any model that includes positive prior probabilities for the null hypotheses in the multiple comparisons and that allows for efficient marginal and posterior simulation, possibly by dependent Markov chain Monte Carlo simulation."], ["False Discovery Control for Random Fields", "This article extends false discovery rates to random fields, for which there are uncountably many hypothesis tests. We develop a method for finding regions in the field's domain where there is a significant signal while controlling either the proportion of area or the proportion of clusters in which false rejections occur. The method produces confidence envelopes for the proportion of false discoveries as a function of the rejection threshold. From the confidence envelopes, we derive threshold procedures to control either the mean or the specified tail probabilities of the false discovery proportion. An essential ingredient of this construnction is a new algorithm to compute a confidence superset for the set of all true-null locations. We demonstrate our method with applications to scan statistics and functional neuroimaging."], ["Cross-Validation and the Estimation of Conditional Probability Densities", null], ["On a Likelihood Approach for Monte Carlo Integration", "The use of estimating equations has been a common approach for constructing Monte Carlo estimators. Recently, Kong et al. proposed a formulation of Monte Carlo integration as a statistical model, making explicit what information is ignored and what is retained about the baseline measure. From simulated data, the baseline measure is estimated by maximum likelihood, and then integrals of interest are estimated by substituting the estimated measure. For two different situations in which independent observations are simulated from multiple distributions, we show that this likelihood approach achieves the lowest asymptotic variance possible by using estimating equations. In the first situation, the normalizing constants of the design distributions are estimated, and Meng and Wong's bridge sampling estimating equation is considered. In the second situation, the values of the normalizing constants are known, thereby imposing linear constraints on the baseline measure. Estimating equations including Hesterberg's stratified importance sampling estimator, Veach and Guibas's multiple importance sampling estimator, and Owen and Zhou's method of control variates are considered."], ["Fully Exponential Laplace Approximations Using Asymptotic Modes", null], ["Bayesian Estimation of the Spectral Density of a Time Series", "This article describes a Bayesian approach to estimating the spectral density of a stationary time series. A nonparametric prior on the spectral density is described through Bernstein polynomials. Because the actual likelihood is very complicated, a pseudoposterior distribution is obtained by updating the prior using the Whittle likelihood. A Markov chain Monte Carlo algorithm for sampling from this posterior distribution is described that is used for computing the posterior mean, variance, and other statistics. A consistency result is established for this pseudoposterior distribution that holds for a short-memory Gaussian time series and under some conditions on the prior. To prove this asymptotic result, a general consistency theorem of Schwartz is extended for a triangular array of independent, nonidentically distributed observations. This extension is also of independent interest. A simulation study is conducted to compare the proposed method with some existing methods. The method is illustrated with the well-studied sunspot dataset."], ["Models and Confidence Intervals for True Values in Interlaboratory Trials", "We consider the one-way random-effects model with unequal sample sizes and heterogeneous variances. Using the method of generalized confidence intervals, we develop a new confidence interval procedure for the mean. Additionally, we investigate two alternative models based on different sets of assumptions regarding between-group variability and derive generalized confidence interval procedures for the mean. These procedures are applicable to small samples. Statistical simulation is used to demonstrate that the coverage probabilities of these procedures are close enough to the nominal value so that they are useful in practice. Although the methods are quite general, the procedures are explained with the backdrop of interlaboratory studies."], ["Smooth and Accurate Multivariate Confidence Regions", null], ["Testing for Trend in the Presence of Autoregressive Error", "A popular model for assessing dependence on time is the time series model composed of a linear trend plus a zero mean autoregressive (AR) process. Although considerable effort has been devoted developing tests for linear trend in the presence of serial correlation, the testing procedures used in practice are less than satisfactory for portions of the parameter space for the AR coefficient. This is because the variance of the feasible generalized least squares (FGLS) estimator of the trend coefficient is heavily dependent on the parameters of the AR process. A test based on the Gauss\u2013Newton procedure is shown to have more uniform behavior over the parameter space than tests based on FGLS. The test based on Gauss\u2013Newton procedure also has good power."], ["Local Global Neural Networks", "We propose the local-global neural networks model within the context of time series models. This formulation encompasses some already existing nonlinear models and also admits the mixture of experts approach. We emphasize the linear expert case and extensively discuss the theoretical aspects of the model: stationarity conditions, existence, consistency and asymptotic normality of the parameter estimates, and model identifiability. The proposed model consists of a mixture of stationary and nonstationary linear models and is able to describe \u201cintermittent\u201d dynamics; the system spends a large fraction of time in a bounded region, but sporadically develops an instability that grows exponentially for some time and then suddenly collapses. Intermittency is a commonly observed behavior in ecology and epidemiology, fluid dynamics, and other natural systems. A model-building strategy is also considered, and the parameters are estimated by concentrated maximum likelihood. The procedure is illustrated with two real time series."], ["Predicting the Conditional Probability of Discovering a New Class", "Consider a population comprising disjoint classes. An important problem arising from various fields is prediction of the random conditional probability of discovering a new class. The asymptotic normality of the discovery probability is established in a Poisson model, where the number of individuals from each class is a Poisson process with a class-specific rate. A new derivation is presented for the well-known Good\u2013Toulmin predictor as a moment-based estimator for the asymptotic limit of the discovery probability. The Good\u2013Toulmin predictor is also shown to be a nonparametric empirical Bayes estimator for the expectation of the discovery probability given the rates of the Poisson processes and an approximation to an unbiased estimator only for the identifiable part of the expectation of the discovery probability in a multinomial model. The properties of the moment-based estimator are investigated so that confidence and prediction intervals can be constructed. The Good\u2013Toulmin predictor and the discovery probability are shown to have a nonnegative correlation. A conditional nonparametric maximum likelihood estimator is developed as an alternative to the moment-based estimator. As an application, the methods are used to predict the probability of discovering a new gene from expressed sequence tags in a genomic sequencing experiment."], ["Predicting Random Effects From Finite Population Clustered Samples With Response Error", "In many situations there is interest in parameters (e. g., mean) associated with the response distribution of individual clusters in a finite clustered population. We develop predictors of such parameters using a two-stage sampling probability model with response error. The probability model stems directly from finite population sampling without additional assumptions and thus is design-based. The predictors are closely related to best linear unbiased predictors (BLUP) that arise from common mixed-model methods, as well as to model-based predictors obtained via super population approaches for survey sampling. The context assumes clusters of equal size and equal size sampling of units within clusters. Target parameters may correspond to clusters realized in the sample, as well as nonrealized clusters. In either case, the predictors are linear and unbiased, and minimize the expected mean squared error. They correspond to the sum of predictors of responses for realized and nonrealized units in the cluster, accounting directly for the second-stage sampling fraction. In contrast, the BLUP commonly used in mixed models can be interpreted as predicting only the responses of second-stage units not observed for a cluster, not the cluster mean. The development reveals that two-stage sampling does not give rise to a more general variance structure often assumed in superpopulation models, even when variances within clusters are heterogeneous. With response error present, we predict target random variables defined as an expected (or average) response over units in a cluster."], ["Combining Independent Regression Estimators From Multiple Surveys", "Efficient generalized regression (GREG) procedures are proposed for the combination of comparable information collected independently from multiple surveys of the same population. In particular, for combination through alignment of estimated totals of common target characteristics, an efficiency improvement of Zieschang's original composite GREG method is developed, involving a correction of the GREG estimation for different effective sample sizes in a multiple-sample setting. The proposed method is nearly as efficient as the alternative method of Renssen and Nieuwenbroek for general sampling designs, and considerably more practical. Under broad sampling design conditions, the proposed method produces composite estimators that are equally efficient as the estimators of Renssen and Nieuwenbroek for common characteristics and more efficient for noncommon characteristics. Under some of these sampling design conditions, an equivalence of extended GREG estimation and extended optimal regression estimation is established."], ["Analysis of Time-to-Event Data With Incomplete Event Adjudication", "In many multicenter, randomized clinical trials, the primary outcome is the time to the first of a number of possible clinical events. An event classification committee may be convened to determine whether events that have been reported by investigators meet the predetermined criteria for primary endpoint events. When interim analyses are performed in such trials, the final classification for many reported events will not be known. Failure to account for the uncertain status of these events may result in incorrect interim analysis. The probability that an unadjudicated event will be confirmed as a primary event can typically be estimated from those events for which adjudication is complete. We show that if each unadjudicated event is weighted according to the probability that it will be the first primary event, then consistent estimates of survival probabilities and regression parameters can be obtained and unbiased log-rank tests of treatment differences performed. Moderate sample consistency of point estimates and variance estimates is verified by simulation. The procedure is illustrated using data from the Coumadin Aspiring Reinfarction Study (CARS) and the Weekly Intervention with Zithromax for Atherosclerosis and Related Diseases (WIZARD) study."], ["Joint Modeling and Estimation for Recurrent Event Processes and Failure Time Data", "Recurrent event data are commonly encountered in longitudinal follow-up studies related to biomedical science, econometrics, reliability, and demography. In many studies, recurrent events serve as important measurements for evaluating disease progression, health deterioration, or insurance risk. When analyzing recurrent event data, an independent censoring condition is typically required for the construction of statistical methods. In some situations, however, the terminating time for observing recurrent events could be correlated with the recurrent event process, thus violating the assumption of independent censoring. In this article, we consider joint modeling of a recurrent event process and a failure time in which a common subject-specific latent variable is used to model the association between the intensity of the recurrent event process and the hazard of the failure time. The proposed joint model is flexible in that no parametric assumptions on the distributions of censoring times and latent variables are made, and under the model, informative censoring is allowed for observing both the recurrent events and failure times. We propose a \u201cborrow-strength estimation procedure\u201d by first estimating the value of the latent variable from recurrent event data, then using the estimated value in the failure time model. Some interesting implications and trajectories of the proposed model are presented. Properties of the regression parameter estimates and the estimated baseline cumulative hazard functions are also studied."], ["Smoothing Spline Nonlinear Nonparametric Regression Models", "Almost all of the current nonparametric regression methods, such as smoothing splines, generalized additive models, and varyingcoefficients models, assume a linear relationship when nonparametric functions are regarded as parameters. In this article we propose a general class of smoothing spline nonlinear nonparametric models that allow nonparametric functions to act nonlinearly. They arise in many fields as either theoretical or empirical models. Our new estimation methods are based on an extension of the Gauss\u2013Newton method to infinite-dimensional spaces and the backfitting procedure. We extend the generalized cross-validation and generalized maximum likelihood methods to estimate smoothing parameters. We establish connections between some nonlinear nonparametric models and nonlinear mixed-effects models. We derive approximate Bayesian confidence intervals for inference. We illustrate the methods with an application to term structure of interest rates and conduct simulations to evaluate the finite-sample performance of our methods."], ["Nonparametric and Semiparametric Models for Missing Covariates in Parametric Regression", "Robustness of covariate modeling for the missing-covariate problem in parametric regression is studied under the missing-at-random assumption. For a simple missing-covariate pattern, nonparametric covariate model is proposed and is shown to yield a consistent and semiparametrically efficient estimator for the regression parameter. Total robustness is achieved in this situation. For more general missingcovariate patterns, a novel semiparametric modeling approach is proposed for the covariates. In this approach, the covariate distribution is first decomposed into the product of a series of conditional distributions according to the overall missing-data patterns, and the conditional distributions are then represented in the general odds ratio form. The general odds ratios are modeled parametrically, and the other components of the covariate distribution are modeled nonparametrically. Maximum semiparametric likelihood is used to find the parameter estimates. The proposed method yields a consistent estimator for the regression parameter when the odds ratios are modeled correctly. In general, the semiparametric covariate modeling strategy increases the robustness against covariate model misspecification when compared with the parametric modeling strategy proposed by Lipsitz and Ibrahim. The new covariate modeling approach can also be incorporated into the doubly robust procedure of Robins et al. to increase protection against misspecification of the missing-data mechanism. In addition, the proposed modeling strategy avoids the usually intractable integrations involved in the maximization of the incomplete-data likelihood with parametric covariate models. The proposed method can be applied to many regression models to handle incomplete covariates."], ["Optimal Experimental Designs When Some Independent Variables Are Not Subject to Control", "This article considers the problem of constructing optimal designs for regression models when the design space is a product space and some of the variables are not under the control of the practitioner. A variable that is not under control can have known values before the experiment is performed or else unknown values before the experiment is realized. The first case is briefly discussed in the literature. The aim of this work is to provide equivalence theorems for the second case and the mixture of both cases. Iterative algorithms for generating approximate optimal designs are given, and a real case of lung cancer is discussed."], ["Bayesian Reasoning in Data Analysis: A Critical Introduction", null], ["Bayesian Field Theory", null], ["Bayesian Survival Analysis", null], ["The Design and Analysis of Computer Experiments", null], ["Measures of Interobserver Agreement", null], ["Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control", null], ["Statistical Methods for Six Sigma in R&d and Manufacturing", null], ["Statistical Size Distributions in Economics and Actuarial Sciences", null], ["Power Analysis for Experimental Research: A Practical Guide For The Biological, Medical and Social Sciences and Statistical Power Analysis: A Simple and General Model for Traditional and Modern Hypothesis Tests", null], ["Applied Longitudinal Data Analysis for Epidemiology: A Practical Guide", null], ["Cross-over Trials in Clinical Research", null], ["Health Science Research: A Handbook of Quantitative Methods", null], ["Telegraphic Reviews", null], ["Calibrated Probabilistic Mesoscale Weather Field Forecasting", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Relative Risk Forests for Exercise Heart Rate Recovery as a Predictor of Mortality", null], ["Hierarchical Bayesian Neural Networks", "Prostate cancer is one of the most common cancers in American men. Management depends on the staging of prostate cancer. Only cancers that are confined to organs of origin are potentially curable. The article considers a hierarchical Bayesian neural network approach for posterior prediction probabilities of certain features indicative of non-organ-confined prostate cancer. The Bayesian procedure is implemented by an application of the Markov chain Monte Carlo numerical integration technique. For the problem at hand, the hierarchical Bayesian neural network approach is shown to be superior to the approach based on hierarchical Bayesian logistic regression model as well as the classical feedforward neural networks."], ["Full Matching in an Observational Study of Coaching for the SAT", "To accommodate missing data, regression-based analyses by ETS researchers rejected a subset of the available sample that differed significantly from the subsample they analyzed. Full matching on the propensity score handles the same problem simply and without rejecting observations. In addition, it eases the detection and handling of nonconstancy of treatment effects, which the regression-based analyses had obscured, and it makes fuller use of covariate information. It estimates a somewhat larger effect of coaching on the math score than did ETS's methods."], ["The Estimation of Prediction Error", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Smooth Design-Adapted Wavelets for Nonparametric Stochastic Regression", "We treat nonparametric stochastic regression using smooth design-adapted wavelets built by means of the lifting scheme. The proposed method automatically adapts to the nature of the regression problem, that is, to the irregularity of the design, to data on the interval, and to arbitrary sample sizes (which do not need to be a power of 2). As such, this method provides a uniform solution to the usual criticisms of first-generation wavelet estimators. More precisely, starting from the unbalanced Haar basis orthogonal with respect to the empirical design measure, we use weighted average interpolation to construct biorthogonal wavelets with a higher number of vanishing analyzing moments. We include a lifting step that improves the conditioning through constrained local semiorthogonalization. We propose a wavelet thresholding algorithm and show its numerical performance both on real data and in simulations including white, correlated, and heteroscedastic noise."], ["Variable Selection and Model Building via Likelihood Basis Pursuit", null], ["Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models", "Representation of generalized additive models (GAM's) using penalized regression splines allows GAM's to be employed in a straightforward manner using penalized regression methods. Not only is inference facilitated by this approach, but it is also possible to integrate model selection in the form of smoothing parameter selection into model fitting in a computationally efficient manner using well founded criteria such as generalized cross-validation. The current fitting and smoothing parameter selection methods for such models are usually effective, but do not provide the level of numerical stability to which users of linear regression packages, for example, are accustomed. In particular the existing methods cannot deal adequately with numerical rank deficiency of the GAM fitting problem, and it is not straightforward to produce methods that can do so, given that the degree of rank deficiency can be smoothing parameter dependent. In addition, models with the potential flexibility of GAM's can also present practical fitting difficulties as a result of indeterminacy in the model likelihood: Data with many zeros fitted by a model with a log link are a good example. In this article it is proposed that GAM's with a ridge penalty provide a practical solution in such circumstances, and a multiple smoothing parameter selection method suitable for use in the presence of such a penalty is developed. The method is based on the pivoted QR decomposition and the singular value decomposition, so that with or without a ridge penalty it has good error propagation properties and is capable of detecting and coping elegantly with numerical rank deficiency. The method also allows mixtures of user specified and estimated smoothing parameters and the setting of lower bounds on smoothing parameters. In terms of computational efficiency, the method compares well with existing methods. A simulation study compares the method to existing methods, including treating GAM's as mixed models."], ["Functional Convex Averaging and Synchronization for Time-Warped Random Curves", null], ["Exact and Approximate Inferences for Nonlinear Mixed-Effects Models With Missing Covariates", "Nonlinear mixed-effects (NLME) models are popular in many longitudinal studies, including human immunodeficiency virus (HIV) viral dynamics, pharmacokinetic analyses, and studies of growth and decay. In practice, covariates in these studies often contain missing data, and so standard complete-data methods are not directly applicable. In this article we propose Monte Carlo parameter-expanded (PX)-EM algorithms for exact and approximate likelihood inferences for NLME models with missing covariates when the missing-data mechanism is ignorable. We allow arbitrary missing-data patterns and allow the covariates to be categorical, continuous, and mixed. The PX-EM algorithm maintains the simplicity and stability of the standard EM algorithm and may converge much faster than EM. The approximate method is computationally more efficient and may be preferable to the exact method when the exact method exhibits convergence problems, such as slow convergence or nonconvergence. It becomes an exact method for linear mixed-effects models and certain NLME models with missing covariates. We also discuss several sampling methods and convergence of the Monte Carlo (PX) EM algorithms. We illustrate the methods using a real data example from the study of HIV viral dynamics and compare the methods via a simulation study."], ["New Estimation and Model Selection Procedures for Semiparametric Modeling in Longitudinal Data Analysis", "Semiparametric regression models are very useful for longitudinal data analysis. The complexity of semiparametric models and the structure of longitudinal data pose new challenges to parametric inferences and model selection that frequently arise from longitudinal data analysis. In this article, two new approaches are proposed for estimating the regression coefficients in a semiparametric model. The asymptotic normality of the resulting estimators is established. An innovative class of variable selection procedures is proposed to select significant variables in the semiparametric models. The proposed procedures are distinguished from others in that they simultaneously select significant variables and estimate unknown parameters. Rates of convergence of the resulting estimators are established. With a proper choice of regularization parameters and penalty functions, the proposed variable selection procedures are shown to perform as well as an oracle estimator. A robust standard error formula is derived using a sandwich formula and is empirically tested. Local polynomial regression techniques are used to estimate the baseline function in the semiparametric model."], ["Survival Analysis With Heterogeneous Covariate Measurement Error", "This article is motivated by a time-to-event analysis where the covariate of interest was measured at the wrong time. We show that the problem can be formulated as a special case of survival analysis with heterogeneous covariate measurement error and develop a general analytic framework. We study the asymptotic behavior of the naive partial likelihood estimates and analytically demonstrate that under the heterogeneous measurement error structure and the assumption that all components of the covariate vector and the measurement error vector combined are mutually independent, these naive estimates will shrink toward 0, and that the degree of attenuation increases as the measurement error increases. We also give counterexamples for reverse attenuation when the independence conditions are violated. We use our analytical results to derive a simple bias-correcting estimator that performs well in simulations for small and moderate amounts of measurement error. Our framework can be used to provide insight into the behavior of the commonly used partial likelihood score test for testing no association between a failure outcome and an exposure, for example, in the presence of measurement error or mistiming error. In particular, we derive the asymptotic distribution of the naive partial likelihood score test under a series of local alternatives and discuss the asymptotic relative efficiency. As a result, a simple sample size formula to account for the contamination of covariates is obtained."], ["Nonlinear and Nonparametric Regression and Instrumental Variables", "We consider regression when the predictor is measured with error and an instrumental variable (IV) is available. The regression function can be modeled linearly, nonlinearly, or nonparametrically. Our major new result shows that the regression function and all parameters in the measurement error model are identified under relatively weak conditions, much weaker than previously known to imply identifiability. In addition, we exploit a characterization of the IV estimator as a classical \u201ccorrection for attenuation\u201d method based on a particular estimate of the variance of the measurement error. This estimate of the measurement error variance allows us to construct functional nonparametric regression estimators making no assumptions about the distribution of the unobserved predictor and structural estimators that use parametric assumptions about this distribution. The functional estimators uses simulation extrapolation or deconvolution kernels and the structural method uses Bayesian Markov chain Monte Carlo. The Bayesian estimator is found to significantly outperform the functional approach."], ["Inference After Model Selection", "Typical modeling strategies involve model selection, which has a significant effect on inference of estimated parameters. Common practice is to use a selected model ignoring uncertainty introduced by the process of model selection. This could yield overoptimistic inferences, resulting in false discovery. In this article we develop a general methodology via optimal approximation for estimating the mean and variance of complex statistics that involve the process of model selection. This allows us to make approximately unbiased inferences, taking into account the selection process. We examine the operating characteristics of the proposed methodology via asymptotic analyses and simulations. These results show that the proposed methodology yields correct inferences and outperforms common alternatives."], ["Discrimination and Classification of Nonstationary Time Series Using the SLEX Model", "Statistical discrimination for nonstationary random processes is important in many applications. Our goal was to develop a discriminant scheme that can extract local features of the time series, is consistent, and is computationally efficient. Here, we propose a discriminant scheme based on the SLEX (smooth localized complex exponential) library. The SLEX library forms a collection of Fourier-type bases that are simultaneously orthogonal and localized in both time and frequency domains. Thus, the SLEX library has the ability to extract local spectral features of the time series. The first step in our procedure, which is the feature extraction step based on work by Saito, is to find a basis from the SLEX library that can best illuminate the difference between two or more classes of time series. In the next step, we construct a discriminant criterion that is related to the Kullback\u2013Leibler divergence between the SLEX spectra of the different classes. The discrimination criterion is based on estimates of the SLEX spectra that are computed using the SLEX basis selected in the feature extraction step. We show that the discrimination method is consistent and demonstrate via finite sample simulation studies that our proposed method performs well. Finally, we apply our method to a seismic waves dataset with the primary purpose of classifying the origin of an unknown seismic recording as either an earthquake or an explosion."], ["Unit Root Quantile Autoregression Inference", null], ["Optimal Bayesian Design by Inhomogeneous Markov Chain Simulation", "We consider decision problems defined by a utility function and an underlying probability model for all unknowns. The utility function quantifies the decision maker's preferences over consequences. The optimal decision maximizes the expected utility function where the expectation is taken with respect to all unknowns, that is, future data and parameters. In many problems, the solution is not analytically tractable. For example, the utility function might involve moments that can be computed only by numerical integration or simulation. Also, the nature of the decision space (i.e., the set of all possible actions) might have a shape or dimension that complicates the maximization. The motivating application for this discussion is the choice of a monitoring network when the optimization is performed over the high-dimensional set of all possible locations of monitoring stations, possibly including choice of the number of locations. We propose an approach to optimal Bayesian design based on inhomogeneous Markov chain simulation. We define a chain such that the limiting distribution identifies the optimal solution. The approach is closely related to simulated annealing. Standard simulated annealing algorithms assume that the target function can be evaluated for any given choice of the variable with respect to which we wish to optimize. For optimal design problems the target function (i. e., expected utility) is in general not available for efficient evaluation and might require numerical integration. We overcome the problem by defining an inhomogeneous Markov chain on an appropriately augmented space. The proposed inhomogeneous Markov chain Monte Carlo method addresses within one simulation both problems, evaluation of the expected utility and maximization."], ["Getting It Right", "Analytical or coding errors in posterior simulators can produce reasonable but incorrect approximations of posterior moments. This article develops simple tests of posterior simulators that detect both kinds of errors, and uses them to detect and correct errors in two previously published articles. The tests exploit the fact that a Bayesian model specifies the joint distribution of observables (data) and unobservables (parameters). There are two joint distribution simulators. The marginal-conditional simulator draws unobservables from the prior and then observables conditional on unobservables. The successive-conditional simulator alternates between the posterior simulator and an observables simulator. Formal comparison of moment approximations of the two simulators reveals existing analytical or coding errors in the posterior simulator."], ["The Invariance of Some Score Tests in the Linear Model With Classical Measurement Error", "The linear model with classical measurement error is an alternative to the standard regression model, in which it is assumed that the independent variables are subject to error. This assumption can cause statistical inferences and parameter estimators to differ dramatically from those obtained by the standard regression model. However, in some cases, inferences remain unchanged even though the independent variables are assumed to be subject to error. This article investigates the invariance property of score tests for assessing heteroscedasticity, first-order autoregressive disturbance, and the need for a Box\u2013Cox power transformation. Under specific constraints, we show that the score tests for measurement error models are identical to the corresponding well-established tests derived from standard regression models. Hence practitioners can assess assumptions of constant variance and independent errors, as well as the need for a Box\u2013Cox transformation, irrespective of whether or not the variables are measured with error. We also discuss some possible generalizations."], ["A Nonparametric Test for Spatial Isotropy Using Subsampling", null], ["The Impact of Dichotomization on the Efficiency of Testing for an Interaction Effect in Exponential Family Models", "In a number of special cases, it has been shown that categorization of a continuous explanatory variable for use in a regression model can lead to efficiency losses. Nevertheless, categorization remains a popular means of dealing with continuous variables, particularly in medical statistics. Two topics of current interest in medical statistics are subgroup effects in clinical trials and gene\u2013environment interactions in genetic studies. In a regression model setting, these topics involve the examination of interaction effects, often between a binary explanatory variable and a continuous explanatory variable. In this article the efficiency losses associated with dichotomization of a continuous explanatory variable for the testing of such interaction effects are calculated and compared with those associated with the testing of main effects. It is shown that considerable additional efficiency loss can arise because of dichotomization in both a main effect and an interaction. The theoretical development is done in the context of exponential family models, thus also generalizing earlier results on efficiency loss for main effects. Some indication is also given of the losses in efficiency associated with more detailed categorization. The further impact of censoring in time-to-event models is investigated and shown to not alter the qualitative conclusions. The practical importance of the findings is illustrated through the analysis of data from a clinical trial in patients with prostate cancer."], ["Improving the Efficiency of Relative-Risk Estimation in Case-Cohort Studies", "The case-cohort design is a common means of reducing the cost of covariate measurements in large failure-time studies. Under this design, complete covariate data are collected only on the cases (i. e., the subjects whose failure times are uncensored) and on a subcohort randomly selected from the whole cohort. In many applications, certain covariates are readily measured on all cohort members, and surrogate measurements of the expensive covariates also may be available. The existing relative-risk estimators for the case-cohort design disregard the covariate data collected outside the case-cohort sample and thus incur loss of efficiency. To make better use of the available data, we develop a class of weighted estimators with general time-varying weights that are related to a class of estimators proposed by Robins, Rotnitzky, and Zhao. The estimators are shown to be consistent and asymptotically normal under appropriate conditions. We identify the estimator within this class that maximizes efficiency, numerical studies demonstrate that the efficiency gains of the proposed estimator over the existing ones can be substantial in realistic settings. We also study the estimation of the cumulative hazard function. An illustration with data taken from Wilms' tumor studies is provided."], ["Unbiased Estimating Equations From Working Correlation Models for Irregularly Timed Repeated Measures", "The method of generalized estimating equations (GEEs) has been criticized recently for a failure to protect against misspecification of working correlation models, which in some cases leads to loss of efficiency or infeasibility of solutions. However, the feasibility and efficiency of GEE methods can be enhanced considerably by using flexible families of working correlation models. We propose two ways of constructing unbiased estimating equations from general correlation models for irregularly timed repeated measures to supplement and enhance GEE. The supplementary estimating equations are obtained by differentiation of the Cholesky decomposition of the working correlation, or as score equations for decoupled Gaussian pseudolikelihood. The estimating equations are solved with computational effort equivalent to that required for a first-order GEE. Full details and analytic expressions are developed for a generalized Markovian model that was evaluated through simulation. Large-sample \u201csandwich\u201d standard errors for working correlation parameter estimates are derived and shown to have good performance. The proposed estimating functions are further illustrated in an analysis of repeated measures of pulmonary function in children."], ["Causal Inference With General Treatment Regimes", "In this article we develop the theoretical properties of the propensity function, which is a generalization of the propensity score of Rosenbaum and Rubin. Methods based on the propensity score have long been used for causal inference in observational studies; they are easy to use and can effectively reduce the bias caused by nonrandom treatment assignment. Although treatment regimes need not be binary in practice, the propensity score methods are generally confined to binary treatment scenarios. Two possible exceptions have been suggested for ordinal and categorical treatments. In this article we develop theory and methods that encompass all of these techniques and widen their applicability by allowing for arbitrary treatment regimes. We illustrate our propensity function methods by applying them to two datasets; we estimate the effect of smoking on medical expenditure and the effect of schooling on wages. We also conduct simulation studies to investigate the performance of our methods."], ["Membership Functions and Probability Measures of Fuzzy Sets", "The notion of fuzzy sets has proven useful in the context of control theory, pattern recognition, and medical diagnosis. However, it has also spawned the view that classical probability theory is unable to deal with uncertainties in natural language and machine learning, so that alternatives to probability are needed. One such alternative is what is known as \u201cpossibility theory.\u201d Such alternatives have come into being because past attempts at making fuzzy set theory and probability theory work in concert have been unsuccessful. The purpose of this article is to develop a line of argument that demonstrates that probability theory has a sufficiently rich structure for incorporating fuzzy sets within its framework. Thus probabilities of fuzzy events can be logically induced. The philosophical underpinnings that make this happen are a subjectivistic interpretation of probability, an introduction of Laplace's famous genie, and the mathematics of encoding expert testimony. The benefit of making probability theory work in concert with fuzzy set theory is an ability to deal with different kinds of uncertainties that may arise within the same problem."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["The Price of Kaplan\u2013Meier", "Miller has studied the asymptotic efficiency of the nonparametric, Kaplan\u2013Meier survival estimator relative to parametric estimates based on the exponential and Weibull distributions. He concluded that in certain cases, the asymptotic efficiency is low and recommended that analysts give more consideration to parametric estimators, particularly for estimation of small tail probabilities. In this article we revisit this issue and examine the performance of the nonparametric procedure for estimation not only of a point on the survival curve, but also of the mean (or restricted mean) lifetime. In addition to the exponential and Weibull families, we consider the performance of the Kaplan\u2013Meier procedure relative to a more flexible parametric model proposed by Efron. We find that the reduction in efficiency of the Kaplan\u2013Meier survival estimate becomes negligible fairly quickly as the number of parameters in the parametric model increases. Moreover, for estimation of the mean or restricted mean, the loss in efficiency, even relative to the exponential distribution, is small or nil. We conclude that a parametric estimate of the survival curve may be necessary in certain extreme situations, such as when the sample size is very small. In these cases, careful attention must be given to considering the degree of fit, although with sparse data, this must be assessed from outside sources. For certain functionals of the survival curve, such as the mean or restricted mean, the nonparametric approach is unbiased and entails little or no loss in efficiency, and therefore would generally be preferred over a parametric-based estimate."], ["Book Reviews", null], ["Spatial Cluster Modeling", null], ["Bayesian Nonparametrics", null], ["Components of Variance", null], ["Survival Analysis: Techniques for Censored and Truncated Data", null], ["Statistical Tools for Environmental Quality Measurement", null], ["Data Analysis and Graphics Using R: An Example-Based Approach", null], ["Statistical Tools for Nonlinear Regression: a Practical Guide With S-PLUS and R Examples", null], ["2D Object Detection and Recognition: Models, Algorithms, and Networks", null], ["The Stress-Strength Model and Its Generalizations: Theory and Applications", null], ["Advanced Calculus with Applications in Statistics", null], ["Handbook of Statistics 22: Statistics in Industry", null], ["Telegraphic Reviews", null], ["Variable Selection in Data Mining", "We predict the onset of personal bankruptcy using least squares regression. Although well publicized, only 2,244 bankruptcies occur in our dataset of 2.9 million months of credit-card activity. We use stepwise selection to find predictors of these from a mix of payment history, debt load, demographics, and their interactions. This combination of rare responses and over 67,000 possible predictors leads to a challenging modeling question: How does one separate coincidental from useful predictors? We show that three modifications turn stepwise regression into an effective methodology for predicting bankruptcy. Our version of stepwise regression (1) organizes calculations to accommodate interactions, (2) exploits modern decision theoretic criteria to choose predictors, and (3) conservatively estimates p-values to handle sparse data and a binary response. Omitting any one of these leads to poor performance. A final step in our procedure calibrates regression predictions. With these modifications, stepwise regression predicts bankruptcy as well as, if not better than, recently developed data-mining tools. When sorted, the largest 14,000 resulting predictions hold 1,000 of the 1,800 bankruptcies hidden in a validation sample of 2.3 million observations. If the cost of missing a bankruptcy is 200 times that of a false positive, our predictions incur less than 2/3 of the costs of classification errors produced by the tree-based classifier C4.5."], ["Bayesian Factor Analysis for Spatially Correlated Data, With Application to Summarizing Area-Level Material Deprivation From Census Data", null], ["A Classical Study of Catch-Effort Models for Hector's Dolphins", "Effective management is the key to the protection of many endangered species. Identification of the primary factors that affect their survival often lead to the introduction of strategies to improve survival rates. In this article, we consider a small population of Hector's dolphins located off the coast of New Zealand and the impact that the establishment of a seasonal sanctuary has on their survival and migration rates. Using Akaike's information criterion and an extension of the simulated annealing algorithm, we distinguish between a wide range of competing models that correspond to different assumptions as to the region and temporal dependence of the survival rates. We also examine the impact of the inclusion of catch-effort information and demonstrate the added value that these data provide in terms of both model discrimination and parameter estimation. In particular, we find a whole class of models that provide a far better fit to the data (and therefore better prediction and ultimately better management) than those adopted for previous analyses."], ["Semiparametric Regression Analysis With Missing Response at Random", "We develop inference tools in a semiparametric partially linear regression model with missing response data. A class of estimators is defined that includes as special cases a semiparametric regression imputation estimator, a marginal average estimator, and a (marginal) propensity score weighted estimator. We show that any of our class of estimators is asymptotically normal. The three special estimators have the same asymptotic variance. They achieve the semiparametric efficiency bound in the homoscedastic Gaussian case. We show that the jackknife method can be used to consistently estimate the asymptotic variance. Our model and estimators are defined with a view to avoid the curse of dimensionality, which severely limits the applicability of existing methods. The empirical likelihood method is developed. It is shown that when missing responses are imputed using the semiparametric regression method the empirical log-likelihood is asymptotically a scaled chi-squared variable. An adjusted empirical log-likelihood ratio, which is asymptotically standard chisquared, is obtained. Also, a bootstrap empirical log-likelihood ratio is derived and its distribution is used to approximate that of the imputed empirical log-likelihood ratio. A simulation study is conducted to compare the adjusted and bootstrap empirical likelihood with the normal approximation-based method in terms of coverage accuracies and average lengths of confidence intervals. Based on biases and standard errors, a comparison is also made by simulation between the proposed estimators and the related estimators."], ["Sieve Maximum Likelihood Estimator for Semiparametric Regression Models With Current Status Data", "In a randomized controlled clinical trial study where the response variable of interest is the time to occurrence of a certain event, it is often too expensive or even impossible to observe the exact time. However, the current status of the subject at a random time of inspection is much more natural, feasible, and practical in terms of cost-effectiveness. This article considers a semiparametric regression model that consists of parametric and nonparametric regression components. A sieve maximum likelihood estimator (MLE) is proposed to estimate the regression parameter, allowing exploration of the nonlinear relationship between a certain covariate and the response function. Asymptotic properties of the proposed sieve MLEs are discussed. Under some mild conditions, the estimators are shown to be strongly consistent. Moreover, the estimators of the unknown parameters are asymptotically efficient and normally distributed, and the estimator of the nonparametric function has an optimal convergence rate. Simulation studies were carried out to investigate the performance of the proposed method. For illustration purposes, the method is applied to a dataset from a study of the calcification of the hydrogel intraocular lenses, a complication of cataract treatment."], ["Estimation in Partially Linear Models With Missing Covariates", null], ["Heteroscedastic One-Way ANOVA and Lack-of-Fit Tests", null], ["Cholesky Residuals for Assessing Normal Errors in a Linear Model With Correlated Outcomes", "Despite the widespread popularity of linear models for correlated outcomes (e.g., linear mixed models and time series models), distribution diagnostic methodology remains relatively underdeveloped in this context. In this article we present an easy-to-implement approach that lends itself to graphical displays of model fit. Our approach involves multiplying the estimated marginal residual vector by the Cholesky decomposition of the inverse of the estimated marginal variance matrix. The resulting \u201crotated\u201d residuals are used to construct an empirical cumulative distribution function and pointwise standard errors. The theoretical framework, including conditions and asymptotic properties, involves technical details that are motivated by Lange and Ryan, Pierce, and Randles. Our method appears to work well in a variety of circumstances, including models having independent units of sampling (clustered data) and models for which all observations are correlated (e.g., a single time series). Our methods can produce satisfactory results even for models that do not satisfy all of the technical conditions stated in our theory."], ["Multiple Comparison of Several Linear Regression Models", "Research on multiple comparison during the past 50 years or so has focused mainly on the comparison of several population means. Several years ago, Spurrier considered the multiple comparison of several simple linear regression lines. He constructed simultaneous confidence bands for all of the contrasts of the simple linear regression lines over the entire range (-\u221e, \u221e) when the models have the same design matrices. This article extends Spurrier's work in several directions. First, multiple linear regression models are considered and the design matrices are allowed to be different. Second, the predictor variables are either unconstrained or constrained to finite intervals. Third, the types of comparison allowed can be very flexible, including pairwise, many\u2013one, and successive. Two simulation methods are proposed for the calculation of critical constants. The methodologies are illustrated with examples."], ["On Priors With a Kullback\u2013Leibler Property", "In this paper, we highlight properties of Bayesian models in which the prior puts positive mass on all Kullback\u2013Leibler neighborhoods of all densities. These properties are concerned with model choice via the Bayes factor, density estimation and the maximization of expected utility for decision problems. In four illustrations we focus on the Bayes factor and show that whatever models are being compared, the [log(Bayes factor)]/[sample size] converges to a non-random number which has a nice interpretation. A parametric versus semiparametric model comparison provides a fifth illustration."], ["A Bayesian Insertion/Deletion Algorithm for Distant Protein Motif Searching via Entropy Filtering", "Bayesian models have been developed that find ungapped motifs in multiple protein sequences. In this article, we extend the model to allow for deletions and insertions in motifs. Direct generalization of the ungapped algorithm, based on Gibbs sampling, proved unsuccessful because the configuration space became much larger. To alleviate the convergence difficulty, a two-stage procedure is introduced. At the first stage, we develop a method called entropy filtering, which quickly searchs \u201cgood\u201d starting points for the alignment approach without the concern of deletion/insertion patterns. At the second stage, we switch to an algorithm that generates both a random vector that represents insertion/deletion patterns and a random variable of motif locations. After the two steps, gapped-motif alignments are obtained for multiple sequences. When applied to datasets that consist of helix\u2013loop\u2013helix proteins and high mobility group proteins, respectively, our methods show great improvements over those that produce ungapped alignments."], ["Propriety of the Posterior Distribution and Existence of the MLE for Regression Models With Covariates Missing at Random", "Characterizing model identifiability in the presence of missing covariate data is a very important issue in missing data problems. In this article, we characterize the propriety of the posterior distribution of the regression coefficients for some general classes of regression models, including the class of generalized linear models (GLM's) and parametric survival models with right-censored data. Toward this goal, we derive some very general and easy-to-check conditions for the matrix of covariates. We also derive sufficient conditions for the existence of the maximum likelihood estimates and establish novel results for checking propriety of the posterior when the sample size is large. Several theorems are given to establish propriety of the posterior and the existence of the maximum likelihood estimator. The conditions reduce to solving a system of linear equations, which can be carried out using software such as MAPLE, IMSL, or SAS. We assume that the missing covariates are missing at random and assume an improper uniform prior for the regression coefficients. In addition, we establish these results assuming a very general form for the covariate distribution, allowing for both missing categorical and/or continuous covariates. A small dataset is used to illustrate that the posterior can be improper based on complete cases while proper when all of the cases are used in the analysis. Two real datasets are presented to demonstrate verification of posterior propriety for GLM's and parametric survival models, and also to illustrate propriety for large datasets."], ["Inferential Aspects of the Skew Exponential Power Distribution", "When the analysis of real data indicates that normality assumptions are untenable, more flexible models that cope with the most prevalent deviations from normality can be adopted. In this context, the skew exponential power (SEP) distribution warrants special attention, because it encompasses distributions having both heavy tails and skewness, it allows likelihood inference, and it includes the normal model as a special case. This article concerns likelihood inference about the parameters of the SEP family. In particular, the information matrix of the maximum likelihood estimators (MLEs) is obtained and finite-sample properties of the estimators are investigated numerically. Special attention is given to the properties of the MLEs and likelihood ratio statistics when the data are drawn from a normal distribution, because this case is relevant for using the SEP distribution to test for normality. Application of the SEP distribution in robust estimation problems is considered for both independent and dependent data. Under moderate deviations from normality, estimators obtained under the SEP distribution are shown to outperform the normal-based estimators and to compete with robust estimators; furthermore, the SEP distribution offers the benefits of having a specified distribution, such as interpretable location and scale parameters under nonnormality."], ["Robust Analysis of Generalized Linear Mixed Models", "The method of maximum likelihood (ML) is widely used for analyzing generalized linear mixed models (GLMM's). A full maximum likelihood analysis requires numerical integration techniques for calculation of the log-likelihood, and to avoid the computational problems involving irreducibly high-dimensional integrals, several maximum likelihood algorithms have been proposed in the literature to estimate the model parameters by approximating the log-likelihood function. Although these likelihood algorithms are useful for fitting the GLMM's efficiently under strict model assumptions, they can be highly influenced by the presence of unusual data points. In this article, the author develops a technique for finding robust maximum likelihood (RML) estimates of the model parameters in GLMM's, which appears to be useful in downweighting the influential data points when estimating the parameters. The asymptotic properties of the robust estimators are investigated under some regularity conditions. Small simulations are carried out to study the behavior of the robust estimates in the presence of outliers, and these estimates are also compared to the ordinary classical estimates. To avoid the computational problems involving high-dimensional integrals, the author proposes a robust Monte Carlo Newton\u2013Raphson (RMCNR) algorithm for fitting GLMM's. The proposed robust method is illustrated in an analysis of data from a clinical experiment described in a biometrical journal."], ["Universal Optimality for Selected Crossover Designs", null], ["Modified Large-Sample Confidence Intervals for Linear Combinations of Variance Components", "We consider the problem of setting a confidence interval or bound for a linear combination of variance components related to a multivariate normal distribution, which includes important applications such as comparing variance components and testing the bioequivalence between two drug products. The lack of an exact confidence interval for a general linear combination of variance components spurred the development of a modified large-sample (MLS) method that was shown to be superior to many other approximation methods. But existing MLS method requires the use of independent estimators of variance components. Using a chi-squared representation of a quadratic form of a multivariate normal vector, we extend the MLS method to situations in which estimators of variance components are dependent. Using Edgeworth and Cornish\u2013Fisher expansions, we explicitly derive the second-order asymptotic coverage error of the MLS confidence bound. Our results show that the MLS confidence bound is not second-order accurate in general, but is much better than the confidence bound based on normal approximation and is nearly second-order accurate in some special cases. Our results also show how to construct an MLS confidence bound that is second-order accurate. As an application, we discuss the use of the MLS method in assessing population bioequivalence, with some simulation results and an example."], ["A Semiparametric Basis for Combining Estimation Problems Under Quadratic Loss", null], [null, null], ["A Conditionally Distribution-Free Multivariate Sign Test for One-Sided Alternatives", null], ["Equivalence Between Conditional and Mixture Approaches to the Rasch Model and Matched Case-Control Studies, With Applications", "Analyses of data using Rasch models, including the special case of matched case-control studies, are common applications of conditional likelihood in which the usual inferential procedures are applied only after conditioning on an approximately ancillary statistic. Another common approach to the analysis of Rasch models is to integrate the nuisance parameters over a mixing distribution, using the marginal likelihood obtained as the basis for inference. We show that the full conditional likelihood can always be obtained exactly via the marginal approach, given a particular choice of mixing distribution, and derive necessary and sufficient conditions for the two approaches to agree. Previous work has shown that with sufficient flexibility in the mixing distribution, the maxima of the marginal and conditional likelihoods will be equivalent under concordance criteria. Our argument requires no such criteria, and for any dataset guarantees the equivalence of the whole of the two likelihoods, not just their maxima. This substantially enhances the previous results and provides an alternative derivation for any existing conditional analysis. We give examples of mixing distributions that guarantee the agreement of the two approaches, and explore equivalence classes of such distributions, together with some of their attractive symmetry properties. Our argument also allows for the adaption and extension of analytic techniques already widely used with Rasch data, and in particular with matched case-control studies; potential applications of these advances are illustrated with several examples. These include new numerical algorithms for evaluating the conditional likelihood without directly specifying its computationally awkward functional form, inferences about complex functions of the parameters of interest obtained using existing Markov chain Monte Carlo methods, powerful measures of goodness of fit derived from likelihood contributions that are ignored by the conditional approach, and the justifiable addition of prior knowledge to existing conditional analyses."], ["Monte Carlo State-Space Likelihoods by Weighted Posterior Kernel Density Estimation", "Maximum likelihood estimation and likelihood ratio tests for nonlinear, non-Gaussian state-space models require numerical integration for likelihood calculations. Several methods, including Monte Carlo (MC) expectation maximization, MC likelihood ratios, direct MC integration, and particle filter likelihoods, are inefficient for the motivating problem of stage-structured population dynamics models in experimental settings. An MC kernel likelihood (MCKL) method is presented that estimates classical likelihoods up to a constant by weighted kernel density estimates of Bayesian posteriors. MCKL is derived by using Bayesian posteriors as importance sampling densities for unnormalized kernel smoothing integrals. MC error and mode bias due to kernel smoothing are discussed and two methods for reducing mode bias are proposed: \u201czooming in\u201d on the maximum likelihood parameters using a focused prior based on an initial estimate and using a posterior cumulant-based approximation of mode bias. A simulated example shows that MCKL can be much more efficient than previous approaches for the population dynamics problem. The zooming-in and cumulant-based corrections are illustrated with a multivariate variance estimation problem for which accurate results are obtained even in 20 parameter dimensions."], ["Parameterization and Bayesian Modeling", "Progress in statistical computation often leads to advances in statistical modeling. For example, it is surprisingly common that an existing model is reparameterized, solely for computational purposes, but then this new configuration motivates a new family of models that is useful in applied statistics. One reason why this phenomenon may not have been noticed in statistics is that reparameterizations do not change the likelihood. In a Bayesian framework, however, a transformation of parameters typically suggests a new family of prior distributions. We discuss examples in censored and truncated data, mixture modeling, multivariate imputation, stochastic processes, and multilevel models."], ["To Model or Not To Model? Competing Modes of Inference for Finite Population Sampling", "Finite population sampling is perhaps the only area of statistics in which the primary mode of analysis is based on the randomization distribution, rather than on statistical models for the measured variables. This article reviews the debate between design-based and model-based inference. The basic features of the two approaches are illustrated using the case of inference about the mean from stratified random samples. Strengths and weakness of design-based and model-based inference for surveys are discussed. It is suggested that models that take into account the sample design and make weak parametric assumptions can produce reliable and efficient inferences in surveys settings. These ideas are illustrated using the problem of inference from unequal probability samples. A model-based regression analysis that leads to a combination of design-based and model-based weighting is described."], ["Estimating Animal Abundance: Closed Populations", null], ["Block Designs: a Randomization Approach. Vol. I: Analysis and Block Designs: a Randomization Approach. Vol. II: Design", null], ["Current Topics in Computational Molecular Biology", null], ["Probability Models for DNA Sequence Evolution", null], ["Nonlinear Estimation and Classification", null], ["Nonparametric Goodness-of-Fit Testing Under Gaussian Models", null], ["Applied Probability", null], ["Generalized Poisson Models and Their Applications in Insurance and Finance", null], ["An Elementary Introduction to Mathematical Finance: Options and Other Topics", null], ["Risk Analysis in Engineering and Economics", null], ["Modeling Financial Time Series With S-PLUS", null], ["Applied Computational Economics and Finance", null], ["Computational Statistics Handbook With MATLAB", null], ["The Elements of Statistical Learning: Data Mining, Inference, and Prediction", null], ["Statistical Process Adjustment Methods for Quality Control", null], ["A Primer on Statistical Distributions", null], ["Statistical Tables, Explained and Applied", null], ["The Cognitive Style of PowerPoint", null], ["Telegraphic Reviews", null], ["Does the Statistics Profession Have an Identity Crisis?", "The field of statistics has numerous areas of application and many positive attributes. Various companies and organizations around the world have chosen not only to adopt statistical tools, but also to integrate them into their associated work and ultimately into their decision making processes. Consequently, the value of statistics as a quantitative tool of importance to society appears to be continually increasing. In contrast, statisticians are less visible to society, and their importance and necessity is not always as evident. This is a result of many factors, some tied to the statistics profession itself and others due to external causes that are not easily controlled. This address presents some basic ideas for improving the recognition of statistics as a profession and of statisticians as the profession's core component. Included is a discussion of ways to promote the recognition of the value of statisticians to society."], ["Randomization Inference With Imperfect Compliance in the ACE-Inhibitor After Anthracycline Randomized Trial", "Anthracyclines are quite effective at curing certain cancers of childhood, but they may damage the heart. The ACE-Inhibitor After Anthracycline (AAA) study compared enalapril to placebo in a randomized trial in an effort to determine whether treatment with enalapril would preserve or improve cardiac function among children previously treated with anthracylines. As is true in many clinical trials, patient compliance with the study protocol was imperfect; some children took less than the prescribed dose of enalapril or placebo. Most analytical procedures that acknowledge imperfect compliance do so at significant cost, abandoning the tight logic of random assignment. With noncompliance, assignment to enalapril or placebo is randomized, but the dose of enalapril actually received is not, and self-selection effects parallel to those in observational studies can exist and have been documented in some instances. Some researchers advocate adherence to the strict logic of randomization by reporting only, or else strongly emphasizing, the so-called \u201cintent-to-treat\u201d analysis, which makes no use of information about compliance. Other researchers report analyses that are not justified by random assignment and can be subject to substantial biases, such as \u201cper protocol\u201d analyses or \u201ctreatment received\u201d analyses. Here we apply a recent proposal for randomization inference with an instrumental variable that uses randomization as the \u201creasoned basis for inference\u201d in Fisher's phrase. We make no assumption that compliance is random; indeed, compliance may be severely biased. Importantly, the proposed analysis will find a statistically significant effect of the treatment if and only if the intent-to-treat analysis finds a significant effect; yet, unlike intent-to-treat analysis, our analysis acknowledges that a patient assigned to a drug that he or she does not take will not receive the drug's pharmacological benefits."], ["Causal Models for Randomized Physician Encouragement Trials in Treating Primary Care Depression", "This article addresses unique causal issues in the context of a randomized study on improving adherence to best practice guidelines by primary care physicians (PCP's) in treating their depressed patients. The study assessed an encouragement strategy to improve PCP guideline adherence. In this context, we compare two causal approaches: the conditional-compliance (CC) Bayesian latent class and the conditional-observable (CO) structural mean model methods. The CC methods estimate contrasts between randomized encouragement and no-encouragement arms [intent-to-treat (ITT) estimand] given latent PCP guideline complier classes. The CO methods estimate contrasts between PCP guideline adherence and nonadherence conditions (as-treated estimand) given observed PCP adherence status. The CC ITT estimand for patients with PCP compliers equals the CO as-treated estimand depending on assumptions. One such assumption pertains to the absence of physician defiers, who do the opposite of what they are encouraged to do in treating patients for depression. We relate these two estimands to each other in our clinical context when the no-defier assumption is not plausible. In other contexts, previous statistical literature has appropriately assumed the absence of defiers. However, indications in the behavioral literature, anecdotal evidence in the study, and results from the data analysis and simulations suggest that defiers do exist in the context of physician-based interventions in primary care. Both simulation and empirical results show that even with a small estimated proportion of defiers under Bayesian model assumptions, inference is sensitive to different assumptions about this class of PCP noncompliers."], ["Dose-Finding Based on Multiple Toxicities in a Soft Tissue Sarcoma Trial", "The scientific goal of a phase I oncology trial of a new chemotherapeutic agent is to find a dose with an acceptable level of toxicity. For ethical reasons, dose-finding is done adaptively, with doses chosen for successive cohorts of patients based on the data obtained from previous cohorts. Typically, patients are at risk for several qualitatively different toxicities, each occurring at several possible severity levels. In this article, we describe how we addressed the dose-finding problem in a phase I trial of gemcitabine for treatment of soft tissue sarcoma. The oncologists planning the trial wanted to account for differences in importance among the toxicities that they had identified. They also requested that the dose-finding method utilize the fact that a low-grade toxicity observed at a given dose, although not dose-limiting, provides a warning that a higher grade of that toxicity is likely to occur at a higher dose. Conventional phase I methods reduce each type of toxicity to an indicator of its occurrence at or above a severity level considered dose-limiting, define \u201ctoxicity\u201d as the maximum of these indicators, and base dose-finding on that single binary variable. Because conventional methods do not address the aforementioned concerns, we developed a Bayesian method for dose-finding in the sarcoma trial based on a vector of correlated, ordinal-valued toxicities with severity levels varying with dose. We also developed a method for jointly eliciting the prior, a vector of weights quantifying the clinical importance of each level of each type of toxicity, and a target total toxicity burden acceptable to the physicians. Our method assigns each cohort the dose with a current posterior mean total toxicity burden closest to the target. The elicitation process is iterative, with the oncologists repeatedly shown the algorithm's behavior and asked to adjust their weights to ensure that the statistical decisions reflect appropriate clinical behavior. We describe how this methodology has worked in the sarcoma trial, present simulations and sensitivity analyses of the trial under several clinical scenarios, and provide guidelines for general application."], ["Bayesian Survival Analysis With Nonproportional Hazards", "Both pravastatin and aspirin are approved by the U.S. Food and Drug Administration (FDA) for secondary prevention of cardiovascular events. This article describes statistical analyses used for a successful submission to the FDA that contends that copackaging pravastatin and aspirin provides a health benefit. From the efficacy perspective this is taken to mean that the combination is more effective than either agent considered alone. We present three Bayesian hierarchical survival models and apply them to the results of five randomized clinical trials. These trials evaluated the benefit of pravastatin in the secondary-prevention setting. Aspirin use was recorded for patients in these trials, but assignment to aspirin was not randomized. We compare the effects of pravastatin and aspirin considered in combination and when given alone. We focus on time to myocardial infarction, although it was just one of several endpoints considered in the presentation to the FDA. Two of the models assume proportional hazards and the third does not. In all three models we adjust for known covariates. Our principal focus is the probability that the combination of pravastatin and aspirin is at least as effective as the agents considered separately. We also find the probability that the combination is synergistic in the sense that the effect of the combination is better than the sum of the effects of the two agents taken alone."], ["Analysis of Contaminant Co-Occurrence in Community Water Systems", "The current framework for U.S. Environmental Protection Agency regulation of water quality in community drinking water supplies produces sequential rules for either single contaminants or small groups of similar contaminants. For both substantive and pragmatic reasons, some water industry experts have advocated the development of a more holistic regulatory process in which rules are promulgated less frequently but for larger contaminant classes. Such a framework would require the expansion of existing regulatory evaluation technologies to account for joint occurrence distributions of multiple contaminants. This article presents an analysis, using two national contaminant databases, of the joint distributions of seven contaminants (arsenic, nitrate, uranium, manganese, magnesium, calcium, and sulfate) in community water system source waters. Inferences are based on a flexible Bayesian hierarchical modeling structure with numerous features desirable for empirical exploration of multicontaminant regulations, including the simultaneous estimation of spatial heterogeneity in contaminant levels and covariations among contaminants, applicability to sparse data collected over a large spatial scale, and coherent assimilation of information provided by censored observations. The model is used to estimate a family of joint distributions for the contaminants indexed by water system characteristics, with empirically appropriate complexity given the resolution of the available data. The resulting distributions provide insights about the nature of, and uncertainty about, contaminant co-occurrence patterns, quantify the impact on national assessments of jointly modeling the contaminants, and facilitate identification of critical classes of water systems where uncertainty is highest."], ["Estimating the Interest Rate Term Structure of Corporate Debt With a Semiparametric Penalized Spline Model", null], ["Multicategory Support Vector Machines", "Two-category support vector machines (SVM) have been very popular in the machine learning community for classification problems. Solving multicategory problems by a series of binary classifiers is quite common in the SVM paradigm; however, this approach may fail under various circumstances. We propose the multicategory support vector machine (MSVM), which extends the binary SVM to the multicategory case and has good theoretical properties. The proposed method provides a unifying framework when there are either equal or unequal misclassification costs. As a tuning criterion for the MSVM, an approximate leave-one-out cross-validation function, called Generalized Approximate Cross Validation, is derived, analogous to the binary case. The effectiveness of the MSVM is demonstrated through the applications to cancer classification using microarray data and cloud classification with satellite radiance profiles."], ["Subsampling Methods to Estimate the Variance of Sample Means Based on Nonstationary Spatial Data With Varying Expected Values", null], ["Large-Scale Simultaneous Hypothesis Testing", "Current scientific techniques in genomics and image processing routinely produce hypothesis testing problems with hundreds or thousands of cases to consider simultaneously. This poses new difficulties for the statistician, but also opens new opportunities. In particular, it allows empirical estimation of an appropriate null hypothesis. The empirical null may be considerably more dispersed than the usual theoretical null distribution that would be used for any one case considered separately. An empirical Bayes analysis plan for this situation is developed, using a local version of the false discovery rate to examine the inference issues. Two genomics problems are used as examples to show the importance of correctly choosing the null hypothesis."], ["Semiparametric Failure Time Regression With Replicates of Mismeasured Covariates", "This article proposes a general strategy for the regression analysis of univariate and multivariate failure time data when a subset of covariates cannot be measured precisely but replicate measurements of their surrogates are available. Multivariate failure time data include recurrent events and clustered survival data. The number of replicate measurements can vary from subject to subject and can even depend on the failure time. No parametric assumption is imposed on the error or on any other random variable. Several semiparametric regression models are considered, including the Cox proportional hazards model for univariate failure time data, multiplicative intensity/rate models for recurrent events data, and marginal Cox proportional hazards models for general multivariate failure time data. The existing estimating functions in the absence of measurement error are corrected to yield consistent and asymptotically normal estimators of the regression parameters. The estimation of the underlying failure time distribution is also studied. The operating characteristics of the proposed estimators are assessed through extensive simulation studies. An application to multiple tumor recurrences from a cancer prevention trial is provided."], ["Kernel Estimators for Univariate Binary Regression", "We present a rather thorough investigation of the use of kernel-based nonparametric estimators of the binary regression function in the case of a single covariate. We consider various versions of Nadaraya\u2013Watson and local linear estimators, some involving a single bandwidth and others involving two bandwidths. The locally linear logistic estimator proves to be a good single-bandwidth estimator, although the basic Nadaraya\u2013Watson estimator also fares quite well. Two-bandwidth methods show great potential when bandwidths are selected with knowledge of the target function, but much of their potential vanishes when data-based bandwidths are used. Likelihood cross-validation and plug-in approaches are the data-based bandwidth selection methods tested; both prove quite useful, with a preference for the latter. Adaptive two-bandwidth methods retain particularly good performance only in certain special situations (and separate estimation of the two bandwidths as for optimal density estimation is never recommended). We therefore propose a hybrid estimation procedure in which the local linear logistic estimator is used unless the ratio of (robust) variances of the covariate in the success and failure groups is greater than 2, in which case we switch to a two-bandwidth Nadaraya\u2013Watson-type estimator, each using plug-in bandwidth selection."], ["A Two-Stage Regression Model for Epidemiological Studies With Multivariate Disease Classification Data", "Polytomous logistic regression is commonly used to analyze epidemiological data with disease subtype information. In this approach effects of exposures on different disease subtypes are studied through separate exposure odds ratios comparing different case groups to the common control group. This article considers the situation where disease subtypes can be defined using multiple characteristics of a disease. For efficient analysis of such data, a two-stage modeling approach is proposed. At the first stage, a standard polytomous logistic regression model is considered for all possible distinct disease subtypes that can be defined by the cross-classification of the different disease characteristics. At the second stage, the exposure odds ratio parameters for the first-stage disease subtypes are further modeled in terms of the defining characteristics of the subtypes. When the total number of first-stage disease subtypes is small, standard maximum likelihood methods can be used for inference in the proposed model. For dealing with a large number of disease subtypes, a novel semiparametric pseudo-conditional-likelihood approach is proposed that does not require any model assumption about the baseline probabilities for the different disease subtypes. This article develops the asymptotic theory for the estimator and studies its small-sample properties using simulation experiments. The proposed method is applied to study the effect of fiber on the risk of various forms of colorectal adenoma using data available from a large screening study, the Prostate, Lung, Colorectal and Ovarian Cancer (PLCO) Screening Trial."], ["Likelihood-Based Local Linear Estimation of the Conditional Variance Function", "We consider estimation of mean and variance functions with kernel-weighted local polynomial fitting in a heteroscedastic nonparametric regression model. Our preferred estimators are based on a localized normal likelihood, using a standard local linear form for estimating the mean and a local log-linear form for estimating the variance. It is important to allow two bandwidths in this problem, separate ones for mean and variance estimation. We provide data-based methods for choosing the bandwidths. We also consider asymptotic results, and study and use them. The methodology is compared with a popular competitor and is seen to perform better for small and moderate sample sizes in simulations. A brief example is provided."], ["Testing Independence for Bivariate Current Status Data", "This article develops a nonparametric procedure for testing marginal independence based on bivariate current status data. Asymptotic properties of the proposed tests are derived, and their finite-sample performance is studied via simulations. The method is applied to analyze data from a community-based study of cardiovascular epidemiology in Taiwan."], ["Monte Carlo Smoothing for Nonlinear Time Series", "We develop methods for performing smoothing computations in general state-space models. The methods rely on a particle representation of the filtering distributions, and their evolution through time using sequential importance sampling and resampling ideas. In particular, novel techniques are presented for generation of sample realizations of historical state sequences. This is carried out in a forward-filtering backward-smoothing procedure that can be viewed as the nonlinear, non-Gaussian counterpart of standard Kalman filter-based simulation smoothers in the linear Gaussian case. Convergence in the mean squared error sense of the smoothed trajectories is proved, showing the validity of our proposed method. The methods are tested in a substantial application for the processing of speech signals represented by a time-varying autoregression and parameterized in terms of time-varying partial correlation coefficients, comparing the results of our algorithm with those from a simple smoother based on the filtered trajectories."], ["Testing for Differences Between Conditional Means in a Time Series Context", "In this article we study tests for equality of two regression curves when the inputs are driven by a time series. The basic process underlying the test statistics is the empirical process of the time series marked by the difference in the pertaining dependent variables. The main results hold under strict stationarity of the input variables, but no mixing condition or special modeling of the time series will be necessary. A simulation study is reported on, which illustrates the quality of the distributional approximation and the power of the tests for small to moderate sample sizes. An application to a real dataset is also included."], ["Computational Methods for Multiplicative Intensity Models Using Weighted Gamma Processes", "We develop computational procedures for a class of Bayesian nonparametric and semiparametric multiplicative intensity models incorporating kernel mixtures of spatial weighted gamma measures. A key feature of our approach is that explicit expressions for posterior distributions of these models share many common structural features with the posterior distributions of Bayesian hierarchical models using the Dirichlet process. Using this fact, along with an approximation for the weighted gamma process, we show that with some care, one can adapt efficient algorithms used for the Dirichlet process to this setting. We discuss blocked Gibbs sampling procedures and P\u00f3lya urn Gibbs samplers. We illustrate our methods with applications to proportional hazard models, Poisson spatial regression models, recurrent events, and panel count data."], ["On the Statistical Analysis of Smoothing by Maximizing Dirty Markov Random Field Posterior Distributions", "We consider Bayesian nonparametric function estimation using a Markov random field prior based on the Laplace distribution. We describe efficient methods for finding the exact maximum a posteriori estimate, which handle constraints naturally and avoid the problems posed by nondifferentiability of the posterior distribution; the methods also make links to spline and wavelet smoothers and to a dual posterior distribution. Three automatic smoothing parameter selection procedures are described: empirical Bayes, two-fold cross-validation, and a universal rule for the Laplace prior. Monte Carlo simulation with Gaussian and Poisson responses demonstrates that the new estimator can give better estimates of nonsmooth functions than can a similar prior based on the Gaussian distribution or wavelet-based competitors. Applications are given to spectral density estimation and to Poisson image denoising."], ["An ANOVA Model for Dependent Random Measures", "We consider dependent nonparametric models for related random probability distributions. For example, the random distributions might be indexed by a categorical covariate indicating the treatment levels in a clinical trial and might represent random effects distributions under the respective treatment combinations. We propose a model that describes dependence across random distributions in an analysis of variance (ANOVA)-type fashion. We define a probability model in such a way that marginally each random measure follows a Dirichlet process (DP) and use the dependent Dirichlet process to define the desired dependence across the related random measures. The resulting probability model can alternatively be described as a mixture of ANOVA models with a DP prior on the unknown mixing measure. The main features of the proposed approach are ease of interpretation and computational simplicity. Because the model follows the standard ANOVAstructure, interpretation and inference parallels conventions for ANOVA models. This includes the notion of main effects, interactions, contrasts, and the like. Of course, the analogies are limited to structure and interpretation. The actual objects of the inference are random distributions instead of the unknown normal means in standard ANOVA models. Besides interpretation and model structure, another important feature of the proposed approach is ease of posterior simulation. Because the model can be rewritten as a DP mixture of ANOVAmodels, it inherits all computational advantages of standard DP mixture models. This includes availability of efficient Gibbs sampling schemes for posterior simulation and ease of implementation of even high-dimensional applications. Complexity of implementing posterior simulation is\u2014at least conceptually\u2014dimension independent."], ["The IOS Test for Model Misspecification", null], [null, null], ["Methodology for Evaluating a Partially Controlled Longitudinal Treatment Using Principal Stratification, With Application to a Needle Exchange Program", "We consider studies for evaluating the short-term effect of a treatment of interest on a time-to-event outcome. The studies we consider are partially controlled in the following sense: (1) Subjects' exposure to the treatment of interest can vary over time, but this exposure is not directly controlled by the study; (2) subjects' follow-up time is not directly controlled by the study; and (3) the study directly controls another factor that can affect subjects' exposure to the treatment of interest as well as subjects' follow-up time. When factors (1) and (2) are both present in the study, evaluating the treatment of interest using standard methods, including instrumental variables, does not generally estimate treatment effects. We develop the methodology for estimating the effect of treatment in this setting of partially controlled studies under explicit assumptions using the framework for principal stratification for causal inference. We illustrate our methods by a study to evaluate the efficacy of the Baltimore Needle Exchange Program to reduce the risk of human immunodeficiency virus (HIV) transmission, using data on distance of the program's sites from the subjects."], ["Inconsistent Estimation and Asymptotically Equal Interpolations in Model-Based Geostatistics", "It is shown that in model-based geostatistics, not all parameters in the Mat\u00e9rn class can be estimated consistently if data are observed in an increasing density in a fixed domain, regardless of the estimation methods used. Nevertheless, one quantity can be estimated consistently by the maximum likelihood method, and this quantity is more important to spatial interpolation. The results are established by using the properties of equivalence and orthogonality of probability measures. Some sufficient conditions are provided for both Gaussian and non-Gaussian equivalent measures, and necessary conditions are provided for Gaussian equivalent measures. Two simulation studies are presented that show that the fixed-domain asymptotic properties can explain some finite-sample behavior of both interpolation and estimation when the sample size is moderately large."], ["Spatially Balanced Sampling of Natural Resources", "The spatial distribution of a natural resource is an important consideration in designing an efficient survey or monitoring program for the resource. Generally, sample sites that are spatially balanced, that is, more or less evenly dispersed over the extent of the resource, are more efficient than simple random sampling. We review a unified strategy for selecting spatially balanced probability samples of natural resources. The technique is based on creating a function that maps two-dimensional space into one-dimensional space, thereby defining an ordered spatial address. We use a restricted randomization to randomly order the addresses, so that systematic sampling along the randomly ordered linear structure results in a spatially well-balanced random sample. Variable inclusion probability, proportional to an arbitrary positive ancillary variable, is easily accommodated. The basic technique selects points in a two-dimensional continuum, but is also applicable to sampling finite populations or one-dimensional continua embedded in two-dimensional space. An extension of the basic technique gives a way to order the sample points so that any set of consecutively numbered points is in itself a spatially well-balanced sample. This latter property is extremely useful in adjusting the sample for the frame imperfections common in environmental sampling."], ["Methods and Criteria for Model Selection", "Model selection is an important part of any statistical analysis and, indeed, is central to the pursuit of science in general. Many authors have examined the question of model selection from both frequentist and Bayesian perspectives, and many tools for selecting the \u201cbest model\u201d have been suggested in the literature. This paper considers the various proposals from a Bayesian decision\u2013theoretic perspective."], ["Visualizing Statistical Models and Concepts", null], ["Configural Frequency Analysis: Methods, Models, and Applications", null], ["Testing Statistical Hypotheses of Equivalence", null], ["Small Area Estimation", null], ["Stochastic Approximation and Its Applications", null], ["Optimal Design of Blocked and Split-Plot Experiments", null], ["Parametric and Nonparametric Inference From Record-Breaking Data", null], ["Generalized Estimating Equations", null], ["Statistical Models and Methods for Lifetime Data", null], ["Regression Models for Time Series Analysis", null], ["Telegraphic Reviews", null]]}