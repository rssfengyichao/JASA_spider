{"2015": [["Statistics: Making Better Decisions", null], ["Wanna Get Away? Regression Discontinuity Estimation of Exam School Effects Away From the Cutoff", "In regression discontinuity (RD) studies exploiting an award or admissions cutoff, causal effects are nonparametrically identified for those near the cutoff. The effect of treatment on inframarginal applicants is also of interest, but identification of such effects requires stronger assumptions than those required for identification at the cutoff. This article discusses RD identification and estimation away from the cutoff. Our identification strategy exploits the availability of dependent variable predictors other than the running variable. Conditional on these predictors, the running variable is assumed to be ignorable. This identification strategy is used to study effects of Boston exam schools for inframarginal applicants. Identification based on the conditional independence assumptions imposed in our framework yields reasonably precise and surprisingly robust estimates of the effects of exam school attendance on inframarginal applicants. These estimates suggest that the causal effects of exam school attendance for 9th grade applicants with running variable values well away from admissions cutoffs differ little from those for applicants with values that put them on the margin of acceptance. An extension to fuzzy designs is shown to identify causal effects for compliers away from the cutoff. Supplementary materials for this article are available online."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Bayesian Partition Models for Identifying Expression Quantitative Trait Loci", "Expression quantitative trait loci (eQTLs) are genomic locations associated with changes of expression levels of certain genes. By assaying gene expressions and genetic variations simultaneously on a genome-wide scale, scientists wish to discover genomic loci responsible for expression variations of a set of genes. The task can be viewed as a multivariate regression problem with variable selection on both responses (gene expression) and covariates (genetic variations), including also multi-way interactions among covariates. Instead of learning a predictive model of quantitative trait given combinations of genetic markers, we adopt an inverse modeling perspective to model the distribution of genetic markers conditional on gene expression traits. A particular strength of our method is its ability to detect interactive effects of genetic variations with high power even when their marginal effects are weak, addressing a key weakness of many existing eQTL mapping methods. Furthermore, we introduce a hierarchical model to capture the dependence structure among correlated genes. Through simulation studies and a real data example in yeast, we demonstrate how our Bayesian hierarchical partition model achieves a significantly improved power in detecting eQTLs compared to existing methods. Supplementary materials for this article are available online."], ["Bayesian Phylogenetic Inference Using a Combinatorial Sequential Monte Carlo Method", "The application of Bayesian methods to large-scale phylogenetics problems is increasingly limited by computational issues, motivating the development of methods that can complement existing Markov chain Monte Carlo (MCMC) schemes. Sequential Monte Carlo (SMC) methods are approximate inference algorithms that have become very popular for time series models. Such methods have been recently developed to address phylogenetic inference problems but currently available techniques are only applicable to a restricted class of phylogenetic tree models compared to MCMC. In this article, we propose an original combinatorial SMC (CSMC) method to approximate posterior phylogenetic tree distributions, which is applicable to a general class of models and can be easily combined with MCMC to infer evolutionary parameters. Our method only relies on the existence of a flexible partially ordered set structure and is more generally applicable to sampling problems on combinatorial spaces. We demonstrate that the proposed CSMC algorithm provides consistent estimates under weak assumptions, is computationally fast, and is additionally easily parallelizable. Supplementary materials for this article are available online."], ["Temporal Autocorrelation-Based Beamforming With MEG Neuroimaging Data", "Characterizing the brain source activity using magnetoencephalography (MEG) requires solving an ill-posed inverse problem. Most source reconstruction procedures are performed in terms of power comparison. However, in the presence of voxel-specific noises, the direct power analysis can be misleading due to the power distortion as suggested by our multiple trial MEG study on a face-perception experiment. To tackle the issue, we propose a temporal autocorrelation-based method for the above analysis. The new method improves the face-perception analysis and identifies several differences between neuronal responses to face and scrambled-face stimuli. By the simulated and real data analyses, we demonstrate that compared to the existing methods, the new proposal can be more robust to voxel-specific noises without compromising on its accuracy in source localization. We further establish the consistency for estimating the proposed index when the number of sensors and the number of time instants are sufficiently large. In particular, we show that the proposed procedure can make a better focus on true sources than its precedents in terms of peak segregation coefficient. Supplementary materials for this article are available online."], ["Some Counterclaims Undermine Themselves in Observational Studies", "Claims based on observational studies that a treatment has certain effects are often met with counterclaims asserting that the treatment is without effect, that associations are produced by biased treatment assignment. Some counterclaims undermine themselves in the following specific sense: presuming the counterclaim to be true may strengthen the support that the original data provide for the original claim, so that the counterclaim fails in its role as a critique of the original claim. In mathematics, a proof by contradiction supposes a proposition to be true en route to proving that the proposition is false. Analogously, the supposition that a particular counterclaim is true may justify an otherwise unjustified statistical analysis, and this added analysis may interpret the original data as providing even stronger support for the original claim. More precisely, the original study is sensitive to unmeasured biases of a particular magnitude, but an analysis that supposes the counterclaim to be true may be insensitive to much larger unmeasured biases. The issues are illustrated using data from the U.S. Fatal Accident Reporting System. Supplementary materials for this article are available online."], ["Randomized Controlled Field Trials of Predictive Policing", "The concentration of police resources in stable crime hotspots has proven effective in reducing crime, but the extent to which police can disrupt dynamically changing crime hotspots is unknown. Police must be able to anticipate the future location of dynamic hotspots to disrupt them. Here we report results of two randomized controlled trials of near real-time epidemic-type aftershock sequence (ETAS) crime forecasting, one trial within three divisions of the Los Angeles Police Department and the other trial within two divisions of the Kent Police Department (United Kingdom). We investigate the extent to which (i) ETAS models of short-term crime risk outperform existing best practice of hotspot maps produced by dedicated crime analysts, (ii) police officers in the field can dynamically patrol predicted hotspots given limited resources, and (iii) crime can be reduced by predictive policing algorithms under realistic law enforcement resource constraints. While previous hotspot policing experiments fix treatment and control hotspots throughout the experimental period, we use a novel experimental design to allow treatment and control hotspots to change dynamically over the course of the experiment. Our results show that ETAS models predict 1.4\u20132.2\u00a0times as much crime compared to a dedicated crime analyst using existing criminal intelligence and hotspot mapping practice. Police patrols using ETAS forecasts led to an average 7.4% reduction in crime volume as a function of patrol time, whereas patrols based upon analyst predictions showed no significant effect. Dynamic police patrol in response to ETAS crime forecasts can disrupt opportunities for crime and lead to real crime reductions."], ["Rerandomization to Balance Tiers of Covariates", "When conducting a randomized experiment, if an allocation yields treatment groups that differ meaningfully with respect to relevant covariates, groups should be rerandomized. The process involves specifying an explicit criterion for whether an allocation is acceptable, based on a measure of covariate balance, and rerandomizing units until an acceptable allocation is obtained. Here, we illustrate how rerandomization could have improved the design of an already conducted randomized experiment on vocabulary and mathematics training programs, then provide a rerandomization procedure for covariates that vary in importance, and finally offer other extensions for rerandomization, including methods addressing computational efficiency. When covariates vary in a priori importance, better balance should be required for more important covariates. Rerandomization based on Mahalanobis distance preserves the joint distribution of covariates, but balances all covariates equally. Here, we propose rerandomizing based on Mahalanobis distance within tiers of covariate importance. Because balancing covariates in one tier will in general also partially balance covariates in other tiers, for each subsequent tier we explicitly balance only the components orthogonal to covariates in more important tiers."], ["An Adaptive Resampling Test for Detecting the Presence of Significant Predictors", "This article investigates marginal screening for detecting the presence of significant predictors in high-dimensional regression. Screening large numbers of predictors is a challenging problem due to the nonstandard limiting behavior of post-model-selected estimators. There is a common misconception that the oracle property for such estimators is a panacea, but the oracle property only holds away from the null hypothesis of interest in marginal screening. To address this difficulty, we propose an adaptive resampling test (ART). Our approach provides an alternative to the popular (yet conservative) Bonferroni method of controlling family-wise error rates. ART is adaptive in the sense that thresholding is used to decide whether the centered percentile bootstrap applies, and otherwise adapts to the nonstandard asymptotics in the tightest way possible. The performance of the approach is evaluated using a simulation study and applied to gene expression data and HIV drug resistance data."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["A Simple Formula for Mixing Estimators With Different Convergence Rates", null], ["Dirichlet\u2013Laplace Priors for Optimal Shrinkage", null], ["Exact Optimal Confidence Intervals for Hypergeometric Parameters", null], ["Bayesian Compressed Regression", null], ["Groupwise Dimension Reduction via Envelope Method", "The family of sufficient dimension reduction (SDR) methods that produce informative combinations of predictors, or indices, are particularly useful for high-dimensional regression analysis. In many such analyses, it becomes increasingly common that there is available a priori subject knowledge of the predictors; for example, they belong to different groups. While many recent SDR proposals have greatly expanded the scope of the methods\u2019 applicability, how to effectively incorporate the prior predictor structure information remains a challenge. In this article, we aim at dimension reduction that recovers full regression information while preserving the predictor group structure. Built upon a new concept of the direct sum envelope, we introduce a systematic way to incorporate the group information in most existing SDR estimators. As a result, the reduction outcomes are much easier to interpret. Moreover, the envelope method provides a principled way to build a variety of prior structures into dimension reduction analysis. Both simulations and real data analysis demonstrate the competent numerical performance of the new method."], ["Uniformly Semiparametric Efficient Estimation of Treatment Effects With a Continuous Treatment", "This article studies identification, estimation, and inference of general unconditional treatment effects models with continuous treatment under the ignorability assumption. We show identification of the parameters of interest, the dose\u2013response functions, under the assumption that selection to treatment is based on observables. We propose a semiparametric two-step estimator, and consider estimation of the dose\u2013response functions through moment restriction models with generalized residual functions that are possibly nonsmooth. This general formulation includes average and quantile treatment effects as special cases. The asymptotic properties of the estimator are derived, namely, uniform consistency, weak convergence, and semiparametric efficiency. We also develop statistical inference procedures and establish the validity of a bootstrap approach to implement these methods in practice. Monte Carlo simulations show that the proposed methods have good finite sample properties. Finally, we apply the proposed methods to estimate the unconditional average and quantile effects of mothers\u2019 weight gain and age on birthweight. Supplementary materials for this article are available online."], ["A Cluster-Based Outlier Detection Scheme for Multivariate Data", "Detection power of the squared Mahalanobis distance statistic is significantly reduced when several outliers exist within a multivariate dataset of interest. To overcome this masking effect, we propose a computer-intensive cluster-based approach that incorporates a reweighted version of Rousseeuw\u2019s minimum covariance determinant method with a multi-step cluster-based algorithm that initially filters out potential masking points. Compared to the most robust procedures, simulation studies show that our new method is better for outlier detection. Additional real data comparisons are given. Supplementary materials for this article are available online."], ["Plausibility Functions and Exact Frequentist Inference", "In the frequentist program, inferential methods with exact control on error rates are a primary focus. The standard approach, however, is to rely on asymptotic approximations, which may not be suitable. This article presents a general framework for the construction of exact frequentist procedures based on plausibility functions. It is shown that the plausibility function-based tests and confidence regions have the desired frequentist properties in finite samples\u2014no large-sample justification needed. An extension of the proposed method is also given for problems involving nuisance parameters. Examples demonstrate that the plausibility function-based method is both exact and efficient in a wide variety of problems."], ["Bayesian Factorizations of Big Sparse Tensors", null], ["Semiparametric Pseudo-Likelihoods in Generalized Linear Models With Nonignorable Missing Data", "We consider identifiability and estimation in a generalized linear model in which the response variable and some covariates have missing values and the missing data mechanism is nonignorable and unspecified. We adopt a pseudo-likelihood approach that makes use of an instrumental variable to help identifying unknown parameters in the presence of nonignorable missing data. Explicit conditions for the identifiability of parameters are given. Some asymptotic properties of the parameter estimators based on maximizing the pseudo-likelihood are established. Explicit asymptotic covariance matrix and its estimator are also derived in some cases. For the numerical maximization of the pseudo-likelihood, we develop a two-step iteration algorithm that decomposes a nonconcave maximization problem into two problems of maximizing concave functions. Some simulation results and an application to a dataset from cotton factory workers are also presented."], ["Robust Filtering", "Filtering methods are powerful tools to estimate the hidden state of a state-space model from observations available in real time. However, they are known to be highly sensitive to the presence of small misspecifications of the underlying model and to outliers in the observation process. In this article, we show that the methodology of robust statistics can be adapted to sequential filtering. We define a filter as being robust if the relative error in the state distribution caused by misspecifications is uniformly bounded by a linear function of the perturbation size. Since standard filters are nonrobust even in the simplest cases, we propose robustified filters which provide accurate state inference in the presence of model misspecifications. The robust particle filter naturally mitigates the degeneracy problems that plague the bootstrap particle filler (Gordon, Salmond, and Smith) and its many extensions. We illustrate the good properties of robust filters in linear and nonlinear state-space examples. Supplementary materials for this article are available online."], [null, null], ["Marginal Inferential Models: Prior-Free Probabilistic Inference on Interest Parameters", "The inferential models (IM) framework provides prior-free, frequency-calibrated, and posterior probabilistic inference. The key is the use of random sets to predict unobservable auxiliary variables connected to the observable data and unknown parameters. When nuisance parameters are present, a marginalization step can reduce the dimension of the auxiliary variable which, in turn, leads to more efficient inference. For regular problems, exact marginalization can be achieved, and we give conditions for marginal IM validity. We show that our approach provides exact and efficient marginal inference in several challenging problems, including a many-normal-means problem. In nonregular problems, we propose a generalized marginalization technique and prove its validity. Details are given for two benchmark examples, namely, the Behrens\u2013Fisher and gamma mean problems."], ["Testing the Number of Components in Normal Mixture Regression Models", null], ["Latent Space Models for Dynamic Networks", "Dynamic networks are used in a variety of fields to represent the structure and evolution of the relationships between entities. We present a model which embeds longitudinal network data as trajectories in a latent Euclidean space. We propose Markov chain Monte Carlo (MCMC) algorithm to estimate the model parameters and latent positions of the actors in the network. The model yields meaningful visualization of dynamic networks, giving the researcher insight into the evolution and the structure, both local and global, of the network. The model handles directed or undirected edges, easily handles missing edges, and lends itself well to predicting future edges. Further, a novel approach is given to detect and visualize an attracting influence between actors using only the edge information. We use the case-control likelihood approximation to speed up the estimation algorithm, modifying it slightly to account for missing data. We apply the latent space model to data collected from a Dutch classroom, and a cosponsorship network collected on members of the U.S. House of Representatives, illustrating the usefulness of the model by making insights into the networks. Supplementary materials for this article are available online."], ["A High-Dimensional Nonparametric Multivariate Test for Mean Vector", null], ["Smoothed and Corrected Score Approach to Censored Quantile Regression With Measurement Errors", "Censored quantile regression is an important alternative to the Cox proportional hazards model in survival analysis. In contrast to the usual central covariate effects, quantile regression can effectively characterize the covariate effects at different quantiles of the survival time. When covariates are measured with errors, it is known that naively treating mismeasured covariates as error-free would result in estimation bias. Under censored quantile regression, we propose smoothed and corrected estimating equations to obtain consistent estimators. We establish consistency and asymptotic normality for the proposed estimators of quantile regression coefficients. Compared with the naive estimator, the proposed method can eliminate the estimation bias under various measurement error distributions and model error distributions. We conduct simulation studies to examine the finite-sample properties of the new method and apply it to a lung cancer study. Supplementary materials for this article are available online."], ["Latent Surface Models for Networks Using Aggregated Relational Data", "Despite increased interest across a range of scientific applications in modeling and understanding social network structure, collecting complete network data remains logistically and financially challenging, especially in the social sciences. This article introduces a latent surface representation of social network structure for partially observed network data. We derive a multivariate measure of expected (latent) distance between an observed actor and unobserved actors with given features. We also draw novel parallels between our work and dependent data in spatial and ecological statistics. We demonstrate the contribution of our model using a random digit-dial telephone survey and a multiyear prospective study of the relationship between network structure and the spread of infectious disease. The model proposed here is related to previous network models which represents high-dimensional structure through a projection to a low-dimensional latent geometric surface-encoding dependence as distance in the space. We develop a latent surface model for cases when complete network data are unavailable. We focus specifically on aggregated relational data (ARD) which measure network structure indirectly by asking respondents how many connections they have with members of a certain subpopulation (e.g., How many individuals do you know who are HIV positive?) and are easily added to existing surveys. Instead of conditioning on the (latent) distance between two members of the network, the latent surface model for ARD conditions on the expected distance between a survey respondent and the center of a subpopulation on a latent manifold surface. A spherical latent surface and angular distance across the sphere\u2019s surface facilitate tractable computation of this expectation. This model estimates relative homogeneity between groups in the population and variation in the propensity for interaction between respondents and group members. The model also estimates features of groups which are difficult to reach using standard surveys (e.g., the homeless). Supplementary materials for this article are available online."], ["Sequentially Refined Latin Hypercube Designs: Reusing Every Point", "The use of iteratively enlarged Latin hypercube designs for running computer experiments has recently gained popularity in practice. This approach conducts an initial experiment with a computer code using a Latin hypercube design and then runs a follow-up experiment with additional runs elaborately chosen so that the combined design set for the two experiments forms a larger Latin hypercube design. This augmenting process can be repeated multiple stages, where in each stage the augmented design set is guaranteed to be a Latin hypercube design. We provide a theoretical framework to put this approach on a firm footing. Numerical examples are given to corroborate the derived theoretical results. Supplementary materials for this article are available online."], ["A Permutation Approach to Testing Interactions for Binary Response by Comparing Correlations Between Classes", null], ["On Asymptotic Distributions and Confidence Intervals for LIFT Measures in Data Mining", "A LIFT measure, such as the response rate, lift, or the percentage of captured response, is a fundamental measure of effectiveness for a scoring rule obtained from data mining, which is estimated from a set of validation data. In this article, we study how to construct confidence intervals of the LIFT measures. We point out the subtlety of this task and explain how simple binomial confidence intervals can have incorrect coverage probabilities, due to omitting variation from the sample percentile of the scoring rule. We derive the asymptotic distribution using some advanced empirical process theory and the functional delta method in the Appendix. The additional variation is shown to be related to a conditional mean response, which can be estimated by a local averaging of the responses over the scores from the validation data. Alternatively, a subsampling method is shown to provide a valid confidence interval, without needing to estimate the conditional mean response. Numerical experiments are conducted to compare these different methods regarding the coverage probabilities and the lengths of the resulting confidence intervals."], ["Conditional Distance Correlation", "Statistical inference on conditional dependence is essential in many fields including genetic association studies and graphical models. The classic measures focus on linear conditional correlations and are incapable of characterizing nonlinear conditional relationship including nonmonotonic relationship. To overcome this limitation, we introduce a nonparametric measure of conditional dependence for multivariate random variables with arbitrary dimensions. Our measure possesses the necessary and intuitive properties as a correlation index. Briefly, it is zero almost surely if and only if two multivariate random variables are conditionally independent given a third random variable. More importantly, the sample version of this measure can be expressed elegantly as the root of a V or U-process with random kernels and has desirable theoretical properties. Based on the sample version, we propose a test for conditional independence, which is proven to be more powerful than some recently developed tests through our numerical simulations. The advantage of our test is even greater when the relationship between the multivariate random variables given the third random variable cannot be expressed in a linear or monotonic function of one random variable versus the other. We also show that the sample measure is consistent and weakly convergent, and the test statistic is asymptotically normal. By applying our test in a real data analysis, we are able to identify two conditionally associated gene expressions, which otherwise cannot be revealed. Thus, our measure of conditional dependence is not only an ideal concept, but also has important practical utility. Supplementary materials for this article are available online."], ["Small Area Estimation With Uncertain Random Effects", null], ["Treatment Effects With Censoring and Endogeneity", "This article develops a nonparametric approach to identification and estimation of treatment effects on censored outcomes when treatment may be endogenous and have arbitrarily heterogenous effects. Identification is based on an instrumental variable that satisfies the exclusion and monotonicity conditions standard in the local average treatment effects framework. The article proposes a censored quantile treatment effects estimator, derives its asymptotic distribution, and illustrates its performance using Monte Carlo simulations. Even in the exogenous case, the estimator performs better in finite samples than existing censored quantile regression estimators, and performs nearly as well as maximum likelihood estimators in cases where their distributional assumptions hold. An empirical application to a subsidized job training program finds that participation significantly and dramatically reduced the duration of jobless spells, especially at the right tail of the distribution."], ["Optimal Data-Driven Regression Discontinuity Plots", "Exploratory data analysis plays a central role in applied statistics and econometrics. In the popular regression-discontinuity (RD) design, the use of graphical analysis has been strongly advocated because it provides both easy presentation and transparent validation of the design. RD plots are nowadays widely used in applications, despite its formal properties being unknown: these plots are typically presented employing ad hoc choices of tuning parameters, which makes these procedures less automatic and more subjective. In this article, we formally study the most common RD plot based on an evenly spaced binning of the data, and propose several (optimal) data-driven choices for the number of bins depending on the goal of the researcher. These RD plots are constructed either to approximate the underlying unknown regression functions without imposing smoothness in the estimator, or to approximate the underlying variability of the raw data while smoothing out the otherwise uninformative scatterplot of the data. In addition, we introduce an alternative RD plot based on quantile spaced binning, study its formal properties, and propose similar (optimal) data-driven choices for the number of bins. The main proposed data-driven selectors employ spacings estimators, which are simple and easy to implement in applications because they do not require additional choices of tuning parameters. Altogether, our results offer an array of alternative RD plots that are objective and automatic when implemented, providing a reliable benchmark for graphical analysis in RD designs. We illustrate the performance of our automatic RD plots using several empirical examples and a Monte Carlo study. All results are readily available in R and STATA using the software packages described in Calonico, Cattaneo, and Titiunik. Supplementary materials for this article are available online."], ["Reinforcement Learning Trees", null], ["Nonparametric and Parametric Estimators of Prevalence From Group Testing Data With Aggregated Covariates", "Group testing is a technique employed in large screening studies involving infectious disease, where individuals in the study are grouped before being observed. Parametric and nonparametric estimators of conditional prevalence have been developed in the group testing literature, in the case where the binary variable indicating the disease status is available only for the group, but the explanatory variable is observed for each individual. However, for reasons such as the high cost of assays, the confidentiality of the patients, or the impossibility of measuring a concentration under a detection limit, the explanatory variable is observable only in an aggregated form and the existing techniques are no longer valid. We develop consistent parametric and nonparametric estimators of the conditional prevalence in this complex problem. We establish theoretical properties of our estimators and illustrate their practical performance on simulated and real data. We extend our techniques to the case where the group status is measured imperfectly, and to the setting where the covariate is aggregated and the individual status is available. Supplementary materials for this article are available online."], ["Self-Normalization for Time Series: A Review of Recent Developments", null], ["Book Reviews", null], ["Editorial Collaborators", null], ["Editorial Board EOV", null], ["Semiparametric Bayesian Density Estimation With Disparate Data Sources: A Meta-Analysis of Global Childhood Undernutrition", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Stable Weights that Balance Covariates for Estimation With Incomplete Outcome Data", "Weighting methods that adjust for observed covariates, such as inverse probability weighting, are widely used for causal inference and estimation with incomplete outcome data. Part of the appeal of such methods is that one set of weights can be used to estimate a range of treatment effects based on different outcomes, or a variety of population means for several variables. However, this appeal can be diminished in practice by the instability of the estimated weights and by the difficulty of adequately adjusting for observed covariates in some settings. To address these limitations, this article presents a new weighting method that finds the weights of minimum variance that adjust or balance the empirical distribution of the observed covariates up to levels prespecified by the researcher. This method allows the researcher to balance very precisely the means of the observed covariates and other features of their marginal and joint distributions, such as variances and correlations and also, for example, the quantiles of interactions of pairs and triples of observed covariates, thus, balancing entire two- and three-way marginals. Since the weighting method is based on a well-defined convex optimization problem, duality theory provides insight into the behavior of the variance of the optimal weights in relation to the level of covariate balance adjustment, answering the question, how much does tightening a balance constraint increases the variance of the weights? Also, the weighting method runs in polynomial time so relatively large datasets can be handled quickly. An implementation of the method is provided in the new package sbw for R. This article shows some theoretical properties of the resulting weights and illustrates their use by analyzing both a dataset from the 2010 Chilean earthquake and a simulated example."], ["An Integrated Bayesian Nonparametric Approach for Stochastic and Variability Orders in ROC Curve Estimation: An Application to Endometriosis Diagnosis", "In estimating ROC curves of multiple tests, some a priori constraints may exist, either between the healthy and diseased populations within a test or between tests within a population. In this article, we proposed an integrated modeling approach for ROC curves that jointly accounts for stochastic and variability orders. The stochastic order constrains the distributional centers of the diseased and healthy populations within a test, while the variability order constrains the distributional spreads of the tests within each of the populations. Under a Bayesian nonparametric framework, we used features of the Dirichlet process mixture to incorporate these order constraints in a natural way. We applied the proposed approach to data from the Physician Reliability Study that investigated the accuracy of diagnosing endometriosis using different clinical information. To address the issue of no gold standard in the real data, we used a sensitivity analysis approach that exploited diagnosis from a panel of experts. To demonstrate the performance of the methodology, we conducted simulation studies with varying sample sizes, distributional assumptions, and order constraints. Supplementary materials for this article are available online."], ["The Role of CPS Nonresponse in the Measurement of Poverty", "The Current Population Survey Annual Social and Economic Supplement (CPS ASEC) serves as the data source for official income, poverty, and inequality statistics in the United States. There is a concern that the rise in nonresponse to earnings questions could deteriorate data quality and distort estimates of these important metrics. We use a dataset of internal ASEC records matched to Social Security Detailed Earnings Records (DER) to study the impact of earnings nonresponse on estimates of poverty from 1997\u20132008. Our analysis does not treat the administrative data as the \u201ctruth\u201d; instead, we rely on information from both administrative and survey data. We compare a \u201cfull response\u201d poverty rate that assumes all ASEC respondents provided earnings data to the official poverty rate to gauge the nonresponse bias. On average, we find the nonresponse bias is about 1.0 percentage point."], ["Clustering High-Dimensional Landmark-Based Two-Dimensional Shape Data", null], [null, null], ["IsoDOT Detects Differential RNA-Isoform Expression/Usage With Respect to a Categorical or Continuous Covariate With High Sensitivity and Specificity", "We have developed a statistical method named IsoDOT to assess differential isoform expression (DIE) and differential isoform usage (DIU) using RNA-seq data. Here isoform usage refers to relative isoform expression given the total expression of the corresponding gene. IsoDOT performs two tasks that cannot be accomplished by existing methods: to test DIE/DIU with respect to a continuous covariate, and to test DIE/DIU for one case versus one control. The latter task is not an uncommon situation in practice, for example, comparing the paternal and maternal alleles of one individual or comparing tumor and normal samples of one cancer patient. Simulation studies demonstrate the high sensitivity and specificity of IsoDOT. We apply IsoDOT to study the effects of haloperidol treatment on the mouse transcriptome and identify a group of genes whose isoform usages respond to haloperidol treatment. Supplementary materials for this article are available online."], ["Simultaneous Edit-Imputation for Continuous Microdata", "Many statistical organizations collect data that are expected to satisfy linear constraints; as examples, component variables should sum to total variables, and ratios of pairs of variables should be bounded by expert-specified constants. When reported data violate constraints, organizations identify and replace values potentially in error in a process known as edit-imputation. To date, most approaches separate the error localization and imputation steps, typically using optimization methods to identify the variables to change followed by hot deck imputation. We present an approach that fully integrates editing and imputation for continuous microdata under linear constraints. Our approach relies on a Bayesian hierarchical model that includes (i) a flexible joint probability model for the underlying true values of the data with support only on the set of values that satisfy all editing constraints, (ii) a model for latent indicators of the variables that are in error, and (iii) a model for the reported responses for variables in error. We illustrate the potential advantages of the Bayesian editing approach over existing approaches using simulation studies. We apply the model to edit faulty data from the 2007 U.S. Census of Manufactures. Supplementary materials for this article are available online."], ["Smoothed Lexis Diagrams With Applications to Lung and Breast Cancer Trends in Taiwan", "Cancer surveillance research often begins with a rate matrix, also called a Lexis diagram, of cancer incidence derived from cancer registry and census data. Lexis diagrams with 3- or 5-year intervals for age group and for calendar year of diagnosis are often considered. This simple smoothing approach suffers from a significant limitation; important details useful in studying time trends may be lost in the averaging process involved in generating a summary rate. This article constructs a smoothed Lexis diagram and indicates its use in cancer surveillance research. Specifically, we use a Poisson model to describe the relationship between the number of new cases, the number of people at risk, and a smoothly varying incidence rate for the study of the incidence rate function. Based on the Poisson model, we use the standard Lexis diagram to introduce priors through the coefficients of Bernstein polynomials and propose a Bayesian approach to construct a smoothed Lexis diagram for the study of the effects of age, period, and cohort on incidence rates in terms of straightforward graphical displays. These include the age-specific rates by year of birth, age-specific rates by year of diagnosis, year-specific rates by age of diagnosis, and cohort-specific rates by age of diagnosis. We illustrate our approach by studying the trends in lung and breast cancer incidence in Taiwan. We find that for nearly every age group the incidence rates for lung adenocarcinoma and female invasive breast cancer increased rapidly in the past two decades and those for male lung squamous cell carcinoma started to decrease, which is consistent with the decline in the male smoking rate that began in 1985. Since the analyses indicate strong age, period, and cohort effects, it seems that both lung cancer and breast cancer will become more important public health problems in Taiwan. Supplementary materials for this article are available online."], ["Robust Estimation of Inverse Probability Weights for Marginal Structural Models", "Marginal structural models (MSMs) are becoming increasingly popular as a tool for causal inference from longitudinal data. Unlike standard regression models, MSMs can adjust for time-dependent observed confounders while avoiding the bias due to the direct adjustment for covariates affected by the treatment. Despite their theoretical appeal, a main practical difficulty of MSMs is the required estimation of inverse probability weights. Previous studies have found that MSMs can be highly sensitive to misspecification of treatment assignment model even when the number of time periods is moderate. To address this problem, we generalize the covariate balancing propensity score (CBPS) methodology of Imai and Ratkovic to longitudinal analysis settings. The CBPS estimates the inverse probability weights such that the resulting covariate balance is improved. Unlike the standard approach, the proposed methodology incorporates all covariate balancing conditions across multiple time periods. Since the number of these conditions grows exponentially as the number of time period increases, we also propose a low-rank approximation to ease the computational burden. Our simulation and empirical studies suggest that the CBPS significantly improves the empirical performance of MSMs by making the treatment assignment model more robust to misspecification. Open-source software is available for implementing the proposed methods."], ["Bias-Reduced Doubly Robust Estimation", null], ["Testing for Nodal Dependence in Relational Data Matrices", "Relational data are often represented as a square matrix, the entries of which record the relationships between pairs of objects. Many statistical methods for the analysis of such data assume some degree of similarity or dependence between objects in terms of the way they relate to each other. However, formal tests for such dependence have not been developed. We provide a test for such dependence using the framework of the matrix normal model, a type of multivariate normal distribution parameterized in terms of row- and column-specific covariance matrices. We develop a likelihood ratio test (LRT) for row and column dependence based on the observation of a single relational data matrix. We obtain a reference distribution for the LRT statistic, thereby providing an exact test for the presence of row or column correlations in a square relational data matrix. Additionally, we provide extensions of the test to accommodate common features of such data, such as undefined diagonal entries, a nonzero mean, multiple observations, and deviations from normality. Supplementary materials for this article are available online."], ["Testing and Modeling Dependencies Between a Network and Nodal Attributes", "Network analysis is often focused on characterizing the dependencies between network relations and node-level attributes. Potential relationships are typically explored by modeling the network as a function of the nodal attributes or by modeling the attributes as a function of the network. These methods require specification of the exact nature of the association between the network and attributes, reduce the network data to a small number of summary statistics, and are unable to provide predictions simultaneously for missing attribute and network information. Existing methods that model the attributes and network jointly also assume the data are fully observed. In this article, we introduce a unified approach to analysis that addresses these shortcomings. We use a previously developed latent variable model to obtain a low-dimensional representation of the network in terms of node-specific network factors. We introduce a novel testing procedure to determine if dependencies exist between the network factors and attributes as a surrogate for a test of dependence between the network and attributes. We also present a joint model for the network relations and attributes, for use if the hypothesis of independence is rejected, which can capture a variety of dependence patterns and be used to make inference and predictions for missing observations."], ["Blood Flow Velocity Field Estimation Via Spatial Regression With PDE Penalization", "We propose an innovative method for the accurate estimation of surfaces and spatial fields when prior knowledge of the phenomenon under study is available. The prior knowledge included in the model derives from physics, physiology, or mechanics of the problem at hand, and is formalized in terms of a partial differential equation governing the phenomenon behavior, as well as conditions that the phenomenon has to satisfy at the boundary of the problem domain. The proposed models exploit advanced scientific computing techniques and specifically make use of the finite element method. The estimators have a penalized regression form and the usual inferential tools are derived. Both the pointwise and the areal data frameworks are considered. The driving application concerns the estimation of the blood flow velocity field in a section of a carotid artery, using data provided by echo-color Doppler. This applied problem arises within a research project that aims at studying atherosclerosis pathogenesis. Supplementary materials for this article are available online."], ["An Objective Approach to Prior Mass Functions for Discrete Parameter Spaces", null], ["Whittle Likelihood Estimation of Nonlinear Autoregressive Models With Moving Average Residuals", "The Whittle likelihood estimation (WLE) has played a fundamental role in the development of both theory and computation of time series analysis. However, WLE is only applicable to models whose theoretical spectral density function (SDF) is known up to the parameters in the models. In this article, we propose a residual-based WLE, called extended WLE (XWLE), which can estimate models with their SDFs only partially available, including many popular time series models with correlated residuals. Asymptotic properties of XWLE are established. In particular, XWLE is asymptotically equivalent to WLE in estimating linear ARMA models, and is also capable of estimating nonlinear AR models with MA residuals and even with exogenous variables. The finite-sample performances of XWLE are checked by simulated examples and real data analysis."], [null, null], ["Convergence Properties of a Sequential Regression Multiple Imputation Algorithm", "A sequential regression or chained equations imputation approach uses a Gibbs sampling-type iterative algorithm that imputes the missing values using a sequence of conditional regression models. It is a flexible approach for handling different types of variables and complex data structures. Many simulation studies have shown that the multiple imputation inferences based on this procedure have desirable repeated sampling properties. However, a theoretical weakness of this approach is that the specification of a set of conditional regression models may not be compatible with a joint distribution of the variables being imputed. Hence, the convergence properties of the iterative algorithm are not well understood. This article develops conditions for convergence and assesses the properties of inferences from both compatible and incompatible sequence of regression models. The results are established for the missing data pattern where each subject may be missing a value on at most one variable. The sequence of regression models are assumed to be empirically good fit for the data chosen by the imputer based on appropriate model diagnostics. The results are used to develop criteria for the choice of regression models. Supplementary materials for this article are available online."], ["Likelihood Inferences on Semiparametric Odds Ratio Model", "A flexible semiparametric odds ratio model has been proposed to unify and to extend both the log-linear model and the joint normal model for data with a mix of discrete and continuous variables. The semiparametric odds ratio model is particularly useful for analyzing biased sampling designs. However, statistical inference of the model has not been systematically studied when more than one nonparametric component is involved in the model. In this article, we study the maximum semiparametric likelihood approach to estimation and inference of the semiparametric odds ratio model. We show that the maximum semiparametric likelihood estimator of the odds ratio parameter is consistent and asymptotically normally distributed. We also establish statistical inference under a misspecified semiparametric odds ratio model, which is important when handling weak identifiability in conditionally specified models under biased sampling designs. We use simulation studies to demonstrate that the proposed approaches have satisfactory finite sample performance. Finally, we illustrate the proposed approach by analyzing multiple traits in a genome-wide association study of high-density lipid protein. Supplementary materials for this article are available online."], ["The E-MS Algorithm: Model Selection With Incomplete Data", "We propose a procedure associated with the idea of the E-M algorithm for model selection in the presence of missing data. The idea extends the concept of parameters to include both the model and the parameters under the model, and thus allows the model to be part of the E-M iterations. We develop the procedure, known as the E-MS algorithm, under the assumption that the class of candidate models is finite. Some special cases of the procedure are considered, including E-MS with the generalized information criteria (GIC), and E-MS with the adaptive fence (AF; Jiang et al.). We prove numerical convergence of the E-MS algorithm as well as consistency in model selection of the limiting model of the E-MS convergence, for E-MS with GIC and E-MS with AF. We study the impact on model selection of different missing data mechanisms. Furthermore, we carry out extensive simulation studies on the finite-sample performance of the E-MS with comparisons to other procedures. The methodology is also illustrated on a real data analysis involving QTL mapping for an agricultural study on barley grains. Supplementary materials for this article are available online."], ["Regression Analysis of Additive Hazards Model With Latent Variables", "We propose an additive hazards model with latent variables to investigate the observed and latent risk factors of the failure time of interest. Each latent risk factor is characterized by correlated observed variables through a confirmatory factor analysis model. We develop a hybrid procedure that combines the expectation\u2013maximization (EM) algorithm and the borrow-strength estimation approach to estimate the model parameters. We establish the consistency and asymptotic normality of the parameter estimators. Various nice features, including finite sample performance of the proposed methodology, are demonstrated by simulation studies. Our model is applied to a study concerning the risk factors of chronic kidney disease for Type 2 diabetic patients. Supplementary materials for this article are available online."], ["Bandwidth Selection for High-Dimensional Covariance Matrix Estimation", "The banding estimator of Bickel and Levina and its tapering version of Cai, Zhang, and Zhou are important high-dimensional covariance estimators. Both estimators require a bandwidth parameter. We propose a bandwidth selector for the banding estimator by minimizing an empirical estimate of the expected squared Frobenius norms of the estimation error matrix. The ratio consistency of the bandwidth selector is established. We provide a lower bound for the coverage probability of the underlying bandwidth being contained in an interval around the bandwidth estimate. Extensions to the bandwidth selection for the tapering estimator and threshold level selection for the thresholding covariance estimator are made. Numerical simulations and a case study on sonar spectrum data are conducted to demonstrate the proposed approaches. Supplementary materials for this article are available online."], ["Estimation of Multiple-Regime Threshold Autoregressive Models With Structural Breaks", "The threshold autoregressive (TAR) model is a class of nonlinear time series models that have been widely used in many areas. Due to its nonlinear nature, one major difficulty in fitting a TAR model is the estimation of the thresholds. As a first contribution, this article develops an automatic procedure to estimate the number and values of the thresholds, as well as the corresponding AR order and parameter values in each regime. These parameter estimates are defined as the minimizers of an objective function derived from the minimum description length (MDL) principle. A genetic algorithm (GA) is constructed to efficiently solve the associated minimization problem. The second contribution of this article is the extension of this framework to piecewise TAR modeling; that is, the time series is partitioned into different segments for which each segment can be adequately modeled by a TAR model, while models from adjacent segments are different. For such piecewise TAR modeling, a procedure is developed to estimate the number and locations of the breakpoints, together with all other parameters in each segment. Desirable theoretical results are derived to lend support to the proposed methodology. Simulation experiments and an application to an U.S. GNP data are used to illustrate the empirical performances of the methodology. Supplementary materials for this article are available online."], ["Analysis of the Proportional Hazards Model With Sparse Longitudinal Covariates", "Regression analysis of censored failure observations via the proportional hazards model permits time-varying covariates that are observed at death times. In practice, such longitudinal covariates are typically sparse and only measured at infrequent and irregularly spaced follow-up times. Full likelihood analyses of joint models for longitudinal and survival data impose stringent modeling assumptions that are difficult to verify in practice and that are complicated both inferentially and computationally. In this article, a simple kernel weighted score function is proposed with minimal assumptions. Two scenarios are considered: half kernel estimation in which observation ceases at the time of the event and full kernel estimation for data where observation may continue after the event, as with recurrent events data. It is established that these estimators are consistent and asymptotically normal. However, they converge at rates that are slower than the parametric rates that\u00a0may be achieved with fully observed covariates, with the full kernel method achieving an optimal convergence rate that is superior to that of the half kernel method. Simulation results demonstrate that the large sample approximations are adequate for practical use and may yield improved performance relative to last value carried forward approach and joint modeling method. The analysis of the data from a cardiac arrest study demonstrates the utility of the proposed methods. Supplementary materials for this article are available online."], ["Detection of Changes in Multivariate Time Series With Application to EEG Data", "The primary contributions of this article are rigorously developed novel statistical methods for detecting change points in multivariate time series. We extend the class of score type change point statistics considered in 2007 by Hu\u0161kov\u00e1, Pr\u00e1\u0161kov\u00e1, and Steinebach to the vector autoregressive (VAR) case and the epidemic change alternative. Our proposed procedures do not require the observed time series to actually follow the VAR model. Instead, following the strategy implicitly employed by practitioners, our approach takes model misspecification into account so that our detection procedure uses the model background merely for feature extraction. We derive the asymptotic distributions of our test statistics and show that our procedure has asymptotic power of 1. The proposed test statistics require the estimation of the inverse of the long-run covariance matrix which is particularly difficult in higher-dimensional settings (i.e., where the dimension of the time series and the dimension of the parameter vector are both large). Thus we robustify the proposed test statistics and investigate their finite sample properties via extensive numerical experiments. Finally, we apply our procedure to electroencephalograms and demonstrate its potential impact in identifying change points in complex brain processes during a cognitive motor task."], ["The Empirical Distribution of a Large Number of Correlated Normal Variables", "Motivated by the advent of high-dimensional, highly correlated data, this work studies the limit behavior of the empirical cumulative distribution function (ecdf) of standard normal random variables under arbitrary correlation. First, we provide a necessary and sufficient condition for convergence of the ecdf to the standard normal distribution. Next, under general correlation, we show that the ecdf limit is a random, possible infinite, mixture of normal distribution functions that depends on a number of latent variables and can serve as an asymptotic approximation to the ecdf in high dimensions. We provide conditions under which the dimension of the ecdf limit, defined as the smallest number of effective latent variables, is finite. Estimates of the latent variables are provided and their consistency proved. We demonstrate these methods in a real high-dimensional data example from brain imaging where it is shown that, while the study exhibits apparently strongly significant results, they can be entirely explained by correlation, as captured by the asymptotic approximation developed here. Supplementary materials for this article are available online."], [null, null], ["Tracking Cross-Validated Estimates of Prediction Error as Studies Accumulate", null], ["An Equivalent Measure of Partial Correlation Coefficients for High-Dimensional Gaussian Graphical Models", null], ["Localized Functional Principal Component Analysis", "We propose localized functional principal component analysis (LFPCA), looking for orthogonal basis functions with localized support regions that explain most of the variability of a random process. The LFPCA is formulated as a convex optimization problem through a novel deflated Fantope localization method and is implemented through an efficient algorithm to obtain the global optimum. We prove that the proposed LFPCA converges to the original functional principal component analysis (FPCA) when the tuning parameters are chosen appropriately. Simulation shows that the proposed LFPCA with tuning parameters chosen by cross-validation can almost perfectly recover the true eigenfunctions and significantly improve the estimation accuracy when the eigenfunctions are truly supported on some subdomains. In the scenario that the original eigenfunctions are not localized, the proposed LFPCA also serves as a nice tool in finding orthogonal basis functions that balance between interpretability and the capability of explaining variability of the data. The analyses of a country mortality data reveal interesting features that cannot be found by standard FPCA methods. Supplementary materials for this article are available online."], ["A Regression Framework for Rank Tests Based on the Probabilistic Index Model", "We demonstrate how many classical rank tests, such as the Wilcoxon\u2013Mann\u2013Whitney, Kruskal\u2013Wallis, and Friedman test, can be embedded in a statistical modeling framework and how the method can be used to construct new rank tests. In addition to hypothesis testing, the method allows for estimating effect sizes with an informative interpretation, resulting in a better understanding of the data. Supplementary materials for this article are available online."], ["Model Estimation, Prediction, and Signal Extraction for Nonstationary Stock and Flow Time Series Observed at Mixed Frequencies", "An important practical problem for statistical agencies and central banks that publish economic data is the seasonal adjustment of mixed frequency stock and flow time series. This may arise in practice due to changes in funding of a particular survey. Mathematically, the problem can be reduced to the need to compute imputations, forecasts, and backcasts from a given model of the highest available frequency data. The nonstationarity of the economic time series coupled with the alteration of sampling frequency makes the problem of model estimation and imputation challenging. For flow data the analysis cannot be recast as a missing value problem, so that time series imputation methods are ineffective. We provide explicit formulas and algorithms that allow one to compute the log Gaussian likelihood of the mixed sample, as well as any imputations and forecasts. Formulas for the relevant mean squared error are also derived. We evaluate the methodology through simulations, and illustrate the techniques on some economic time series."], ["Design and Analysis of the Randomized Response Technique", "About a half century ago, in 1965, Warner proposed the randomized response method as a survey technique to reduce potential bias due to nonresponse and social desirability when asking questions about sensitive behaviors and beliefs. This method asks respondents to use a randomization device, such as a coin flip, whose outcome is unobserved by the interviewer. By introducing random noise, the method conceals individual responses and protects respondent privacy. While numerous methodological advances have been made, we find surprisingly few applications of this promising survey technique. In this article, we address this gap by (1) reviewing standard designs available to applied researchers, (2) developing various multivariate regression techniques for substantive analyses, (3) proposing power analyses to help improve research designs, (4) presenting new robust designs that are based on less stringent assumptions than those of the standard designs, and (5) making all described methods available through open-source software. We illustrate some of these methods with an original survey about militant groups in Nigeria."], ["Book Reviews", null], ["False Discovery Rate Regression: An Application to Neural Synchrony Detection in Primary Visual Cortex", null], ["Analysis of Longitudinal Multivariate Outcome Data From Couples Cohort Studies: Application to HPV Transmission Dynamics", "We consider a specific situation of correlated data where multiple outcomes are repeatedly measured on each member of a couple. Such multivariate longitudinal data from couples may exhibit multi-faceted correlations that can be further complicated if there are polygamous partnerships. An example is data from cohort studies on human papillomavirus (HPV) transmission dynamics in heterosexual couples. HPV is a common sexually transmitted disease with 14 known oncogenic types causing anogenital cancers. The binary outcomes on the multiple types measured in couples over time may introduce inter-type, intra-couple, and temporal correlations. Simple analysis using generalized estimating equations or random effects models lacks interpretability and cannot fully use the available information. We developed a hybrid modeling strategy using Markov transition models together with pairwise composite likelihood for analyzing such data. The method can be used to identify risk factors associated with HPV transmission and persistence, estimate difference in risks between male-to-female and female-to-male HPV transmission, compare type-specific transmission risks within couples, and characterize the inter-type and intra-couple associations. Applying the method to HPV couple data collected in a Ugandan male circumcision (MC) trial, we assessed the effect of MC and the role of gender on risks of HPV transmission and persistence. Supplementary materials for this article are available online."], ["Survival Analysis of Loblolly Pine Trees With Spatially Correlated Random Effects", "Loblolly pine, a native pine species of the southeastern United States, is the most-planted species for commercial timber. Predicting survival of loblolly pine following planting is of great interest to researchers in forestry science as it is closely related to the yield of timber. Data were collected from a region-wide thinning study, where permanent plots, located at 182 sites ranging from central Texas east to Florida and north to Delaware, were established in 1980\u20131981. One of the main objectives of this study was to investigate the relationship between the survival of loblolly pine trees and several important covariates such as age, thinning types, and physiographic regions, while adjusting for spatial correlation among different sites. We use a semiparametric proportional hazards model to describe the effects of covariates on the survival time, and incorporate the spatial random effects in the model to describe the spatial correlation among different sites. We apply the expectation-maximization (EM) algorithm to estimate the parameters in the model and conduct simulations to validate the estimation procedure. We also compare the proposed method with existing methods through simulations and discussions. Then we apply the developed method to the large-scale loblolly pine tree survival data and interpret the results. We conclude this article with discussions on the advantages of the proposed method, major findings of data analysis, and directions for future research. Supplementary materials for this article are available online."], ["MAD Bayes for Tumor Heterogeneity\u2014Feature Allocation With Exponential Family Sampling", null], ["Large, Sparse Optimal Matching With Refined Covariate Balance in an Observational Study of the Health Outcomes Produced by New Surgeons", "Every newly trained surgeon performs her first unsupervised operation. How do the health outcomes of her patients compare with the patients of experienced surgeons? Using data from 498 hospitals, we compare 1252 pairs comprised of a new surgeon and an experienced surgeon working at the same hospital. We introduce a new form of matching that matches patients of each new surgeon to patients of an otherwise similar experienced surgeon at the same hospital, perfectly balancing 176 surgical procedures and closely balancing a total of 2.9 million categories of patients; additionally, the individual patient pairs are as close as possible. A new goal for matching is introduced, called \u201crefined covariate balance,\u201d in which a sequence of nested, ever more refined, nominal covariates is balanced as closely as possible, emphasizing the first or coarsest covariate in that sequence. A new algorithm for matching is proposed and the main new results prove that the algorithm finds the closest match in terms of the total within-pair covariate distances among all matches that achieve refined covariate balance. Unlike previous approaches to forcing balance on covariates, the new algorithm creates multiple paths to a match in a network, where paths that introduce imbalances are penalized and hence avoided to the extent possible. The algorithm exploits a sparse network to quickly optimize a match that is about two orders of magnitude larger than is typical in statistical matching problems, thereby permitting much more extensive use of fine and near-fine balance constraints. The match was constructed in a few minutes using a network optimization algorithm implemented in R. An R package called rcbalance implementing the method is available from CRAN."], ["Bayesian Inference for Multivariate Meta-Regression With a Partially Observed Within-Study Sample Covariance Matrix", null], ["Unifying Amplitude and Phase Analysis: A Compositional Data Approach to Functional Multivariate Mixed-Effects Modeling of Mandarin Chinese", null], ["Analysis of Sequence Data Under Multivariate Trait-Dependent Sampling", "High-throughput DNA sequencing allows for the genotyping of common and rare variants for genetic association studies. At the present time and for the foreseeable future, it is not economically feasible to sequence all individuals in a large cohort. A cost-effective strategy is to sequence those individuals with extreme values of a quantitative trait. We consider the design under which the sampling depends on multiple quantitative traits. Under such trait-dependent sampling, standard linear regression analysis can result in bias of parameter estimation, inflation of Type I error, and loss of power. We construct a likelihood function that properly reflects the sampling mechanism and uses all available data. We implement a computationally efficient EM algorithm and establish the theoretical properties of the resulting maximum likelihood estimators. Our methods can be used to perform separate inference on each trait or simultaneous inference on multiple traits. We pay special attention to gene-level association tests for rare variants. We demonstrate the superiority of the proposed methods over standard linear regression through extensive simulation studies. We provide applications to the Cohorts for Heart and Aging Research in Genomic Epidemiology Targeted Sequencing Study and the National Heart, Lung, and Blood Institute Exome Sequencing Project. Supplementary materials for this article are available online."], ["Joint Bayesian Modeling of Binomial and Rank Data for Primate Cognition", "In recent years, substantial effort has been devoted to methods for analyzing data containing mixed response types, but such techniques typically do not include rank data among the response types. Some unique challenges exist in analyzing rank data, particularly when ties are prevalent. We present techniques for jointly modeling binomial and rank data using Bayesian latent variable models. We apply these techniques to compare the cognitive abilities of nonhuman primates based on their performance on 17 cognitive tasks scored on either a rank or binomial scale. To jointly model the rank and binomial responses, we assume that responses are implicitly determined by latent cognitive abilities. We then model the latent variables using random effects models, with identifying restrictions chosen to promote parsimonious prior specification and model inferences. Results from the primate cognitive data are presented to illustrate the methodology. Our results suggest that the ordering of the cognitive abilities of species varies significantly across tasks, suggesting a partially independent evolution of cognitive abilities in primates. Supplementary materials for this article are available online."], ["New Statistical Learning Methods for Estimating Optimal Dynamic Treatment Regimes", null], ["Foundations for Envelope Models and Methods", "Envelopes were recently proposed by Cook, Li and Chiaromonte as a method for reducing estimative and predictive variations in multivariate linear regression. We extend their formulation, proposing a general definition of an envelope and a general framework for adapting envelope methods to any estimation procedure. We apply the new envelope methods to weighted least squares, generalized linear models and Cox regression. Simulations and illustrative data analysis show the potential for envelope methods to significantly improve standard methods in linear discriminant analysis, logistic regression and Poisson regression. Supplementary materials for this article are available online."], ["Post-Fisherian Experimentation: From Physical to Virtual", "Fisher\u2019s pioneering work in design of experiments has inspired further work with broader applications, especially in industrial experimentation. This article discusses three topics in physical experiments: principles of effect hierarchy, sparsity, and heredity for factorial designs, a new method called conditional main effect (CME) for de-aliasing aliased effects, and robust parameter design. I also review the recent emergence of virtual experiments on a computer. Some major challenges in computer experiments, which must go beyond Fisherian principles, are outlined."], ["Semiparametric Accelerated Failure Time Modeling for Clustered Failure Times From Stratified Sampling", "Clustered failure times often arise from studies with stratified sampling designs where it is desired to reduce both cost and sampling error. Semiparametric accelerated failure time (AFT) models have not been used as frequently as Cox relative risk models in such settings due to lack of efficient and reliable computing routines for inferences. The challenge roots in the nonsmoothness of the rank-based estimating functions, and for clustered data, the asymptotic properties of the estimator from the weighted version have not been available. The recently proposed induced smoothing approach, which provides fast and accurate rank-based inferences for AFT models, is generalized to incorporate weights to accommodate stratified sampling designs. The estimator from the induced smoothing weighted estimating equations are shown to be consistent and have the same asymptotic distribution as that from the nonsmooth version, which has not been developed before. The variance of the estimator is estimated by computationally efficient sandwich estimators aided by a multiplier bootstrap. The proposed method is assessed in extensive simulation studies where the estimators appear to provide valid and efficient inferences. A stratified case-cohort design with clustered times to tooth extraction in a dental study illustrates the usefulness of the method."], ["Model-Free Feature Screening for Ultrahigh Dimensional Discriminant Analysis", null], [null, null], ["Detection of Multiple Structural Breaks in Multivariate Time Series", "We propose a new nonparametric procedure (referred to as MuBreD) for the detection and estimation of multiple structural breaks in the autocovariance function of a multivariate (second-order) piecewise stationary process, which also identifies the components of the series where the breaks occur. MuBreD is based on a comparison of the estimated spectral distribution on different segments of the observed time series and consists of three steps: it starts with a consistent test, which allows us to prove the existence of structural breaks at a controlled Type I error. Second, it estimates sets containing possible break points and finally these sets are reduced to identify the relevant structural breaks and corresponding components which are responsible for the changes in the autocovariance structure. In contrast to all other methods proposed in the literature, our approach does not make any parametric assumptions, is not especially designed for detecting one single change point, and addresses the problem of multiple structural breaks in the autocovariance function directly with no use of the binary segmentation algorithm. We prove that the new procedure detects all components and the corresponding locations where structural breaks occur with probability converging to one as the sample size increases and provide data-driven rules for the selection of all regularization parameters. The results are illustrated by analyzing financial asset returns, and in a simulation study it is demonstrated that MuBreD outperforms the currently available nonparametric methods for detecting breaks in the dependency structure of multivariate time series. Supplementary materials for this article are available online."], ["Testing Hypotheses of Covariate-Adaptive Randomized Clinical Trials", null], ["Functional and Structural Methods With Mixed Measurement Error and Misclassification in Covariates", "Covariate measurement imprecision or errors arise frequently in many areas. It is well known that ignoring such errors can substantially degrade the quality of inference or even yield erroneous results. Although in practice both covariates subject to measurement error and covariates subject to misclassification can occur, research attention in the literature has mainly focused on addressing either one of these problems separately. To fill this gap, we develop estimation and inference methods that accommodate both characteristics simultaneously. Specifically, we consider measurement error and misclassification in generalized linear models under the scenario that an external validation study is available, and systematically develop a number of effective functional and structural methods. Our methods can be applied to different situations to meet various objectives."], ["Objective Bayesian Survival Analysis Using Shape Mixtures of Log-Normal Distributions", "Survival models such as the Weibull or log-normal lead to inference that is not robust to the presence of outliers. They also assume that all heterogeneity between individuals can be modeled through covariates. This article considers the use of infinite mixtures of lifetime distributions as a solution for these two issues. This can be interpreted as the introduction of a random effect in the survival distribution. We introduce the family of shape mixtures of log-normal distributions, which covers a wide range of density and hazard functions. Bayesian inference under nonsubjective priors based on the Jeffreys\u2019 rule is examined and conditions for posterior propriety are established. The existence of the posterior distribution on the basis of a sample of point observations is not always guaranteed and a solution through set observations is implemented. In addition, we propose a method for outlier detection based on the mixture structure. A simulation study illustrates the performance of our methods under different scenarios and an application to a real dataset is provided. Supplementary materials for the article, which include R code, are available online."], ["Bayesian Dose-Finding in Two Treatment Cycles Based on the Joint Utility of Efficacy and Toxicity", "This article proposes a phase I/II clinical trial design for adaptively and dynamically optimizing each patient\u2019s dose in each of two cycles of therapy based on the joint binary efficacy and toxicity outcomes in each cycle. A dose-outcome model is assumed that includes a Bayesian hierarchical latent variable structure to induce association among the outcomes and also facilitate posterior computation. Doses are chosen in each cycle based on posteriors of a model-based objective function, similar to a reinforcement learning or Q-learning function, defined in terms of numerical utilities of the joint outcomes in each cycle. For each patient, the procedure outputs a sequence of two actions, one for each cycle, with each action being the decision to either treat the patient at a chosen dose or not to treat. The cycle 2 action depends on the individual patient\u2019s cycle 1 dose and outcomes. In addition, decisions are based on posterior inference using other patients\u2019 data, and therefore, the proposed method is adaptive both within and between patients. A simulation study of the method is presented, including comparison to two-cycle extensions of the conventional 3 + 3 algorithm, continual reassessment method, and a Bayesian model-based design, and evaluation of robustness. Supplementary materials for this article are available online."], ["Efficient Quantile Regression Analysis With Missing Observations", null], ["Matching a Distribution by Matching Quantiles Estimation", "Motivated by the problem of selecting representative portfolios for backtesting counterparty credit risks, we propose a matching quantiles estimation (MQE) method for matching a target distribution by that of a linear combination of a set of random variables. An iterative procedure based on the ordinary least-squares estimation (OLS) is proposed to compute MQE. MQE can be easily modified by adding a LASSO penalty term if a sparse representation is desired, or by restricting the matching within certain range of quantiles to match a part of the target distribution. The convergence of the algorithm and the asymptotic properties of the estimation, both with or without LASSO, are established. A measure and an associated statistical test are proposed to assess the goodness-of-match. The finite sample properties are illustrated by simulation. An application in selecting a counterparty representative portfolio with a real dataset is reported. The proposed MQE also finds applications in portfolio tracking, which demonstrates the usefulness of combining MQE with LASSO."], ["Generalized Fiducial Inference for\u00a0Ultrahigh-Dimensional Regression", null], ["An Improved Transformation-Based Kernel Estimator of Densities on the Unit Interval", "The kernel density estimator (KDE) suffers boundary biases when applied to densities on bounded supports, which are assumed to be the unit interval. Transformations mapping the unit interval to the real line can be used to remove boundary biases. However, this approach may induce erratic tail behaviors when the estimated density of transformed data is transformed back to its original scale. We propose a modified, transformation-based KDE that employs a tapered and tilted back-transformation. We derive the theoretical properties of the new estimator and show that it asymptotically dominates the naive transformation based estimator while maintains its simplicity. We then propose three automatic methods of smoothing parameter selection. Our Monte Carlo simulations demonstrate the good finite sample performance of the proposed estimator, especially for densities with poles near the boundaries. An example with real data is provided."], ["LADE-Based Inference for ARMA Models With Unspecified and Heavy-Tailed Heteroscedastic Noises", null], ["Scalable Bayesian Model Averaging Through Local Information Propagation", null], ["Clustering from Categorical Data Sequences", "The three-parameter cluster model is a combinatorial stochastic process that generates categorical response sequences by randomly perturbing a fixed clustering parameter. This clear relationship between the observed data and the underlying clustering is particularly attractive in cluster analysis, in which supervised learning is a common goal and missing data is a familiar issue. The model is well equipped for this task, as it can handle missing data, perform out-of-sample inference, and accommodate both independent and dependent data sequences. Moreover, its clustering parameter lies in the unrestricted space of partitions, so that the number of clusters need not be specified beforehand. We establish these and other theoretical properties and also demonstrate the model on datasets from epidemiology, genetics, political science, and legal studies."], ["Index Models for Sparsely Sampled Functional Data", null], ["A Two-Sample Test for Equality of Means in High Dimension", null], [null, null], ["Likelihood Ratio Test for Multi-Sample Mixture Model and Its Application to Genetic Imprinting", null], ["Book Reviews", null], ["Why Your Involvement Matters", "The International Year of Statistics, 2013, focused on outreach in a wonderful way. As we celebrate the ASA's 175th anniversary in 2014, it is worthwhile to look inward as well and think about how to keep our association and profession strong, so that our successors will be able to celebrate the 275th anniversary. The ASA, with its long history, its fine staff and organization, and its financial resource base, is well positioned to serve the profession, and indeed society, and it is very successful at doing so. But the real measure of the health of our association is the size and level of engagement of its membership, whose participation is a major source of the ASA's strength. So, what is it that compels people to be members? One might argue that it is the tangible benefits that we receive in exchange for our dues\u2014magazine and journal subscriptions, discounted meeting registrations, and so on. Although such benefits are attractive, I believe they are not the primary reasons people are ASA members. What compels people is the value they find through involvement in the association. Unlike benefits, which are objective, value is subjective, varying over time and varying from member to member or group to group. And unlike benefits, which can be listed as bullet points, value is best borne out in personal experiences. In this address, I will use experiences that ASA members have shared with me, along with experiences of my own, to paint a picture of the deep value that involvement in the ASA has provided. I also will challenge you to continue to find the extraordinary value available through involvement in our association."], ["A Spatio-Temporal Point Process Model for Ambulance Demand", "Ambulance demand estimation at fine time and location scales is critical for fleet management and dynamic deployment. We are motivated by the problem of estimating the spatial distribution of ambulance demand in Toronto, Canada, as it changes over discrete 2 hr intervals. This large-scale dataset is sparse at the desired temporal resolutions and exhibits location-specific serial dependence, daily, and weekly seasonality. We address these challenges by introducing a novel characterization of time-varying Gaussian mixture models. We fix the mixture component distributions across all time periods to overcome data sparsity and accurately describe Toronto\u2019s spatial structure, while representing the complex spatio-temporal dynamics through time-varying mixture weights. We constrain the mixture weights to capture weekly seasonality, and apply a conditionally autoregressive prior on the mixture weights of each component to represent location-specific short-term serial dependence and daily seasonality. While estimation may be performed using a fixed number of mixture components, we also extend to estimate the number of components using birth-and-death Markov chain Monte Carlo. The proposed model is shown to give higher statistical predictive accuracy and to reduce the error in predicting emergency medical service operational performance by as much as two-thirds compared to a typical industry practice."], ["Risk-Adjusted Cumulative Sum Charting Procedure Based on\u00a0Multiresponses", "The cumulative sum charting procedure is traditionally used in the manufacturing industry for monitoring the quality of products. Recently, it has been extended to monitoring surgical outcomes. Unlike a manufacturing process where the raw material is usually reasonably homogeneous, patients\u2019 risks of surgical failure are usually different. It has been proposed in the literature that the binary outcomes from a surgical procedure be adjusted using the preoperative risk based on a likelihood-ratio scoring method. Such a crude classification of surgical outcome is naive. It is unreasonable to regard a patient who has a full recovery, the same quality outcome as another patient who survived but remained bed-ridden for life. For a patient who survives an operation, there can be many different grades of recovery. Thus, it makes sense to consider a risk-adjusted cumulative sum charting procedure based on more than two outcomes to better monitor surgical performance. In this article, we develop such a chart and study its performance."], ["Estimating a Structured Covariance Matrix From Multilab Measurements in High-Throughput Biology", "We consider the problem of quantifying the degree of coordination between transcription and translation, in yeast. Several studies have reported a surprising lack of coordination over the years, in organisms as different as yeast and humans, using diverse technologies. However, a close look at this literature suggests that the lack of reported correlation may not reflect the biology of regulation. These reports do not control for between-study biases and structure in the measurement errors, ignore key aspects of how the data connect to the estimand, and systematically underestimate the correlation as a consequence. Here, we design a careful meta-analysis of 27 yeast datasets, supported by a multilevel model, full uncertainty quantification, a suite of sensitivity analyses, and novel theory, to produce a more accurate estimate of the correlation between mRNA and protein levels\u2014a proxy for coordination. From a statistical perspective, this problem motivates new theory on the impact of noise, model misspecifications, and nonignorable missing data on estimates of the correlation between high-dimensional responses. We find that the correlation between mRNA and protein levels is quite high under the studied conditions, in yeast, suggesting that post-transcriptional regulation plays a less prominent role than previously thought."], ["A Flexible Bayesian Approach to Monotone Missing Data in Longitudinal Studies With Nonignorable Missingness With Application to an Acute Schizophrenia Clinical Trial", "We develop a Bayesian nonparametric model for a longitudinal response in the presence of nonignorable missing data. Our general approach is to first specify a working model that flexibly models the missingness and full outcome processes jointly. We specify a Dirichlet process mixture of missing at random (MAR) models as a prior on the joint distribution of the working model. This aspect of the model governs the fit of the observed data by modeling the observed data distribution as the marginalization over the missing data in the working model. We then separately specify the conditional distribution of the missing data given the observed data and dropout. This approach allows us to identify the distribution of the missing data using identifying restrictions as a starting point. We propose a framework for introducing sensitivity parameters, allowing us to vary the untestable assumptions about the missing data mechanism smoothly. Informative priors on the space of missing data assumptions can be specified to combine inferences under many different assumptions into a final inference and accurately characterize uncertainty. These methods are motivated by, and applied to, data from a clinical trial assessing the efficacy of a new treatment for acute schizophrenia. Supplementary materials for this article are available online."], ["Power Curve Estimation With Multivariate Environmental Factors for Inland and Offshore Wind\u00a0Farms", "In the wind industry, a power curve refers to the functional relationship between the power output generated by a wind turbine and the wind speed at the time of power generation. Power curves are used in practice for a number of important tasks including predicting wind power production and assessing a turbine\u2019s energy production efficiency. Nevertheless, actual wind power data indicate that the power output is affected by more than just wind speed. Several other environmental factors, such as wind direction, air density, humidity, turbulence intensity, and wind shears, have potential impact. Yet, in industry practice, as well as in the literature, current power curve models primarily consider wind speed and, sometimes, wind speed and direction. We propose an additive multivariate kernel method that can include the aforementioned environmental factors as a new power curve model. Our model provides, conditional on a given environmental condition, both the point estimation and density estimation of power output. It is able to capture the nonlinear relationships between environmental factors and the wind power output, as well as the high-order interaction effects among some of the environmental factors. Using operational data associated with four turbines in an inland wind farm and two turbines in an offshore wind farm, we demonstrate the improvement achieved by our kernel method."], ["Calibration of Computational Models With Categorical Parameters and Correlated Outputs via Bayesian Smoothing Spline ANOVA", null], ["What Happens Depends on When It Happens: Copula-Based Ordered Event History Analysis of Civil War Duration and Outcome", "Scholars are interested in not just what event happens but also when the event happens. If there is dependence among events or dependence between time and events, however, the currently common methods (e.g., competing risks approaches) produce biased estimates. To deal with these problems, this article proposes a new method of copula-based ordered event history analysis (COEHA). A merit of working with copulas is that, whatever marginal distributions time and event variables follow (including the Cox model), researchers can derive whatever joint distribution exists between the two. Application of the COEHA model to a dataset from civil wars supports two controversial hypotheses. First, as wars become longer, rebel victory becomes more likely but settlement does not (there is dependence between time and events at both tails). Second, stronger rebels make wars shorter but do not necessarily tend to win, as experts predict but fail to establish (rebels\u2019 strength shortens time but has no effect on which events occur). Supplementary materials for this article are available online."], ["A Dynamic Directional Model for Effective Brain Connectivity Using Electrocorticographic (ECoG) Time Series", "We introduce a dynamic directional model (DDM) for studying brain effective connectivity based on intracranial electrocorticographic (ECoG) time series. The DDM consists of two parts: a set of differential equations describing neuronal activity of brain components (state equations), and observation equations linking the underlying neuronal states to observed data. When applied to functional MRI or EEG data, DDMs usually have complex formulations and thus can accommodate only a few regions, due to limitations in spatial resolution and/or temporal resolution of these imaging modalities. In contrast, we formulate our model in the context of ECoG data. The combined high temporal and spatial resolution of ECoG data result in a much simpler DDM, allowing investigation of complex connections between many regions. To identify functionally segregated subnetworks, a form of biologically economical brain networks, we propose the Potts model for the DDM parameters. The neuronal states of brain components are represented by cubic spline bases and the parameters are estimated by minimizing a log-likelihood criterion that combines the state and observation equations. The Potts model is converted to the Potts penalty in the penalized regression approach to achieve sparsity in parameter estimation, for which a fast iterative algorithm is developed. The methods are applied to an auditory ECoG dataset."], ["Simulating and Analyzing Order Book Data: The Queue-Reactive Model", "Through the analysis of a dataset of ultra high frequency order book updates, we introduce a model which accommodates the empirical properties of the full order book together with the stylized facts of lower frequency financial data. To do so, we split the time interval of interest into periods in which a well chosen reference price, typically the midprice, remains constant. Within these periods, we view the limit order book as a Markov queuing system. Indeed, we assume that the intensities of the order flows only depend on the current state of the order book. We establish the limiting behavior of this model and estimate its parameters from market data. Then, to design a relevant model for the whole period of interest, we use a stochastic mechanism that allows to switch from one period of constant reference price to another. Beyond enabling to reproduce accurately the behavior of market data, we show that our framework can be very useful for practitioners, notably as a market simulator or as a tool for the transaction cost analysis of complex trading algorithms."], ["An Analysis of an Incomplete Marked Point Pattern of Heat-Related 911 Calls", null], ["Robust Principal Component Analysis for Power Transformed Compositional Data", "Geochemical surveys collect sediment or rock samples, measure the concentration of chemical elements, and report these typically either in weight percent or in parts per million (ppm). There are usually a large number of elements measured and the distributions are often skewed, containing many potential outliers. We present a new robust principal component analysis (PCA) method for geochemical survey data, that involves first transforming the compositional data onto a manifold using a relative power transformation. A flexible set of moment assumptions are made which take the special geometry of the manifold into account. The Kent distribution moment structure arises as a special case when the chosen manifold is the hypersphere. We derive simple moment and robust estimators (RO) of the parameters which are also applicable in high-dimensional settings. The resulting PCA based on these estimators is done in the tangent space and is related to the power transformation method used in correspondence analysis. To illustrate, we analyze major oxide data from the National Geochemical Survey of Australia. When compared with the traditional approach in the literature based on the centered log-ratio transformation, the new PCA method is shown to be more successful at dimension reduction and gives interpretable results."], ["Multi-Agent Inference in Social Networks: A Finite Population Learning Approach", null], ["Bayesian Inference of Multiple Gaussian Graphical Models", "In this article, we propose a Bayesian approach to inference on multiple Gaussian graphical models. Specifically, we address the problem of inferring multiple undirected networks in situations where some of the networks may be unrelated, while others share common features. We link the estimation of the graph structures via a Markov random field (MRF) prior, which encourages common edges. We learn which sample groups have a shared graph structure by placing a spike-and-slab prior on the parameters that measure network relatedness. This approach allows us to share information between sample groups, when appropriate, as well as to obtain a measure of relative network similarity across groups. Our modeling framework incorporates relevant prior knowledge through an edge-specific informative prior and can encourage similarity to an established network. Through simulations, we demonstrate the utility of our method in summarizing relative network similarity and compare its performance against related methods. We find improved accuracy of network estimation, particularly when the sample sizes within each subgroup are moderate. We also illustrate the application of our model to infer protein networks for various cancer subtypes and under different experimental conditions."], ["Homogeneity Pursuit", "This article explores the homogeneity of coefficients in high-dimensional regression, which extends the sparsity concept and is more general and suitable for many applications. Homogeneity arises when regression coefficients corresponding to neighboring geographical regions or a similar cluster of covariates are expected to be approximately the same. Sparsity corresponds to a special case of homogeneity with a large cluster of known atom zero. In this article, we propose a new method called clustering algorithm in regression via data-driven segmentation (CARDS) to explore homogeneity. New mathematics are provided on the gain that can be achieved by exploring homogeneity. Statistical properties of two versions of CARDS are analyzed. In particular, the asymptotic normality of our proposed CARDS estimator is established, which reveals better estimation accuracy for homogeneous parameters than that without homogeneity exploration. When our methods are combined with sparsity exploration, further efficiency can be achieved beyond the exploration of sparsity alone. This provides additional insights into the power of exploring low-dimensional structures in high-dimensional regression: homogeneity and sparsity. Our results also shed lights on the properties of the fused Lasso. The newly developed method is further illustrated by simulation studies and applications to real data. Supplementary materials for this article are available online."], ["A Unifying Model for Capture\u2013Recapture and Distance Sampling Surveys of Wildlife Populations", "A fundamental problem in wildlife ecology and management is estimation of population size or density. The two dominant methods in this area are capture\u2013recapture (CR) and distance sampling (DS), each with its own largely separate literature. We develop a class of models that synthesizes them. It accommodates a spectrum of models ranging from nonspatial CR models (with no information on animal locations) through to DS and mark-recapture distance sampling (MRDS) models, in which animal locations are observed without error. Between these lie spatially explicit capture\u2013recapture (SECR) models that include only capture locations, and a variety of models with less location data than are typical of DS surveys but more than are normally used on SECR surveys. In addition to unifying CR and DS models, the class provides a means of improving inference from SECR models by adding supplementary location data, and a means of incorporating measurement error into DS and MRDS models. We illustrate their utility by comparing inference on acoustic surveys of gibbons and frogs using only capture locations, using estimated angles (gibbons) and combinations of received signal strength and time-of-arrival data (frogs), and on a visual MRDS survey of whales, comparing estimates with exact and estimated distances. Supplementary materials for this article are available online."], ["Bahadur Efficiency of Sensitivity Analyses in Observational Studies", "An observational study draws inferences about treatment effects when treatments are not randomly assigned, as they would be in a randomized experiment. The naive analysis of an observational study assumes that adjustments for measured covariates suffice to remove bias from nonrandom treatment assignment. A sensitivity analysis in an observational study determines the magnitude of bias from nonrandom treatment assignment that would need to be present to alter the qualitative conclusions of the naive analysis, say leading to the acceptance of a null hypothesis rejected in the naive analysis. Observational studies vary greatly in their sensitivity to unmeasured biases, but a poor choice of test statistic can lead to an exaggerated report of sensitivity to bias. The Bahadur efficiency of a sensitivity analysis is introduced, calculated, and connected to established concepts, such as the power of a sensitivity analysis and the design sensitivity. The Bahadur slope equals zero when the sensitivity parameter equals the design sensitivity, but the Bahadur slope permits more refined distinctions. Specifically, the Bahadur relative efficiency can also compare the relative performance of two test statistics at a value of the sensitivity parameter below the minimum of their design sensitivities. Adaptive procedures that combine several tests can achieve the best design sensitivity and the best Bahadur slope of their component tests. Ultimately, in sufficiently large sample sizes, design sensitivity is more important than efficiency for the power of a sensitivity analysis, and the exponential rate at which rate design sensitivity overtakes efficiency is characterized."], [null, null], ["Model-Robust Designs for Quantile Regression", "We give methods for the construction of designs for regression models, when the purpose of the investigation is the estimation of the conditional quantile function, and the estimation method is quantile regression. The designs are robust against misspecified response functions, and against unanticipated heteroscedasticity. The methods are illustrated by example, and in a case study in which they are applied to growth charts."], ["Quantile Correlations and Quantile Autoregressive Modeling", "In this article, we propose two important measures, quantile correlation (QCOR) and quantile partial correlation (QPCOR). We then apply them to quantile autoregressive (QAR) models, and introduce two valuable quantities, the quantile autocorrelation function (QACF) and the quantile partial autocorrelation function (QPACF). This allows us to extend the Box\u2013Jenkins three-stage procedure (model identification, model parameter estimation, and model diagnostic checking) from classical autoregressive models to quantile autoregressive models. Specifically, the QPACF of an observed time series can be employed to identify the autoregressive order, while the QACF of residuals obtained from the fitted model can be used to assess the model adequacy. We not only demonstrate the asymptotic properties of QCOR and QPCOR, but also show the large sample results of QACF, QPACF, and the quantile version of the Box\u2013Pierce test. Moreover, we obtain the bootstrap approximations to the distributions of parameter estimators and proposed measures. Simulation studies indicate that the proposed methods perform well in finite samples, and an empirical example is presented to illustrate usefulness. Supplementary materials for this article are available online."], ["Tuning Parameter Selection for the Adaptive Lasso Using ERIC", null], ["Regularization Methods for High-Dimensional Instrumental Variables Regression With an Application to Genetical Genomics", null], ["SPReM: Sparse Projection Regression Model For High-Dimensional Linear Regression", null], ["Inference for Subgroup Analysis With a Structured Logistic-Normal Mixture Model", "In this article, we propose a statistical model for the purpose of identifying a subgroup that has an enhanced treatment effect as well as the variables that are predictive of the subgroup membership. The need for such subgroup identification arises in clinical trials and in market segmentation analysis. By using a structured logistic-normal mixture model, our proposed framework enables us to perform a confirmatory statistical test for the existence of subgroups, and at the same time, to construct predictive scores for the subgroup membership. The inferential procedure proposed in the article is built on the recent literature on hypothesis testing for Gaussian mixtures, but the structured logistic-normal mixture model enjoys some distinctive properties that are unavailable to the simpler Gaussian mixture models. With the bootstrap approximations, the proposed tests are shown to be powerful and, equally importantly, insensitive to the choice of tuning parameters. As an illustration, we analyze a dataset from the AIDS Clinical Trials Group 320 study and show how the proposed methodology can help detect a potential subgroup of AIDS patients who may react much more favorably to the addition of a protease inhibitor to a conventional regimen than other patients."], ["Semiparametric Relative-Risk Regression for Infectious Disease Transmission Data", null], ["Multivariate Meta-Analysis of Heterogeneous Studies Using Only Summary Statistics: Efficiency and Robustness", null], ["Varying Index Coefficient Models", "It has been a long history of using interactions in regression analysis to investigate alterations in covariate-effects on response variables. In this article, we aim to address two kinds of new challenges arising from the inclusion of such high-order effects in the regression model for complex data. The first kind concerns a situation where interaction effects of individual covariates are weak but those of combined covariates are strong, and the other kind pertains to the presence of nonlinear interactive effects directed by low-effect covariates. We propose a new class of semiparametric models with varying index coefficients, which enables us to model and assess nonlinear interaction effects between grouped covariates on the response variable. As a result, most of the existing semiparametric regression models are special cases of our proposed models. We develop a numerically stable and computationally fast estimation procedure using both profile least squares method and local fitting. We establish both estimation consistency and asymptotic normality for the proposed estimators of index coefficients as well as the oracle property for the nonparametric function estimator. In addition, a generalized likelihood ratio test is provided to test for the existence of interaction effects or the existence of nonlinear interaction effects. Our models and estimation methods are illustrated by simulation studies, and by an analysis of child growth data to evaluate alterations in growth rates incurred by mother\u2019s exposures to endocrine disrupting compounds during pregnancy. Supplementary materials for this article are available online."], ["A Unified Family of Covariate-Adjusted Response-Adaptive Designs Based on Efficiency and Ethics", "Response-adaptive designs have recently attracted more and more attention in the literature because of its advantages in efficiency and medical ethics. To develop personalized medicine, covariate information plays an important role in both design and analysis of clinical trials. A challenge is how to incorporate covariate information in response-adaptive designs while considering issues of both efficiency and medical ethics. To address this problem, we propose a new and unified family of covariate-adjusted response-adaptive (CARA) designs based on two general measurements of efficiency and ethics. Important properties (including asymptotic properties) of the proposed procedures are studied under categorical covariates. This new family of designs not only introduces new desirable CARA designs, but also unifies several important designs in the literature. We demonstrate the proposed procedures through examples, simulations, and a discussion of related earlier work."], ["Size and Shape Analysis of Error-Prone Shape Data", "We consider the problem of comparing sizes and shapes of objects when landmark data are prone to measurement error. We show that naive implementation of ordinary Procrustes analysis that ignores measurement error can compromise inference. To account for measurement error, we propose the conditional score method for matching configurations, which guarantees consistent inference under mild model assumptions. The effects of measurement error on inference from naive Procrustes analysis and the performance of the proposed method are illustrated via simulation and application in three real data examples. Supplementary materials for this article are available online."], ["On the Prediction of Stationary Functional Time Series", "This article addresses the prediction of stationary functional time series. Existing contributions to this problem have largely focused on the special case of first-order functional autoregressive processes because of their technical tractability and the current lack of advanced functional time series methodology. It is shown here how standard multivariate prediction techniques can be used in this context. The connection between functional and multivariate predictions is made precise for the important case of vector and functional autoregressions. The proposed method is easy to implement, making use of existing statistical software packages, and may, therefore, be attractive to a broader, possibly nonacademic, audience. Its practical applicability is enhanced through the introduction of a novel functional final prediction error model selection criterion that allows for an automatic determination of the lag structure and the dimensionality of the model. The usefulness of the proposed methodology is demonstrated in a simulation study and an application to environmental data, namely the prediction of daily pollution curves describing the concentration of particulate matter in ambient air. It is found that the proposed prediction method often significantly outperforms existing methods."], ["Risk Classification With an Adaptive Naive Bayes Kernel Machine Model", "Genetic studies of complex traits have uncovered only a small number of risk markers explaining a small fraction of heritability and adding little improvement to disease risk prediction. Standard single marker methods may lack power in selecting informative markers or estimating effects. Most existing methods also typically do not account for nonlinearity. Identifying markers with weak signals and estimating their joint effects among many noninformative markers remains challenging. One potential approach is to group markers based on biological knowledge such as gene structure. If markers in a group tend to have similar effects, proper usage of the group structure could improve power and efficiency in estimation. We propose a two-stage method relating markers to disease risk by taking advantage of known gene-set structures. Imposing a naive Bayes kernel machine (KM) model, we estimate gene-set specific risk models that relate each gene-set to the outcome in stage I. The KM framework efficiently models potentially nonlinear effects of predictors without requiring explicit specification of functional forms. In stage II, we aggregate information across gene-sets via a regularization procedure. Estimation and computational efficiency is further improved with kernel principal component analysis. Asymptotic results for model estimation and gene-set selection are derived and numerical studies suggest that the proposed procedure could outperform existing procedures for constructing genetic risk models."], ["Bayesian Generalized Additive Models for Location, Scale, and Shape for Zero-Inflated and Overdispersed\u00a0Count\u00a0Data", "Frequent problems in applied research preventing the application of the classical Poisson log-linear model for analyzing count data include overdispersion, an excess of zeros compared to the Poisson distribution, correlated responses, as well as complex predictor structures comprising nonlinear effects of continuous covariates, interactions or spatial effects. We propose a general class of Bayesian generalized additive models for zero-inflated and overdispersed count data within the framework of generalized additive models for location, scale, and shape where semiparametric predictors can be specified for several parameters of a count data distribution. As standard options for applied work we consider the zero-inflated Poisson, the negative binomial and the zero-inflated negative binomial distribution. The additive predictor specifications rely on basis function approximations for the different types of effects in combination with Gaussian smoothness priors. We develop Bayesian inference based on Markov chain Monte Carlo simulation techniques where suitable proposal densities are constructed based on iteratively weighted least squares approximations to the full conditionals. To ensure practicability of the inference, we consider theoretical properties like the involved question whether the joint posterior is proper. The proposed approach is evaluated in simulation studies and applied to count data arising from patent citations and claim frequencies in car insurances. For the comparison of models with respect to the distribution, we consider quantile residuals as an effective graphical device and scoring rules that allow us to quantify the predictive ability of the models. The deviance information criterion is used to select appropriate predictor specifications once a response distribution has been chosen. Supplementary materials for this article are available online."], ["Sufficient Reductions in Regressions With Elliptically Contoured Inverse Predictors", null], ["Decoupling Shrinkage and Selection in Bayesian Linear Models: A Posterior Summary Perspective", "Selecting a subset of variables for linear models remains an active area of research. This article reviews many of the recent contributions to the Bayesian model selection and shrinkage prior literature. A posterior variable selection summary is proposed, which distills a full posterior distribution over regression coefficients into a sequence of sparse linear predictors."], ["Book Reviews", null], ["Letter To the Editor", null], ["Reply", null], ["Reply", null]]}