{"1998": [["Statistics among the Liberal Arts", "The liberal arts are usually understood to be general and flexible modes of reasoning. By this definition, statistics qualifies as a liberal art, and it is important to the health of the discipline that it be recognized as such. The \u201cphilosophical\u201d tradition of the liberal arts that is now dominant has alternated with an \u201coratorical\u201d tradition that also gives insight, as do ideas of \u201cevolutionary psychology.\u201d This paper considers how understanding statistics as a liberal art influences our appreciation of the discipline and especially our teaching of beginners."], ["Hierarchical Bayesian Analysis of Arrest Rates", "A Bayesian hierarchical model provides the basis for calibrating the crimes avoided by incarceration of individuals convicted of drug offenses compared to those convicted of nondrug offenses. Two methods for constructing reference priors for hierarchical models both lead to the same prior in the final model. We use Markov chain Monte Carlo methods to fit the model to data from a random sample of past arrest records of all felons convicted of drug trafficking, drug possession, robbery, or burglary in Los Angeles County in 1986 and 1990. The value of this formal analysis, as opposed to a simpler analysis that does not use the formal machinery of a Bayesian hierarchical model, is to provide interval estimates that account for the uncertainty due to the random effects."], ["Scheduling Periodic Examinations for the Early Detection of Disease: Applications to Breast Cancer", null], ["Using Historical Controls to Adjust for Covariates in Trend Tests for Binary Data", "Historical data often play an important role in helping interpret the results of a current study. This article is motivated primarily by one specific application: the analysis of data from rodent carcinogenicity studies. By proposing a suitable informative prior distribution on the relationship between control outcome data and covariates, we derive modified trend test statistics that incorporate historical control information to adjust for covariate effects. Frequentist and fully Bayesian methods are presented, and novel computational techniques are developed to compute the test statistics. Several attractive theoretical and computational properties of the proposed priors are derived. In addition, a semiautomatic elicitation scheme for the priors is developed. Our approach is used to modify a widely used prevalence test for carcinogenicity studies. The proposed methodology is applied to data from a National Toxicology Program carcinogenicity experiment and is shown to provide helpful insight on the results of the analysis."], ["Approximately Exact Inference for the Common Odds Ratio in Several 2 \u00d7 2 Tables", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Semiparametric Regression for Repeated Outcomes with Nonignorable Nonresponse", "We consider inference about the parameter \u03b2* indexing the conditional mean of a vector of correlated outcomes given a vector of explanatory variables when some of the outcomes are missing in a subsample of the study and the probability of response depends on both observed and unobserved data values; that is, nonresponse is nonignorable. We propose a class of augmented inverse probability of response weighted estimators that are consistent and asymptotically normal (CAN) for estimating \u03b2* when the response probabilities can be parametrically modeled and a CAN estimator exists. The proposed estimators do not require full specification of a parametric likelihood, and their computation does not require numerical integration. Our estimators can be viewed as an extension of generalized estimating equation estimators that allows for nonignorable nonresponse. We show that our class essentially consists of all CAN estimators of \u03b2*. We also show that the asymptotic variance of the optimal estimator in our class attains the semiparametric variance bound for the model. When the model for nonresponse is richly parameterized, joint estimation of the regression parameter \u03b2* and the nonresponse model parameter \u03c4* which encodes the magnitude of nonignorable selection bias, may be difficult or impossible. Therefore we propose regarding the selection bias parameter \u03c4* as known, rather than estimating it from the data. We then perform a sensitivity analysis that examines how inference concerning the regression parameter \u03b2* changes as we vary \u03c4* over a range of plausible values. We apply our approach to the analysis of ACTG Trial 002, an AIDS clinical trial."], ["Variability Assessment in Positron Emission Tomography and Related Generalized Deconvolution Models", "The problem of variance assessment for positron emission tomography (PET) image reconstructions is considered in the context of generalized deconvolution. A refinement of an approximate technique proposed by Carson and colleagues is examined. Computational implications of representing the reconstruction kernel in terms of a weighted sum of Gaussian densities are developed. Bias and variance characteristics of the resulting variance estimators are examined by numerical simulation. For typical regions, the error in estimated standard deviations is found to be on the order of 10%. The use of smoothing to obtain more reliable pointwise variance estimators is described, and some theoretical analysis of this technique is carried out. For the PET application, simulations suggest that the percent improvement in the root mean squared error accuracy of pointwise variance estimators obtained by smoothing can be on the order of 30%. A practical application of the methodology to a PET study is presented."], ["Using Smoothed Receiver Operating Characteristic Curves to Summarize and Compare Diagnostic Systems", "Receiver operating characteristic (ROC) curves are used for summarizing the performance of imperfect diagnostic systems, especially in biomedical research. These curves are also appropriate for summarizing the performance of a discriminant analysis but are under-utilized by statisticians. This article is illustrates the use of these curves for comparing competing diagnostic systems, develops new estimation methods based on kernel density estimation, and studies the statistical performance of the new method. A transform of the ROC curve is further suggested based on the idea of \u201clocal population separation.\u201d This graphic is quite generally useful for displaying the differences between two populations. The methods are applied to a dataset comprising the results of seven diagnostics for predicting cancer activity on 353 patients. The distributions of these diagnostics are not well modeled parametrically, and so either completely nonparametric or kernel density estimation seems appropriate. Construction of the ROC curves shows not only if, but also how the diagnostics differ. The use of bootstrap simulation to check and adjust for the bias of smoothing is also demonstrated using these data."], ["Extended Generalized Estimating Equations for Clustered Data", "Typically, analysis of data consisting of multiple observations on a cluster is complicated by within-cluster correlation. Estimating equations for generalized linear modeling of clustered data have recently received much attention. This article proposes an extension to the generalized estimating equation method proposed by Liang and Zeger, which treats within-cluster correlations as nuisance parameters. Using ideas from extended quasi-likelihood, estimating equations for regression and association parameters are provided simultaneously. The resulting estimators are proven to be asymptotically normal and consistent under certain conditions. The consistency of regression estimators allows incorrect modeling of the correlation among repeated responses. The method is illustrated with an analysis of data from a developmental toxicity study."], ["Quasi-Likelihood Regression with Unknown Link and Variance Functions", null], ["Asymptotic Confidence Regions for Kernel Smoothing of a Varying-Coefficient Model with Longitudinal Data", null], ["Nonparametric Regression Analysis of Longitudinal Data", "Nonparametric methods are developed for estimating the dose effect when a response consists of correlated observations over time measured in a dose\u2013response experiment. The methods can also be applied to data collected from a completely randomized design experiment. Methods are developed for the detection and description of the effects of dose, time, and their interaction. The methods allow for individual variation in the timing and number of observations. A generalization allowing baseline covariates to be incorporated is addressed. These results may be used in an exploratory fashion in the process of building a random-effects model for longitudinal data."], ["Analysis of Two-Way Layout of Count Data Involving Multiple Counts in Each Cell", null], ["Orthogonal Column Latin Hypercubes and Their Application in Computer Experiments", "Latin hypercubes have been frequently used in conducting computer experiments. In this paper, a class of orthogonal Latin hypercubes that preserves orthogonality among columns is proposed. Applying an orthogonal Latin hypercube design to a computer experiment benefits the data analysis in two ways. First, it retains the orthogonality of traditional experimental designs. The estimates of linear effects of all factors are uncorrelated not only with each other, but also with the estimates of all quadratic effects and bilinear interactions. Second, it can facilitate nonparametric fitting procedures, because one can select good space-filling designs within the class of orthogonal Latin hypercubes according to selection criteria."], ["Minimax Robust Designs and Weights for Approximately Specified Regression Models with Heteroscedastic Errors", null], ["An Intrinsic Limiting Procedure for Model Selection and Hypotheses Testing", null], ["Testing Parametric versus Semiparametric Modeling in Generalized Linear Models", null], ["Estimating the Number of Classes in a Finite Population", "We use an extension of the generalized jackknife approach of Gray and Schucany to obtain new nonparametric estimators for the number of classes in a finite population of known size. We also show that generalized jackknife estimators are closely related to certain Horvitz\u2013Thompson estimators, to an estimator of Shlosser, and to estimators based on sample coverage. In particular, the generalized jackknife approach leads to a modification of Shlosser's estimator that does not suffer from the erratic behavior of the original estimator. The performance of both new and previous estimators is investigated by means of an asymptotic variance analysis and a Monte Carlo simulation study."], ["Time-Dependent Spectral Analysis of Nonstationary Time Series", "Modeling of nonstationary stochastic time series has found wide applications in speech processing, biomedical signal processing, seismology, and failure detection. Data from these fields have often been modeled as piecewise stationary processes with abrupt changes, and their time-varying spectral features have been studied with the help of spectrograms. A general class of piecewise locally stationary processes is introduced here that allows both abrupt and smooth changes in the spectral characteristics of the nonstationary time series. It is shown that this class of processes behave as approximately piecewise stationary processes and can be used to model various naturally occuring phenomena. An adaptive segmentation method of estimating the time-dependent spectrum is proposed for this class of processes. The segmentation procedure uses binary trees and windowed spectra to nonparametrically and adaptively partition the data into approximately stationary intervals. Results of simulation studies demonstrate that the method has excellent ability to adapt to the rate at which the spectrum is changing. Applications of the method to speech signals and earthquake data are considered."], ["Sequentially Deciding between Two Experiments for Estimating a Common Success Probability", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["1998 Editorial Collaborators", null], ["Editorial Board Page", "This article has no abstract"], ["Adrian Raftery", "Editor's Report for 1997"], ["Not Asked and Not Answered: Multiple Imputation for Multiple Surveys", "We present a method of analyzing a series of independent cross-sectional surveys in which some questions are not answered in some surveys and some respondents do not answer some of the questions posed. The method is also applicable to a single survey in which different questions are asked or different sampling methods are used in different strata or clusters. Our method involves multiply imputing the missing items and questions by adding to existing methods of imputation designed for single surveys a hierarchical regression model that allows covariates at the individual and survey levels. Information from survey weights is exploited by including in the analysis the variables on which the weights were based, and then reweighting individual responses (observed and imputed) to estimate population quantities. We also develop diagnostics for checking the fit of the imputation model based on comparing imputed data to nonimputed data. We illustrate with the example that motivated this project: a study of pre-election public opinion polls in which not all the questions of interest are asked in all the surveys, so that it is infeasible to impute within each survey separately."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["The Balance of Self-Reported Heterosexual Activity in KAP Surveys and the AIDS Epidemic in Africa", "The AIDS epidemic in Africa depends on heterosexual behavior, about which little is known. National Knowledge, Attitudes and Practices (KAP) surveys are a potentially important data source but are compromised because women report less\u2014often, much less\u2014sexual activity than men. This imbalance is investigated using questions on the type of activity (with regular or casual partners and involving compensation) and on the duration of abstinence (time since last act), as well as other socioeconomic variables. Evidence from a hazard analysis and other information suggest that the imbalance may originate in the failure of respondents to answer accurately rather than the systematic undersampling of particular respondents, such as prostitutes. If so, the imbalance does not reflect an epidemic driven by a small group of high-activity women\u2014the core group model\u2014but rather a more diffuse sexual network. The contrasting implications of these hypotheses for the dynamics of the epidemic have been stressed by theoretical epidemiologists. The imbalance seems to originate with respondents who have particular socioeconomic characteristics. Furthermore, the absolute level of activity inferred from the hazard analysis is substantially higher than what is directly reported with possible importance for the evolution of the epidemic. These findings have implications for the usefulness of KAP surveys in general and how they can be refined in particular."], ["Purposive Program Placement and the Estimation of Family Planning Program Effects in Tanzania", "Most studies evaluating the impact of family planning on fertility treat the presence of family planning clinics as being \u201crandomly\u201d assigned among the areas included in the study. They tend to ignore the possibility that the distribution of services may be related to the fertility level observed in a particular area. In some cases the distribution of services may respond to a conscious effort by public authorities or funding agencies to target areas with observed higher fertility. Even in absence of program planning, the factors determining service placement might be related to the determinants of high, or low, fertility in a particular area. If that is the case, and one fails to account for the endogeneity of family planning services, then the estimated impact of family planning programs will be biased. This article presents a modeling approach to address this issue. The model extends the simultaneous equation framework by integrating an individual-level model of timing and spacing of children with the dynamic process of program placement. Individual-level data from the 1991/1992 Tanzania Demographic and Health Survey are augmented with data on the timing of, and factors influencing, family planning service placement to demonstrate the approach. The empirical results show that standard methods yield misleading results on the impact of different components of the family planning program on fertility. In particular, the effect of access to family planning hospitals on births is overstated, and the impact of access to health centers that offer family planning is understated significantly. We quantify the size of these effects through simulations."], ["Parametric and Nonparametric Approaches to Price and Tax Reform", "In many public policy problems, we need to estimate the way in which policy changes affect people's behavior. In the analysis of tax and subsidy reform\u2014the topic of this article\u2014we need to know how tax-induced price changes affect the amounts that people buy of the taxed goods. We present various economic and statistical approaches to obtaining the required estimates. We consider the standard structural methods in economics, where the behavior and welfare of individual agents are captured simultaneously by the specification of utility functions whose parameters are to be estimated. We argue that these methods are less useful than alternatives that directly consider the derivatives of the regression function of average behavior. We consider both parametric and nonparametric estimators of these derivatives in the context of price reform for foods in Pakistan, focussing on the advantages and disadvantages of \u201caverage derivative estimation\u201d (ADE). ADE is attractive in principle, because it directly estimates the statistics required for policy analysis. In the practical case considered here, neither technique is a clear winner; each has strengths and weaknesses."], ["Bayesian Identification of Outliers in Computerized Adaptive Tests", "We consider the problem of identifying examinees with aberrant response patterns in a computerized adaptive test (CAT). The vector Y of responses of an examinee from a CAT is a multivariate response vector. Multivariate observations may be outlying in many different directions, and we characterize specific directions as corresponding to outliers with different interpretations. We develop a class of outlier statistics to identify different types of outliers based on a control chart\u2013type methodology. The outlier methodology is adaptable to general longitudinal discretes data structures. We consider several procedures to judge how extreme a particular outlier is. Data from a nationally administered CAT examination motivates our development and is used to illustrate the results."], ["A Model for Evaluating Sensitivity and Specificity for Correlated Diagnostic Tests in Efficacy Studies with an Imperfect Reference Test", null], ["Analysis of Survival Data from a Randomized Trial with All-or-None Compliance: Estimating the Cost-Effectiveness of a Cancer Screening Program", "Many randomized cancer screening trials involve all-or-none compliance. Some subjects randomized to an offer of screening refuse screening, and some subjects randomized to no offer of screening obtain screening outside the trial. The primary analysis to test whether or not cancer screening reduces cancer mortality is by intent-to-treat. To estimate the cost-effectiveness of screening, it is necessary to adjust for all-or-none compliance. Heretofore, adjustments for all-or-none compliance have been limited to a fixed-time endpoint. Estimating cost-effectiveness as dollars per life year saved requires an extension to the analysis of yearly survival data. In general, this involves modeling both the hazard for death from cancer and death from competing risk. Unconstrained estimates and variances can be written in closed-form notation. For the four yearly breast cancer screens with physical examination and mammography in the Health Insurance Plan of Greater New York study, the estimated cost-effectiveness for a $100 mammogram and $900 biopsy is $16,000 per life year saved with 95% confidence interval ($10,000, $45,000). In contrast, under an inappropriate intent-to-treat analysis, the estimated cost-effectiveness is $23,000 with 95% confidence interval ($14,000, $66,000)."], ["Bayesian CART Model Search", "In this article we put forward a Bayesian approach for finding classification and regression tree (CART) models. The two basic components of this approach consist of prior specification and stochastic search. The basic idea is to have the prior induce a posterior distribution that will guide the stochastic search toward more promising CART models. As the search proceeds, such models can then be selected with a variety of criteria, such as posterior probability, marginal likelihood, residual sum of squares or misclassification rates. Examples are used to illustrate the potential superiority of this approach over alternative methods."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Smoothing Spline Models for the Analysis of Nested and Crossed Samples of Curves", "We introduce a class of models for an additive decomposition of groups of curves stratified by crossed and nested factors, generalizing smoothing splines to such samples by associating them with a corresponding mixed-effects model. The models are also useful for imputation of missing data and exploratory analysis of variance. We prove that the best linear unbiased predictors (BLUPs) from the extended mixed-effects model correspond to solutions of a generalized penalized regression where smoothing parameters are directly related to variance components, and we show that these solutions are natural cubic splines. The model parameters are estimated using a highly efficient implementation of the EM algorithm for restricted maximum likelihood (REML) estimation based on a preliminary eigenvector decomposition. Variability of computed estimates can be assessed with asymptotic techniques or with a novel hierarchical bootstrap resampling scheme for nested mixed-effects models. Our methods are applied to menstrual cycle data from studies of reproductive function that measure daily urinary progesterone; the sample of progesterone curves is stratified by cycles nested within subjects nested within conceptive and nonconceptive groups."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Correlation and High-Dimensional Consistency in Pattern Recognition", "Classical discriminant analysis breaks down when the feature vectors are of extremely high dimension; for example, when the basic observation is a random function observed over a fine grid. Alternative methods have been developed assuming a simplified form for the covariance structure. We analyze the high-dimensional asymptotics of some of these methods, emphasizing the effects of correlations such as occur when the baseline is random. For instance, the Euclidean distance classifier, which has been proposed for generic use in high-dimensional classification problems, is dimensionally inconsistent under a simple repeated measurement model. We provide exponential bounds for the error rates of several classifiers. We develop new dimensionally consistent methods to deal with the effects of correlation in high-dimensional problems."], ["Test of Significance When Data are Curves", "With modern technology, massive data can easily be collected in a form of multiple sets of curves. New statistical challenge includes testing whether there is any statistically significant difference among these sets of curves. In this article we propose some new tests for comparing two groups of curves based on the adaptive Neyman test and the wavelet thresholding techniques introduced earlier by Fan. We demonstrate that these tests inherit the properties outlined by Fan and that they are simple and powerful for detecting differences between two sets of curves. We then further generalize the idea to compare multiple sets of curves, resulting in an adaptive high-dimensional analysis of variance, called HANOVA. These newly developed techniques are illustrated by using a dataset on pizza commercials where observations are curves and an analysis of cornea topography in ophthalmology where images of individuals are observed. A simulation example is also presented to illustrate the power of the adaptive Neyman test."], ["Rejection Control and Sequential Importance Sampling", null], ["Sequential Monte Carlo Methods for Dynamic Systems", "We provide a general framework for using Monte Carlo methods in dynamic systems and discuss its wide applications. Under this framework, several currently available techniques are studied and generalized to accommodate more complex features. All of these methods are partial combinations of three ingredients: importance sampling and resampling, rejection sampling, and Markov chain iterations. We provide guidelines on how they should be used and under what circumstance each method is most suitable. Through the analysis of differences and connections, we consolidate these methods into a generic algorithm by combining desirable features. In addition, we propose a general use of Rao-Blackwellization to improve performance. Examples from econometrics and engineering are presented to demonstrate the importance of Rao\u2013Blackwellization and to compare different Monte Carlo procedures."], ["Adaptive Markov Chain Monte Carlo through Regeneration", "Markov chain Monte Carlo (MCMC) is used for evaluating expectations of functions of interest under a target distribution \u03c0. This is done by calculating averages over the sample path of a Markov chain having \u03c0 as its stationary distribution. For computational efficiency, the Markov chain should be rapidly mixing. This sometimes can be achieved only by careful design of the transition kernel of the chain, on the basis of a detailed preliminary exploratory analysis of \u03c0. An alternative approach might be to allow the transition kernel to adapt whenever new features of \u03c0 are encountered during the MCMC run. However, if such adaptation occurs infinitely often, then the stationary distribution of the chain may be disturbed. We describe a framework, based on the concept of Markov chain regeneration, which allows adaptation to occur infinitely often but does not disturb the stationary distribution of the chain or the consistency of sample path averages."], ["Discretization of Continuous Markov Chains and Markov Chain Monte Carlo Convergence Assessment", "We show that continuous state-space Markov chains can be rigorously discretized into finite Markov chains. The idea is to subsample the continuous chain at renewal times related to small sets that control the discretization. Once a finite Markov chain is derived from the Markov chain Monte Carlo output, general convergence properties on finite state spaces can be exploited for convergence assessment in several directions. Our choice is based on a divergence criterion derived from Kemeny and Snell, which is first evaluated on parallel chains with a stopping time and then implemented, more efficiently, on two parallel chains only, using Birkhoff's pointwise ergodic theorem for stopping rules. The performance of this criterion is illustrated on three standard examples."], ["Projected Multivariate Linear Models for Directional Data", "We introduce the spherically projected multivariate linear model for directional data. This model treats directional observations as projections onto the unit sphere of unobserved responses from a multivariate linear model. Focusing on the important case of circular data, we show that maximum likelihood estimates for the model are readily computed using iterative methods, in sharp contrast with competing approaches. Examples are given to demonstrate the resulting methodology in realistic applications."], ["Large Cluster Results for Two Parametric Multinomial Extra Variation Models", "Two parametric extra variation models are considered. Approximate closed-form expressions are given for the Fisher information matrices. The expressions are useful in computing maximum likelihood estimates and obtaining large cluster efficiencies. A simulation study shows that the approximations perform very well even in clusters of moderate size. The models are applied in illustrative examples. A goodness-of-fit test is developed that is applicable even when the cluster sizes are unequal. The null distribution of the test statistic is shown to be well approximated by a chi-squared distribution. For the cluster size configurations in the examples, the test also has high power in distinguishing between the two models considered. The goodness-of-fit test shows that the new model provides adequate description of the data from the three experiments designed to study induced mutagenic effect."], ["Assessing the Number of Linear Components in a General Regression Problem", "This article presents a method for testing the dimension of a general regression model without assuming any specific model for the distribution of the regressors. Examples of application and comparisons with existing methods are also studied."], ["A Composite Likelihood Approach to Binary Spatial Data", "Conventional geostatistics addresses the problem of estimation and prediction for continuous observations. But in many practical applications in public health, environmental remediation, or ecological research the most commonly available data are in the form of counts (e.g., number of cases) or indicator variables denoting above or below threshold values. Also, in many situations it is less expensive to obtain an imprecise categorical observation than to obtain precise measurements of the variable of interest (such as a contaminant). This article proposes a computationally simple method for estimation and prediction using binary or indicator data in space. The proposed method is based on pairwise likelihood contributions, and the large-sample properties of the estimators are obtained in a straightforward manner. We illustrate the methodology through application to indicator data related to gypsy moth defoliation in Massachusetts."], ["A Unified Approach to Likelihood Inference on Stochastic Orderings in a Nonparametric Context", null], ["Estimation Efficiency With Omitted Covariates in Generalized Linear Models", "Although omitting covariates associated with the response in generalized linear models can yield seriously biased estimates of the effects of the included covariates, in certain settings there is no bias. This article considers the effect of omitted covariates on the efficiency of the estimated effects of the included covariates in these cases. I consider the case where the omitted covariate is independent of the included covariates, as in studies with randomized treatment assignment, as well as the case where the omitted covariate is correlated with the included but not a confounder. I present expressions to show that omitting covariates associated with the response and independent of the included covariate can result in serious efficiency losses for estimates of the effects of the included covariates. I also present expressions to show that omitting nonconfounding covariates can lead to efficiency gains. These expressions also quantify the magnitude of estimation efficiency with omitted covariates. The findings suggest that to improve the efficiency of estimated covariate effects of interest, analysts of randomized clinical trial data should adjust for covariates that are strongly associated with the outcome, and that analysts of observational data should not adjust for covariates that do not confound the association of interest."], ["Bayesian Multiple Comparisons Using Dirichlet Process Priors", "We consider the problem of multiple comparisons from a Bayesian viewpoint. The family of Dirichlet process priors is applied in the form of baseline prior/likelihood combinations to obtain posterior probabilities for various hypotheses of equality among population means. The baseline prior/likelihood combinations considered here are beta/binomial and normal/inverted gamma with equal variances on treatment means. The prior probabilities of the hypotheses depend directly on the concentration parameter of the Dirichlet process prior. Finding posterior distributions is analytically intractable; we use Gibbs sampling. The posterior probabilities of hypotheses of interest are easily obtained as a by-product in evaluating the marginal posterior distributions of the parameters. The proposed procedure is compared to Duncan's multiple range test and shown to be more powerful under certain alternative hypotheses."], [null, null], ["An Adaptive Concatenated Failure Rate Model for Software Reliability", "This article introduces a software reliability model whose concatenated failure rate function is motivated via considerations that reflect an engineer's knowledge about the stochastic nature of software failures. The model is adaptive (in a sense explained), has two parameters, and has characteristics that generalize those of existing models. A Bayesian approach for estimating the model parameters and for testing hypotheses about reliability growth is proposed. The prior distributions reflect structural considerations, and Markov chain Monte Carlo techniques are used to implement the approach."], ["Marginal Regression Models for Multivariate Failure Time Data", "In this article we propose a general Cox-type regression model to formulate the marginal distributions of multivariate failure time data. This model has a nested structure in that it allows different baseline hazard functions among distinct failure types and imposes a common baseline hazard function on the failure times of the same type. We prove that the maximum \u201cquasi-partial-likelihood\u201d estimator for the vector of regression parameters under the independence working assumption is consistent and asymptotically normal with a covariance matrix for which a consistent estimator is provided. Furthermore, we establish the uniform consistency and joint weak convergence of the Aalen-Breslow type estimators for the cumulative baseline hazard functions, and develop a resampling technique to approximate the joint distribution of these processes, which enables one to make simultaneous inference about the survival functions over the time axis and across failure types. Finally, we assess the small-sample properties of the proposed methods through Monte Carlo simulation, and present an application to a real dental study."], ["Optimal and Efficient Repeated-Measurements Designs for Uncorrelated Observations", null], ["Testing and Modeling Multivariate Threshold Models", "Threshold autoregressive models in which the process is piecewise linear in the threshold space have received much attention in recent years. In this article I use predictive residuals to construct a test statistic for detecting threshold nonlinearity in a vector time series and propose a procedure for building a multivariate threshold model. The thresholds and the model are selected jointly based on the Akaike information criterion. The finite-sample performance of the proposed test is studied by simulation. The modeling procedure is then used to study arbitrage in security markets and results in a threshold cointegration between logarithms of future contracts and spot prices of a security after adjusting for the cost of carrying the contracts. In this particular application, thresholds are determined in part by the transaction costs. I also apply the proposed procedure to U.S. monthly interest rates and two river flow series of Iceland."], ["Theory and Methods", "A self-organizing filter and smoother for the general nonlinear non-Gaussian state-space model is proposed. An expanded state-space model is defined by augmenting the state vector with the unknown parameters of the original state-space model. The state of the augmented state-space model, and hence the state and the parameters of the original state-space model, are estimated simultaneously by either a non-Gaussian filter/smoother or a Monte Carlo filter/smoother. In contrast to maximum likelihood estimation of model parameters in ordinary state-space modeling, for which the recursive filter computation has to be done many times, model parameter estimation in the proposed self-organizing filter/smoother is achieved with only two passes of the recursive filter and smoother operations. Examples such as automatic tuning of dispersion and the shape parameters, adaptation to changes of the amplitude of a signal in seismic data, state estimation for a nonlinear state space model with unknown parameters, and seasonal adjustment with a nonlinear model with changing variance parameters are shown to exemplify the usefulness of the proposed method."], ["A Review of Composite Sampling Methods", "A composite is formed by collecting multiple sample units and combining them in their entirety or in part, to form a new sample. The sample units that make up the composite may retain their integrity or be homogenized through physical processes such as ball milling, sieving, shaking, or centrifuging. One or more subsequent measurements are taken on the composite and the information on the sample units is lost. This counterintuitive loss of information has fueled opposition to composite sampling, while the methodology's adherents find their motivation in its ability to reduce measurement costs for many classes of problems. This article reviews the scientific literature related to the development of composite sampling methods. The literature on compositing exists only as a compendium derived from disparate disciplines in which terms such as compositing, group screening, pooling, and weighing designs are used. The goal of this review is to synthesize this body of literature. The articles reviewed are limited to those that offer original applications or methodologies. A novel application in numerical regularization, is illustrated using data from an environmental investigation into mercury contamination at a waste disposal site in New Mexico. What is unusual about these data is the existence of measurements at both the composite and sample unit levels, thus allowing an opportunity to evaluate estimates based on composite sampling methods."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["A New Strategy for Evaluating the Impact of Epidemiologic Risk Factors for Cancer with Application to Melanoma", "A new stochastic framework is proposed for evaluating the individual and collective impact of cancer risk factors, and is applied to data on the incidence of melanoma. It is demonstrated that the standardized incidence ratio of second primary melanoma can be used to estimate the total coefficient of variation in risk in the population, subject to some simplifying assumptions. The coefficient of variation estimated in this manner thus can be used as a benchmark against which to judge the contributions to this total variance of individual risk factors. A nonparametric estimator of the coefficient of variation attributable to a single risk factor on the basis of data from a case-control study is derived, and its statistical properties are examined using simulations. It is shown that the categorization of a continuous risk factor can attenuate the estimate substantially, and that estimation of the joint contribution of several risk factors will usually require statistical modeling. Applying the methods to the epidemiology of melanoma, the results indicate that the known risk factors for melanoma explain only a relatively small fraction of the population variation in risk, in contrast to conventional views on this topic."], ["Analysis of Censored Survival Data with Intermittently Observed Time-Dependent Binary Covariates", "In survival analyses that include a time-dependent covariate, the standard partial likelihood approach is often complicated by the fact that the necessary observations of the time-dependent covariate are not always available for the subjects in each risk set. In this article, we consider the situation of a binary time-dependent covariate. We specify a longitudinal model for the binary covariate with missing observations and a proportional hazards model for survival time conditional on fixed covariates and the completed time-dependent covariate history. The joint posterior distribution of the missing covariates and the parameters in the covariate and hazard models is simulated using Gibbs sampling. The joint modeling approach allows the missing covariates to be generated using all of the observed covariates as well as the disease outcome and failure time information. Simulation studies show that this approach often leads to lower bias of relative risk estimates associated with the time-dependent covariate and superior coverage of interval estimates as compared to variations of approaches based on imputing missing covariate values using previous values and then performing partial likelihood analysis. Our research is motivated by a study of survival of lung cancer patients after surgery in which current smoking status is a binary time-dependent covariate. We apply the methods to data from this study and explore the advantages of the joint modeling approach. The application demonstrates that the joint modeling approach can have improved efficiency in estimating the coefficients of both the fixed and the time-dependent covariates. Moreover, it can facilitate analyses using more complicated functions of the time-dependent covariate, such as cumulative smoking, that are difficult or inappropriate to use with the comparison methods."], ["Model for the Analysis of Binary Longitudinal Pain Data Subject to Informative Dropout through Remedication", "We address the problem of accounting for informative dropout in the form of rescue medication when comparing pain relievers with respect to longitudinal binary pain-relief outcomes. We present a selection model approach for binary longitudinal data that accommodates informative dropout. The relationship between dropout or remedication and the binary pain-relief response is assumed to be characterized by a random effect. That is, conditional on this random effect, response and dropout are independent. Unlike previous approaches to this problem, which rely on numerical or approximation methods, we obtain a closed-form expression for the marginal log-likelihood of response and dropout by specifying a complementary log-log link function for both components and a conjugate log-gamma random effect distribution. A data analysis supported by simulation results suggest that the model fits reasonably well. Results are compared to those obtained from conventional, but somewhat inappropriate analyses."], ["Estimating Bowhead Whale Population Size and Rate of Increase from the 1993 Census", null], ["On the Fairness of Death-Penalty Jurors: A Comparison of Bayesian Models with Different Levels of Hierarchy and Various Missing-Data Mechanisms", "Jurors who are opposed to the death penalty are excluded from serving on trials in which the defendant may be sentenced to death. The exclusion of such jurors from death-penalty trials raises some concern about the fairness of these juries in deciding the guilt or innocence of a defendant. This article uses data from a survey of behaviors and beliefs of jurors conducted at the conclusion of non\u2013capital offense trials to study this issue. A Bayesian hierarchical model is proposed for the probabilities of voting guilty or not guilty on a particular trial given various death-penalty beliefs. The Gibbs sampler is used to fit the model. The posterior distributions of the log odds of voting guilty for jurors who would be excluded from death-penalty juries versus those who would not be excluded suggest that there may be differences between the decisions of jurors on non\u2013capital offense trials. We believe that they are likely to differ on death-penalty trials as well. The results from fitting the hierarchical model, which includes a jury effect, are compared with a previous analysis by Kadane that took the jurors' responses to be independent. At issue is the comparability of the priors under models with different hierarchical structures. Another issue in the analysis is that some jurors did not respond to the survey concerning their death-penalty beliefs. Because the views of nonrespondents may differ from those of respondents, our analysis includes an investigation of how various assumptions about the missing-data mechanism affect the conclusions."], ["Forecasting the U.S. Unemployment Rate", "This article presents a comparison of forecasting performance for a variety of linear and nonlinear time series models using the U.S. unemployment rate. Our main emphases are on measuring forecasting performance during economic expansions and contractions by exploiting the asymmetric cyclical behavior of unemployment numbers, on building vector models that incorporate initial jobless claims as a leading indicator, and on utilizing additional information provided by the monthly rate for forecasting the quarterly rate. Comparisons are also made with the consensus forecasts from the Survey of Professional Forecasters. In addition, the forecasts of nonlinear models are combined with the consensus forecasts. The results show that significant improvements in forecasting accuracy can be obtained over existing methods."], ["Correcting for Omitted-Variables and Measurement-Error Bias in Regression with an Application to the Effect of Lead on IQ", "Ordinary least squares (OLS) regression estimates are biased, in general, when relevant variables are omitted from the regression equation or when included variables are measured with error. The errors-in-variables bias can be corrected using auxiliary information about unobservable measurement errors. In this article we demonstrate how auxiliary information can also be used to correct for omitted-variables bias. We illustrate our methods with an application to four published studies of the effect on IQ of childhood exposure to lead. Each of the published studies used OLS methods (or equivalent). None of the studies includes a father IQ variable, and none accounts for the biasing effect of measurement error in the right-side variables. For each of the studies we demonstrate that bias-corrected estimates of the effect of lead on IQ are much reduced in size and are not significantly different from 0. Our methods can be used in other applications involving omitted variables or errors of measurement in the included variables."], ["Comment: Problems with Using Auxiliary Information to Correct for Omitted Variables when Estimating the Effect of Lead on IQ", null], ["Comment", null], ["Rejoinder", null], ["Testing Covariance Structure in Multivariate Models: Application to Family Disease Data", "Recent interest in modeling multivariate responses for members of groups has emphasized the need for testing goodness of fit. Here we describe a way to test the covariance structure of a multivariate distribution parameterized by a vector \u03b8. The idea is to extend this distribution, the \u201cnull\u201d distribution, to a more general distribution that depends on \u03b8, an additional scalar \u03b3, and a specific quadratic function of the response vector chosen to capture features of an alternative covariance structure. When \u03b3 = 0, the more general distribution reduces to the null one. Standard likelihood theory yields a score test for \u03b3 = 0; that is, a test of fit of the null distribution. The score statistic is the standardized difference between observed and expected values of the quadratic function, where the expectation is taken with respect to the null distribution, with \u03b8 replaced by its maximum likelihood estimate. Applying the methods to case-control data on familial cancers of the ovary and breast, we illustrate their use with nonrandomly sampled groups, with censored response data, and with complex multivariate distributions. The application shows that this kind of model extension can succeed where more obvious approaches fail."], ["Edge-Preserving Smoothers for Image Processing", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Bayesian Inference on Network Traffic Using Link Count Data", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Nearest-Neighbor Clutter Removal for Estimating Features in Spatial Point Processes", null], ["Auxiliary Variable Methods for Markov Chain Monte Carlo with Applications", null], ["Saddlepoint Approximation for Multivariate Cumulative Distribution Functions and Probability Computations in Sampling Theory and Outlier Testing", "Four multivariate distributions commonly arise in sampling theory: the multinomial, multivariate hypergeometric, Dirichlet, and multivariate P\u00f3lya distributions. Second-order saddlepoint approximations are given for approximating these multivariate cumulative distribution functions (cdf's) in their most general settings. Probabilities of rectangular regions associated with these cdf's are also approximated directly using second-order saddlepoint methods. All the approximations follow from characterizations of the multivariate distributions as conditional distributions. Applications to outlier discordancy tests and slippage tests are discussed."], ["A Fully Automated Bandwidth Selection Method for Fitting Additive Models", null], ["One-Sided Cross-Validation", "A new method of selecting the smoothing parameters of nonparametric regression estimators is introduced. The method, termed one-sided cross-validation (OSCV), has the objectivity of cross-validation and statistical properties comparable to those of a plug-in rule. The new method may be viewed as an application of the prequential model selection method of Dawid. As such, our results identify a situation in which the prequential method is a more efficient model selector than cross-validation. An example, simulations, and theoretical results demonstrate the utility of OSCV when used with local linear and kernel estimators."], ["Some Linear Models for Estimating the Motion of Rigid Bodies with Applications to Geometric Quality Assurance", "A geometrical method to assess the quality of a part is to sample points on the surface of the part and to bring an ideal computer representation of the part as close as possible to the sampled locations. The points are sampled with a coordinate measuring machine (CMM) that gives the coordinates, with respect to the machine's own three-dimensional system of axis, of the sampled locations. Computer-assisted design (CAD) gives the ideal representation of the part. The matching involves estimating a rotation and a translation vector characterizing the rigid body transformation needed to go from the computer model to the sampled locations. The sum of the squared residual distances between the sampled locations and the fitted computer part is a measure of the quality of the part. This article investigates the statistical properties of this method when the surface of the part is made of planar regions. Estimating the translation vector and the rotation matrix is shown to be \u201clocally\u201d equivalent to fitting a linear model to the localization data. Three components of the residual sum of squared distances are identified. They measure within planar region variability, the orientation conformity of the planar regions, and the size discrepancies of the regions. Diagnostics for linear models are adapted to yield statistical tests for specific geometric failures of a part. The developments use first-order asymptotic expansions of the estimators and of the sum of squared residuals for the local linear model, as the errors associated with the sampled locations go to 0."], [null, null], ["Filtered-Variate Prior Distributions for Histogram Smoothing", "We develop prior distributions for histogram inference favoring smooth population frequencies; that is, probability vectors with small differences for neighboring categories. We give a theory of prior-random probability vectors representable as a linear transform, or \u201cfilter,\u201d of a standard random probability vector, or equivalently, a random weighted average of nonrandom smooth probability vectors. Promising methods of prior assessment are given based on elicitation of a list of typically smooth probability vectors, the empirical moments of which can then be matched by the mean vector and variance matrix of a constructed continuous-type filtered-variate prior distribution."], ["Achieving Higher-Order Convergence Rates for Density Estimation with Binned Data", null], ["Smooth Goodness-of-Fit Tests for the Baseline Hazard in Cox's Proportional Hazards Model", null], ["Locally Efficient Estimation with Current Status Data and Time-Dependent Covariates", null], ["Restricted Mean Life with Covariates: Modification and Extension of a Useful Survival Analysis Method", "Karrison has presented a method for covariate-adjusted comparison of two groups with respect to survival when the group effect is expected to not be of proportional hazards form. This article describes a simplified procedure for implementing Karrison's basic approach, along with an extended version designed to achieve robustness against misspecification in the underlying analytical model. A simulation study of these techniques is presented, and the techniques are illustrated with an example taken from Karrison."], ["Semiparametric Stochastic Mixed Models for Longitudinal Data", "We consider inference for a semiparametric stochastic mixed model for longitudinal data. This model uses parametric fixed effects to represent the covariate effects and an arbitrary smooth function to model the time effect and accounts for the within-subject correlation using random effects and a stationary or nonstationary stochastic process. We derive maximum penalized likelihood estimators of the regression coefficients and the nonparametric function. The resulting estimator of the nonparametric function is a smoothing spline. We propose and compare frequentist inference and Bayesian inference on these model components. We use restricted maximum likelihood to estimate the smoothing parameter and the variance components simultaneously. We show that estimation of all model components of interest can proceed by fitting a modified linear mixed model. We illustrate the proposed method by analyzing a hormone dataset and evaluate its performance through simulations."], ["Consistent Estimators in Generalized Linear Mixed Models", "A simple method based on simulated moments is proposed for estimating the fixed-effects and variance components in a generalized linear mixed model (GLMM). It is shown that the method is not only computationally attractive but also leads to consistent estimators. On the other hand, simulation shows that the method can be quite inefficient."], ["Partial Residual Plots in Generalized Linear Models", null], ["Weighted Likelihood Equations with Bootstrap Root Search", "We discuss a method of weighting likelihood equations with the aim of obtaining fully efficient and robust estimators. We discuss the case of continuous probability models using unimodal weighting functions. These weighting functions downweight observations that are inconsistent with the assumed model. At the true model, therefore, the proposed estimating equations behave like the ordinary likelihood equations. We investigate the number of solutions of the estimating equations via a bootstrap root search; the estimators obtained are consistent and asymptotically normal and have desirable robustness properties. An extensive simulation study and real data examples illustrate the operating characteristics of the proposed methodology."], ["The Power of Some Tests for Difference Stationarity under Local Heteroscedastic Integration", "This article considers the power properties of the McCabe and Tremayne (MT) test for the difference stationarity of a time series. The limiting distribution of the MT test is derived under sequences of locally heteroscedastic and locally explosive autoregressive (AR) alternatives. The limiting distribution of Dickey\u2013Fuller (DF) statistics is also considered under a sequence of locally heteroscedastic alternatives. Whereas the MT test possesses asymptotic power against both forms of nonstationary local alternative, the DF tests set up to test against explosive AR alternatives display little or no ability to reject difference stationarity under local heteroscedastic integration."], ["Data-Driven Efficient Estimation of the Spectral Density", "A nonparametric data-driven spectral density estimator is suggested for a class of processes with the exponentially decaying autocovariance function. This particular class is motivated by causal ARMA processes. The estimator is asymptotically efficient; that is, its mean integrated squared error converges with optimal minimax constant and rate as the sample size increases. The article also presents a Monte Carlo study of the estimator for the case of small sample sizes and an illustrative example of its application in the spectral domain analysis of insulin secretion data. The estimator is both simple and reliable, and does not require human supervision; thus it can be recommended to a practitioner with little or even no experience in spectral analysis of time series."], ["A Unified Approach to Identifying Multivariate Time Series Models", "This article proposes a Bayesian procedure for simultaneous identification of the Kronecker indices and model parameters of a multivariate linear system. The model parameters include the starting values and innovations of the system so that the series considered may be co-integrated or non-invertible. The procedure uses some recent developments in stochastic search variable selection in linear regression analysis and Markov chain Monte Carlo methods in statistical computing. It also takes into consideration the row structure of a vector model implied by the Kronecker indices. Comparison with other existing methods is discussed. Simulated and real examples are used to illustrate the proposed procedure."], ["Fully Modified Vector Autoregressive Inference in Partially Nonstationary Models", null], ["Diagnosing Shocks in Time Series", "Efficient means of modeling aberrant behavior in times series are developed. Our methods are based on state-space forms and allow test statistics for various interventions to be computed from a single run of the Kalman filter smoother. The approach encompasses existing detection methodologies. Departures commonly observed in practice, such as outlying values, level shifts, and switches, are readily dealt with. New diagnostic statistics are proposed. Implications for structural models, autoregressive integrated moving average models, and models with explanatory variables are given."], ["A Multivariate MultiSample Quantile Test for Ordered Alternatives", null], ["Balanced Repeated Replication for Stratified Multistage Survey Data under Imputation", null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Estimating the Probability of Events That have Never Occurred: When is Your Vote Decisive?", "Researchers sometimes argue that statisticians have little to contribute when few realizations of the process being estimated are observed. We show that this argument is incorrect even in the extreme situation of estimating the probabilities of events so rare that they have never occurred. We show how statistical forecasting models allow us to use empirical data to improve inferences about the probabilities of these events. Our application is estimating the probability that your vote will be decisive in a U.S. presidential election, a problem that has been studied by political scientists for more than two decades. The exact value of this probability is of only minor interest, but the number has important implications for understanding the optimal allocation of campaign resources, whether states and voter groups receive their fair share of attention from prospective presidents, and how formal \u201crational choice\u201d models of voter behavior might be able to explain why people vote at all. We show how the probability of a decisive vote can be estimated empirically from state-level forecasts of the presidential election and illustrate with the example of 1992. Based on generalizations of standard political science forecasting models, we estimate the (prospective) probability of a single vote being decisive as about 1 in 10 million for close national elections such as 1992, varying by about a factor of 10 among states. Our results support the argument that subjective probabilities of many types are best obtained through empirically based statistical prediction models rather than solely through mathematical reasoning. We discuss the implications of our findings for the types of decision analyses used in public choice studies."], ["Reprojecting Partially Observed Systems with Application to Interest Rate Diffusions", "We introduce reprojection as a general purpose technique for characterizing the dynamic response of a partially observed nonlinear system to its observable history. Reprojection is the third step of a procedure wherein first data are summarized by projection onto a Hermite series representation of the unconstrained transition density for observables; second, system parameters are estimated by minimum chi-squared, where the chi-squared criterion is a quadratic form in the expected score of the projection; and third, the constraints on dynamics implied by the nonlinear system are imposed by projecting a long simulation of the estimated system onto a Hermite series representation of the constrained transition density for observables. The constrained transition density can be used to study the response of the system to its observable history. We utilize the technique to assess the dynamics of several diffusion models for the short-term interest rate that have been proposed and to compare them to a new model that has feedback from the interest rate into both the drift and diffusion coefficients of a volatility equation."], ["A State-Space Model for National Football League Scores", "This article develops a predictive model for National Football League (NFL) game scores using data from the period 1988\u20131993. The parameters of primary interest\u2014measures of team strength\u2014are expected to vary over time. Our model accounts for this source of variability by modeling football outcomes using a state-space model that assumes team strength parameters follow a first-order autoregressive process. Two sources of variation in team strengths are addressed in our model; week-to-week changes in team strength due to injuries and other random factors, and season-to-season changes resulting from changes in personnel and other longer-term factors. Our model also incorporates a home-field advantage while allowing for the possibility that the magnitude of the advantage may vary across teams. The aim of the analysis is to obtain plausible inferences concerning team strengths and other model parameters, and to predict future game outcomes. Iterative simulation is used to obtain samples from the joint posterior distribution of all model parameters. Our model appears to outperform the Las Vegas \u201cbetting line\u201d on a small test set consisting of the last 110 games of the 1993 NFL season."], ["Achieving Uniformity in a Semiconductor Fabrication Process using Spatial Modeling", "Material is deposited onto the wafer surface during several steps of wafer fabrication. This material must be deposited evenly across the entire wafer surface, close to the targeted thickness, and with little wafer-to-wafer variability. But unequal variances across the wafer and under different process conditions, as well as nonstationary correlation across a wafer, make these goals difficult to achieve, because traditional methods for optimizing deposition processes assume homogeneity and independence. We avoid these assumptions and determine the best settings of process variables using physically motivated statistical models for the mean response, unequal variances, and nonstationary spatial correlation structure. Data from a rapid thermal chemical vapor deposition process is used to illustrate the approach. A simulation exercise demonstrates the advantages of fitting flexible variance models and using appropriate performance measures."], ["Panel Data with Survival: Hospitalization of HIV-Positive Patients", "This article provides an analysis of the hospitalization experience of a panel of HIV-positive patients. It is part of a program of work designed to study the medical expenditures of such patients and their variation both between people and over time. We model the joint distribution of the inpatient episodes and the survival times of a panel of patients over 15 months. The model induces correlation between hospitalization and death via an unmeasured, person-specific, frailty term, and it allows rates of hospitalization and of death each to be affected by time-invariant and time-varying covariates. We subject the model to a variety of predictive tests and show that it is generally consistent with the data. We study and present estimates of the time variation in the rate of hospitalization. We also report the effects of a large number of covariates on rates of hospitalization and mortality. The model generalizes fairly easily in a number of ways, one of which is to handle vector-valued measures of medical expenditure and of other outcomes, such as the employment record, associated with the illness and evolving through time. Our model thus points toward a general solution to the problem of analyzing panel data in which the outcome variables of interest are correlated with the rate of mortality."], ["Cost-Containment and Adverse Selection in Medicaid HMOs", "This article examines whether substituting a health maintenance organization (HMO) for traditional fee-for-service (FFS) Medicaid insurance reduces the cost of children's health care. The estimation is complicated by the fact that in nonrandomized settings, unobserved selection can bias estimates of HMO performance. To control for selection, researchers often rely on parametric assumptions or instrumental variables estimation to compute selection-free estimates. But the robustness of these approaches has been questioned. We pursue a different approach based on semiparametric maximum likelihood techniques. Monte Carlo and applied economic studies have shown this method to be quite robust in a variety of contexts. We apply this model to data from a self-selected sample of children in either a Medicaid HMO or a traditional FFS in Florida. After controlling for selection, we estimate that the HMO reduced expenditures on children by 9.1%. Conversely, a model assuming no selection predicts no savings from the HMO. We also validate our estimates by comparing our results with those obtained from a randomized sample of HMO and FFS enrollees. These indicate that the HMO reduces expenditures by 13.6%. We conclude that selection can substantially bias estimates of HMO impact and that this technique provides a potentially useful method for accounting for this bias."], ["Health Service Utilization and Insurance Coverage: A Multivariate Probit Analysis", "A computationally practical form of probit analysis for multiple response variables was applied to data on health service utilization as it relates to type of supplemental insurance coverage and other demographic covariates in a sample of 4,658 individuals over age 65 on Medicare. The quantal response variables comprised use of five health services: Medical provider visit, hospital outpatient visit, emergency room visit, hospital inpatient stay, and home health care visit. An individual could use any or all of these services. Supplemental insurance coverage was categorized as Medicaid, employer-related private insurance, and privately purchased insurance, and any combination of these insurance coverages was possible. Covariates included health status, education, urban location, race, sex, marital status, employment status, age, and income. The multivariate probit model estimates association between insurance coverage variables and health service utilization patterns jointly for the five health service variables while controlling for the effects of the covariates and correlation among the health service utilization variables. The estimated correlation among the five health service utilization variables is then taken into account when testing the hypothesis of no insurance coverage effects, providing an advance over traditional approaches such as univariate logistic regression. Data from all five service utilization variables are used to perform a single test of insurance coverage effects taking into account their intercorrelation and effects of covariates\u2014the major contribution of this article. In addition, the multivariate probit model provides probability estimates for specific patterns of service use conditional on type of insurance coverage and demographic factors."], ["Signal Detection in Underwater Sound using Wavelets", "This article considers the use of wavelet methods in relation to a common signal processing problem, that of detecting transient features in sound recordings that contain interference or distortion. In this particular case, the data are various types of underwater sounds, and the objective is to detect intermittent departures (potential \u201csignals\u201d) from the background sound environment in the data (\u201cnoise\u201d), where the latter may itself be evolving and changing over time. We develop an adaptive model of the background interference, using recursive density estimation of the joint distribution of certain summary features of its wavelet decomposition. Observations considered to be outliers from this density estimate at any time are then flagged as potential \u201csignals.\u201d The performance of our method is illustrated on artificial data, where a known \u201csignal\u201d is contaminated with simulated underwater \u201cnoise\u201d using a range of different signal-to-noise ratios, and a \u201cbaseline\u201d comparison is made with results obtained from a relatively unsophisticated, but commonly used, time-frequency approach. A similar comparison is then reported in relation to the more significant problem of detecting various types of dolphin sound in real conditions."], ["Principal Hessian Directions Revisited", "Li has suggested the method of principal Hessian directions for estimating plotting directions that capture curvature in regression functions. This article revisits Li's proposal, offering a number of suggestions for improved application."], ["Comment", null], ["Rejoinder", null], ["Triogram Models", "In this article we introduce the Triogram method for function estimation using piecewise linear, bivariate splines based on an adaptively constructed triangulation. We illustrate the technique for bivariate regression and log-density estimation and indicate how our approach can be applied directly to model bivariate functions in the broader context of an extended linear model. The entire estimation procedure is invariant under affine transformations and is a natural approach for modeling data when the domain of the predictor variables is a polygonal region in the plane. Although our examples deal exclusively with estimating bivariate functions, the use of Triograms for modeling two-factor interactions in analysis of variance decompositions of functions depending on more than two variables is straightforward."], ["On Measuring and Correcting the Effects of Data Mining and Model Selection", "In the theory of linear models, the concept of degrees of freedom plays an important role. This concept is often used for measurement of model complexity, for obtaining an unbiased estimate of the error variance, and for comparison of different models. I have developed a concept of generalized degrees of freedom (GDF) that is applicable to complex modeling procedures. The definition is based on the sum of the sensitivity of each fitted value to perturbation in the corresponding observed value. The concept is nonasymptotic in nature and does not require analytic knowledge of the modeling procedures. The concept of GDF offers a unified framework under which complex and highly irregular modeling procedures can be analyzed in the same way as classical linear models. By using this framework, many difficult problems can be solved easily. For example, one can now measure the number of observations used in a variable selection process. Different modeling procedures, such as a tree-based regression and a projection pursuit regression, can be compared on the basis of their residual sums of squares and the GDF that they cost. I apply the proposed framework to measure the effect of variable selection in linear models, leading to corrections of selection bias in various goodness-of-fit statistics. The theory also has interesting implications for the effect of general model searching by a human modeler."], ["Determining the Dimension in Sliced Inverse Regression and Related Methods", "Sliced inverse regression (SIR) and principal Hessian directions aim to reduce the dimensionality of regression problems. An important step in the method is the determination of a suitable dimension. Although statistical tests based on the nullity eigenvalues are usually suggested, this article focuses on the quality of the estimation of the effective dimension reduction (EDR) spaces. Essentially, the goal is to retain only sufficiently stable subspaces. The goodness of the estimation is measured by the squared trace correlation between the subspaces of the EDR space and their estimates. Asymptotic expansions are derived and estimates deduced. Simulations give an insight on the behavior of the criterion and indicate how it can be used in practice."], ["Bootstrap Approximations in Model Checks for Regression", null], ["Lorelogram: A Regression Approach to Exploring Dependence in Longitudinal Categorical Responses", null], ["Change Curve Estimation via Wavelets", "The recently developed theory of wavelets has a remarkable ability to \u201czoom in\u201d on very short-lived frequency phenomena, such as transients in signals and singularities in functions, and hence provides an ideal tool to study localized changes. This article proposes a wavelet method for estimating jump and sharp cusp curves of a function in the plane. The method involves first computing wavelet transformation of data and then estimating jump and sharp cusp curves by wavelet transformation across fine scales. Asymptotic theory is established, and simulations are carried out to lend some credence to the asymptotic theory. The wavelet estimate is nearly optimal and can be computed by fast algorithms. The method is applied to a real image."], ["Nonlinear Wavelet Shrinkage with Bayes Rules and Bayes Factors", "Wavelet shrinkage, the method proposed by the seminal work of Donoho and Johnstone is a disarmingly simple and efficient way of denoising data. Shrinking wavelet coefficients was proposed from several optimality criteria. In this article a wavelet shrinkage by coherent Bayesian inference in the wavelet domain is proposed. The methods are tested on standard Donoho-Johnstone test functions."], ["Classification Trees for Multiple Binary Responses", "Multiple binary responses arise from many applications for which an array of health related symptoms are of primary interest. These symptoms are usually correlated. I generalize the tree-based methodology for a single response to the case involving multiple binary responses. In particular, I apply the method to analyze building-related occupant complaint syndrome based on a recently collected large database. Using the same database, I compare the performance of several node-splitting criteria for trees."], ["Assessing the Order of Dependence for Partially Exchangeable Binary Data", "The problem we consider is how to assess the order of serial dependence within partially exchangeable binary sequences. We obtain exact conditional tests comparing any two orders by finding the conditional distribution of data given certain transition counts. These tests are facilitated with a new Monte Carlo scheme. Asymptotic tests are also discussed. In particular, we show that the likelihood ratio tests have an asymptotic chi-squared distribution, thus generalizing the results of Billingsley for the particular case of Markov chains. We apply these methods to several datasets, and perform a simulation to study their properties."], ["A Bayesian Approach to Robust Binary Nonparametric Regression", "This article presents a Bayesian approach to binary nonparametric regression that assumes that the argument of the link is an additive function of the explanatory variables and their multiplicative interactions. The article makes the following contributions. First, a comprehensive approach is presented in which the function estimates are smoothing splines with the smoothing parameters integrated out and the estimates are made robust to outliers. Second, the approach can handle a wide range of link functions. Third, efficient state-space-based algorithms are used to carry out the computations. Fourth, an extensive set of simulations is carried out, which show that the Bayesian estimator works well and compares favorably to two estimators that have recently been proposed and used in practice."], ["Local Estimating Equations", "Estimating equations have found wide popularity recently in parametric problems, yielding consistent estimators with asymptotically valid inferences obtained via the sandwich formula. Motivated by a problem in nutritional epidemiology, we use estimating equations to derive nonparametric estimators of a \u201cparameter\u201d depending on a predictor. The nonparametric component is estimated via local polynomials with loess or kernel weighting; asymptotic theory is derived for the latter. In keeping with the estimating equation paradigm, variances of the nonparametric function estimate are estimated using the sandwich method, in an automatic fashion, without the need (typical in the literature) to derive asymptotic formulas and plug-in an estimate of a density function. The same philosophy is used in estimating the bias of the nonparametric function; that is, an empirical method is used without deriving asymptotic theory on a case-by-case basis. The methods are applied to a series of examples. The application to nutrition is called \u201cnonparametric calibration\u201d after the term used for studies in that field. Other applications include local polynomial regression for generalized linear models, robust local regression, and local transformations in a latent variable model. Extensions to partially parametric models are discussed."], ["Local Linear Quantile Regression", null], ["A Coupling-Regeneration Scheme for Diagnosing Convergence in Markov Chain Monte Carlo Algorithms", "Here I propose a convergence diagnostic for Markov chain Monte Carlo (MCMC) algorithms based on couplings of a Markov chain with an auxiliary chain that is periodically restarted from a fixed parameter value. The diagnostic provides a mechanism for estimating the specific constants governing the rate of convergence of geometrically and uniformly ergodic chains, and provides a lower bound on the effective sample size of a MCMC run. It also provides a simple procedure for obtaining what is, with high probability, an independent sample from the stationary distribution."], ["Bias Analysis and SIMEX Approach in Generalized Linear Mixed Measurement Error Models", "We consider generalized linear mixed models (GLMMs) for clustered data when one of the predictors is measured with error. When the measurement error is additive and normally distributed and the error-prone predictor is itself normally distributed, we show that the observed data also follow a GLMM but with a different fixed effects structure from the original model, a different and more complex random effects structure, and restrictions on the parameters. This characterization enables us to compute the biases that result in common GLMMs when one ignores measurement error. For instance, in one common situation the biases in parameter estimates become larger as the number of observations within a cluster increases, both for regression coefficients and for variance components. Parameter estimation is described using the SIMEX method, a relatively new functional method that makes no assumptions about the structure of the unobservable predictors. Simulations and an example illustrate the results."], ["Standard Errors of Prediction in Generalized Linear Mixed Models", "The unconditional mean squared error of prediction (UMSEP) is widely used as a measure of prediction variance for inferences concerning linear combinations of fixed and random effects in the classical normal theory mixed model. But the UMSEP is inappropriate for generalized linear mixed models where the conditional variance of the random effects depends on the data. When the random effects describe variation between independent small domains and domain-specific prediction is of interest, we propose a conditional mean squared error of prediction (CMSEP) as a general measure of prediction variance. The CMSEP is shown to be the sum of the conditional variance and a positive correction that accounts for the sampling variability of parameter estimates. We derive a second-order-correct estimate of the CMSEP that consists of three components: (a) a plug-in estimate of the conditional variance, (b) a plug-in estimate of a Taylor series approximation to the correction term, and (c) a bootstrap estimate of the bias incurred in (a). In the normal case our formulas based on the CMSEP provide a conditional alternative to the unconditional expansions of Fuller and Harter, Kackar and Harville, and Prasad and Rao. In addition, we show that the prediction variance formula obtained by Wolfinger and O'Connell and suggested by Breslow and Clayton is in fact Laplace's approximation to the CMSEP based on the assumption that the variance components are known and ignoring the bias-correction term. Thus this formula has a conditional interpretation in the small-domain setting and should not be interpreted unconditionally. Finally, although use of the CMSEP is motivated using entirely frequentist arguments, our second-order approximation to the CMSEP closely resembles a corresponding expansion for the Bayesian posterior variance."], ["Generalized Linear Models for Small-Area Estimation", "Bayesian methods have been used quite extensively in recent years for solving small-area estimation problems. Particularly effective in this regard has been the hierarchical or empirical Bayes approach, which is especially suitable for a systematic connection of local areas through models. However, the development to date has mainly concentrated on continuous-valued variates. Often the survey data are discrete or categorical, so that hierarchical or empirical Bayes techniques designed for continuous variates are inappropriate. This article considers hierarchical Bayes generalized linear models for a unified analysis of both discrete and continuous data. A general theorem is provided that ensures the propriety of posteriors under diffuse priors. This result is then extended to the case of spatial generalized linear models. The hierarchical Bayes procedure is implemented via Markov chain Monte Carlo integration techniques. Two examples (one featuring spatial correlation structure) are given to illustrate the general method."], ["A Sample Coverage Approach to Multiple-System Estimation with Application to Census Undercount", "The concept of \u201csample coverage\u201d used in animal abundance estimation is modified to evaluate the undercount of a census. Lack of independence between the census and its postenumeration survey leads to correlation bias for the standard estimator of population size. An additional recapture sample (besides the census and postenumeration survey) can be used to estimate the correlation bias due to two types of dependences. This work expresses the correlation bias for the three-sample model as a function of expected sample coverage and measures of dependence between lists. A nonparametric population size estimator that incorporates the correlation bias is proposed. A simulation study investigates the performance of the proposed procedure. Data from a 1988 dress rehearsal study for the 1990 census conducted by the U.S. Bureau of the Census are used to illustrate the proposed three-system estimation procedure and to compare the resulting estimates with those given by Darroch, Fienberg, Glonek, and Junker and Zaslavsky and Wolfgang."], ["Detecting Features in Spatial Point Processes with Clutter via Model-Based Clustering", "We consider the problem of detecting features, such as minefields or seismic faults, in spatial point processes when there is substantial clutter. We use model-based clustering based on a mixture model for the process, in which features are assumed to generate points according to highly linear multivariate normal densities, and the clutter arises according to a spatial Poisson process. Nonlinear features are represented by several densities, giving a piecewise linear representation. Hierarchical model-based clustering provides a first estimate of the features, and this is then refined using the EM algorithm. The number of features is estimated from an approximation to its posterior distribution. The method gives good results for the minefield and seismic fault problems. Software to implement it is available on the World Wide Web."], ["Estimation of a Common Mean and Weighted Means Statistics", "Measurements made by several laboratories may exhibit nonnegligible between-laboratory variability, as well as different within-laboratory variances. Also, the number of measurements made at each laboratory often differ. Questions of fundamental importance in the analysis of such data are how to form a best consensus mean, and what uncertainty to attach to this estimate. An estimation equation approach due to Mandel and Paule is often used at the National Institute of Standards and Technology (NIST), particularly when certifying standard reference materials. Primary goals of this article are to study the theoretical properties of this method, and to compare it with some alternative methods, in particular to the maximum likelihood estimator (MLE). Toward this end, we show that the Mandel-Paule solution can be interpreted as a simplified version of the maximum likelihood method. A class of weighted means statistics is investigated for situations where the number of laboratories is large. This class includes a modified MLE and the Mandel-Paule procedure. Large-sample behavior of the distribution of these estimators is investigated. This study leads to a utilizable estimate of the variance of the Mandel-Paule statistic and to an approximate confidence interval for the common mean. It is shown that the Mandel-Paule estimator of the between-laboratory variance is inconsistent in this setting. The results of numerical comparison of mean squared errors of these estimators for a special distribution of within-laboratory variances are also reported."], ["Confidence Intervals with More Power to Determine the Sign: Two Ends Constrain the Means", null], ["Inference Based on Imputed Failure Times for the Proportional Hazards Model with Interval-Censored Data", "We propose an approach to the proportional hazards model for interval-censored data in which parameter estimates are obtained by solving estimating equations that are the partial likelihood score equations for the full-data proportional hazards model, averaged over all rankings of imputed failure times consistent with the observed censoring intervals. Imputed failure times are generated using a parametric estimate of the baseline distribution; the parameters of the baseline distribution are estimated simultaneously with the proportional hazards regression parameters. Although a parametric form for the baseline distribution must be specified, simulation studies show that the method performs well even when the baseline distribution is misspecified. The estimating equations are solved using Monte Carlo techniques. We present a recursive stochastic approximation scheme that converges to the zero of the estimating equations; the solution has a random error that is asymptotically normally distributed with a variance-covariance matrix that can itself be estimated recursively."], ["Discrimination and Clustering for Multivariate Time Series", "Minimum discrimination information provides a useful generalization of likelihood methodology for classification and clustering of multivariate time series. Discrimination between different classes of multivariate time series that can be characterized by differing covariance or spectral structures is of importance in applications occurring in the analysis of geophysical and medical time series data. For discrimination between such multivariate series, Kullback-Leibler discrimination information and the Chernoff information measure are developed for the multivariate non-Gaussian case. Asymptotic error rates and limiting distributions are given for a generalized spectral disparity measure that includes the foregoing criteria as special cases. Applications to problems of clustering and classifying earthquakes and mining explosions are given."], ["Smoothing Spline Models with Correlated Random Errors", "Spline-smoothing techniques are commonly used to estimate the mean function in a nonparametric regression model. Their performances depend greatly on the choice of smoothing parameters. Many methods of selecting smoothing parameters such as generalized maximum likelihood (GML), generalized cross-validation (GCV), and unbiased risk (UBR), have been developed under the assumption of independent observations. They tend to underestimate smoothing parameters when data are correlated. In this article, I assume that observations are correlated and that the correlation matrix depends on a parsimonious set of parameters. I extend the GML, GCV, and UBR methods to estimate the smoothing parameters and the correlation parameters simultaneously. I also relate a smoothing spline model to three mixed-effects models. These relationships show that the smoothing spline estimates evaluated at design points are best linear unbiased prediction (BLUP) estimates and that the GML estimates of the smoothing parameters and the correlation parameters are restricted maximum likelihood (REML) estimates. They also provide a way to fit a spline model with correlated errors using the SAS procedure proc mixed. Simulations are conducted to evaluate and compare the performance of the GML, GCV, UBR methods and the method proposed by Diggle and Hutchinson. The GML method is recommended, because it is stable and works well in all simulations. It performs better than other methods, especially when the sample size is not large. I illustrate my methods with applications to time series data and to spatial data."], ["Median Unbiased Estimation of Coefficient Variance in a Time-Varying Parameter Model", "This article considers inference about the variance of coefficients in time-varying parameter models with stationary regressors. The Gaussian maximum likelihood estimator (MLE) has a large point mass at 0. We thus develop asymptotically median unbiased estimators and asymptotically valid confidence intervals by inverting quantile functions of regression-based parameter stability test statistics, computed under the constant-parameter null. These estimators have good asymptotic relative efficiencies for small to moderate amounts of parameter variability. We apply these results to an unobserved components model of trend growth in postwar U.S. per capita gross domestic product. The MLE implies that there has been no change in the trend growth rate, whereas the upper range of the median-unbiased point estimates imply that the annual trend growth rate has fallen by 0.9% per annum since the 1950s."], ["On Bayesian Modeling of Fat Tails and Skewness", null], ["Estimating the Prediction Function and the Number of Unseen Species in Sampling with Replacement", null], ["Conditional Likelihood Ratio Test for a Nonnegative Normal Mean Vector", null], ["Permutation Tests Using Estimated Distribution Functions", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Correction", null], ["Addendum", null], ["Editorial Board Page", "This article has no abstract"]]}