{"2001": [["Statistical Meetings", null], ["Empirical Bayes Analysis of a Microarray Experiment", "Microarrays are a novel technology that facilitates the simultaneous measurement of thousands of gene expression levels. A typical microarray experiment can produce millions of data points, raising serious problems of data reduction, and simultaneous inference. We consider one such experiment in which oligonucleotide arrays were employed to assess the genetic effects of ionizing radiation on seven thousand human genes. A simple nonparametric empirical Bayes model is introduced, which is used to guide the efficient reduction of the data to a single summary statistic per gene, and also to make simultaneous inferences concerning which genes were affected by the radiation. Although our focus is on one specific experiment, the proposed methods can be applied quite generally. The empirical Bayes inferences are closely related to the frequentist false discovery rate (FDR) criterion."], ["Analysis of Data From Viral DNA Microchips", "Viral DNA microchips, arrays of viral genes printed over a glass slide, are powerful tools for rapidly characterizing the expression pattern of these genes in an infection. The chips are exposed to a solution of fluorescently labeled cDNAs prepared from either mock or true infected human fibroblast cells and the expression levels of the various genes are recorded with the objective of detecting which viral genes are expressed to a significantly higher degree when exposed to the true infection as compared to the mock infection. The data were initially examined visually via image plots and scatterplots. These reveal that analysis of such data presents many challenges owing to, among other problems, high interchip and intrachip variability with low signal-to-noise ratio, differential intensity scales that have to be adjusted nonlinearly, nonGaussian data, data for a large number of genes with little replication, scratches and dark spots on the chips, dust, outliers, and an inability to quantitate intensities below a detection limit, or above a threshold. The first step of the analysis was to standardize the chips to a single intensity scale using a photograph analogy. Next, the average expression level of each gene was estimated using a highly resistant repeated median estimator to avoid being misled by aberrant values. Finally, a simulation-based approach was used to make a distribution-free assessment of significance."], ["Multivariate Receptor Modeling for Temporally Correlated Data by Using MCMC", "Multivariate receptor modeling aims to estimate pollution source profiles and the amounts of pollution based on a series of ambient concentrations of multiple chemical species over time. Air pollution data often show temporal dependence due to meteorology and/or background sources. Previous approaches to receptor modeling do not incorporate this dependence. We model dependence in the data using a time series approach so that we can incorporate extra sources of variability in parameter estimation and uncertainty estimation. We estimate parameters using the Markov chain Monte Carlo method, which makes simultaneous estimation of parameters and uncertainties possible. The methods are applied to simulated data and 1990 Atlanta air pollution data. The results show promise towards the goal of accounting for the dependence in the data."], ["Characterization of Arsenic Occurrence in Source Waters of U.S. Community Water Systems", "We develop a Bayesian hierarchical model linking arsenic concentrations in the source waters of U.S. community water systems to system characteristics such as source water type and location. This characterization provides a starting point for the assessment of current and required water treatment to meet proposed maximum contaminant level (MCL) standards. After a model validation study based on predictive densities, we use a national census of treatment systems and their associated covariates to predict the national distribution of raw water arsenic concentrations. We then examine the relationship between alternative MCLs and the number of systems requiring treatment modification and identify classes of systems which are most likely to be problematic. The posterior distribution of the model parameters, obtained using Markov chain Monte Carlo (MCMC), quantifies the uncertainty in model predictions. We use this quantification to designate classes of water systems where future sampling would most substantially reduce uncertainties in national estimates of arsenic occurrence."], ["Crossed Random Effect Models for Multiple Outcomes in a Study of Teratogenesis", null], ["Statistical Interpretation of Species Composition", "The relative abundance of different species characterizes the structure of a biological community. We analyze an experiment addressing the relationship between omnivorous feeding linkages and community stability. Our goal is to determine whether communities with different predator compositions respond similarly to environmental disturbance. To evaluate these data, we develop a hierarchical statistical model that combines Aitchison's logistic normal distribution with a conditional multinomial observation distribution. In addition, we present an algebra for compositions that includes addition, scalar multiplication, and a metric for differences in compositions. The algebra aids interpretation of treatment effects, treatment interactions, and covariates. Markov chain Monte Carlo (MCMC) is used for inference in a Bayesian framework. Our experimental results indicate that a high degree of omnivory can help to stabilize community dynamics and prevent radical shifts in community composition. This result is at odds with classical food-web predictions, but agrees with recent theoretical formulations."], ["A Bayesian Model for Detecting Acute Change in Nonlinear Profiles", "We propose a model for longitudinal data with random effects that includes model-based smoothing of measurements over time. This research is motivated by experiments evaluating the hemodynamic effects of various agents in tumor-bearing rats. In one set of experiments, the rats breathed room air, followed by carbogen (a mixture of pure oxygen and carbon dioxide). The experimental responses are longitudinal measurements of oxygen pressure measured in tissue, tumor blood flow, and mean arterial pressure. The nature of the recorded responses does not allow any meaningful parametric form to model these profiles over time. Additionally, response patterns differ widely across individuals. Therefore, we propose a nonparametric regression to model the profile data over time. We propose a dynamic state-space model to smooth the data at the profile level. Using the state parameters, we formally define \u201cchange\u201d in the measured responses. A hierarchical extension allows inference to include a regression on covariates. The proposed approach provides a modeling framework for any longitudinal data, where no parsimonious parametric model is available at the level of the repeated measurements and a hierarchical modeling of some feature of a smooth fit for these profiles data is desired. The proposed MCMC algorithm for inference on the hierarchical extension is appropriate in any hierarchical model in which posterior simulation for the submodels is significantly easier."], ["Moment Estimation With Attrition", "We study the effects of the attrition of firms from longitudinal samples on the estimates of dynamic labor demand models. The reasons for attrition from business-based longitudinal samples are extremely varied and are related to both the economic activity of the business and the methods of acquiring sampling frame information for those businesses. We do an exhaustive study of the available information regarding the attrition of French firms from our analysis sample. We propose flexible attrition models based on a longitudinal generalization of the missing at random assumption. We implement these models with a weighted generalized method of moments estimator that is consistent and efficient (in the class of moment estimators). Our flexible attrition models substantially alter and improve the estimation results for dynamic factor demand models. We attribute the improvement to the ability of our models to handle the very diverse reasons for attrition that our audit uncovered without requiring specific knowledge of which reason applies to a particular exiting firm."], ["Inference for the Complier-Average Causal Effect From Longitudinal Data Subject to Noncompliance and Missing Data, With Application to a Job Training Assessment for the Unemployed", "Longitudinal studies involving human participants are often complicated by subjects who do not comply with their treatment assignment or do not provide complete data. A treatment effect of interest in the presence of noncompliance is the complier-average causal effect (CACE; Imbens and Rubin 1997a), which is the treatment effect for subjects who would comply regardless of the assigned treatment. Imbens and Rubin (1997a,b) proposed maximum likelihood and Bayesian inferential methods for CACE, which make explicit assumptions for causal inference in the presence of noncompliance and are more efficient than standard instrumental variable methods. A model for inference about the CACE based on this approach is developed which allows for the inclusion of baseline covariates and handles missing data in the repeated outcome measures. Our methods are applied to a randomized trial of a job training intervention for unemployed workers. Results suggest that the intervention trial significantly reduced depression for high-risk compliers up to six months postintervention but not for low-risk compliers."], ["Matching With Doses in an Observational Study of a Media Campaign Against Drug Abuse", "Multivariate matching with doses of treatment differs from the treatment-control matching in three ways. First, pairs must not only balance covariates, but also must differ markedly in dose. Second, any two subjects may be paired, so that the matching is nonbipartite, and different algorithms are required. Finally, a propensity score with doses must be used in place of the conventional propensity score. We illustrate multivariate matching with doses using pilot data from a media campaign against drug abuse. The media campaign is intended to change attitudes and intentions related to illegal drugs, and the evaluation compares stated intentions among ostensibly comparable teens who reported markedly different exposures to the media campaign."], ["Should the DEA's STRIDE Data Be Used for Economic Analyses of Markets for Illegal Drugs?", "The United States Drug Enforcement Administration's (DEA's) System to Retrieve Information from Drug Evidence (STRIDE) data contain records of acquisitions of illegal drugs by undercover agents and informants of the DEA and Metropolitan Police of the District of Columbia. These data are widely used in economic analyses of markets for illegal drugs. The STRIDE data are mainly records of acquisitions made to support criminal investigations and are not a random sample of an identifiable population. This article presents evidence that the STRIDE data on cocaine and heroin prices are not representative of market prices for those drugs. Specifically, there are large differences among price estimates obtained from different subsets of STRIDE. It is concluded that STRIDE is not a reliable source of price data for economic and policy analyses that require accurate measures of price levels and variations."], ["Semiparametric Nonlinear Mixed-Effects Models and Their Applications", "Nonlinear mixed effects models (NLMMs) and self-modeling nonlinear regression (SEMOR) models are often used to fit repeated measures data. They use a common function shared by all subjects to model variation within each subject and some fixed and/or random parameters to model variation between subjects. The parametric NLMM may be too restrictive, and the semiparametric SEMOR model ignores correlations within each subject. In this article we propose a class of semiparametric nonlinear mixed effects models (SNMMs) that extend NLMMs, SEMOR models, and many other existing models in a natural way. A SNMM assumes that the mean function depends on some parameters and nonparametric functions. The parameters provide an interpretable data summary. The nonparametric functions provide flexibility to allow the data to decide some unknown or uncertain components, such as the shape of the mean response over time. A second-stage model with fixed and random effects is used to model the parameters. Smoothing splines are used to model the nonparametric functions. Covariate effects on parameters can be built into the second-stage model, and covariate effects on nonparametric functions can be constructed using smoothing spline ANOVA decompositions. Laplace approximations to the marginal likelihood and penalized marginal likelihood are used to estimate all parameters and nonparametric functions. We propose and compare two estimation procedures, and also show how to construct approximate Bayesian confidence intervals for the nonparametric functions based on a Bayesian formulation of SNMMs. We evaluate the proposed estimation and inference procedures through a simulation study. Applications of SNMMs are illustrated with analyses of Canadian temperature data."], ["Nonparametric Estimation With Recurrent Event Data", "The problem of nonparametric estimation for the distribution function governing the time to occurrence of a recurrent event in the presence of censoring is considered. We derive Nelson\u2013Aalen and Kaplan\u2013Meier-type estimators for the distribution function, and establish their respective finite-sample and asymptotic properties. We allow for random observation periods for each subject under study and explicitly account for the informative sum-quota nature of the data accrual scheme. These allowances complicate technical matters considerably and, in particular, invalidate the direct use of martingale methods. Consistency and weak convergence of our estimators are obtained by extending an approach due to Sellke, who considered a single renewal process (i.e., recurrent events on a single subject) observed over an infinite time period. A useful feature of the present analysis is that strong parallels are drawn to the usual \u201csingle-event\u201d setting, providing a natural route toward developing extensions that involve covariates (e.g., weighted log-rank tests, Cox-type regression, and frailty models). Another advantage is that we obtain explicit, closed-form expressions for the asymptotic variances for these estimators. This enables, for instance, the characterization of the efficiency loss that results from employing only the first, possibly right-censored, observation per subject. An interesting feature of these results is the prominent role of the renewal function. Finally, we discuss the case of correlated interoccurrence times, propose an estimator in the case where the within-unit interoccurrence times follow a gamma frailty model, and compare the performance of our estimators to an estimator recently proposed by Wang and Chang."], ["Bayesian Model Selection in Finite Mixtures by Marginal Density Decompositions", null], ["Structural Tests in Additive Regression", "We consider the component analysis problem for a regression model with an additive structure. The problem is to test whether some of the additive components are of polynomial structure (e.g., linear) without specifying the structure of the remaining components. A particular case is the problem of selecting the significant covariates. The method that we present is based on the wavelet transform using the Haar basis, which allows for applications under mild conditions on the design and smoothness of the regression function. The results demonstrate that each component of the model can be tested with the rate corresponding to the case if all of the remaining components were known. The proposed procedure is also computationally straightforward. Simulation results and a real data example about female labor supply demonstrate the test's good performance."], ["Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties", "Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, \u221e), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications."], ["Objective Bayesian Analysis of Spatially Correlated Data", "Spatially varying phenomena are often modeled using Gaussian random fields, specified by their mean function and covariance function. The spatial correlation structure of these models is commonly specified to be of a certain form (e.g., spherical, power exponential, rational quadratic, or Mat\u00e9rn) with a small number of unknown parameters. We consider objective Bayesian analysis of such spatial models, when the mean function of the Gaussian random field is specified as in a linear model. It is thus necessary to determine an objective (or default) prior distribution for the unknown mean and covariance parameters of the random field. We first show that common choices of default prior distributions, such as the constant prior and the independent Jeffreys prior, typically result in improper posterior distributions for this model. Next, the reference prior for the model is developed and is shown to yield a proper posterior distribution. A further attractive property of the reference prior is that it can be used directly for computation of Bayes factors or posterior probabilities of hypotheses to compare different correlation functions, even though the reference prior is improper. An illustration is given using a spatial dataset of topographic elevations."], ["Bayesian Selection of Decomposable Models With Incomplete Data", "This article describes a new approach to Bayesian selection of decomposable models with incomplete data. This approach requires the characterization of new ignorability conditions for the missing-data mechanism and the development of new computational methods. Both issues are considered, and solutions are proposed. Theory and methods are assessed in controlled experiments and in the analysis of one real-life incomplete dataset."], ["A Note on the Efficiency of Sandwich Covariance Matrix Estimation", null], ["Semiparametric Bayesian Analysis of Selection Models", null], ["Marginal Mean Models for Dynamic Regimes", "A dynamic treatment regime is a list of rules for how the level of treatment will be tailored through time to an individual's changing severity. In general, individuals who receive the highest level of treatment are the individuals with the greatest severity and need for treatment. Thus, there is planned selection of the treatment dose. In addition to the planned selection mandated by the treatment rules, staff judgment results in unplanned selection of the treatment level. Given observational longitudinal data or data in which there is unplanned selection of the treatment level, the methodology proposed here allows the estimation of a mean response to a dynamic treatment regime under the assumption of sequential randomization."], ["Optimal Permutation Tests for the Analysis of Group Randomized Trials", "Two facts complicate the comparison of interventions in group randomized trials (GRT's), a family of clinical trials in which each member of a particular group receives the same treatment assignment. First, individual outcomes within each group are often correlated. Second, the number of groups in a GRT is often not sufficient to make asymptotic approximations possible. Therefore, the tests used with methods, such as generalized estimating equations (GEE) and penalized quasi likelihood (PQL), originally developed for longitudinal studies, may not be valid. As an alternative, a class of permutation tests is derived to maximize the power of testing for an intervention effect in a GRT while maintaining a nominal test size. The test uses a statistic that is a weighted sum of residuals, with the weights based on the group sizes and the variability of each individual outcome. Through simulation, we demonstrate the importance of weights to a permutation test's power and compare the power of permutation tests, GEE, and PQL. Last, we apply our methods to an actual GRT to study smoking cessation, discuss the findings based on permutation tests, and compare those findings with traditional methods."], ["Cluster Identification Using Projections", "This article describes a procedure to identify clusters in multivariate data using information obtained from the univariate projections of the sample data onto certain directions. The directions are chosen as those that minimize and maximize the kurtosis coefficient of the projected data. It is shown that, under certain conditions, these directions provide the largest separation for the different clusters. The projected univariate data are used to group the observations according to the values of the gaps or spacings between consecutive-ordered observations. These groupings are then combined over all projection directions. The behavior of the method is tested on several examples, and compared to k-means, MCLUST, and the procedure proposed by Jones and Sibson in 1987. The proposed algorithm is iterative, affine equivariant, flexible, robust to outliers, fast to implement, and seems to work well in practice."], ["Weighted Semiparametric Likelihood Method for Fitting a Proportional Odds Regression Model to Data From the Case-Cohort Design", "The problem of fitting a proportional odds regression model to data from the case-cohort design proposed by Prentice is considered. A weighted semiparametric likelihood method is proposed. Under the proportional odds model, the maximum weighted-semiparametric likelihood estimators of both the regression parameter and the transformation function are shown to be consistent and normally distributed. The applicability of the weighted semiparametric likelihood method to the semiparametric transformation regression models is also discussed. In particular, when the proportional hazards regression model is fitted, estimators proposed by Chen and Lo can be generated by the weighted semiparametric likelihood method under different weighting schemes. A simulation study suggests that the case-cohort design is also useful under the proportional odds regression model and the proposed method performs well with practical finite sample sizes."], ["Bayesian Semiparametric Median Regression Modeling", "Median regression models become an attractive alternative to mean regression models when employing flexible families of distributions for the errors. Classical approaches are typically algorithmic with desirable properties emerging asymptotically. However, nonparametric error models may be most attractive in the case of smaller sample sizes where parametric specifications are difficult to justify. Hence, a Bayesian approach, enabling exact inference given the observed data, may be appealing. In this context there is little Bayesian work. We develop two fully Bayesian modeling approaches, employing mixture models, for the errors in a median regression model. The associated families of error distributions allow for increased variability, skewness, and flexible tail behavior. The first family is semiparametric with extra variability captured nonparametrically through mixing and skewness handled parametrically. The second family, a fully nonparametric one, includes all unimodal densities on the real line with median (and mode) equal to zero. Inconjunction with a parametric regression specification, two semiparametric median regression models arise. After fitting such models by using Gibbs sampling, full posterior inference for general population functionals is possible. The approach can also be applied when censored observations are present, leading to semiparametric censored median regression modeling. We illustrate with two examples, one involving censoring."], ["Consistent Functional Methods for Logistic Regression With Errors in Covariates", "We propose consistent functional methods for logistic regression in which some covariates are not accurately ascertainable. Among existing methods for generalized linear models, the conditional-score approach to normal errors does not guarantee the convergence of its estimators, and the corrected-score method generally is not applicable to the logistic-regression score function. In this article, after constructing a correction-amenable estimation procedure with the true covariates, we formulate parametric- and nonparametric-correction estimation procedures in the presence of additive errors in covariates. The former procedure accommodates the situation with known (but not necessarily normal) error distribution, whereas the latter further relieves this distributional assumption requirement, given that additional replicated mismeasured covariates or instrumental variables are available. Large-sample theory is developed; the proposed estimators are consistent and asymptotically normal. We investigate their asymptotic relative efficiency and, through simulations, examine their finite-sample properties. Application to an acquired immunodeficiency syndrome study is provided to illustrate the proposed methods."], ["One-Level Rotation Design Balanced on Time in Monthly Sample and in Rotation Group", "We introduce the two-way balanced one-level rotation design for which balancing is done on interview time in sample and in rotation group and provide the necessary and sufficient condition for the two-way balancing and an algorithm to construct such design. Using this design, we obtain generalized composite estimators (GCE) and the minimum variance linear unbiased estimator (MVLUE). We then calculate their variance and mean squared error (MSE) when there exist two types of correlations and the rotation group bias. Minimizing the weighted sum of variances (or MSEs), we derive one set of the compromise coefficients for all important estimators and characteristics to retain the consistency in total. We compared the GCE with the compromise coefficients to MVLUE and other GCEs with the previous coefficients. We show that the second-order correlations have very significant effects on the variance and MSE. We also investigate the design efficiencies for some selected two-way balanced designs."], ["Positive Quadrant Dependence and Marginal Modeling in Two-Way Tables With Ordered Margins", null], ["A Bivariate Bayes Method for Improving the Estimates of Mortality Rates With a Twofold Conditional Autoregressive Model", "A bivariate Bayes method is proposed for estimating the mortality rates of a single disease for a given population, using additional information from a second disease. The information on the two diseases is assumed to be from the same population groups or areas. The joint frequencies of deaths for the two diseases for given populations are assumed to have a bivariate Poisson distribution with joint means proportional to the population sizes. The relationship between the mortality rates of the two different diseases if formulated through the twofold conditional autoregressive (CAR) model, where spatial effects as well as indexes of spatial dependence are introduced to capture the structured clusterings among areas. This procedure is compared to a univariate hierarchical Bayes procedure that uses information from one disease only. Comparisons of two procedures are made by the optimal property, a Monte Carlo study, real data, and the Bayes factor. All of the methods that we consider demonstrate a substantial improvement in the bivariate over the univariate procedure. For analyzing male and female lung cancer data from the state of Missouri, Markov chain Monte Carlo methods are used to estimate mortality rates."], ["Nonparametric Regression and Spline Smoothing", null], ["SAS System for Regression", null], ["Probabilistic Networks and Expert Systems", null], ["Introduction to Graphical Modeling", null], ["Statistical Modeling With Quantile Functions", null], ["Statistical Science in the Courtroom", null], ["Statistics of Random Processes I: General Theory, Statistics of Random Processes II: Applications", null], ["Stochastic Calculus and Financial Applications", null], ["Probability Theory and Statistical Inference: Econometric Modeling With Observational Data", null], ["Reliability: Modeling, Prediction and Optimization", null], ["Contrasts and Effect Sizes in Behavioral Research: A Correlational Approach", null], ["Design and Analysis of Cluster Randomization Trials in Health Research", null], ["Stochastic Processes in Epidemiology: HIV/AIDS, Other Infectious Diseases and Computers", null], ["Stochastic Population Models: A Compartmental Perspective", null], ["Limit Theorems of Probability Theory", null], ["Telegraphic Reviews", null], ["Corrections", null], ["Estimation of the Head Sensitivity Function in Scanning Magnetoresistance Microscopy", "We apply Bayesian image analysis techniques to a problem in a newly developed scanned probe technology that uses commercial magnetoresistive (MR) record and playback heads as probes to sense magnetic fields. This technology can be used for magnetic imaging and for evaluating playback and record processes in magnetic recording. In MR microscopy, an MR head is raster scanned while in physical contact with a magnetic sample (e.g., hard disk media, tape, or fine magnetic particles). By plotting the MR resistance as a function of position, a very high resolution (on the order of .1 \u00d7 1.0\u03bcm) magnetic image of the sample is constructed. This case study focuses on characterizing the head sensitivity function (HSF), which depends on the physical dimensions and the magnetic properties of the MR head. These sensitivity functions are of great practical interest because they ultimately relate to the head's performance in a high-density data storage environment. Estimating the HSF requires a deconvolution that has features that prevent the problem from being straightforward: both the HSF and the source being scanned are unknown, and there is a substantial amount of correlated noise in the scanned image. We take a Bayesian approach to model and estimate the HSF, while accounting for noise and other nuisance effects such as thermal drift. Besides yielding a point estimate, which is a fairly difficult task here, this approach also quantifies uncertainty so we can assess whether certain features of the estimated head sensitivity function appear to be genuine."], ["Statistical Correction of a Deterministic Numerical Weather Prediction Model", "The forecasting skill of meteorologists is determined largely by their ability to interpret output from deterministic numerical weather prediction (NWP) models in the light of local conditions. Biases in the deterministic model may arise for many reasons, including the inability to account for physical processes at a scale smaller than the grid used in the numerical solution of the model equations. A statistical method for correction and interpretation of NWP model output is the model output statistics technique (MOS), where regression analysis is used to relate observations and NWP model predictions. We describe a Bayesian hierarchical approach to MOS that was motivated by the need to develop sensible statistical corrections for recently opened stations with a short historical data record. The strength of the Bayesian hierarchical formulation is its ability to combine information from stations with short and long observational records in a sensible way. Markov chain Monte Carlo methods are used for computation, and our approach is illustrated by using daily maximum temperature data from a network of 29 stations in the Sydney area."], ["Statistical Analysis of Ion Channel Data Using Hidden Markov Models With Correlated State-Dependent Noise and Filtering", "A hidden Markov model that describes ion channel data, including correlated, state-dependent noise and filter characteristics, and a Markov chain Monte Carlo algorithm that enables Bayesian inference under this model are presented. The method provides parameter estimates and an estimate of the noiseless signal. It was tested on simulated data and applied to real recordings to estimate the model parameters. Modeling the noise and filter correctly turned out to be crucial for the analyzed data sets. The assumption of white noise is too simple, and negligence of the smoothing effect of the low-pass filter leads to errors in the detection of rapid transitions. The hidden Markov model that we propose treats these effects simultaneously."], ["Parametric and Nonparametric Methods for Understanding the Relationship Between Carcinogen-Induced DNA Adduct Levels in Distal and Proximal Regions of the Colon", "An important problem in studying the etiology of colon cancer is understanding the relationship between DNA adduct levels (broadly, DNA damage) in cells within colonic crypts in distal and proximal parts of the colon, following treatment with a carcinogen and different types of diet. In particular, it is important to understand whether rats who have elevated adduct levels in particular positions in distal region crypts also have elevated levels in the same positions of the crypts in proximal regions, and whether this relationship depends on diet. We cast this problem as estimating the correlation function of two responses as a function of a covariate for studies where both responses are measured on the same experimental units but not the same subsampling units. Parametric and nonparametric methods are developed and applied to a dataset from an ongoing study, leading to potentially important and surprising biological results. Theoretical calculations suggest that the nonparametric method, based on nonparametric regression, should in fact have statistical properties nearly the same as if the functions nonparametrically estimated were known. The methodology used in this article can be applied to other settings when the goal of the study is to model the correlation of two continuous repeated measurement responses as a function of a covariate, whereas the two responses of interest can be measured on the same experimental units but not on the same subsampling units. In our example, the two responses were measured in two different regions of the colon."], ["Using Priors to Improve Multiple Animal Carcinogenicity Tests", "This article reviews and summarizes methods for controlling false positives in animal carcinogenicity studies and promotes an alternative that incorporates historical control information via Bayesian methods. The Bayesian paradigm is used as a procedure generator; however, frequentist multisample, age-stratified exact trend tests are used in the ultimate analysis. Critical values for the exact tests are chosen to maximize total expected power, conditional on tumor totals, by using prior distributions. To control the risk of a false-positive finding for one or more tumor types, the sum of the individual critical levels is constrained to be less than a nominal familywise error rate, such as .05. The resulting tests give more power to tumor types with higher-than-expected tumor totals. We use historical control data from animal carcinogenicity studies obtained from a large pharmaceutical company to train and evaluate the tests. There is greatly enhanced power of the proposed method, with concurrent error rate control, because the targeting procedure gives higher power to affected sites, and the procedure tends to produce critical values that are as small as possible overall (implying higher power), subject to the overall risk level constraint. Randomly sampling from real historical animal populations, we compare operating characteristics of various methods proposed in the literature and requested by regulatory agencies. Commonly used methods can have greatly inflated false-positive rates, particularly with larger studies. In some cases, we find greater power for the proposed method, even compared to methods that do not control false positives."], ["Multiple Test Procedures for Identifying the Maximum Safe Dose", null], ["A Marginal Model for Analyzing Discrete Outcomes From Longitudinal Surveys With Outcomes Subject to Multiple-Cause Nonresponse", "Techniques for analyzing categorical outcomes obtained from longitudinal survey samples, with outcomes subject to multiple-cause nonresponse, are developed within the framework of weighted generalized estimating equations. Development of these techniques was motivated by disability data obtained from the Longitudinal Study of Aging (LSOA), a longitudinal survey sample containing missing follow-up for many elderly participants. We posit a model that combines different multivariate link functions to permit fitting Markov models to an outcome with categories represented by a mixture of ordinal success states and multiple failure states. Extending the missing data approach of Robins, Rotnitzky, and Zhao to longitudinal survey sample settings, we use multiple-logit models to model the probability of multiple reasons for missing success or failure outcomes. Given the assumption that the probability of nonresponse depends only on observed responses and covariates specified in the missing data model, weighted estimating equations that permit the incorporation of both survey and missing data weights are used in estimation of parameters specified in the Markov models. Taylor series and jackknife variance estimators are developed for parameters estimated from these models and are presented within the context of adjusting for survey considerations and multiple-cause nonresponse. The sensitivity of marginal model results to different features of the survey design and missing data considerations are explored. Analyses of the LSOA suggest that participation in physical activity may be an important predictor of transitions in functional limitations among older adults."], ["Downweighting Influential Clusters in Surveys", "Certain clusters may be extremely influential on survey estimates and consequently contribute disproportionately to their variance. We propose a general approach to estimation that downweights highly influential clusters, with the amount of downweighting based on M-estimation applied to the empirical influence of the clusters. The method is motivated by a problem in census coverage estimation, and we illustrate it by using data from the 1990 Post Enumeration Survey (PES). In this context, an objective, prespecified methodology for handling influential observations is essential to avoid having to justify judgmental post hoc adjustment of weights. In 1990, both extreme weights and large errors in the census led to extreme influence. We estimated influence by Taylor linearization of the survey estimator, and we applied M-estimators based on the t distribution and the Huber \u03c8-function. As predicted by theory, the robust procedures greatly reduced the estimated variance of estimated coverage rates, more so than did truncation of weights. On the other hand, the procedure may introduce bias into survey estimates when the distributions of the influence statistics are asymmetric. We consider the properties of the estimators in the presence of asymmetry, and we demonstrate techniques for assessing the bias-variance trade-off, finding that estimated mean squared error is reduced by applying the robust procedure to our dataset. We also suggest PES design improvements to reduce the impact of influential clusters."], ["Balanced Risk Set Matching", null], ["Effect of Insurance on Mortality in an HIV-Positive Population in Care", "As policymakers consider expanding insurance coverage for individuals infected with human immunodeficiency virus (HIV), it is useful to ask if insurance has any affect on health outcomes and, if so, whether its magnitude has changed with recent efficacious but expensive treatments. By using data from a nationally representative cohort of HIV-infected (HIV+) persons receiving regular medical care, we estimate the impact of insurance on mortality in this population. A na\u00efve single-equation model confirms the perverse result found by others in the literature\u2013that insurance increases the probability of death for HIV+ patients. We attribute this finding to a correlation between unobserved health status and insurance status in the mortality equation for two reasons. First, the eligibility rules for Medicaid and Medicare require HIV+ patients to demonstrate a disability, almost always defined as advanced disease, to qualify. Second, if unobserved health status is the cause of the positive correlation, then including measures of HIV+ disease as controls should mitigate the effect. Including measures of immune function (CD4 lymphocyte counts) reduces the effect size by approximately 50%, although it does not change sign. To deal with this correlation, we develop a two-equation parametric model of both insurance and mortality. The effect of insurance on mortality is identified through the judicious use of state policy variables as instruments (variables related to insurance status but not mortality, except through insurance). The results from this model indicate that insurance does have a beneficial effect on outcomes, lowering the probability of 6-month mortality by 71% at baseline and 85% at follow-up. The larger effect at followup can be attributed to the recent introduction of effective therapies for HIV infection, which have magnified the returns to insurance for HIV+ patients (as measured by mortality rates)."], ["Jointly Modeling Longitudinal and Event Time Data With Application to Acquired Immunodeficiency Syndrome", "In many clinical and epidemiologic studies, periodically measured disease markers are used to monitor progression to the onset of disease. Motivated by a study of CD4 counts in men infected with human immunodeficiency virus (HIV) at risk for acquired immunodeficiency syndrome (AIDS), we developed a joint model for analysis of both longitudinal and event time data. We use a longitudinal model for continuous data that incorporates a mean structure dependent on covariates, a random intercept, a stochastic process, and measurement error. A central component of the longitudinal model is an integrated Ornstein\u2013Uhlenbeck stochastic process, which represents a family of covariance structures with a random effects model and Brownian motion as special cases. The regression model for the event time data is a proportional hazards model that includes the longitudinal marker as a time-dependent variable and other covariates. A Markov chain Monte Carlo algorithm was developed for fitting the joint model. The joint modeling approach is evaluated and compared with the approach of separate modeling through simulation studies, and it is applied to CD4 counts and AIDS event time data from a cohort study of HIV-infected men. The joint estimation approach allows the simultaneous estimation of the effect of baseline covariates on the progression of CD4 counts and the effect of the current CD4 count and baseline covariates on the hazard of AIDS. The joint modeling approach also gives a way to incorporate measurement error in CD4 counts into a hazard model."], ["Latent Variable Model for Joint Analysis of Multiple Repeated Measures and Bivariate Event Times", "This article presents a novel approach to analyzing a complex dataset from a prevention trial, where outcomes comprise multiple repeated mental health items and times to initiation of alcohol and tobacco use. The dataset has a nonnegligible portion of missing values and interval or left censored events. The substantive interest of the trial suggests a psychiatric distress latent variable that is reflected in the mental health items and potentially affects initiation of alcohol and tobacco use. We describe the data with a combination of three types of component model: a marginal model for the longitudinal latent process for psychiatric distress given study interventions and covariates; logistic regression models for the repeated mental health items given the latent process; and hazard models for times to initiation of alcohol and tobacco use given the latent process, study interventions, and covariates. To aid in fitting these models simultaneously, we use automatic differentiation to find the first two derivatives of the total log-likelihood function, thus speeding up convergence relative to the regular expectation\u2013maximization algorithm with a direct calculation of valid variance estimates."], ["Estimation of Fetal Growth and Gestation in Bowhead Whales", "We address estimating fetal growth and gestation for bowhead whales of the Bering, Chukchi, and Beaufort seas stock. This population is subject to a subsistence hunt by Eskimo whale hunters, which is monitored via a quota system established by the International Whaling Commission. Quota determination is assisted by biological information, such as fetal growth and gestation, which is the basis of a population dynamics model used to estimate the annual replacement yield of the stock. We developed a Bayesian hierarchical nonlinear model for fetal growth with computation carried out via Markov chain Monte Carlo techniques. Our model allows for unique conception and parturition dates and provides predictive distributions for gestation length and conception dates. These results are used to propose estimates of geographic locations for conception and parturition. A sensitivity analysis indicated caution when specifying some hyperparameters related to growth rate, conception dates, and parturition dates."], ["Regularization of Wavelet Approximations", "In this paper, we introduce nonlinear regularized wavelet estimators for estimating nonparametric regression functions when sampling points are not uniformly spaced. The approach can apply readily to many other statistical contexts. Various new penalty functions are proposed. The hard-thresholding and soft-thresholding estimators of Donoho and Johnstone are specific members of nonlinear regularized wavelet estimators. They correspond to the lower and upper envelopes of a class of the penalized least squares estimators. Necessary conditions for penalty functions are given for regularized estimators to possess thresholding properties. Oracle inequalities and universal thresholding parameters are obtained for a large class of penalty functions. The sampling properties of nonlinear regularized wavelet estimators are established and are shown to be adaptively minimax. To efficiently solve penalized least squares problems, nonlinear regularized Sobolev interpolators (NRSI) are proposed as initial estimators, which are shown to have good sampling properties. The NRSI is further ameliorated by regularized one-step estimators, which are the one-step estimators of the penalized least squares problems using the NRSI as initial estimators. The graduated nonconvexity algorithm is also introduced to handle penalized least squares problems. The newly introduced approaches are illustrated by a few numerical examples."], ["Stochastic Neural Networks With Applications to Nonlinear Time Series", null], ["On a Mixture Autoregressive Conditional Heteroscedastic Model", null], ["Extending Sliced Inverse Regression", "Sliced inverse regression (SIR) and an associated chi-squared test for dimension have been introduced as a method for reducing the dimension of regression problems whose predictor variables are normal. In this article the assumptions on the predictor distribution, under which the chi-squared test was proved to apply, are relaxed, and the result is extended. A general weighted chi-squared test that does not require normal regressors for the dimension of a regression is given. Simulations show that the weighted chi-squared test is more reliable than the chi-squared test when the regressor distribution digresses from normality significantly, and that it compares well with the chi-squared test when the regressors are normal."], ["Locally Ancillary Quasi-Score Models for Errors-in-Covariates", null], ["Monte Carlo Deconvolution of Digital Signals Guided by the Inverse Filter", "Digital deconvolution concerns the restoration of an underlying discrete signal from a blurred noisy observation sequence. The problem can be formulated in a Bayesian framework. As is usual in the Bayesian context, the computation of relevant posterior quantities is the major challenge. Previous work made substantial progress toward making this computation feasible, most notably through the Gibbs sampling approach of Chen and Li and the sequential importance sampling approach of Liu and Chen. Yet, there is room for improvement, because both global Monte Carlo strategies can be very slow to reach the target distribution. We propose two new sampling strategies for efficient restoration of digital signals. The key idea is to use inverse filtering to transform the posterior integration into a problem that is substantially more local than the direct formulation, leading to stable Monte Carlo procedures that are very fast to converge. In the final section, we consider extensions to the case of blind deconvolution, where the filter coefficients are unknown and must be estimated as the signal is being restored."], ["Robust Inference for Generalized Linear Models", "By starting from a natural class of robust estimators for generalized linear models based on the notion of quasi-likelihood, we define robust deviances that can be used for stepwise model selection as in the classical framework. We derive the asymptotic distribution of tests based on robust deviances, and we investigate the stability of their asymptotic level under contamination. The binomial and Poisson models are treated in detail. Two applications to real data and a sensitivity analysis show that the inference obtained by means of the new techniques is more reliable than that obtained by classical estimation and testing procedures."], ["Bayesian Multidimensional Scaling and Choice of Dimension", "Multidimensional scaling is widely used to handle data that consist of similarity or dissimilarity measures between pairs of objects. We deal with two major problems in metric multidimensional scaling\u2013configuration of objects and determination of the dimension of object configuration\u2013within a Bayesian framework. A Markov chain Monte Carlo algorithm is proposed for object configuration, along with a simple Bayesian criterion, called MDSIC, for choosing their dimension. Simulation results are presented, as are real data. Our method provides better results than does classical multidimensional scaling and ALSCAL for object configuration, and MDSIC seems to work well for dimension choice in the examples considered."], ["Semiparametric Regression for Clustered Data Using Generalized Estimating Equations", null], ["Analyzing Recurrent Event Data With Informative Censoring", "Recurrent event data are frequently encountered in longitudinal follow-up studies. In statistical literature, noninformative censoring is typically assumed when statistical methods and theory are developed for analyzing recurrent event data. In many applications, however, the observation of recurrent events could be terminated by informative dropouts or failure events, and it is unrealistic to assume that the censoring mechanism is independent of the recurrent event process. In this article we consider recurrent events of the same type and allow the censoring mechanism to be possibly informative. The occurrence of recurrent events is modeled by a subject-specific nonstationary Poisson process via a latent variable. A multiplicative intensity model is used as the underlying model for nonparametric estimation of the cumulative rate function. The multiplicative intensity model is also extended to a regression model by taking the covariate information into account. Statistical methods and theory are developed for estimation of the cumulative rate function and regression parameters. As a major feature of this article, we treat the distributions of both the censoring and latent variables as nuisance parameters. We avoid modeling and estimating the nuisance parameters by proper procedures. An analysis of the AIDS Link to Intravenous Experiences cohort data is presented to illustrate the proposed methods."], ["Testing That a Dependent Process Is Uncorrelated", "An analysis is presented of a new testing procedure for the null hypothesis that a stochastic process is uncorrelated when the process is possibly dependent. Unlike with existing procedures, the user does not need to choose any arbitrary number to implement the proposed test. The asymptotic null distribution of the proposed test statistic is not standard, but it is tabulated by means of simulations. The test is compared with two alternative test procedures that require selection of user-chosen numbers on the basis of asymptotic local power and finite sample behavior. Although the asymptotic local power of the proposed test is lower than those corresponding to the alternative tests, in a Monte Carlo study I show that in small samples the test typically better controls the type I error and that the loss of power is not substantial."], ["Estimation and Prediction for Stochastic Blockstructures", "A statistical approach to a posteriori blockmodeling for digraphs and valued digraphs is proposed. The probability model assumes that the vertices of the digraph are partitioned into several unobserved (latent) classes and that the probability distribution of the relation between two vertices depends only on the classes to which they belong. A Bayesian estimator based on Gibbs sampling is proposed. The basic model is not identified, because class labels are arbitrary. The resulting identifiability problems are solved by restricting inference to the posterior distributions of invariant functions of the parameters and the vertex class membership. In addition, models are considered where class labels are identified by prior distributions for the class membership of some of the vertices. The model is illustrated by an example from the social networks literature (Kapferer's tailor shop)."], ["Simple Robust Testing of Hypotheses in Nonlinear Models", "We develop test statistics to test hypotheses in nonlinear weighted regression models with serial correlation or conditional heteroscedasticity of unknown form. The novel aspect is that these tests are simple and do not require the use of heteroscedasticity autocorrelationconsistent (HAC) covariance matrix estimators. This new class of tests uses stochastic transformations to eliminate nuisance parameters as a substitute for consistently estimating the nuisance parameters. We derive the limiting null distributions of these new tests in a general nonlinear setting, and show that although the tests have nonstandard distributions, the distributions depend only on the number of restrictions being tested. We perform some simulations on a simple model and apply the new method of testing to an empirical example and illustrate that the size of the new test is less distorted than tests using HAC covariance matrix estimators."], ["Estimation for the Box-Cox Transformation Model Without Assuming Parametric Error Distribution", "Box and Cox proposed a power transformation for the response variable that yields a linear regression model with normal error and constant variance. Inference procedures for the regression coefficients and transformation parameter under this model setting have been studied extensively. In this article we propose a simple semiparametric estimation method for the Box\u2013Cox transformation model with no specific parametric assumption on the distribution of the error term. The resulting estimators are strongly consistent and asymptotically normal. Their covariance matrix can be estimated through a novel resampling method without involving nonparametric function estimates of the underlying unknown density function for the error term. The new proposal is illustrated with a well-known dataset in the literature of transformation, and its efficiency and robustness are closely examined through numerical studies."], ["A Correlated Probit Model for Joint Modeling of Clustered Binary and Continuous Responses", "A difficulty in joint modeling of continuous and discrete response variables is the lack of a natural multivariate distribution. For joint modeling of clustered observations on binary and continuous responses, we study a correlated probit model that has an underlying normal latent variable for the binary responses. Catalano and Ryan have factored the model into a marginal and a conditional component and used generalized estimating equations methodology to estimate the effects. We propose a Monte Carlo expectation\u2013conditional maximization algorithm for finding maximum likelihood estimates of the mixed model itself, extending and accelerating an algorithm for models with binary responses. We demonstrate the methodology with a developmental toxicity study measuring fetal weight and a binary malformation status for several litters of mice. A simulation study suggests that efficiency gains of joint fittings over separate fittings of the response variables occur mainly for small datasets with strong correlations between the responses within cluster."], ["Bayesian Heuristic for Multiperiod Control", null], ["Markov Chain Monte Carlo Methods for Computing Bayes Factors", null], ["Applied Regression Including Computing and Graphics", null], ["Mixed-Effects Models in S and S-Plus", null], ["Matrix Analysis and Applied Linear Algebra, Numerical Linear Algebra, and Applied Numerical Linear Algebra", null], ["Environmental Statistics With S-Plus", null], ["Linear Models in Statistics", null], ["Non-Parametric Statistical Diagnosis: Problems and Methods", null], ["Risk Modeling for Determining Value and Decision Making", null], ["A Primer of Probability Logic", null], ["Telegraphic Reviews", null], ["Local Harmonic Estimation in Musical Sound Signals", "Statistical modeling and analysis have been applied to different music-related fields, including sound synthesis and analysis. Sound can be represented as a real-valued function of time. This function can be sampled at a sufficiently small rate such that the resulting discrete version is a good approximation of the continuous version, thus enabling the study of musical sounds as a discrete time series, an entity for which many statistical techniques are available. Physical modeling suggests that many musical instruments' sounds may be characterized by a deterministic periodic and stochastic signal model. In this article the interest is in separating these two elements of the sound and finding parametric representations with musical meaning. To do so, a local harmonic model that tracks changes in pitch and in the amplitudes of the harmonics is fitted. Deterministic changes in the signal, such as pitch change, suggest that different temporal window sizes should be considered. Ways to choose appropriate window sizes are studied. Among other things, the analysis provides estimates of the harmonic signal and noise signal. Different musical composition applications may be based on these estimates."], [null, null], ["Spatiotemporal Hierarchical Bayesian Modeling Tropical Ocean Surface Winds", "Spatiotemporal processes are ubiquitous in the environmental and physical sciences. This is certainly true of atmospheric and oceanic processes, which typically exhibit many different scales of spatial and temporal variability. The complexity of these processes and the large number of observation/prediction locations preclude the use of traditional covariance-based spatiotemporal statistical methods. Alternatively, we focus on conditionally specified (i.e., hierarchical) spatiotemporal models. These methods offer several advantages over traditional approaches. Primarily, physical and dynamical constraints can be easily incorporated into the conditional formulation, so that the series of relatively simple yet physically realistic conditional models leads to a much more complicated spatiotemporal covariance structure than can be specified directly. Furthermore, by making use of the sparse structure inherent in the hierarchical approach, as well as multiresolution (wavelet) bases, the models can be computed with very large datasets. This modeling approach was necessitated by a scientifically meaningful problem in the geosciences. Satellite-derived wind estimates have high spatial resolution but limited global coverage. In contrast, wind fields provided by the major weather centers provide complete coverage but have low spatial resolution. The goal is to combine these data in a manner that incorporates the space-time dynamics inherent in the surface wind field. This is an essential task to enable meteorological research, because no complete high-resolution surface wind datasets exist over the world oceans. High-resolution datasets of this type are crucial for improving our understanding of global air\u2013sea interactions affecting climate and tropical disturbances, and for driving large-scale ocean circulation models."], ["Bayesian Wavelet Regression on Curves With Application to a Spectroscopic Calibration Problem", "Motivated by calibration problems in near-infrared (NIR) spectroscopy, we consider the linear regression setting in which the many predictor variables arise from sampling an essentially continuous curve at equally spaced points and there may be multiple predictands. We tackle this regression problem by calculating the wavelet transforms of the discretized curves, then applying a Bayesian variable selection method using mixture priors to the multivariate regression of predictands on wavelet coefficients. For prediction purposes, we average over a set of likely models. Applied to a particular problem in NIR spectroscopy, this approach was able to find subsets of the wavelet coefficients with overall better predictive performance than the more usual approaches. In the application, the available predictors are measurements of the NIR reflectance spectrum of biscuit dough pieces at 256 equally spaced wavelengths. The aim is to predict the composition (i.e., the fat, flour, sugar, and water content) of the dough pieces using the spectral variables. Thus we have a multivariate regression of four predictands on 256 predictors with quite high intercorrelation among the predictors. A training set of 39 samples is available to fit this regression. Applying a wavelet transform replaces the 256 measurements on each spectrum with 256 wavelet coefficients that carry the same information. The variable selection method could use subsets of these coefficients that gave good predictions for all four compositional variables on a separate test set of samples. Selecting in the wavelet domain rather than from the original spectral variables is appealing in this application, because a single wavelet coefficient can carry information from a band of wavelengths in the original spectrum. This band can be narrow or wide, depending on the scale of the wavelet selected."], ["Inference for a Random Wavelet Packet Model of Single-Channel Event-Related Potentials", "Event-related potentials (ERPs) are brain electrical potentials associated with sensory and cognitive processing. ERP researchers typically wish to separate a recorded time series into functionally distinct component waveforms, and to estimate the effects of experimental conditions on each component. We present an integrated statistical approach to the decomposition of single-channel ERPs and to inference concerning the component waveforms and the effects of experimental conditions on the amplitude and latency (lag from stimulus presentation) of each component. A wavelet packet model of a single individual's data defines a unique decomposition based on prior time/frequency information and variation among experimental conditions. A particular orthogonal wavelet packet basis is selected using the best basis algorithm with a special cost function that incorporates prior information. Our statistical model allows individual-specific parameters to vary randomly among individuals. Because the number of observations on each individual is several orders of magnitude greater than the number of independent individuals, we fit our mixed model using a two-stage approach. In the first stage, a separate wavelet packet model is fit to each individual's data; in the second stage, the parameter estimates from the first stage are analyzed. We evaluated our method using numerical experiments based on design and analysis concepts that are common in applied statistics, but that are rarely used in evaluation of new statistical methods. We applied our methods to auditory evoked responses of cats recorded before and after lesions of the brain association cortex and at several stimulus rates. Our data analysis revealed a surprising lesion effect on the auditory brainstem response."], ["A Proposed Design and Analysis for Comparing Digital and Analog Mammography", null], ["Screening Based on the Risk of Cancer Calculation From Bayesian Hierarchical Changepoint and Mixture Models of Longitudinal Markers", "The standard approach to early detection of disease with a quantitative marker is to set a population-based fixed reference level for making further individual screening or referral decisions. For many types of disease, including prostate and ovarian cancer, additional information is contained in the subject-specific temporal behavior of the marker, which exhibits a characteristic alteration early in the course of the disease. In this article we derive a Bayesian approach to screening based on calculation of the posterior probability of disease given longitudinal marker levels. The method is motivated by a randomized ovarian cancer screening trial in the United Kingdom comprising 22,000 women screened over 4 years with an additional 5 years of follow-up on average. Levels of the antigen CA125 were recorded annually in the screened arm. CA125 profiles of cases and controls from the U.K. trial are modeled using hierarchical changepoint and mixture models, posterior distributions are calculated using Markov chain Monte Carlo methods, and the model is used to calculate the Bayesian posterior risk of having ovarian cancer given a new subject's single or multiple longitudinal CA125 levels. A screening strategy based on the risk calculation is then evaluated using data from an independent screening trial of 5,550 women performed in Sweden. A longitudinal CA125 screening strategy based on calculation of the risk of ovarian cancer is proposed. Simulations of a prospective trial using a strategy based on the risk calculated from longitudinal CA125 values indicate potentially large increases in sensitivity for a given specificity compared to the standard approach based on a fixed CA125 reference level for all subjects."], ["Marginal Structural Models to Estimate the Joint Causal Effect of Nonrandomized Treatments", null], ["Local Likelihood Analysis of Survival Data With Censored Intermediate Events", "AIDS Clinical Trials Group protocol 193A was a randomized trial designed to compare survival and progression-free survival among patients on different treatment regimens. A complicating feature of the analysis of progression-free survival is that different censoring mechanisms operated on progression and survival, which resulted in more complete information on survival. A simple analysis that uses the minimum of the times to progression and survival and the minimum of the corresponding censoring times may sacrifice the extra information available on survival. To address this problem, we have developed a method that exploits the bivariate nature of these data and thereby uses all of the available information. We obtain smooth, nonparametric estimates of the hazard functions for a terminal event, before and after the occurrence of an intermediate event. These hazards can be used to estimate the distribution of progression-free survival. Our method uses local likelihood estimation, which assumes that the underlying true hazard functions can be approximated locally by polynomials. We use an iterative imputation algorithm to perform the estimation when the intermediate events are right censored."], ["Reappraising Medfly Longevity", "In this article we explore the usefulness of a quantile regression formulation of reanalyzing a large experimental study that monitored age-specific mortality in a sample of roughly 1.2 million Mediterranean fruit flies. The quantile regression approach appears useful in refining several of the conclusions drawn from the original study including the apparent decline in mortality rates at advanced ages, and the gender crossover effect in survival functions for medflies."], ["Evaluation of Traffic Injury Prevention Programs Using Counting Process Approaches", "Traumatic brain injury (TBI) is among the most devastating of injuries leading to death and disability among young people today. The major cause of TBI is motor vehicle crashes. One way to reduce the rates of such crashes and thus TBI is through prevention programs. This article analyzes a study conducted for assessing a 1-day educational traffic injury prevention program for young traffic offenders with speeding violations. The obtained data include information about traffic convictions for speeding violations on a group of 16- to 23-year-old drivers. A common method for analyzing such studies is to use simple two-sample rank tests on summary statistics. But this approach ignores the detailed conviction process information and can assess only the long-term overall effect of the program. In this article, we treat the data as recurrent event data and apply a novel approach based on counting processes to evaluate the program. Our approach makes use of the information ignored by the rank tests and allows the assessment of both short- and long-term effects of the program. The analysis results indicate that the prevention program has an effect for a short period and suggest that a long-term effect could be gained if the program is repeated."], ["Pay Phones, Parking Meters, Vending Machines, and Optimal Bayesian Decisions on Collection Times", "Payphones, parking meters, and vending machines illustrate the modern business practice of substituting machinery for manpower. They do not eliminate manual labor completely, because full coin-boxes still must be replaced and vending machines still must be stocked. Deciding when to replace a coin-box is important, with unequal losses resulting from underestimation and overestimation. This article derives optimal methodology for this problem by incorporating collection history and specifying common prior distributions over average daily fill rate and standard deviation at each box. The approach is implemented and analyzed on collection records from 11,308 pay phones over a large geographical region. When the loss from overestimation is 19 times that from underestimation, our methods outperform the one in current use at least 69.9%% of the time, translating into average potential collection-cost reductions exceeding 21%%."], ["Combining Incomplete Information From Independent Assessment Surveys for Estimating Masonry Deterioration", "Construction materials used in building structures, such as masonry, wood, and reinforced concrete, deteriorate over time because of many factors including poor design, defective materials or manufacture, and poor workmanship. This article is concerned with estimates of masonry deterioration and the effects of covariates on the damage to bricks on the walls of five multiple-story buildings of a residential complex located in the Bronx, New York. In this case study, the damage of primary interest was a \u201cspall,\u201d a physical separation of a portion of the brick face from the body of the brick. Eventually, the face becomes so damaged that portions fall off. The result is an unattractive appearance and a hazard to passersby. In this study, spall damage was assessed by means of three different and independent condition assessment surveys: an expensive, precise, and hence very limited scaffold drop survey and two additional inexpensive, but more detailed photographic and visual surveys. The photographic survey was obtained by photographing the walls of the entire residential complex, and the visual survey was done by individuals walking around the periphery of each building and making a visual assessment of the damage to each wall. In the photographic survey, a large amount of incomplete data was unavoidable because of either poor photo angles or various physical obstructions. A binomial regression model using four categorical explanatory variables or factors was fitted to the observed photographic spall data. Sparseness of the data, the presence of outliers, and overdispersion were major problems encountered in selecting and fitting a suitable model. A small pilot survey, in which the relevant portions of the photographic and visual surveys were matched to 11 drop locations of the scaffold survey, recorded spall counts using each survey method. From this pilot survey, photographic and visual spall data were calibrated to the scaffold drop survey data. It was determined that of the two surveys, only the photographic spall survey was needed to predict scaffold spalls. The estimate of total damage from the photographic survey was then adjusted using the calibration results. Finally, a multiple imputation procedure was used to impute values for the missing data and obtain an estimate (and its standard error) of the true spall rate over the entire residential complex. Sources of uncertainty to factfinders in legal trials are discussed and illustrated through the present case."], ["Are Points in Tennis Independent and Identically Distributed? Evidence From a Dynamic Binary Panel Data Model", "This article tests whether points in tennis are independent and identically distributed (iid). We model the probability of winning a point on service and show that points are neither independent nor identically distributed: winning the previous point has a positive effect on winning the current point, and at \u201cimportant\u201d points it is more difficult for the server to win the point than at less important points. Furthermore, the weaker a player, the stronger are these effects. Deviations from iid are small, however, and hence the iid hypothesis will still provide a good approximation in many cases. The results are based on a large panel of matches played at Wimbledon 1992\u20131995, in total almost 90,000 points. Our panel data model takes into account the binary character of the dependent variable, uses random effects to capture the unobserved part of a player's quality, and includes dynamic explanatory variables."], ["Vote Tampering in a District Justice Election in Beaver County, PA", "This article examines the evidence of vote tampering in a District Justice election in Beaver County, Pennsylvania. An informal exploratory data analysis and a legal history are followed by a formal Bayesian model of the data from the vote count on election night and the recount completed 2 months later. The evidence suggests that persons unknown could have gained access to the boxes containing the paper ballots, and surprising patterns of changes in the counts support the inference that certain boxes were tampered with. Three methods are compared not only with respect to the overall matter of whether tampering occurred, but also with respect to which precincts were likely to have been tampered with, and to what extent. The results are generally consistent across methods. The Bayesian model is validated by using it on the data for a race (for Superior Court) in the same election in which vote tampering is not suspected. The results show that the model gives a predictive distribution of just a few votes uncertainty for the Superior Court race but of around 60 votes in the District Justice race, enough to swing the election. Technically, the computations involve a Markov chain Monte Carlo. Because it is not possible to observe how each individual ballot was counted each time, data augmentation is required to fill in a Markov matrix given both margins. The fact that both margins are given restricts the kinds of proposals that the chain considers."], ["Inference for Density Families Using Functional Principal Component Analysis", null], ["Automatic Statistical Analysis of Bivariate Nonstationary Time Series", "We propose a new method for analyzing bivariate nonstationary time series. The proposed method is a statistical procedure that automatically segments the time series into approximately stationary blocks and selects the span to be used to obtain the smoothed estimates of the time-varying spectra and coherence. It is based on the smooth localized complex exponential (SLEX) transform, which forms a library of orthogonal complex-valued transforms that are simultaneously localized in time and frequency. We show that the smoothed SLEX periodograms are consistent estimators, report simulation results, and apply the method to a two-channel electroencephalogram dataset recorded during an epileptic seizure."], ["A Theory for Dynamic Weighting in Monte Carlo Computation", null], ["Adaptive Regression by Mixing", null], ["Classification Trees With Unbiased Multiway Splits", "Two univariate split methods and one linear combination split method are proposed for the construction of classification trees with multiway splits. Examples are given where the trees are more compact and hence easier to interpret than binary trees. A major strength of the univariate split methods is that they have negligible bias in variable selection, both when the variables differ in the number of splits they offer and when they differ in the number of missing values. This is an advantage because inferences from the tree structures can be adversely affected by selection bias. The new methods are shown to be highly competitive in terms of computational speed and classification accuracy of future observations."], ["Smoothing Spline Estimation for Varying Coefficient Models With Repeatedly Measured Dependent Variables", null], ["Semiparametric Transformation Models for Point Processes", "In this article, we propose a family of semiparametric transformation models for point processes with positive jumps of arbitrary sizes. These models offer great flexibilities in formulating the effects of covariates on the mean function of the point process while leaving the stochastic structure completely unspecified. We develop a class of estimating equations for the baseline mean function and the vector-valued regression parameter based on censored point processes and covariate data. These equations can be solved easily by the standard Newton\u2013Raphson algorithm. The resultant estimator of the regression parameter is consistent and asymptotically normal with a covariance matrix that can be estimated consistently. Furthermore, the estimator of the baseline mean function is uniformly consistent and, upon proper normalization, converges weakly to a zero-mean Gaussian process with an easily estimated covariance function. We demonstrate through extensive simulation studies that the proposed inference procedures are appropriate for practical use. The data on recurrent pulmonary exacerbations from a cystic fibrosis clinical trial are provided for illustration."], ["Empirical Bayes Approach to Improve Wavelet Thresholding for Image Noise Reduction", " This article proposes and motivates the particular and original choice of the conditional model. Instead of introducing this Bayesian framework, we could also apply heuristic image processing techniques to find clustered configurations of large coefficients. This article also explains the benefits of the Bayesian approach compared to these simple techniques. The parameter of the prior model is estimated on an empirical basis using a pseudolikelihood criterion."], ["Goodness-of-Fit Tests for Parametric Regression Models", null], ["Real-Parameter Evolutionary Monte Carlo With Applications to Bayesian Mixture Models", "We propose an evolutionary Monte Carlo algorithm to sample from a target distribution with real-valued parameters. The attractive features of the algorithm include the ability to learn from the samples obtained in previous steps and the ability to improve the mixing of a system by sampling along a temperature ladder. The effectiveness of the algorithm is examined through three multimodal examples and Bayesian neural networks. The numerical results confirm that the real-coded evolutionary algorithm is a promising general approach for simulation and optimization."], ["Density Estimation Under Random Censorship and Order Restrictions", "Do a random censorship and/or order restrictions (e.g., nonnegativity, monotonicity, convexity) affect estimation of a smooth density under mean integrated squared error (MISE)? Under mild assumptions, the known asymptotic results, which are concerned only with rates, answer \u201cno.\u201d This answer, especially for censored data, contradicts practical experience and statistical intuition. So what can be said about constants of MISE convergence? It is shown that asymptotically (a) censorship does affect the constant, and this allows one to find a relationship between sample sizes of directly observed and censored datasets that implies the same precision of estimation, and (b) an order restriction does not affect the constant, and thus no isotonic estimation is needed. Intensive Monte Carlo simulations show that the lessons of the sharp asymptotics are valuable for small sample sizes. Also, the estimator developed is illustrated both on simulated data and a dataset of lifetimes of conveyer blades used at wastewater treatment plants."], ["Data-Driven Rank Tests for Classes of Tail Alternatives", "Tail alternatives describe the occurrence of a nonconstant shift in the two-sample problem with a shift function increasing in the tail. The classes of shift functions can be built up using Legendre polynomials. It is important to choose the number of involved polynomials in the right way. Here this choice is based on the data, using a modification of the Schwarz selection rule. Given the data-driven choice of the model, appropriate rank tests are applied. Simulations show that the new data-driven rank tests work very well. Although other tests for detecting shift alternatives, such as Wilcoxon's test, may break down completely for important classes of tail alternatives, the new tests have high and stable power. The new tests also have higher power than data-driven rank tests for the unconstrained two-sample problem. Theoretical support is obtained by proving consistency of the new tests against very large classes of alternatives, including all common tail alternatives. A simple but accurate approximation of the null distribution makes application of the new tests easy."], ["James-Stein-Type Estimators in Large Samples With Application to the Least Absolute Deviations Estimator", "We explore the extension of James\u2013Stein\u2013type estimators in a direction that enables them to preserve their superiority when the sample size goes to infinity. Instead of shrinking a base estimator toward a fixed point, we shrink it toward a data-dependent point. We provide an analytic expression for the asymptotic risk and bias of James\u2013Stein\u2013type estimators shrunk toward a data-dependent point and prove that they have smaller asymptotic risk than the base estimator. Shrinking an estimator toward a data-dependent point turns out to be equivalent to combining two random variables using the James\u2013Stein rule. We propose a general combination scheme that includes random combination (the James\u2013Stein combination) and the usual nonrandom combination as special cases. As an example, we apply our method to combine the least absolute deviations estimator and the least squares estimator. Our simulation study indicates that the resulting combination estimators have desirable finite-sample properties when errors are drawn from symmetric distributions. Finally, using stock return data, we present some empirical evidence that the combination estimators have the potential to improve out-of-sample prediction in terms of both mean squared error and mean absolute error."], ["Linear Transformation Models for Failure Time Data With Covariate Measurement Error", "In medical studies, patients' biological parameters are often imprecisely measured due to the measuring mechanism or the biological variability. In the presence of covariate measurement error, survival analysis using the Cox model with the observed covariate may yield a biased estimate for the regression parameter. Existing research on this topic has focused on adapting the Cox model to covariates with measurement errors. In this article we generalize linear transformation models to accommodate covariate measurement error. We derive inference procedures for the regression coefficients of examining the covariate effects on survival times under a generalized estimating equation framework. Our method relaxes the normality assumption on the unobserved true covariates and the measurement errors and can be easily adopted to conduct sensitivity analyses when the magnitude of the measurement error variance is unknown. The extra variation owing to the measurement error corrections is accounted for through an asymptotic U statistic expression of the estimator for the measurement error model parameter. We illustrate the numerical performance of our estimator with an example, and investigate it through simulation studies."], ["Bayesian Forecasting for Complex Systems Using Computer Simulators", "Although computer models are often used for forecasting future outcomes of complex systems, the uncertainties in such forecasts are not usually treated formally. We describe a general Bayesian approach for using a computer model or simulator of a complex system to forecast system outcomes. The approach is based on constructing beliefs derived from a combination of expert judgments and experiments on the computer model. These beliefs, which are systematically updated as we make runs of the computer model, are used for either Bayesian or Bayes linear forecasting for the system. Issues of design and diagnostics are described in the context of forecasting. The methodology is applied to forecasting for an active hydrocarbon reservoir."], ["A Two-Part Random-Effects Model for Semicontinuous Longitudinal Data", "A semicontinuous variable has a portion of responses equal to a single value (typically 0) and a continuous, often skewed, distribution among the remaining values. In cross-sectional analyses, variables of this type may be described by a pair of regression models; for example, a logistic model for the probability of nonzero response and a conditional linear model for the mean response given that it is nonzero. We extend this two-part regression approach to longitudinal settings by introducing random coefficients into both the logistic and the linear stages. Fitting a two-part random-effects model poses computational challenges similar to those found with generalized linear mixed models. We obtain maximum likelihood estimates for the fixed coefficients and variance components by an approximate Fisher scoring procedure based on high-order Laplace approximations. To illustrate, we apply the technique to data from the Adolescent Alcohol Prevention Trial, examining reported recent alcohol use for students in grades 7\u201311 and its relationships to parental monitoring and rebelliousness."], ["Model Selection and the Principle of Minimum Description Length", null], ["Data Analysis by Resampling: Concepts and Applications", null], ["Modern Applied Biostatistical Methods Using S-PLUS", null], ["Monte Carlo Methods in Statistical Physics", null], ["Linear Algebra and Linear Models", null], ["Rethinking the Foundations of Statistics", null], ["Mathematical Statistics", null], ["Towing Icebergs, Falling Dominoes and Other Adventures in Applied Mathematics", null], ["Empirical Processes in M-Estimation", null], ["Gaussian and Non-Gaussian Linear Time Series and Random Fields", null], ["Coupling, Stationarity, and Regeneration", null], ["Flood Frequency Analysis", null], ["Quantitative Fish Dynamics", null], ["Statistical Process Control In Industry: Implementation and Assurance of SPC", null], ["Continuous Multivariate Distributions, Volume 1: Models and Applications", null], ["Telegraphic Reviews", null], ["Corrections", null], ["Poststratification Without Population Level Information on the Poststratifying Variable With Application to Political Polling", "We investigate the construction of more precise estimates of a collection of population means using information about a related variable in the context of repeated sample surveys. The method is illustrated using poll results concerning presidential approval rating (our related variable is political party identification). We use poststratification to construct these improved estimates, but because we do not have population level information on the poststratifying variable, we construct a model for the manner in which the poststratifier develops over time. In this manner, we obtain more precise estimates without making possibly untenable assumptions about the dynamics of our variable of interest, the presidential approval rating."], ["Investigating Child Mortality in Malawi Using Family and Community Random Effects", "The Malawi Demographic and Health Survey conducted in 1992 collected the retrospective birth histories for a national sample of 4,878 women aged between 15 and 49 years. The sample was randomly selected by a two-stage sampling design. The data consist of biological, demographic, and social variables collected for each birth. This article models the infant and early childhood survival using family and community random effect multipliers on the fixed effect proportional hazards model, which allows the dependence between observations in the same family and community into the model. A Markov chain Monte Carlo sample from the posterior distribution of the parameters given the data is found. The standard errors of the fixed effect estimates are more correct than those found from the standard model, which are underestimated because of the ignored correlation structure."], ["Overcoming Scale Usage Heterogeneity", "Questions that use a discrete ratings scale are commonplace in survey research. Examples in marketing include customer satisfaction measurement and purchase intention. Survey research practitioners have long commented that respondents vary in their usage of the scale: Common patterns include using only the middle of the scale or using the upper or lower end. These differences in scale usage can impart biases to correlation and regression analyses. To capture scale usage differences, we developed a new model with individual scale and location effects and a discrete outcome variable. We model the joint distribution of all ratings scale responses rather than specific univariate conditional distributions as in the ordinal probit model. We apply our model to a customer satisfaction survey and show that the correlation inferences are much different once proper adjustments are made for the discreteness of the data and scale usage. We also show that our adjusted or latent ratings scale is more closely related to actual purchase behavior."], ["Iterative Automated Record Linkage Using Mixture Models", "The goal of record linkage is to link quickly and accurately records that correspond to the same person or entity. Whereas certain patterns of agreements and disagreements on variables are more likely among records pertaining to a single person than among records for different people, the observed patterns for pairs of records can be viewed as arising from a mixture of matches and nonmatches. Mixture model estimates can be used to partition record pairs into two or more groups that can be labeled as probable matches (links) and probable nonmatches (nonlinks). A method is proposed and illustrated that uses marginal information in the database to select mixture models, identifies sets of records for clerks to review based on the models and marginal information, incorporates clerically reviewed data, as they become available, into estimates of model parameters, and classifies pairs as links, nonlinks, or in need of further clerical review. The procedure is illustrated with five datasets from the U.S. Bureau of the Census. It appears to be robust to variations in record-linkage sites. The clerical review corrects classifications of some pairs directly and leads to changes in classification of others through reestimation of mixture models."], ["The Distribution of Realized Exchange Rate Volatility", "Using high-frequency data on deutschemark and yen returns against the dollar, we construct model-free estimates of daily exchange rate volatility and correlation that cover an entire decade. Our estimates, termed realized volatilities and correlations, are not only model-free, but also approximately free of measurement error under general conditions, which we discuss in detail. Hence, for practical purposes, we may treat the exchange rate volatilities and correlations as observed rather than latent. We do so, and we characterize their joint distribution, both unconditionally and conditionally. Noteworthy results include a simple normality-inducing volatility transformation, high contemporaneous correlation across volatilities, high correlation between correlation and volatilities, pronounced and persistent dynamics in volatilities and correlations, evidence of long-memory dynamics in volatilities and correlations, and remarkably precise scaling laws under temporal aggregation."], ["Fitting Mixtures of Kent Distributions to Aid in Joint Set Identification", "When examining a rock mass, joint sets and their orientations can play a significant role with regard to how the rock mass will behave. To identify joint sets present in the rock mass, the orientation of individual fracture planes can be measured on exposed rock faces and the resulting data can be examined for heterogeneity. In this article, the expectation\u2013maximization algorithm is used to fit mixtures of Kent component distributions to the fracture data to aid in the identification of joint sets. An additional uniform component is also included in the model to accommodate the noise present in the data."], ["Bayesian Methods Applied to Survey Data From Archeological Magnetometry", "A model is presented for the interpretation of magnetometer data in terms of archeological features beneath the ground. It describes the detector's response to the assemblage of buried features, incorporating both a spread function and a statistical error process as well as appropriate prior beliefs about the nature of archeological features. The problem is to estimate the magnetic susceptibility of the buried features at each horizontal location. A Monte Carlo Markov chain approach is used to estimate magnetic susceptibilities and all prior parameters. This requires estimation of the normalization constant of the Gibbs prior distribution. The approach is illustrated with both simulated data and measurements from an archeological site. In the latter case, the reconstruction of the buried features corresponds well with the archeologist's observations during subsequent excavation."], ["The Bayesian Modeling of Disease Risk in Relation to a Point Source", "Recently there has been increased interest, from both the media and the public, in the question, \u201cIs there an excess of disease risk close to a prespecified point source?\u201d To address this question, routinely available public health data may be analyzed. In the United Kingdom, as in many countries, health data and the associated population data that are required for comparison, are available as aggregated counts. In this article we propose to analyze such data using a Bayesian disease mapping framework. This framework allows the extra-Poisson variability that is frequently encountered to be accommodated through random effects that may be unstructured or display spatial dependence. The disease risk-spatial location relationship is modeled using a simple but realistic parametric form. The random effects may be used for diagnostic purposes, in particular to assess the appropriateness of the distance-risk model. The choice of prior distribution is extremely important in this context and we develop an informative prior distribution that is based on epidemiological considerations and on additional analyses of data that are obtained from a larger \u201creference\u201d region within which the study region is embedded. We argue that a particularly useful inferential summary for public health purposes is the predictive distribution. For example, we may obtain the distribution of the number of cases that would be expected to occur within a specified distance of the putative source (given a population size, by age and sex, and a time period). The approach is illustrated using data from an investigation into the incidence of stomach cancer close to a municipal solid waste incinerator. The sensitivity to the prior distribution and the presence or absence of spatial random effects is examined. To determine whether the increase in risk detected in the study is persistent, we analyze incidence data from the four-year interval following the study period. We finally describe a number of extensions including the modeling of data from a number of sites using a four-stage hierarchical model. This model is statistically realistic and, more importantly, allows the epidemiological question to be answered with greater reliability."], ["Group Testing With Blockers and Synergism", "Discovery and development of a new drug can cost hundreds of millions of dollars. Pharmaceutical companies have used group testing methodology routinely as one of the efficient high throughput screening techniques to search for \u201clead\u201d compounds among collections of hundreds of thousands of chemical compounds. The lead compounds can be modified to produce new and effective molecules, which eventually may lead to new drugs. This article develops models and estimation procedures to obtain quantitative information from data in such applications. It investigates group testing procedures and studies cost efficiency when the standard assumption adopted by Dorfman, that tested items act independently of one another, is violated. The investigation is focused on, but not limited to, the square array pooling method, and the methodologies developed are illustrated through simulations and a drug discovery dataset from Glaxo Wellcome Inc."], ["Semiparametric and Nonparametric Regression Analysis of Longitudinal Data", null], ["Smoothing Spline ANOVA for Multivariate Bernoulli Observations With Application to Ophthalmology Data", "We combine a smoothing spline analysis of variance (SS-ANOVA) model and a log-linear model to build a partly flexible model for multivariate Bernoulli data. The joint distribution conditioning on the predictor variables is estimated. The log odds ratio is used to measure the association between outcome variables. A numerical scheme based on the block one-step successive over relaxation SOR\u2013Newton-Ralphson algorithm is proposed to obtain an approximate solution for the variational problem. We extend the generalized approximate cross validation (GACV) and the randomized GACV for choosing smoothing parameters to the case of multivariate Bernoulli responses. The randomized version is fast and stable to compute and is used to adaptively select smoothing parameters in each block onestep SOR iteration. Approximate Bayesian confidence intervals are obtained for the flexible estimates of the conditional logit functions. Simulation studies are conducted to check the performance of the proposed method, using the comparative Kullback\u2013Leibler distance as a yardstick. Finally, the model is applied to two-eye observational data from the Beaver Dam Eye Study, to examine the association of pigmentary abnormalities and various covariates."], ["Gibbs Sampling Methods for Stick-Breaking Priors", "A rich and flexible class of random probability measures, which we call stick-breaking priors, can be constructed using a sequence of independent beta random variables. Examples of random measures that have this characterization include the Dirichlet process, its two-parameter extension, the two-parameter Poisson\u2013Dirichlet process, finite dimensional Dirichlet priors, and beta two-parameter processes. The rich nature of stick-breaking priors offers Bayesians a useful class of priors for nonparametric problems, while the similar construction used in each prior can be exploited to develop a general computational procedure for fitting them. In this article we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick-breaking priors. The first type of Gibbs sampler, referred to as a P\u00f3lya urn Gibbs sampler, is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing. This method applies to stick-breaking priors with a known P\u00f3lya urn characterization, that is, priors with an explicit and simple prediction rule. Our second method, the blocked Gibbs sampler, is based on an entirely different approach that works by directly sampling values from the posterior of the random measure. The blocked Gibbs sampler can be viewed as a more general approach because it works without requiring an explicit prediction rule. We find that the blocked Gibbs avoids some of the limitations seen with the P\u00f3lya urn approach and should be simpler for nonexperts to use."], ["Bayesian and Conditional Frequentist Testing of a Parametric Model Versus Nonparametric Alternatives", null], ["A Model-Calibration Approach to Using Complete Auxiliary Information From Survey Data", null], ["Markov chain Monte Carlo Estimation of Classical and Dynamic Switching and Mixture Models", "Bayesian estimation of a very general model class, where the distribution of the observations depends on a latent process taking values in a discrete state space, is discussed in this article. This model class covers finite mixture modeling, Markov switching autoregressive modeling, and dynamic linear models with switching. The consequences the unidentifiability of this type of model has on Markov chain Monte Carlo (MCMC) estimation are explicitly dealt with. Joint Bayesian estimation of all latent variables, model parameters, and parameters that determine the probability law of the latent process is carried out by a new MCMC method called permutation sampling. The permutation sampler first samples from the unconstrained posterior\u2013which often can be done in a convenient multimove manner\u2013and then applies a permutation of the current labeling of the states of the latent process. In a first run, the random permutation sampler used selected the permutation randomly. The MCMC output of the random permutation sampler is explored to find suitable identifiability constraints. In a second run, the permutation sampler was used to sample from the constrained posterior by imposing identifiablity constraints. This time a suitable permutation is applied if the identifiability constraint is violated. For illustration, two detailed case studies are presented, namely finite mixture modeling of fetal lamb data and Markov switching autoregressive modeling of the U.S. quarterly real gross national product data."], ["Stability in the Absence of Treatment", "When subjects are measured twice, once at each of two symmetrical locations or times, stability of responses in the absence of treatment within subjects, together with comparability of untreated responses between subjects, is often viewed as supporting a conclusion that differences between treated and control responses reflect effects actually caused by the treatment. The degree to which this intuitive argument is formally correct is explored in several related models: a multivariate Normal model, a nonparametric model defined by symmetries, an analogous randomized experiment, and a sensitivity analysis model for observational studies in which treatments are not randomly assigned to subjects, nor to locations within subjects. Card and Kreuger's study of the employment effects of the minimum wage is used to illustrate the methods."], ["ANCOVA Methods for Heteroscedastic Nonparametric Regression Models", null], ["Confidence Intervals for Nonparametric Curve Estimates", null], ["Spatially Adaptive Regression Splines and Accurate Knot Selection Schemes", "Spline procedures have proven effective in estimating smooth functions. However, spline procedures based on stepwise addition and/or deletion have some drawbacks. They suffer from the knot compounding problem, making their performance suboptimal. Furthermore, due to computational complexity, spline procedures may not achieve their full potential. In this article, we propose a novel knot selection algorithm for regression spline estimation in nonparametric regression. The algorithm includes three new components: knot relocation, guided search, and local fitting. The local properties of the spline functions are used to efficiently implement the algorithm. Extensive simulation studies are performed to demonstrate the improvement of the new knot selection algorithm over the stepwise addition and deletion scheme, and the advantages of the spline procedure with the new knot selection scheme over alternative adaptive methods. In the simulations, our procedure achieves very competitive performance with alternative methods and has substantial advantage in nonsmooth functions. Finally, the usefulness of the proposed method is illustrated by an application to signal recovery in speech signal processing."], ["Jackknife Variance Estimation for Nearest-Neighbor Imputation", "Nearest-neighbor imputation is a popular hot deck imputation method used to compensate for nonresponse in sample surveys. Although this method has a long history of application, the problem of variance estimation after nearest-neighbor imputation has not been fully investigated. Because nearest-neighbor imputation is a nonparametric method, a nonparametric variance estimation technique, such as the jackknife, is desired. We show that the naive jackknife that treats imputed values as observed data produces serious underestimation. We also show that Rao and Shao's adjusted jackknife, or the jackknife with each pseudoreplicate reimputed, which produces asymptotically unbiased and consistent jackknife variance estimators for other imputation methods (such as mean imputation, random hot deck imputation, and ratio or regression imputation), produces serious overestimation in the case of nearest-neighbor imputation. Two partially reimputed and a partially adjusted jackknife variance estimators are proposed and shown to be asymptotically unbiased and consistent. Some empirical results are provided to examine finite-sample properties of these jackknife variance estimators."], ["Marginal Likelihood From the Metropolis\u2013Hastings Output", "This article provides a framework for estimating the marginal likelihood for the purpose of Bayesian model comparisons. The approach extends and completes the method presented in Chib (1995) by overcoming the problems associated with the presence of intractable full conditional densities. The proposed method is developed in the context of MCMC chains produced by the Metropolis\u2013Hastings algorithm, whose building blocks are used both for sampling and marginal likelihood estimation, thus economizing on prerun tuning effort and programming. Experiments involving the logit model for binary data, hierarchical random effects model for clustered Gaussian data, Poisson regression model for clustered count data, and the multivariate probit model for correlated binary data, are used to illustrate the performance and implementation of the method. These examples demonstrate that the method is practical and widely applicable."], ["Generalized Least Squares, Taylor Series Linearization and Fisher's Scoring in Multivariate Nonlinear Regression", "In this article, we consider a general multivariate nonlinear regression setting in which the marginal mean and variance\u2013covariance structure share a common set of regression parameters. Estimation is carried out via iteratively reweighted generalized least squares (IRGLS) that entails repeated application of Taylor series linearization and estimated generalized least squares (EGLS). Under normality, this IRGLS procedure is equivalent to Fisher's method of scoring and hence maximum likelihood estimation (MLE). However, estimates from this procedure are also shown to minimize a bias-corrected generalized least squares objective function that does not require the assumption of normality. Under fairly mild regularity conditions, the resulting estimates are consistent, asymptotically normal, and\u2013under normality assumptions\u2013asymptotically efficient. The estimates are compared against those obtained as solutions to the usual generalized estimating equations (GEE) using both simulation and numerical examples."], ["Likelihood-Based Methods for Missing Covariates in the Cox Proportional Hazards Model", "Problems associated with missing covariate data are well known but often ignored. We present a method for estimating the parameters in the Cox proportional hazards model when the missing data are missing at random (MAR) and censoring is noninformative. Due to the computational burden of this method, we introduce an approximation that allows us to use a weighted expectation-maximization (EM) algorithm to estimate the parameters more easily. When the missing covariates are continuous rather than categorical, we implement a Monte Carlo version of the EM algorithm along with the Gibbs sampler to obtain parameter estimates. We also give the asymptotic distribution of these estimates. The primary advantage of this method over complete case analysis is that it produces more efficient parameter estimates and corrects for bias in the MAR setting. To motivate the methodology, we present an analysis of a phase III melanoma clinical trial conducted by the Eastern Cooperative Oncology Group."], ["Information and Posterior Probability Criteria for Model Selection in Local Likelihood Estimation", "Local likelihood estimation has proven to be an effective method for obtaining estimates of parameters that vary with a covariate. To obtain useful estimates of such parameters, approximating models are used. In such cases it is useful to consider window based estimates. We may need to choose between competing approximating models. In this article, we propose a modification to the methods used to motivate many information and posterior probability criteria for the weighted likelihood case. We derive weighted versions for two of the most widely known criteria, namely the AIC and BIC. Via a simple modification, the criteria are also made useful for window span selection. The usefulness of the weighted version of these criteria is demonstrated through a simulation study and an application to three datasets."], ["Estimating Millions of Dynamic Timing Patterns in Real Time", null], ["Driving Fast in Reverse", "Structural equation modeling is one of the most widely used statistical techniques in the social sciences, especially psychology. Its popularity and complexity have spawned a large number of \u201cuser-friendly\u201d computer programs, training seminars, introductory textbooks, edited volumes, and an internet discussion group (SEMNET). A review of several introductory textbooks and an edited volume raises disturbing questions about the interplay between commercial development, statistical theory, and \u201cpractical\u201d statistical education in this field."], ["Who Counts? The Politics of Census-Taking in Contemporary America", null], ["Monte Carlo Statistical Methods", null], ["Local Regression and Likelihood", null], ["Nonparametric Econometrics", null], ["Challenging Time Series: Limits to Knowledge, Inertia and Caprice", null], ["The Econometric Modelling of Financial Time Series", null], ["Unit Roots, Cointegration, and Structural Change", null], ["Forecasting Non-Stationary Economic Time Series", null], ["Graphical Methods for the Design of Experiments", null], ["The Analysis of Variance: Fixed, Random and Mixed Models", null], ["Epidemiology: Study Design and Data Analysis", null], ["Analysis of Health Surveys", null], ["Cognition and Survey Research", null], ["Uniform Central Limit Theorems", null], ["Set-Indexed Martingales", null], ["Economic Forecasting", null], ["Nonlinear Econometric Modeling in Time Series Analysis: Proceedings of the Eleventh International Symposium in Economic Theory", null], ["Generalized Linear Models: A Bayesian Perspective", null], ["Statistics in Society: The Arithmetic of Politics.", null], ["A Handbook of Statistical Analyses Using Stata", null], ["Statistics for the 21st Century: Methodologies for Applications of the Future", null], ["Handbook of Tables for Order Statistics From Lognormal Distributions With Applications", null], ["Counting on the Census? Race, Group Identity, and the Evasion of Politics", null], ["Public Policy and Statistics: Case Studies From RAND.", null], ["Correction", null]]}