{"1995": [["Are People Bayesian? Uncovering Behavioral Strategies", "Economists and psychologists have recently been developing new theories of decision making under uncertainty that can accommodate the observed violations of standard statistical decision theoretic axioms by experimental subjects. We propose a procedure that finds a collection of decision rules that best explain the behavior of experimental subjects. The procedure is a combination of maximum likelihood estimation of the rules together with an implicit classification of subjects to the various rules and a penalty for having too many rules. We apply our procedure to data on probabilistic updating by subjects in four different universities. We get remarkably robust results showing that the most important rules used by the subjects (in order of importance) are Bayes's rule, a representativeness rule (ignoring the prior), and, to a lesser extent, conservatism (overweighting the prior)."], ["Stochastic Modeling of Early Hematopoiesis", "Hematopoiesis is the body's way of making the cellular constituents of blood. Oxygen transport, response to infections, and control of bleeding are among the functions of different mature blood cells. These specific functions are acquired as cells mature in the bone marrow. Stem cells are the \u201cmaster cells\u201d at the top of this pedigree, having within them the capacity to reconstitute the entire system. Although the latter stages of hematopoiesis are fairly well understood, the functioning of stem cells and other multipotential cells is currently a matter of intense research. This article presents a statistical analysis providing support for the clonal succession model of early hematopoiesis. J. L. Abkowitz and colleagues at the University of Washington have developed an experimental method for studying the kinetics of early hematopoiesis in a hybrid cat. The essence of the method is to analyze G6PD, an enzyme linked to the X chromosome. The G6PD type of a cell forms a binary marker that is passed down to all its descendant cells. Data record time series of proportions of one G6PD type in cells from the bone marrow, providing information about the number and lifetime of unobservable stem cells. Studies were performed after the autologous transplantation of G6PD heterozygous cats with limited numbers of hematopoietic stem cells. Preliminary analysis of the observed proportions indicates that under these circumstances, the proportion of cells with one type of G6PD is not constant over time. A simple stochastic model is used to quantify the relationship between observed proportions and unobserved stem cell populations. The model has a hidden Markov structure. We develop parameter estimates, confidence sets, and goodness-of-fit tests for this model. For our simple model, a recursive updating algorithm allows computation of the multimodal likelihood functions. A similar algorithm produces estimates of the realized Markov process. The parametric bootstrap is used to calibrate likelihood-based confidence sets and to perform simple goodness-of-fit tests. We address the question of whether stem cells have a constant proliferative potential between cats, and we discuss criticisms of the simple model."], ["Bayesian Models for Multiple Local Sequence Alignment and Gibbs Sampling Strategies", "A wealth of data concerning life's basic molecules, proteins and nucleic acids, has emerged from the biotechnology revolution. The human genome project has accelerated the growth of these data. Multiple observations of homologous protein or nucleic acid sequences from different organisms are often available. But because mutations and sequence errors misalign these data, multiple sequence alignment has become an essential and valuable tool for understanding structures and functions of these molecules. A recently developed Gibbs sampling algorithm has been applied with substantial advantage in this setting. In this article we develop a full Bayesian foundation for this algorithm and present extensions that permit relaxation of two important restrictions. We also present a rank test for the assessment of the significance of multiple sequence alignment. As an example, we study the set of dinucleotide binding proteins and predict binding segments for dozens of its members."], ["A Nonparametric Regression Approach to Syringe Grading for Quality Improvement", "In the biomedical products industry, measures of the quality of individual clinical specimens or manufacturing production units are often available in the form of high-dimensional data such as continuous recordings obtained from an analytical instrument. These recordings are then examined by experts in the field who extract certain features and use these to classify individuals. To formalize and quantify this procedure, an approach for extracting features from recordings based on nonparametric regression is described. These features are then used to build a classification model that incorporates the knowledge of the expert. The procedure is illustrated with the problem of grading of syringes from associated friction profile data. Features of the syringe friction profiles used in the classification are extracted via smoothing splines, and grades of the syringes are assigned by an expert tribologist. A nonlinear classification model is constructed to predict syringe grades based on the extracted features. The classification model makes it possible to grade syringes automatically without expert inspection. Using leave-one-out cross-validation, the prediction accuracy of the classification model is found to be about the same as the accuracy obtained from the expert."], ["Searching for Structure in Curve Samples", null], ["Local Prediction of a Spatio-Temporal Process with an Application to Wet Sulfate Deposition", "A prediction method is given for a first- and second-order nonstationary spatio-temporal process. The predictor uses local data only and consists of a two-stage generalized regression estimate of the local drift at the prediction location added to a kriging prediction of the residual process at that location. This predictor is applied to observations on seasonal, rainfall-deposited sulfate over the conterminous United States between summer 1986 and summer 1992. Analyses suggest that predictions and estimated prediction standard errors have negligible to small biases, there is spatially heterogeneous temporal drift, and temporal covariance is negligible."], ["Adapting to Unknown Smoothness via Wavelet Shrinkage", null], ["Overdispersion Diagnostics for Generalized Linear Models", null], ["Tests of Homogeneity for Generalized Linear Models", "We propose two tests for testing homogeneity among clustered data adjusting for the effects of covariates. The first is a score test for a generalized linear model with random effect, in which the distribution of the response variable given the random effect is entirely defined. In contrast to the likelihood ratio test, however, the score test does not require estimation of the parameters of a mixed-effects model nor specification of the mixing distribution. The second test is proposed in the framework of the generalized estimating equation (GEE) approach. In deriving this test, we need only the specification of the marginal expectation and variance of the response variable and the fourth moment for the overdispersion term, whereas for deriving the score test for mixed effects models, the entire conditional distribution must be specified. We demonstrate that the two tests are identical when the covariance matrix assumed in the GEE approach is that of the random-effects model. In both approaches, the test statistic can be decomposed into a pairwise correlation statistic and a statistic of overdispersion. We performed a simulation study to compare the power of the score test and of the test based on their pairwise correlation statistic only, and also to compare their type I errors in cases where data present overdispersion not due to the clustering studied. On the basis of these results, we recommend using the pairwise correlation statistic, which is more robust than the complete statistic to overdispersion not due to the clustering studied."], ["Simulation-Extrapolation: The Measurement Error Jackknife", "This article provides theoretical support for our simulation-based estimation procedure, SIMEX, for measurement error models. We do so by establishing a strong relationship between SIMEX estimation and jackknife estimation. A result of our investigation is the identification of a variance estimation method for SIMEX that parallels jackknife variance estimation. Data from the Framingham Heart Study are used to illustrate the variance estimation procedure in logistic regression measurement error models."], ["An Effective Bandwidth Selector for Local Least Squares Regression", "Local least squares kernel regression provides an appealing solution to the nonparametric regression, or \u201cscatterplot smoothing,\u201d problem, as demonstrated by Fan, for example. The practical implementation of any scatterplot smoother is greatly enhanced by the availability of a reliable rule for automatic selection of the smoothing parameter. In this article we apply the ideas of plug-in bandwidth selection to develop strategies for choosing the smoothing parameter of local linear squares kernel estimators. Our results are applicable to odd-degree local polynomial fits and can be extended to other settings, such as derivative estimation and multiple nonparametric regression. An implementation in the important case of local linear fits with univariate predictors is shown to perform well in practice. A by-product of our work is the development of a class of nonparametric variance estimators, based on local least squares ideas, and plug-in rules for their implementation."], ["Transformations for Improving Linearization Confidence Intervals in Nonlinear Regression", null], ["Fixed-Domain Asymptotics for Spatial Periodograms", "The periodogram for a spatial process observed on a lattice is often used to estimate the spectral density. The bases for such estimators are two asymptotic properties that periodograms commonly possess: (1) the periodogram at a particular frequency is approximately unbiased for the spectral density, and (2) the correlation of the periodogram at distinct frequencies is approximately zero. For spatial data, it is often appropriate to use fixed-domain asymptotics in which the observations get increasingly dense in some fixed region as their number increases. Using fixed-domain asymptotics, this article shows that standard asymptotic results for periodograms do not apply and that using the periodogram of the raw data can yield highly misleading results. But by appropriately filtering the data before computing the periodogram, it is possible to obtain results similar to the standard asymptotic results for spatial periodograms."], ["Bootstrap Inference for a First-Order Autoregression with Positive Innovations", null], ["Bayesian Inference in Cyclical Component Dynamic Linear Models", "Dynamic linear models (DLM's) with time-varying cyclical components are developed for the analysis of time series with persistent though time-varying cyclical behavior. The development covers inference on wavelengths of possibly several persistent cycles in nonstationary time series, permitting explicit time variation in amplitudes and phases of component waveforms, decomposition of stochastic inputs into purely observational noise and innovations that impact on the waveform characteristics, with extensions to incorporate ranges of (time-varying) time series and regression terms wihin the standard DLM context. Bayesian inference via iterative stochastic simulation methods is developed and illustrated. Some indications of model extensions and generalizations are given. In addition to the specific focus on cyclical component models, the development provides the basis for Bayesian inference, via stochastic simulation, for state evolution matrix parameters and variance components in DLM's, building on recent work on Gibbs sampling for state vectors in such models by other authors."], ["Marginal Likelihood from the Gibbs Output", "In the context of Bayes estimation via Gibbs sampling, with or without data augmentation, a simple approach is developed for computing the marginal density of the sample data (marginal likelihood) given parameter draws from the posterior distribution. Consequently, Bayes factors for model comparisons can be routinely computed as a by-product of the simulation. Hitherto, this calculation has proved extremely challenging. Our approach exploits the fact that the marginal density can be expressed as the prior times the likelihood function over the posterior density. This simple identity holds for any parameter value. An estimate of the posterior density is shown to be available if all complete conditional densities used in the Gibbs sampler have closed-form expressions. To improve accuracy, the posterior density is estimated at a high density point, and the numerical standard error of resulting estimate is derived. The ideas are applied to probit regression and finite mixture models."], ["Optimal Design via Curve Fitting of Monte Carlo Experiments", "This article explores numerical methods for stochastic optimization, with special attention to Bayesian design problems. A common and challenging situation occurs when the objective function (in Bayesian applications, the expected utility) is very expensive to evaluate, perhaps because it requires integration over a space of very large dimensionality. Our goal is to explore a class of optimization algorithms designed to gain efficiency in such situations, by exploiting smoothness of the expected utility surface and borrowing information from neighboring design points. The central idea is that of implementing stochastic optimization by curve fitting of Monte Carlo samples. This is done by simulating draws from the joint parameter/sample space and evaluating the observed utilities. Fitting a smooth surface through these simulated points serves as estimate for the expected utility surface. The optimal design can then be found deterministically. In this article we introduce a general algorithm for curve-fitting-based optimization, discuss implementation options, and present a consistency property for one particular implementation of the algorithm. To illustrate the advantages and limitations of curve-fitting-based optimization, and to compare it with some of the alternatives, we consider in detail two important practical applications: an information theoretical stopping rule for a clinical trial, with an objective function based on the expected amount of information acquired about a subvector of parameters of interest, and the design of exploratory shock levels in the implantation of heart defibrillators. This latter example is also used for comparison with some of the alternative schemes. One of the main attractions of efficient optimization algorithms in design is the application to sequential problems. We conclude with an outlook on how the ideas presented here can be extended to solve stochastic dynamic programming problems such as those occurring in Bayesian sequential design."], ["Modeling and Inference with \u03c5-Spherical Distributions", null], ["Information and Conditional Inference", null], ["Conjugate Parameterizations for Natural Exponential Families", null], ["Some Remarks on Noninformative Priors", null], ["An Empirical Bayes Model for Markov-Dependent Binary Sequences with Randomly Missing Observations", "We develop an improved empirical Bayes estimation methodology for the analysis of two-state Markov chains observed from heterogeneous individuals. First, the two transition probabilities corresponding to each chain are assumed to be drawn from a common, bivariate distribution that has beta marginals. Second, randomly missing observations are incorporated into the likelihood for the hyperparameters by efficiently summing over all possible values for the missing observations. A likelihood ratio test is used to test for dependence between the transition probabilities. Posterior distributions for the transition probabilities are also derived, as is an approximation for the equilibrium probabilities. The proposed procedures are illustrated in a numerical example and in an analysis of longitudinal store display data."], ["Correlation Analysis of Extreme Observations from a Multivariate Normal Distribution", null], ["Control Charts for Multivariate Processes", null], ["Rank Tests for Main and Interaction Effects in Analysis of Variance", "A class of rank tests for analysis of variance based on linear functions of the Wilcoxon/Mann-Whitney statistics between cells is studied. This class includes many popular procedures. Assuming a location-family model, a condition for optimal Pitman efficacy of tests of main effects in this class is given. Tests for main effects in two-way layouts that achieve this optimality are shown to improve on previous proposals when the sample sizes are unbalanced. Nested orthogonal contrasts are introduced that can be used to decompose the overall test statistics into independent components to assess the contribution of individual contrasts. A new definition of interaction appropriate for rank-based analysis of variance is suggested, and test procedures are proposed."], ["Likelihood Ratio-Based Confidence Intervals in Survival Analysis", "Confidence intervals for the survival function and the cumulative hazard function are considered. These confidence intervals are based on an inversion of the likelihood ratio statistic. To do this, two extensions of the likelihood, each of which yields meaningful likelihood ratio hypothesis tests and subsequent confidence intervals, are considered. The choice of the best extension is difficult. In the failure time setting, the binomial extension is best in constructing confidence intervals concerning the survival function and the Poisson extension is best in constructing confidence intervals concerning the cumulative hazard. Simulations indicate that these two methods perform as well as or better than competitors based on the asymptotic normality of the estimator."], ["Estimating the Occurrence Rate for Prevalent Survival Data in Competing Risks Models", "The problem of interest is the estimation of occurrence probabilities based on prevalent data in competing risks models. In the literature, the development of nonparametric methods has relied heavily on the independent competing risks assumption. The primary purpose of this article is to establish a statistical framework without excluding the possibility of dependence among competing risks. This is done through the use of crude hazard functions. The crude hazard functions not only are estimable regardless of whether the competing risks are independent, but also are mathematically more tractable. In this article we show that there is a one-to-one correspondence between the crude hazard functions and the occurrence probabilities. The general inversion formulas for the occurrence probabilities are presented, from which various representations can be derived under different sampling techniques. Maximum likelihood estimators are derived using these representations for nonparametric and length bias data. The maximum likelihood property and asymptotic behavior of both estimation procedures are studied. The simulation results show that the length bias estimators have smaller variances compared to the nonparametric estimators. Nevertheless, the nonparametric estimation procedure appears to be more robust to model assumptions."], ["How Pooling Failure Data May Reverse Increasing Failure Rates", "Although mixtures of decreasing failure rate (DFR) distributions are always DFR, some mixtures of increasing failure rate (IFR) distributions can also be ultimately DFR. In this article various types of discrete and continuous mixtures of IFR distributions are considered, and conditions are developed for such mixtures to be ultimately DFR. These conditions lead to an interesting result\u2014that certain mixtures of IFR distributions, even those with very rapidly increasing failure rates (e.g., Weibull, truncated extreme), ultimately become DFR distributions. It is common practice to pool data from several different IFR distributions to enlarge sample size, for instance. The results of this article sound a warning that such pooling may actually reverse the IFR property of the individual samples to a DFR property."], ["Quantiles in Nonrandom Samples and Observational Studies", null], ["Analyzing Bivariate Ordinal Data Using a Global Odds Ratio", "A moment method is proposed for regression analysis of bivariate ordered categorical data using the global odds ratio as the measure of association. For modeling the margins, this method utilizes the stochastic ordering implicit in the data. This method allows for covariate effects in modeling the association between responses. An application of the proposed model is illustrated using the ophthalmological data from the Wisconsin Epidemiologic Study of Diabetic Retinopathy for identifying risk factors among younger onset diabetics. This model is also extended to a longitudinal data setting with more than two repeated measures."], ["Computation of Maximum Likelihood Estimates in Association Models", "In association models for cross-classified data, computation of maximum likelihood estimates (MLE's) is relatively difficult due to the nonlinear constraints on the parameters. Currently available procedures based on the scoring algorithm for constrained maximum likelihood are relatively unreliable, and other cyclic procedures are relatively slow and do not provide estimated asymptotic standard deviations as by-products of calculations. To facilitate computations, it is noted that in standard association models, removal of constraints results in underidentification of parameters but does not affect the model itself, so that the MLE's of cell probabilities and conditional probabilities are unaffected. Given this observation, maximum likelihood estimation may be accomplished by unconstrained maximization of an objective function with two components: a log-likelihood ratio and a sum of squares representing deviations of parameters from their constraints. The objective function is then maximized by using a modification of the Newton-Raphson algorithm that ensures that successive iterations increase the objective function whenever a local maximum has not been reached. The proposed algorithm is shown to be reliable and relatively rapid. In addition, it is shown that the proposed technique may be used to estimate asymptotic standard deviations of parameter estimates. Use of the algorithm in practice is illustrated through some standard examples of association models."], ["Dispersion of Categorical Variables and Penalty Functions: Derivation, Estimation, and Comparability", "Measures of dispersion for categorical random variables based on penalty functions play a central role in establishing relevant measures of association between such variables. The literature concerning these measures provides little systematic treatment of such aspects of these measures as comparability, efficient estimation, and large-sample properties. This article provides a systematic and rigorous construction of dispersion measures based on penalty functions. Efficient estimation procedures and asymptotic properties of estimates are examined. Conditions from majorization theory that ensure a meaningful comparability of dispersion measures based on penalty functions are discussed. A large class of familiar dispersion measures is then given a new interpretation using these conditions."], ["Network Models for Complementary Cell Suppression", "Complementary cell suppression is a method for protecting data pertaining to individual respondents from statistical disclosure when the data are presented in tabular form. Several mathematical methods for complementary suppression have been proposed in the statistical literature; some have been implemented in large-scale data processing environments by national statistical agencies. Each method has either theoretical or computational limitations. This article presents solutions to the complementary cell suppression problem based on linear optimization over a mathematical network. These methods are shown to be optimal for certain problems and to offer theoretical and practical advantages, including comprehensiveness, comprehensibleness, and computational efficiency."], ["A Class of Sequential Conditional Probability Ratio Tests", null], ["Simultaneous Detection of Shift in Means and Variances", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Correction", null], ["Editorial Board Page", "This article has no abstract"], ["Modeling the Labeling Index Distribution: An Application of Functional Data Analysis", "This article presents exploratory data analytic methodology for visualizing and summarizing data that can be represented as individual-specific curves. We propose a simplified form of functional data analysis. A nonparametric scatterplot smooth is applied to each individual's data, followed by a principal components analysis of the smoothed data. We then display the individual smooth curves in an array organized by principal component scores. The display suggests interpretable summary measures. The methodology is applied to the measurement of proliferative activity, a biomarker for colon cancer risk. We use the summary measures in the analysis of a pilot study clinical trial."], ["Handling \u201cDon't Know\u201d Survey Responses: The Case of the Slovenian Plebiscite", "The critical step in the drive toward an independent Slovenia was the plebiscite held in December 1990, at which the citizens of Slovenia voted overwhelmingly in favor of a sovereign and independent state. The Slovenian Public Opinion (SPO) survey of November/December 1990 was used by the government of Slovenia to prepare for the plebiscite. Because the plebiscite counted as \u201cYES voters\u201d only those voters who attended and voted for independence (nonvoters counted as \u201cNO voters\u201d), \u201cDon't Know\u201d survey responses can be thought of as missing data\u2014the true intention of the voter is unknown but must be either \u201cYES\u201d or \u201cNO.\u201d An analysis of the survey data under the missing-at-random assumption for the missing responses provides remarkably accurate estimates of the eventual plebiscite outcome, substantially better than ad hoc methods and a nonignorable model that allows nonresponse to depend on the intended vote."], ["Modeling Dose and Local Control in Radiotherapy", "We discuss models for predicting local control (prevention of tumor recurrence) after therapeutic radiation in cancer patients. The probability of control is first formulated from theoretical precepts. Biophysical principles dictate that the three factors in therapy that most universally influence outcome are total dose, number of sessions in which the dose is administered, and total time under treatment. We show that these principles also suggest the scale, or link function, on which local control probability for a tumor of given size is a linear function of these predictors. The probabilities are given clinical relevance by assigning a mixing distribution to tumor size; effective size, the number of actively dividing cells in a tumor, is an unmeasurable but of course quite influential quantity. We show in this case that a gamma distribution on tumor size induces linearity on a subset of the class of links first proposed by Burr. Next, we discuss methods of modeling control by a finite follow-up time. We demonstrate a new result, that minor assumptions on the effects of size on recurrence allow models developed for permanent control to be applied directly to recurrence by a finite time. We also describe adjustments for accommodating losses to follow-up before that time. Finally, we develop inference on the mixing distribution of tumor size along with results of the effect of misspecifying the distribution. We illustrate the methods with a new analysis of a radiotherapy study. Though developed for a specific type of failure data, many of the results also apply to any time-dependent binary outcome."], ["Estimating Products in Forensic Identification Using DNA Profiles", "In many areas, such as reliability and forensic identification, it is of interest to estimate a product of unknown parameters, each of which is estimated from test data. The maximum likelihood estimator usually used in both applications is approximately unbiased but has an asymmetric sampling distribution, particularly when the sample size is not large. Consequently, most estimates understate the parameter value, often substantially, and this may be a serious drawback if \u201coveroptimism\u201d is highly undesirable. Further, widely used methods of interval estimation based on confidence limits suffer from a range of theoretical and computational problems. Alternative methods of estimation are developed and discussed, based on frequentist, Bayesian, and likelihood approaches."], ["Regression Models for a Bivariate Discrete and Continuous Outcome with Clustering", "Developmental toxicity studies of laboratory animals play a crucial role in the testing and regulation of chemicals and pharmaceutical compounds. Exposure to developmental toxicants typically causes a variety of adverse effects, such as fetal malformations and reduced fetal weight at term. In this article, we discuss regression methods for jointly analyzing bivariate discrete and continuous outcomes that are motivated by the statistical problems that arise in analyzing data from developmental toxicity studies. We focus on marginal regression models; that is, models in which the marginal expectation of the bivariate response vector is related to a set of covariates by some known link functions. In these models the regression parameters for the marginal expectation are of primary scientific interest, whereas the association between the binary and continuous response is considered to be a nuisance characteristic of the data. We describe a likelihood-based approach, based on the general location model of Olkin and Tate, that yields maximum likelihood estimates of the marginal mean parameters that are robust to misspecification of distributional assumptions. Finally, we describe an extension of this model to allow for clustering, using generalized estimating equations, a multivariate analog of quasi-likelihood. A motivating example, using fetal weight and malformation data from a developmental toxicity study of ethylene glycol in mice, illustrates this methodology."], ["Spatial Interpolation Errors for Monitoring Data", "Separate modeling of the spatial mean field, the spatial variance field, and the space-time residual fields can give a more detailed and possibly more accurate representation of spatial interpolation errors when we have repeated observations on a fixed monitoring network. This article gives expressions for the spatial interpolation errors in terms of the statistics of the component fields, which enable us to assess the relative importance of different kinds of uncertainty. This modeling approach is applied to data of sulfur dioxide concentrations in Europe, and a comparison with neighborhood kriging is done by means of cross-validation."], ["Threshold Models for Combination Data from Reproductive and Developmental Experiments", null], ["A Saturated Model for Analyzing Exchangeable Binary Data: Applications to Clinical and Developmental Toxicity Studies", "Correlated binary data occur very frequently in statistical practice. In many applications, it is reasonable to assume that data from the same cluster are exchangeable. Such data are commonly encountered in cluster sample surveys, teratological experiments, ophthalmologic and otolaryngologic studies, and other clinical trials. The standard methods of analyzing these data include the use of beta-binomial models and generalized estimating equations with third and fourth moments specified by \u201cworking matrices.\u201d The focus of these procedures is an estimation of the mean and variance parameters. More information can be obtained when data are exchangeable. By expressing the joint distribution of a set of exchangeable binary random variables in terms of the probability of similar response within cluster, this article introduces a procedure for obtaining maximum likelihood estimates of population parameters such as the marginal means, moments, and correlations of orders two and higher. Applications are made to data sets from a clinical trial and a developmental toxicity study."], ["Optimal Confidence Sets, Bioequivalence, and the Lima\u00e7on of Pascal", "We begin with a decision-theoretic investigation into confidence sets that minimize expected volume at a given parameter value. Such sets are constructed by inverting a family of uniformly most powerful tests, and hence they also enjoy the optimality property of being uniformly most accurate. In addition, these sets possess Bayesian optimal volume properties and represent the first case (to our knowledge) of a frequentist 1 \u2013 \u03b1 confidence set that possesses a Bayesian optimality property. The hypothesis testing problem that generates these sets is similar to that encountered in bioequivalence testing. Our sets are optimal for testing bioequivalence in certain settings; in the case of the normal distribution, the optimal set is a curve known as the lima\u00e7on of Pascal. We illustrate the use of these curves with a biopharmaceutical example."], ["Estimation of the Number of True Gray Levels, Their Values, and Relative Frequencies in a Noisy Image", null], ["New Loss Functions in Bayesian Imaging", "Unlike the development of more accurate prior distributions for use in Bayesian imaging, the design of more sensible estimators through loss functions has been neglected in the literature. We discuss the design of loss functions with a local structure that depend only on a binary misclassification vector. The proposed approach is similar to modeling with a Markov random field. The Bayes estimate is calculated in a two-step algorithm using Markov chain Monte Carlo and simulated annealing algorithms. We present simulation experiments with the Ising model, where the observations are corrupted with Gaussian and flip noise."], ["Annealing Markov Chain Monte Carlo with Applications to Ancestral Inference", "Markov chain Monte Carlo (MCMC; the Metropolis-Hastings algorithm) has been used for many statistical problems, including Bayesian inference, likelihood inference, and tests of significance. Though the method generally works well, doubts about convergence often remain. Here we propose MCMC methods distantly related to simulated annealing. Our samplers mix rapidly enough to be usable for problems in which other methods would require eons of computing time. They simulate realizations from a sequence of distributions, allowing the distribution being simulated to vary randomly over time. If the sequence of distributions is well chosen, then the sampler will mix well and produce accurate answers for all the distributions. Even when there is only one distribution of interest, these annealing-like samplers may be the only known way to get a rapidly mixing sampler. These methods are essential for attacking very hard problems, which arise in areas such as statistical genetics. We illustrate the methods with an application that is much harder than any problem previously done by MCMC, involving ancestral inference on a very large genealogy (7 generations, 2,024 individuals). The problem is to find, conditional on data on living individuals, the probabilities of each individual having been a carrier of cystic fibrosis. Exact calculation of these conditional probabilities is infeasible. Moreover, a Gibbs sampler for the problem would not mix in a reasonable time, even on the fastest imaginable computers. Our annealing-like samplers have mixing times of a few hours. We also give examples of samplers for the \u201cwitch's hat\u201d distribution and the conditional Strauss process."], ["Gibbs Sampler Convergence Criteria", null], ["A Reference Bayesian Test for Nested Hypotheses and its Relationship to the Schwarz Criterion", null], ["A Bayesian Method for Combining Results from Several Binomial Experiments", null], ["Diagnostics and Robust Estimation in Multivariate Data Transformations", "This article presents a method for detecting multivariate outliers that might be distorting the estimation of a transformation to normality. It also proposes a robust estimator of the transformation parameter."], ["Diagnostics in Linear Discriminant Analysis", null], ["Approximations to Multivariate Normal Rectangle Probabilities Based on Conditional Expectations", "Two new approximations for multivariate normal probabilities for rectangular regions, based on conditional expectations and regression with binary variables, are proposed. One is a second-order approximation that is much more accurate but also more numerically time-consuming than the first-order approximation. A third approximation, based on the moment-generating function of a truncated multivariate normal distribution, is proposed for orthant probabilities only. Its accuracy is between the first- and second-order approximations when the dimension is less than seven and the correlations are not large. All of the approximations get worse as correlations get larger. These new approximations offer substantial improvements on previous approximations. They also compare favorably with the methods of Genz for numerical evaluation of the multivariate normal integral. The approximation methods should be especially useful within a quasi-Newton routine for parameter estimation in discrete models that involve the multivariate normal distribution."], ["Some Applications of Covariance Identities and Inequalities to Functions of Multivariate Normal Variables", "We apply general covariance identities and inequalities to some functions of multivariate normal variables. We recover, in particular, a recent covariance identity due to Siegel and provide simple estimates on the variance of order statistics. We also present some computations."], ["Modeling Satellite Ozone Data", "Starting in the early 1970s, the decline in column ozone over much of the earth has received much attention. Satellite ozone data, with the advantage of global coverage, now play an important role in assessing long-term trends in ozone distributions. We consider a class of space-time regression models for the analysis of satellite data on a fixed latitude, which take into account temporal and longitudinal dependence of the observations. The models can be used to test the uniformity of long-term trends in different longitudinal ozone series. Using the property of circular matrices, explicit expressions of the likelihood functions are obtained. Asymptotic properties of the parameter estimates are briefly discussed. A diagnostic method is proposed to tentatively select the orders in the noise terms of the models. The space-time regression models are applied to the total ozone mapping spectrometer (TOMS) data for trend assessment."], ["Prediction and Creation of Smooth Curves for Temporally Correlated Longitudinal Data", null], ["Tests for Cointegration Based on Canonical Correlation Analysis", "Critical values are provided for four new tests for cointegration based on the canonical correlations and variates of a development of the Box-Tiao procedure. It is found that in finite samples the power of three of these tests, unlike the power of Johansen's and Engle and Yoo's tests, is highly robust to the correlation between the disturbances in the cointegrating relationships and those generating the common trends. The proposed tests perform well against these alternatives, but neither set of tests dominates over the entire parameter space."], ["Nonparametric Likelihood Ratio Estimation of Probabilities for Truncated Data", "We consider the problem of interval estimation of probabilities for randomly truncated data. The best-known method is based on the normal approximation of the product-limit estimator and has a drawback that it may produce intervals containing impossible values outside the range [0, 1]. Its small-sample performance is also not satisfactory. In this article we investigate an alternative approach that derives confidence intervals directly from a conditional nonparametric likelihood ratio. The method is an exact nonparametric analog of the classical parametric likelihood ratio theory, with the parameter space now being the family of all distributions. It has an appealing property that the resulting confidence intervals are always subintervals of [0, 1]. It also demonstrated a better small-sample performance in our simulation studies. This approach is generalized to obtain confidence intervals for the ratio of two probabilities, make joint inferences on any finite number of probabilities, and test goodness of fit of a given distribution function. The small-sample performances of three different methods are investigated in a Monte Carlo study. An illustration is also given using the Centers for Disease Control's transfusion related acquired immune deficiency syndrome data."], ["Nonparametric Methods for Stratified Two-Sample Designs with Application to Multiclinic Trials", "Motivated by some problems arising from multiclinic trials, we consider stratified two-sample designs. Nonparametric effects are defined and nonparametric hypotheses are formulated in a design where treatment, centers (strata), and interactions are assumed to be fixed factors. The interpretation of the nonparametric effects and hypotheses is analyzed in two classes of semiparametric models: the linear models and models with Lehmann alternatives. The case where centers and interactions are assumed to be random factors, the so-called mixed model, is also considered. Nonparametric effects and hypotheses are defined for general models, and their properties are analyzed in corresponding linear models and in models with Lehmann alternatives. The nonparametric effects are estimated by linear rank statistics where the ranks over all centers are used. The mixed model for repeated (baseline and endpoint) observations is briefly considered, and rank procedures are also proposed for this model. All procedures are related to the nonparametric effects and are not restricted to the two classes of semiparametric models, which are used only for interpretation of the nonparametric effects. Moreover, we do not assume continuity of the underlying distribution functions of the observations, to be as general as possible. We exclude only the trivial case where the distribution function arises from a point mass; that is, a \u201cone-point distribution.\u201d Thus, not only data coming from continuous distribution functions, but also data with ties\u2014especially discrete ordinal data\u2014can be handled with the proposed procedures. In all cases, the results are derived for unbalanced designs so that there are no restrictions for practical applications. The small-sample properties of the proposed statistics are investigated by simulation studies, and the relevant asymptotic distribution theory is considered. Applications of the proposed procedures are demonstrated by means of examples related to multicenter clinical trials."], ["Testing Ordered Alternatives in the Presence of Incomplete Data", "In testing the null hypothesis of no treatment effects in a randomized block experiment, a researcher may restrict attention to an ordered alternative and thereby increase the power of his test. Jonckheere and later Page proposed such test statistics based on the Kendall and Spearman correlation coefficients. Motivated by notions of distance between permutations, we generalize Jonckheere's and Page's tests to the situation in which one or more observations are missing from one or more blocks. Conditional on the pattern of missing observations, the resulting statistics are shown to be asymptotically normal. For a particular pattern of missing observations, the asymptotic efficiency of the extended Page test is found, in many cases, to be not much lower than for the standard Page test."], ["Power Robustification of Approximately Linear Tests", "We present a general method of improving the power of linear and approximately linear tests when deviations from a translation family of distributions must be taken into account. This method involves the combination of a linear statistic measuring location and a quadratic statistic measuring change of shape of the underlying distribution. The tests (\u201cfunnel tests\u201d) are constructed as certain Bayes tests. In general they gain a sizeable amount of power over the linear tests adapted to the translation family when a change of shape of the underlying distribution occurs, while losing little for translation alternatives (\u201cpower robustification\u201d). We introduce the concept of funnel tests in an Gaussian framework first. The effect of power robustification is studied by means of a power function expansion, which applies to a large class of tests sharing a certain invariance property. The funnel tests are characterized by a maximin property over a region defined by a rotational cone. The idea of the construction is then carried over to a finite sample situation where the Gaussian model is used as an approximation. As a particular application, we construct power-robustified nonlinear rank tests in the standard two-sample situation. A simulation study demonstrates the good overall performance of these tests as compared to other nonlinear tests."], ["Inference for Likelihood Ratio Ordering in the Two-Sample Problem", "We obtain the maximum likelihood estimators of two multinomial probability vectors under the constraint that they are likelihood ratio ordered. We extend this estimation approach to the case of two univariate distributions and show strong consistency of the estimators. We also derive and study the asymptotic distribution of the likelihood ratio statistic for testing the equality of two discrete probability distributions against the alternative that one distribution is greater than the other in the likelihood ratio ordering sense. Finally, we examine a data set pertaining to average daily insulin dose from the Boston Collaborative Drug Surveillance Program and compare our testing procedure to testing procedures for other stochastic orderings."], ["Modeling Lifetime Data with Application to Fatigue Models", "A new model for the analysis of lifetime data in the presence of a covariate is derived based on physical and statistical considerations. The model depends on five parameters that have clear physical interpretations. Two methods for the estimation of the parameters and of the quantiles are presented: one based on the order statistics and the other a regression estimator. The quantile estimators as well as the estimators of four of the five parameters are given in closed form. The fifth parameter can be estimated independently of the other parameters using either a closed form or a numerically simple algorithm. A simulation study shows that the regression estimators are better for estimating the parameters but the order statistics estimators perform better for the estimation of low quantiles. The methodology is also illustrated by an example of a real life data."], [null, null], ["Testing Homogeneity of Uniform Scale Distributions against Two-Sided and One-Sided Alternatives", null], ["Diagnostics for Linearization Confidence Intervals in Nonlinear Regression", null], ["Estimators of Odds Ratio Regression Parameters in Matched Case-Control Studies with Covariate Measurement Error", "This article studies estimators of the odds ratio and odds ratio regression parameters in finely matched case-control studies containing a binary exposure of primary interest and subject-specific covariates that are subject to measurement error. A retrospective logistic regression model for the binary exposure variable is used. The effect of measurement errors on the conditional maximum likelihood estimator is determined. Three alternatives are considered: bias-corrected, functional, and \u201ctransformation\u201d estimators. The asymptotic and small-sample properties of the three competitors are studied. The results are illustrated using data from a case-control study of diet and colon cancer."], ["Comparison of Regression Curves Using Quasi-Residuals", "We consider testing the equality of two regression functions using two independent samples. Three tests are proposed that are free of the restriction of having the same covariate values or sample sizes for both samples. Asymptotic distributions are given and results from a simulation study are presented that show the superior power properties of these tests over a competing test in a variety of cases, including the testing of hypotheses involving high-frequency curves when the design points for the two samples differ. It is also observed that the tests have good level properties when proper smoothing parameters are selected."], ["Simultaneous Confidence Bands for Linear Regression with Heteroscedastic Errors", "The Scheff\u00e9 method may be used to construct simultaneous confidence bands for a regression surface for the whole predictor space. When the bands need only hold for a subset of that space, previous authors have described how the bands can be appropriately narrowed while still maintaining the desired level of confidence. Data with heteroscedastic errors occur often, and unless some transformation is feasible, there is no obvious way to construct bands using the current methods. This article shows how to construct approximate simultaneous confidence bands when the errors are heteroscedastic and symmetric. The method works when the weights are known or unknown and have to be estimated. The region in which the bands must hold can be quite general and will work for any linear unbiased estimate of the regression surface. The method can even be extended to linear estimates with a small amount of bias such as nonparametric kernel regression smoothers."], ["An Alternative Definition of Finite-Sample Breakdown Point with Applications to Regression Model Estimators", "We propose an alternative definition of the finite-sample breakdown point. This breakdown point is invariant with respect to reparameterization and compatible with the Donoho-Huber breakdown point in linear regression situations. It also overcomes certain limitations of the definition proposed by Stromberg and Ruppert and can be used in a wide range of estimation problems. We investigate the breakdown properties of some nonlinear regression estimators. These results alert us to the danger of using familiar M estimators with data sets containing outliers and to the advantages of using estimators based on Hampel's proposal, such as S estimators."], ["Minimax Estimation of Proportions under Random Sample Size", null], ["Modeling the Drop-Out Mechanism in Repeated-Measures Studies", "Subjects often drop out of longitudinal studies prematurely, yielding unbalanced data with unequal numbers of measures for each subject. Modern software programs for handling unbalanced longitudinal data improve on methods that discard the incomplete cases by including all the data, but also yield biased inferences under plausible models for the drop-out process. This article discusses methods that simultaneously model the data and the drop-out process within a unified model-based framework. Models are classified into two broad classes\u2014random-coefficient selection models and random-coefficient pattern-mixture models\u2014depending on how the joint distribution of the data and drop-out mechanism is factored. Inference is likelihood-based, via maximum likelihood or Bayesian methods. A number of examples in the literature are placed in this framework, and possible extensions outlined. Data collection on the nature of the drop-out process is advocated to guide the choice of model. In cases where the drop-out mechanism is not well understood, sensitivity analyses are suggested to assess the effect on inferences about target quantities of alternative assumptions about the drop-out process."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Correction", null], ["Editorial Board Page", "This article has no abstract"], ["Editors' Report for 1994", null], ["Inference from a Deterministic Population Dynamics Model for Bowhead Whales", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Two-Stage Least Squares Estimation of Average Causal Effects in Models with Variable Treatment Intensity", "Two-stage least squares (TSLS) is widely used in econometrics to estimate parameters in systems of linear simultaneous equations and to solve problems of omitted-variables bias in single-equation estimation. We show here that TSLS can also be used to estimate the average causal effect of variable treatments such as drug dosage, hours of exam preparation, cigarette smoking, and years of schooling. The average causal effect in which we are interested is a conditional expectation of the difference between the outcomes of the treated and what these outcomes would have been in the absence of treatment. Given mild regularity assumptions, the probability limit of TSLS is a weighted average of per-unit average causal effects along the length of an appropriately defined causal response function. The weighting function is illustrated in an empirical example based on the relationship between schooling and earnings."], ["Problems with Instrumental Variables Estimation when the Correlation between the Instruments and the Endogenous Explanatory Variable is Weak", null], ["The Conditional Distribution of Excess Returns: An Empirical Analysis", "In this article we describe the cumulative distribution function of excess returns conditional on a broad set of predictors that summarize the state of the economy. We do so by estimating a sequence of conditional logit models over a grid of values of the response variable. Our method uncovers higher-order multidimensional structure that cannot be found by modeling only the first two moments of the distribution. We compare two approaches to modeling: one based on a conventional linear logit model and the other based on an additive logit. The second approach avoids the \u201ccurse of dimensionality\u201d problem of fully nonparametric methods while retaining both interpretability and the ability to let the data determine the shape of the relationship between the response variable and the predictors. We find that the additive logit fits better and reveals aspects of the data that remain undetected by the linear logit. The additive model retains its superiority even in out-of-sample prediction and portfolio selection performance, suggesting that this model captures genuine features of the data that seem to be important to guide investors' optimal portfolio choices."], ["Analysis of Censored Data from Fractionated Experiments: A Bayesian Approach", "Censored data arise naturally in industrial experiments whose observations are failure times or measurements with mixed continuous-categorical outcomes. When censored data are observed from a fractionated experiment, likelihood-based estimates quite often do not exist, especially when the opportunity for improvement is great. To circumvent this problem, we propose a Bayesian analysis strategy that has a straightforward implementation using the data augmentation and Monte Carlo EM algorithms. For nonregular designs with complex aliasing patterns, a modified analysis strategy is proposed that substantially reduces the computation needed for model selection. The proposed strategy is illustrated using data from three real industrial experiments. For these data sets, the analysis results are fairly insensitive to the specification of the prior."], ["On the Adjustment of Gross Flow Estimates for Classification Error with Application to Data from the Canadian Labour Force Survey", "Gross flows represent transition counts between a finite number of states for individuals in a population from one point in time to the next. Such flows are important for researchers and policy analysts; for example, gross labour flows for understanding labour market dynamics. Unfortunately, the observed flows are typically subject to classification error. As a result, the problem of adjusting observed flows for classification error has received considerable attention. Currently, three methods for adjustment of classification error are available. All these methods use the key assumption of independent classification errors (ICE) in conjunction with interview-reinterview data. We first give modifications to two of the methods that ensure that margins of the adjusted flow table agree with the published \u201cstocks\u201d without requiring a final margin adjustment. We then propose e-response contamination models and procedures for studying the robustness of ICE under different scenarios of departures from ICE applicable for all the methods. Our empirical results, based on data from the Canadian Labour Force Survey, show that for many scenarios the ICE assumption is fairly robust. There are, however, situations where this is not the case. We thus suggest that users apply the proposed procedures to check the robustness of ICE under scenarios relevant for their own applications. Finally, we provide valid chi-squared tests for modeling flow tables adjusted for classification error under ICE so that the adjusted flows could be further smoothed under the model. These tests are also illustrated using data from the Canadian Labour Force Survey."], ["A Random-Effects Model for Ordered Categorical Data", "This article presents a random-effects model for multivariate and grouped univariate ordered categorical data. The assumed family of distributions for the random effects adopts a wide variety of forms and shapes. The model's likelihood has a closed expression and can be computed without recourse to numerical integration or Gaussian quadrature. A bivariate and a grouped univariate example are used to illustrate the proposed model."], ["Visual Error Criteria for Qualitative Smoothing", "An important gap, between the classical mathematical theory and the practice and implementation of nonparametric curve estimation, is due to the fact that the usual norms on function spaces measure something different from what the eye can see visually in a graphical presentation. Mathematical error criteria that more closely follow \u201cvisual impression\u201d are developed and analyzed from both graphical and mathematical viewpoints. Examples from wavelet regression and kernel density estimation are considered."], ["Nonparametric Empirical Bayes Growth Curve Analysis", "Nonparametric and semiparametric regression have been suggested as alternatives to parametric models for growth curve analysis. In this article we demonstrate that empirical Bayes estimation can be used to improve linear smoothing estimates when multiple curves are available."], ["Nonparametric Likelihood Confidence Bands for a Distribution Function", null], ["Nonparametric Estimation of the Transformation in the Transform-Both-Sides Regression Model", "The transform-both-sides (TBS) regression model developed by Carroll and Ruppert is applicable when the relationship between the median response and the independent variables has been identified. Several different families of transformations, such as the Box-Cox power family, have been considered in the parametric approach to this model. In this article, we propose a nonparametric estimator of the transformation in the TBS model allowing general smooth monotonic transformations. Asymptotic properties of this estimator are discussed."], ["Adaptive Bandwidth Choice for Kernel Regression", "A data-based procedure is introduced for local bandwidth selection for kernel estimation of a regression function at a point. The estimated bandwidth is shown to be consistent and asymptotically normal as an estimator of the (asymptotic) optimal value for minimum mean square estimation. Simulation studies indicate satisfactory behavior of the new bandwidth estimator in finite samples. The findings are improvements over a global bandwidth estimator. The same methodology works for local linear regression and extends easily to weighted local polynomial fits."], ["Instrumental Variable Estimation in Binary Regression Measurement Error Models", "We describe two approaches to instrumental variable estimation in binary regression measurement error models. The methods entail constructing approximate mean models for the binary response as a function of the measured predictor, the instrument, and any covariates in the model. Estimates are obtained by exploiting relationships between regression parameters, just as in linear instrumental variable estimation. In the course of deriving the approximate mean models, we obtain an alternative characterization of instrumental variable estimation in linear measurement error models."], ["A Consistent Nonparametric Test of Symmetry in Linear Regression Models", null], ["Minorization Conditions and Convergence Rates for Markov Chain Monte Carlo", null], ["Blind Deconvolution via Sequential Imputations", null], ["Bayesian Density Estimation and Inference Using Mixtures", "We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation and are exemplified by special cases where data are modeled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior, and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models."], ["Reference Prior Bayesian Analysis for Normal Mean Products", null], ["Modeling Expert Opinion Arising as a Partial Probabilistic Specification", "Expert opinion is often sought with regard to unknowns in a decision-making setting. For a univariate unknown, \u03b8, our presumption is that such opinion is elicited as a partial probabilistic specification in the form of either probability assignments regarding the chance of \u03b8 falling in a fixed set of disjoint exhaustive intervals or selected quantiles for \u03b8. Treating such specification as \u201cdata,\u201d our focus is on the development of suitable probability densities for these data given the true \u03b8. In particular, we advocate a rich class of densities created by transformation of random mixtures of beta distributions. These densities become likelihoods when viewed as a function of \u03b8 given the data. We presume that a decision-maker (here a so-called supra Bayesian) presides over the opinion collection, offering his or her assessment as well. All of this opinion is synthesized using Bayes's theorem, resulting in the posterior distribution as the pooling mechanism. The models are applied to opinion collected regarding points per game for participants in the 1991 National Basketball Association championship basketball series."], ["Bayesian Inference for Stable Distributions", "Very little work on stable distribution parameter estimation and inference appears in the literature due to the nonexistence of the probability density function. This has led in particular to a dearth of Bayesian work in this area. But Bayesian computation via Markov chain Monte Carlo allows us to sample from the distribution of the parameters of the stable distributions, by exploiting a particular mathematical representation involving the stable density."], ["Computing Bayes Factors Using a Generalization of the Savage-Dickey Density Ratio", "We present a simple method for computing Bayes factors. The method derives from observing that in general, a Bayes factor can be written as the product of a quantity called the Savage-Dickey density ratio and a correction factor; both terms are easily estimated from posterior simulation. In some cases it is possible to do these computations without ever evaluating the likelihood."], ["The Influence of Variable Selection: A Bayesian Diagnostic Perspective", "Variable selection is ubiquitous in statistical practice. Forward selection, backwards elimination, or a combination thereof are among the most popular techniques. Statistical texts teach direct interpretation of coefficients after variable selection. For scientific purposes, the desired interpretation is unconditional on the variable selection. In this article an influence analysis of variable selection is performed from a Bayesian viewpoint. Variable selection is shown to be surprisingly influential. A new statistic is recommended for routine examination during variable selection."], ["Using Finitely Additive Probability: Uniform Distributions on the Natural Numbers", "In the usual, countably additive definition of probability, it is not possible to have a distribution giving equal probabilities to every one of the natural numbers. Yet such a distribution would be interesting and potentially useful. This article considers an approach to this problem based on finitely additive probability. We give a necessary and sufficient condition for when specifications of the probabilities of an arbitrary collection of subsets of a space \u03a9 can be extended to define a finitely additive probability on all the subsets of \u03a9. This is applied to probability statements modeling the uniform distribution on the natural numbers, using relative frequencies and residue classes to make precise notions of uniformity. Tight bounds are given on the possible values of the probability of an arbitrary set under both interpretations. These bounds are applied to several sets of interest."], ["Improved Exact Inference about Conditional Association in Three-Way Contingency Tables", null], [null, null], [null, null], ["Information Distinguishability with Application to Analysis of Failure Data", null], ["Survival Analysis Using a Scale Change Random Effects Model", "Frailty models are effective in broadening the class of survival models and inducing dependence in multivariate survival distributions. In proportional hazards, the random effect multiplies the hazard function. The scale-change model incorporates unobserved heterogeneity through a random effect that enters the baseline hazard function to change the time scale. We interpret this random effect as frailty, or other unobserved risks that create heterogeneity in the population. This model produces a wide range of shapes for univariate survival and hazard functions. We extend this model to multivariate survival data by assuming that members of a group share a common random effect. This structure induces association among the survival times in a group and provides alternative association structures to the proportional hazards frailty model. We present parametric and semiparametric estimation techniques and illustrate these methods with an example."], ["Efficiency and Power of Tests for Multiple Binary Outcomes", null], ["A Method for Calibrating False-Match Rates in Record Linkage", "Specifying a record-linkage procedure requires both (1) a method for measuring closeness of agreement between records, typically a scalar weight, and (2) a rule for deciding when to classify records as matches or nonmatches based on the weights. Here we outline a general strategy for the second problem, that is, for accurately estimating false-match rates for each possible cutoff weight. The strategy uses a model where the distribution of observed weights are viewed as a mixture of weights for true matches and weights for false matches. An EM algorithm for fitting mixtures of transformed-normal distributions is used to find posterior modes; associated posterior variability is due to uncertainty about specific normalizing transformations as well as uncertainty in the parameters of the mixture model, the latter being calculated using the SEM algorithm. This mixture-model calibration method is shown to perform well in an applied setting with census data. Further, a simulation experiment reveals that, across a wide variety of settings not satisfying the model's assumptions, the procedure is slightly conservative on average in the sense of overstating false-match rates, and the onesided confidence coverage (i.e., the proportion of times that these interval estimates cover or overstate the actual false-match rate) is very close to the nominal rate."], ["The Restricted EM Algorithm for Maximum Likelihood Estimation under Linear Restrictions on the Parameters", "The EM algorithm is one of the most powerful algorithms for obtaining maximum likelihood estimates for many incomplete-data problems. But when the parameters must satisfy a set of linear restrictions, the EM algorithm may be too complicated to apply directly. In this article we propose maximum likelihood estimation procedures under a set of linear restrictions for situations in which the EM algorithm could be used if there were no such restrictions on the parameters. We develop a modification to the EM algorithm, which we call the restricted EM algorithm, incorporating the linear restrictions on the parameters. This algorithm is easily updated by using the code for the complete data information matrix and the code for the usual EM algorithm. Major applications of the restricted EM algorithm are to construct likelihood ratio tests and profile likelihood confidence intervals. We illustrate the procedure with two models: a variance component model and a bivariate normal model."], ["Order-Restricted Inferences in Linear Regression", "Regression analysis constitutes a large portion of the statistical repertoire in applications. In cases where such analysis is used for exploratory purposes with no previous knowledge of the structure, one would not wish to impose any constraints on the problem. But in many applications we are interested in curve fitting with a simple parametric model to describe the structure of a system with some prior knowledge about the structure. An important example of this occurs when the experimenter has a strong belief that the regression function changes monotonically with some or all of the predictor variables in a region of interest. The analyses needed for statistical inferences under such constraints are nonstandard. Considering the present body of knowledge developed for unconstrained regression, it will be an enormous task to derive the analogs of even a small fraction of this for the restricted case. In this article we initiate the study with simple linear regression on a single variable. The estimators of the regression parameters may be intuitively obvious in this case, but, as discussed, very little else is."], ["Likelihood Ratio Methods for Monitoring Parameters of a Nested Random Effect Model", "In many practical situations the variance of a set of measurements can be attributed to several known sources of variability. For example, if several measurements of each item of a lot are taken, then one may need to deal not only with the within-item variability, but also with item-to-item within-lot and lot-to-lot components of variability. In such cases conventional control charts tend to produce an unacceptably high rate of false alarms and in general represent a rather weak diagnostic tool. This article shows how to build a control system based on likelihood ratio tests capable of monitoring the mean and variance components of a nested random effect model. The strong points and weaknesses of this approach are compared to those of competing methods, and some examples related to manufacturing of integrated circuits are discussed."], ["A Family of Bivariate Densities Constructed from Marginals", null], ["Upper and Lower Bound Distributions That Give Simultaneous Confidence Intervals for Quantiles", "We propose a new method for constructing parametric confidence intervals for quantiles of an unknown distribution. These confidence intervals are constructed so that the joint probability that all intervals simultaneously contain their respective percentiles is at least a preset value. Thus we may also use the set of either upper or lower confidence limits to define an upper (or lower) bound distribution function, which we call an upper (or lower) tolerance distribution because this distribution may also be used to construct tolerance intervals for the unknown distribution. We derive tolerance distributions with exact coverage probability for iid samples from distributions in the location-scale family. Tolerance distributions can also be used for best- and worst-case analyses, as we illustrate by considering bounds on the distribution of the time interval between the onset of infectiousness and development of detectable antibody in individuals newly infected with the human immunodeficiency virus (HIV), the virus that causes acquired immune deficiency syndrome (AIDS)."], ["Confidence Intervals for Autocorrelations Based on Cyclic Samples", "Clinger and Van Ness have shown how to sample a stochastic process in a periodic manner such that the autocorrelation function for that process can be estimated at all lags. We derive approximate confidence intervals for these estimates, focusing on low-order autoregressive moving average processes. Our methods are illustrated with an example from plant disease epidemiology."], ["Robust Estimation of Mean Squared Error of Small Area Estimators", null], ["Testing the Mean of Skewed Distributions", null], ["Bayes Factors", "It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Comment on Hadi and Simonoff", null], ["Response to Ryan's Comments", null], ["Correction", null], ["Editorial Board Page", "This article has no abstract"], ["New Paradigms for the Statistics Profession", null], ["Comparing the Contributions of Groups of Predictors: Which Outcomes Vary with Hospital Rather than Patient Characteristics?", "In a model, such as a logit regression model, we wish to compare the relative importance of two groups of predictors for various objectives. In the example that motivated this work, the model predicts patient outcomes during hospital stays, and we wish to measure the relative contribution of patient and hospital characteristics to the variation in outcomes among patients and among hospitals. This is done using the relative dispersion of patient and hospital contributions to the fitted outcomes. As is seen, this question is distinct from other common questions, including the quality of the overall fit, the degree to which the outcome is accurately predicted, the statistical significance of groups of predictors, and the correlations among and between groups of predictors. Relevant point estimates, confidence intervals, and hypothesis tests are developed. In the example, we examine three outcome measures and find that nearly all of the predictable variation in patient outcomes and most of the predictable variation in outcomes among hospitals reflects variation in patient characteristics rather than hospital characteristics; however, this is true in varying degrees for the three outcomes."], ["Optimal Sequential Screening Guidelines for Quantitative Risk Factors Measured with Error", "Any screening program based on a quantitative risk factor has the objective of identifying individuals at high risk of disease. When the risk factor varies within individuals over time or is subject to measurement error, repeat measurements should aid risk assessment. This article presents statistical guidelines for sequential screening based on measurements accumulated over time. The aim for each subject is to decide whether (a) risk is sufficiently low that screening can be stopped, or (b) risk is sufficiently high that intervention is indicated, or (c) further measurements should be obtained to improve risk assessment. The guidelines presented are optimal in a population, public health sense; that is, they are designed to give the maximal expected risk in the group identified for intervention, subject to constraints on that group's size and on the cost of screening (defined in terms of the average number of measurements per person required). An example concerning screening for cardiovascular disease in middle-aged men using diastolic blood pressure measurements is presented."], ["Modeling the Relationship of Survival to Longitudinal Data Measured with Error. Applications to Survival and CD4 Counts in Patients with AIDS", "A question that has received a great deal of attention in evaluating new treatments in acquired immune deficiency syndrome (AIDS) clinical trials is that of finding a good surrogate marker for clinical progression. The identification of such a marker may be useful in assessing the efficacy of new therapies in a shorter period. The number of CD4-lymphocyte counts has been proposed as such a potential marker for human immune virus (HIV) trials because of its observed correlation with clinical outcome. But to evaluate the role of CD4 counts as a potential surrogate marker, we must better understand the relationship of clinical outcome to an individual's CD4 count history over time. The Cox proportional hazards regression model is used to study the relationship between CD4 counts as a time-dependent covariate and survival. Because the CD4 counts are measured only periodically and with substantial measurement error and biological variation, standard methods for estimating the parameters in the Cox model by maximizing the partial likelihood are no longer appropriate. Instead, we propose a two-stage approach. In the first stage the longitudinal CD4 count data are modeled using a repeated measures random components model. In the second stage methods for estimating the parameters in a Cox model when the data are assumed to be of this form are derived. We also considered methods to account for missing data patterns. These methods are applied to CD4 data from a randomized clinical trial of AIDS patients where half of the patients were randomized to receive Zidovudine (ZDV) and the other half were randomized to receive a placebo. Although a strong correlation between CD4 counts and survival is demonstrated, we also show that CD4 counts may not serve as a useful surrogate marker for assessing treatments for this population of patients."], ["Bayesian Analysis of a Protocol for Sampling Preserved Tumor Specimens", "Intratumor DNA heterogeneity\u2014as assessed by flow-cytometric cellular DNA analysis\u2014is predictive of prognosis in many types of cancer. But some truly heterogeneous tumors apparently have a substantial chance of being misclassified as normal. One way to enhance the assay's sensitivity is to test multiple samples from each tumor specimen. This article describes an analysis of data from a study of endometrial cancer, with a view to establishing the number of samples needed to test to reliably assess heterogeneity. The analysis involves selection of a model for the data and estimation of test sensitivity and predictive value. We analyze the data from both Bayesian and frequentist perspectives. The data suggest that the degree of within-tumor correlation is relatively high; consequently, there is little to be gained from taking multiple samples per tumor."], ["Estimating Unknown Transition Times Using a Piecewise Nonlinear Mixed-Effects Model in Men with Prostate Cancer", "It may be clinically useful to know when prostate-specific antigen (PSA) levels first begin to rise rapidly and to determine if the natural history of PSA progression is different in men with locally confined prostate cancers compared to men with metastatic tumors. This article uses a nonlinear mixed-effects model to describe longitudinal changes in PSA in men before their prostate cancers were detected clinically. Repeated measurements of PSA are available for 18 subjects with a diagnosis of prostate cancer based on prostate biopsy. PSA measurements were determined on repeated frozen serum samples collected from subjects with at least 10.0 years and up to 25.6 years of observation before the cancer was detected. A piecewise model is used to describe this data. The model is linear long before the cancer was detected and exponential nearer the time the cancer was detected. The time at which the PSA levels change from linear to exponential PSA progression is unknown but can be estimated by including random terms that allow each subject to have his own transition time. The model also accounts for two groups of patients\u2014those with local or regional cancer and those with advanced cancer or whose cancer has metastasized. Various parameters are allowed to differ between these two groups. By backward elimination of statistically nonsignificant parameters, a model is found that adequately describes the data. The model represents a situation where local/regional and advanced/metastatic cancers have similar rates of PSA progression, but advanced/metastatic cancers are diagnosed later. Piecewise mixed-effects models may be useful in a variety of research settings where it is necessary to estimate the unknown time of an event."], ["A Split Questionnaire Survey Design", "This article develops a survey design where the questionnaire is split into components and individuals are administered the varying subsets of the components. A multiple imputation method for analyzing data from this design is developed, in which the imputations are created by random draws from the posterior predictive distribution of the missing parts, given the observed parts by using Gibbs sampling under a general location scale model. Results from two simulation studies that investigate the properties of the inferences using this design are reported. In the first study several random split questionnaire designs are imposed on the complete data from an existing survey collected using a long questionnaire, and the corresponding data elements are extracted to form split data sets. Inferences obtained using the complete data and the split data are then compared. This comparison suggests that little is lost, at least in the example considered, by administering only parts of the questionnaire to each sampled individual. The second simulation study reports on the investigation of the efficiency of the split questionnaire design and the robustness of the estimates to the distributional assumptions used to create imputations. In this study several complete and split data sets were generated under a variety of distributional assumptions, and the imputations for the split data sets were created assuming the normality of the distributions. The sampling properties of the point and interval estimates of the regression coefficient in a particular logistic regression model using both the complete and split data sets were compared. This comparison suggests that the loss in efficiency of the split questionnaire design decreases as the correlation among the variables that are within different parts increases. The proposed multiple imputation method seems to be sensitive to the skewness and relatively insensitive to the kurtosis, contrary to the assumed normality of the distribution for the observables."], ["An Evaluation of Population Projection Errors for Census Tracts", "Population projections are widely used in both the public and private sectors for planning, budgeting, and analysis. For these purposes, projections are often needed for small areas such as census tracts, zip code areas or traffic analysis zones. Population size, growth constraints, shifting boundaries, and data availability create special problems for small-area projections, however, and very little is known about their forecasting performance. In this article we evaluate the accuracy and bias of projections of total population and population by age group for census tracts in three counties in Florida. We use data from 1970 and 1980 and several simple extrapolation techniques to produce projections for 1990; we then compare these projections with 1990 census counts and evaluate the differences. For the total sample, we find mean absolute errors of 17%-20% for the three most accurate techniques for projecting total population and find no indication of overall bias. For individual age groups, mean absolute errors range from 20%-29%. We believe that this analysis provides valuable information for demographers, planners, marketers, and others who make extensive use of small-area projections."], ["An Exploratory Analysis of a Record of El Ni\u00f1o Events, 1800\u20131987", "This article describes an exploratory analysis of a record of major El Ni\u00f1o events covering the period 1800\u20131987. This record contains both the years during which the events occurred and a binary indication of their magnitude. The record is modeled as a marked point process in which the times of the events follow a renewal process and the sequence of binary marks follows a stationary first-order Markov chain."], ["Hazard Regression", "Linear splines and their tensor products are used to estimate the conditional log-hazard function based on possibly censored, positive response data and one or more covariates. An automatic procedure involving the maximum likelihood method, stepwise addition, stepwise deletion, and the Bayes Information Criterion is used to select the final model. The possible models contain proportional hazards models as a subclass, which makes it possible to diagnose departures from proportionality. Cubic splines and two additional log terms are incorporated into a similar model for the unconditional log-hazard function to allow for greater flexibility in the extreme tails. A user interface based on S is described."], ["Semiparametric Regression Functionals", "A regression method is developed for a general class of functionals. A semiparametric linear model is adopted, and the regression parameters are estimated by maximizing a profiled nonparametric or empirical likelihood based on a local estimate of the conditional distribution function. Simulated and real data examples are shown, including an application of quantile regression to censored survival data from a clinical trial for myeloma."], ["Analysis of Semiparametric Regression Models for Repeated Outcomes in the Presence of Missing Data", null], ["Semiparametric Efficiency in Multivariate Regression Models with Missing Data", null], ["Estimation of Linear and Nonlinear Errors-in-Variables Models Using Validation Data", "Consistent estimators for linear and nonlinear regression models with measurement errors in variables in the presence of validation data are proposed. The estimation procedures are based on least squares methods with regression functions replaced by wide-sense conditional expectation functions. The methods do not depend on distributional assumptions and are robust against the misspecification of a measurement error model. They are computationally and analytically simpler than semiparametric methods based on nonparametric regression or density functions."], ["Local Polynomial Kernel Regression for Generalized Linear Models and Quasi-Likelihood Functions", "We investigate the extension of the nonparametric regression technique of local polynomial fitting with a kernel weight to generalized linear models and quasi-likelihood contexts. In the ordinary regression case, local polynomial fitting has been seen to have several appealing features in terms of intuitive and mathematical simplicity. One noteworthy feature is the better performance near the boundaries compared to the traditional kernel regression estimators. These properties are shown to carry over to generalized linear model and quasi-likelihood settings. We also derive the asymptotic distributions of the proposed class of estimators that allow for straightforward interpretation and extensions of state-of-the-art bandwidth selection methods."], ["Consistent Variable Selection in Linear Models", null], ["Prospective Analysis of Logistic Case-Control Studies", "In a classical case-control study, Prentice and Pyke proposed to ignore the study design and instead base estimation and inference on a random sampling (i.e., prospective) formulation. We generalize this prospective formulation of case-control studies to include multiplicative models, stratification, missing data, measurement error, robustness, and other examples. The resulting estimators, which ignore the case-control study aspect and instead are based on a random-sampling formulation, are typically consistent for nonintercept parameters and are asymptotically normally distributed. We derive the resulting asymptotic covariance matrix of the parameter estimates. The covariance matrix obtained by ignoring the case-control sampling scheme and using prospective formulas instead is shown to be at worst asymptotically conservative and asymptotically correct in a variety of problems; a simple sufficient condition guaranteeing the latter is obtained."], ["The Theil-Sen Estimator with Doubly Censored Data and Applications to Astronomy", "The Theil-Sen estimator of the slope parameter in simple linear regression is extended to data with both the response and the covariate subject to censoring. Based on inverting a suitable version of Kendall's \u03c4 statistic, this estimator requires weak assumptions and is simple to compute, and a simple estimate of its asymptotic variance is obtained. A second extension of the Theil-Sen estimator, based on a direct estimation of the median of pairwise slopes, is given. These estimators are compared numerically with versions of Schmitt's estimator and applied to two data sets from the recent astronomical literature."], ["Survival Analysis with Median Regression Models", "The median is a simple and meaningful measure for the center of a long-tailed survival distribution. To examine the covariate effects on survival, a natural alternative to the usual mean regression model is to regress the median of the failure time variable or a transformation thereof on the covariates. In this article we propose semiparametric procedures to make inferences for such median regression models with possibly censored observations. Our proposals can be implemented efficiently using a simulated annealing algorithm. Numerical studies are conducted to show the advantage of the new procedures over some recently developed methods for the accelerated failure time model, a special type of mean regression models in the survival analysis. The proposals discussed in the article are illustrated with a lung cancer data set."], ["Properties of Hazard-Based Residuals and Implications in Model Diagnostics", "Model diagnostic procedures in failure time models using hazard-based residuals rely on the assumption that the residual vector closely resembles a random sample from a unit exponential distribution when the model holds. This article formally investigates the validity of this critical assumption by deriving and examining the properties of parametrically, semiparametrically, and nonparametrically estimated residuals for complete and right-censored data. The joint distribution of the residual vector is characterized, and the behavior of some tests for exponentiality when applied to the residuals is examined analytically and through Monte Carlo methods. Findings reveal that the critical assumption of approximate unit exponentiality of the residual vector may not be viable and, consequently, the model diagnostic procedures considered, which revolve on checking the approximate unit exponentiality of the residual vector (specifically, hazard plotting and the use of spacings and total-time-on-test statistics on the residual vector) may have serious defects. This is especially evident in situations where the failure time distribution is not exponential or when the residuals are obtained nonparametrically in the no-covariate model or semiparametrically in the Cox proportional hazards model."], ["Tests for Variation over Groups in Survival Data", "When survival data can be divided into natural groups, such as in a clinical trial with patients from a number of different institutions, the question arises whether effects are different for the different groups. For situations where one is concerned with a random pattern of variation over groups, this article proposes testing the null hypothesis of no variation using statistics based on the sums of martingale residuals over subjects within groups. These tests are closely related to the locally most powerful tests of Liang for testing for variation across a large number of strata. The tests are applied to data from a lung cancer trial conducted by the Eastern Cooperative Oncology Group."], [null, "The methods of optimum experimental design are applied to models in which the variance, as well as the mean, is a parametric function of explanatory variables. Extensions to standard optimality theory lead to designs when the parameters of both the mean and the variance functions, or the parameters of only one function, are of interest. The theory also applies whether the mean and variance are functions of the same variables or of different variables, although the mathematical foundations differ. The example studied is a second-order two-factor response surface for the mean with a parametric nonlinear variance function. The theory is used both for constructing designs and for checking optimality. A major potential for application is to experimental design in off-line quality control."], ["Multiple Bayes Factors for Testing Hypotheses", "Partial and multiple Bayes factors are introduced to obtain pairwise comparisons of hypotheses in a statistical experiment with a partition on the parameter space. Robust Bayesian analyses are performed by introducing suitable classes of priors and by calculating lower and upper bounds of Bayes factors and posterior probabilities. Classes of intuitively meaningful priors are introduced, including unimodal densities without the constraint of symmetry for the case of precise hypotheses. Procedures for the corresponding optimizations are specified, and examples are given."], ["Inference and Predictions from Poisson Point Processes Incorporating Expert Knowledge", "We present a Bayesian approach for inference and predictions from nonhomogeneous Poisson point processes. The novel feature of our approach is the use of \u201cexpert knowledge\u201d or \u201cengineering information\u201d on the mean value function of the process. We describe two scenarios from the field of reliability in which engineering information on the mean value function is available. The first scenario pertains to the prediction of software failures during the debugging phase. Here expert knowledge is provided by the published empirical experiences of software engineers involved with the testing and debugging of several software systems. The second scenario pertains to the prediction of defects in a rail segment for which expert knowledge is supplied by an engineering model."], ["The Multiprocess Dynamic Poisson Model", "This article develops the multiprocess dynamic Poisson model for estimating and forecasting a Poisson random variable with a timevarying parameter. Its characteristics are similar to the multiprocess dynamic linear model of Harrison and Stevens. Its precision increases when the parameter remains unchanged, it reacts quickly to real parameter changes, and it is not sensitive to outliers. But the observation distribution is Poisson instead of normal, so the gamma conjugate family is used. Perturbation distributions and observation error distributions are not required, because the extrapolated conditional parameter distributions and conditional observations distributions are found directly in the gamma conjugate family. The conditional posterior distribution is found by Bayes's theorem. The theorem of Pena and Guttman about the optimal condensing of the mixture of normal distributions to a single normal distribution in terms of minimizing Kullbeck-Liebler distance is generalized to the optimal condensing of the mixture of twodimensional exponential family members to a single member. The model uses the optimal condensing for the gamma family. Simulation results show that this gives a substantial improvement."], ["Regeneration in Markov Chain Samplers", "Markov chain sampling has recently received considerable attention, in particular in the context of Bayesian computation and maximum likelihood estimation. This article discusses the use of Markov chain splitting, originally developed for the theoretical analysis of general state-space Markov chains, to introduce regeneration into Markov chain samplers. This allows the use of regenerative methods for analyzing the output of these samplers and can provide a useful diagnostic of sampler performance. The approach is applied to several samplers, including certain Metropolis samplers that can be used on their own or in hybrid samplers, and is illustrated in several examples."], ["Monte Carlo EM Estimation for Time Series Models Involving Counts", null], ["Estimation and Testing for Unit Roots in a Partially Nonstationary Vector Autoregressive Moving Average Model", "The Gaussian estimation of a partially nonstationary autoregressive model and the related issue of testing for cointegration using the likelihood ratio test procedure have been considered by others. In this article we extend the Gaussian estimation procedure to partially nonstationary autoregressive moving average models and derive the asymptotic properties of the full-rank and reduced-rank Gaussian estimators. Based on these results, the asymptotic distribution of the likelihood ratio statistic for testing the number of unit roots is obtained. A numerical example based on three U.S. interest rate series is used to illustrate the estimation and testing procedures. The finite-sample properties of the estimation and likelihood ratio test procedures are examined through a small simulation study."], ["Unit Root Tests in ARMA Models with Data-Dependent Methods for the Selection of the Truncation Lag", "We analyze the choice of the truncation lag in the context of the Said-Dickey test for the presence of a unit root in a general autoregressive moving average model. It is shown that a deterministic relationship between the truncation lag and the sample size is dominated by data-dependent rules that take sample information into account. In particular, we study data-dependent rules that are not constrained to satisfy the lower bound condition imposed by Said-Dickey. Akaike's information criterion falls into this category. The analytical properties of the truncation lag selected according to a class of information criteria are compared to those based on sequential testing for the significance of coefficients on additional lags. The asymptotic properties of the unit root test under various methods for selecting the truncation lag are analyzed, and simulations are used to show their distinctive behavior in finite samples. Our results favor methods based on sequential tests over those based on information criteria, because the former show less size distortions and have comparable power."], ["Exact Maximum Likelihood Estimation of Stationary Vector ARMA Models", "The problems of evaluating and subsequently maximizing the exact likelihood function of vector autoregressive moving average (ARMA) models are considered separately. A new and efficient procedure for evaluating the exact likelihood function is presented. This method puts together a set of useful features that can only be found separately in currently available algorithms. A procedure for maximizing the exact likelihood function, which takes full advantage of the properties offered by the evaluation algorithm, is also considered. Combining these two procedures, a new algorithm for exact maximum likelihood estimation of vector ARMA models is obtained. Comparisons with existing procedures, in terms of both analytical arguments and a numerical example, are given to show that the new estimation algorithm performs at least as well as existing ones, and that relevant real situations occur in which it does better."], ["Supplementing the Intent-to-Treat Analysis: Accounting for Covariates Observed Postrandomization in Clinical Trials", "A problem that arises frequently in the analysis of clinical trials is accounting for covariates observed postrandomization. Two examples are patient compliance measured through pill counts and the dose of a concomitant medication. Typically these are measured concurrently with the primary endpoint at the regularly scheduled follow-ups during the course of the study. To account for these effects, the confounders are sometimes introduced as time-dependent covariates into the statistical model. But there are conceptual and statistical difficulties with this approach. In this article we adopt the view that because the confounder is observed postrandomization, it must be considered as an endpoint in its own right. A seemingly unrelated regression (SUR) model is proposed to relate the two sets of repeated measures to important explanatory variables, and a vector autoregressive moving average (ARMA) time series model is used to characterize the disturbance terms within and across the two series. This accommodates the evolving relationship between the two variables and results in a parsimonious structure in the joint covariance matrix. Maximum likelihood estimation is applied using a Fisher scoring algorithm. Inference in this model is then performed by considering hypotheses on the primary endpoint conditionally on some behavior for the confounder. For example, if the two treatment groups had received an equivalent amount of concomitant medication, is there a significant difference in the primary endpoint? The methodology is illustrated with an example."], ["Filtering and Smoothing via Estimating Functions", "We consider the problem of filtering and smoothing in state-space models, which include nonlinear and non-Gaussian models. We do not make any distributional assumptions about the processes involved. Our approach to these problems is based on the theory of estimating functions. Filter and smoother are obtained as solutions of estimating equations that are optimal in appropriate classes. We illustrate our procedures by simulation studies of a model where the observational variance depends on the state and a binomial logit model with a covariate. In non-Gaussian cases, procedures based on estimating equations often perform considerably better than the existing semiparametric procedures."], ["Estimation of a Stationary Markov Chain", null], ["Robust Likelihoods", "Strong theoretical arguments imply that for directly representing and interpreting statistical data as evidence, the proper vehicle is the likelihood function. These arguments have had limited impact on statistical practice. One reason for this is that likelihood functions are explicitly model dependent and thus appear to be inherently nonrobust. In this article we examine the concept of robustness as it relates to likelihood functions. We note five ways that likelihood functions can be used to represent and interpret statistical data as evidence. These various uses suggest corresponding senses in which one likelihood function can approximate another, and these in turn suggest different senses in which a likelihood function can be \u201crobust.\u201d We establish some general relationships among these senses of robustness, and examine two general techniques for producing robust likelihoods."], ["REML Estimation of Covariance Matrices with Restricted Parameter Spaces", null], ["The Behavior of the Stahel-Donoho Robust Multivariate Estimator", null], ["A Score Test against One-Sided Alternatives", null], [null, null], ["The P-P Plot as a Method for Comparing Treatment Effects", "This article examines the use of the probability-probability plot (p-p plot) as a method for comparing treatment effects. To begin in the context of three examples the p-p plot is contrasted with the quantile-quantile plot (q-q plot), which is an alternative means of describing treatment effects. In these examples it is shown that p-p plots representing different experimental conditions or patient populations allow scale-invariant comparisons of treatment effects but q-q plots do not; that the presentation of the treatment effect by the p-p plot is not obscured by outliers, whereas it may be in the q-q plot; and that the p-p plot encompasses information in the control distributions that is important for the assessment of treatment effects but that is not incorporated in the q-q plot. Theoretical considerations are presented that show that under appropriate assumptions, the p-p plot is a maximal invariant and contains all the information necessary to make scale-invariant comparisons of treatment effects. Further, statistical methods for assessing patterns observed in the p-p plots are presented and illustrated in two examples."], ["Simultaneous Confidence Intervals and Sample Size Determination for Multinomial Proportions", "Simultaneous confidence interval procedures for multinomial proportions are used in many areas of science. In this article two new simultaneous confidence interval procedures are introduced. Numerical results are presented to evaluate these procedures and compare their performance with established methods that have been used in statistical literature. From the results presented in this article, it is evident that the new procedures are more accurate than the established ones, where the accuracy of the procedure is measured by the volume of the confidence region corresponding to the nominal coverage probability and the probability of coverage it achieves. In the sample size determination problem, the new procedures provide a sizable amount of savings as compared to the procedures that have been used in many applications. Because both procedures performed equally well, the procedure that requires the least amount of computing time is recommended."], ["A Stepwise Resampling Method of Multiple Hypothesis Testing", null], ["Optimal and Robust Strategies for Cluster Sampling", "This article extends Royall's (1992) results on optimal and robust sampling strategies to cluster sampling. It also gives the lower bound on the model-based variances of best linear unbiased predictors of finite population totals under certain classes of superpopulation models."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"]]}