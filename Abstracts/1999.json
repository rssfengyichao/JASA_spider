{"1999": [[null, null], ["Multicriterion Decision Merging: Competitive Development of an Aboriginal Whaling Management Procedure", "International Whaling Commission management of aboriginal subsistence whaling will eventually use an aboriginal whaling management procedure (AWMP) chosen from a collection of candidate procedures after grueling simulation testing. An AWMP is a fully automatic algorithm designed to operate on the results of an assessment (i.e., a statistical estimation problem relying on sparse series of whale abundance data) to produce a catch limit in each year of real or simulated management. An AWMP should, as much as possible, meet the conflicting objectives of low population risk, high satisfaction of needed catch, and high rate of population recovery. The choice of the best procedure falls naturally in the multicriterion decision making framework, because one of several candidates must be chosen on the basis of high-dimensional simulated performance summaries over a wide range of assumptions about whales and whaling. However, standard multicriterion decision making methods are impractical and unsatisfying for this problem. A method is developed to merge competing procedures into a new procedure that is an admissible Bayes rule. The approach is constructive rather than selective, meaning that it is not intended to produce an automatic winner, but rather a promising new candidate. This merging approach allows the best performance aspects of competing procedures to be combined. Ideally, and in examples shown, the newly constructed procedure outperforms all previous candidates. The approach also permits tuning of a single procedure to enhance performance or to more closely reflect design goals, without a simulation-intensive search over the tuning parameter space. These methods are generalizable to a larger class of decision problems."], ["Modeling Epidemiologic Typing Data and Likelihood Inference for Disease Spread", "A model for epidemiologic typing data is introduced, and likelihood ratio methods are developed for evaluating these data as evidence about disease spread. The observed data consist of microorganism subtypes from an index case of infectious disease, cases clustered with the index case, and a reference sample. The likelihood methods are evaluated via probabilities of observing epidemiologic subtypes that represent strong and sometimes misleading evidence favoring one hypothesis over another. A general bound is identified for the probability of observing strong evidence favoring a close epidemiologic relationship between the index and cluster cases vis-\u00e0-vis no relationship when in fact there is none. The advantages of this approach versus alternate approaches to measuring the strength of typing evidence are discussed."], ["A Method-of-Moments Estimation Procedure for Categorical Quality-of-Life Data with Nonignorable Missingness", "Quality-of-life outcomes collected during clinical trials often have considerable amounts of missing data, which, if not appropriately accounted for, may lead to bias in inferences. We introduce a method-of-moments (MM) estimating procedure for a model designed to handle nonignorable missingness arising in categorical data measured on independent populations. The missingness mechanism is assumed to be the same across the populations. We derive necessary and sufficient conditions for the identifiability of the model and fit the model to quality-of-life data collected as part of a breast cancer clinical trial. We compare the MM estimator to the maximum likelihood estimator in a simulation study."], ["Comparison of Partially Measured Latent Traits across Nominal Subgroups", "This article presents a method for estimating the subgroup distributions of a latent trait measured as a normal variate in the population. This problem occurs in large-scale assessments, such as the National Assessment of Educational Progress, the National Adult Literacy Survey, and other programs where proficiencies are estimated via marginal maximum likelihood through a model based on item-response theory. Heretofore, estimates of subgroup means were often estimated using ad hoc assumptions about within-group distributions, which conflicted with concurrent assumptions about normality of the population distributions. The method presented here removes the conflict, using consistent distributional assumptions for population and subgroup estimates."], ["An Evaluation of California's Inmate Classification System Using a Generalized Regression Discontinuity Design", "Published studies using the regression discontinuity design have been limited to cases in which linear regression is applied to a categorical treatment indicator and an equal interval outcome. This is unnecessarily narrow. We show here how a generalization the usual regression discontinuity design can be applied in a wider range of situations. We focus on the use of categorical treatment and response variables, but we also consider the more general case of any regression relationship. We also show how a resampling sensitivity analysis may be used to address the credibility of the assumed assignment process. The broader formulation is applied to an evaluation of California's inmate classification system, which is used to allocate prisoners to different kinds of confinement."], ["Causal Effects in Nonexperimental Studies: Reevaluating the Evaluation of Training Programs", "This article uses propensity score methods to estimate the treatment impact of the National Supported Work (NSW) Demonstration, a labor training program, on postintervention earnings. We use data from Lalonde's evaluation of nonexperimental methods that combine the treated units from a randomized evaluation of the NSW with nonexperimental comparison units drawn from survey datasets. We apply propensity score methods to this composite dataset and demonstrate that, relative to the estimators that Lalonde evaluates, propensity score estimates of the treatment impact are much closer to the experimental benchmark estimate. Propensity score methods assume that the variables associated with assignment to treatment are observed (referred to as ignorable treatment assignment, or selection on observables). Even under this assumption, it is difficult to control for differences between the treatment and comparison groups when they are dissimilar and when there are many preintervention variables. The estimated propensity score (the probability of assignment to treatment, conditional on preintervention variables) summarizes the preintervention variables. This offers a diagnostic on the comparability of the treatment and comparison groups, because one has only to compare the estimated propensity score across the two groups. We discuss several methods (such as stratification and matching) that use the propensity score to estimate the treatment impact. When the range of estimated propensity scores of the treatment and comparison groups overlap, these methods can estimate the treatment impact for the treatment group. A sensitivity analysis shows that our estimates are not sensitive to the specification of the estimated propensity score, but are sensitive to the assumption of selection on observables. We conclude that when the treatment and comparison groups overlap, and when the variables determining assignment to treatment are observed, these methods provide a means to estimate the treatment impact. Even though propensity score methods are not always applicable, they offer a diagnostic on the quality of nonexperimental comparison groups in terms of observable preintervention variables."], ["Account-Level Modeling for Trade Promotion: An Application of a Constrained Parameter Hierarchical Model", "We consider the problem of utilizing data at the retail/market level on sales and marketing mix variables to help manufacturers optimize the allocation of trade promotional budgets across areas. Major consumer packaged goods manufacturers budget at least one-half of their total marketing expenses to trade promotions. Trade promotional deals are designed to encourage retailers to promote products by temporarily reducing the price, putting them in in-store displays, or advertising in local media. A profit-based trade promotional allocation system will require estimates of the responsiveness of sales at each retailer to a given promotion. A major barrier to the use of retailer data is the proliferation of incorrectly signed coefficients in standard least squares analyses. Even more sophisticated adaptive shrinkage methods will not remove the problem of improper signs. We propose a hierarchical model to modeling retailer response that uses a first-stage prior with inequality constraints on the regression coefficients. We demonstrate the usefulness of our modeling approach with data on more than 75 retailers. We find substantial profit opportunities from our response-based promotional allocation scheme over and above what might be achieved by a standard volume-oriented allocation scheme."], ["Hierarchical Bayes Estimation of Unemployment Rates for the States of the U.S.", "Under a federal-state cooperative program, the U.S. Bureau of Labor Statistics (BLS) publishes monthly unemployment rate estimates for its 50 states and the District of Columbia. The primary source of data for this estimation problem is the Current Population Survey (CPS). However, the CPS state unemployment rate estimates are unreliable, because the survey provides relatively few observations per state. Various federal agencies use state-level unemployment rate estimates for policy making and fund allocation. Thus it is important to improve on the CPS estimates. For this, we propose a hierarchical Bayes (HB) method using a time series generalization of a widely used cross-sectional model in small-area estimation. The proposed method is compared in detail with the corresponding HB method, which uses the HB analog of the well-known Fay-Herriot cross-sectional model. A third model based on a time series approach to repetitive surveys is found to be very hard to implement for these data; the resulting estimates are very unstable and not meaningful. If we ignore some important factors from this model, then the reduced model can be fit, but the resulting model is found to be less than adequate. Gibbs sampling is used to obtain the posterior means and variances of the state unemployment rates. Based on some diagnostic tools recently developed for hierarchical models, our proposed model emerges as the best. The coefficients of variation of the proposed HB estimates are considerably lower than those of the rival estimates."], ["Evaluation and Comparison of EEG Traces: Latent Structure in Nonstationary Time Series", "We explore and illustrate the use of time series decomposition methods for evaluating and comparing latent structure in nonstationary electroencephalographic (EEG) traces obtained from depressed patients during brain seizures induced as part of electroconvulsive therapy (ECT). Analysis of the patterns of change over time in the frequency structure of such EEG data provides insight into the neurophysiological mechanisms of action of this effective but poorly understood antidepressant treatment, and allows clinicians to modify ECT treatments to optimize therapeutic benefits while minimizing associated side effects. Our work has introduced new methods of time-frequency analysis of EEG series that identify the complete pattern of time evolution of frequency structure over the course of a seizure, and usefully assist in these scientific and clinical studies. New methods of decomposition of flexible dynamic models provide time domain decompositions of individual EEG series into collections of latent components in different frequency bands. This allows us to explore ECT seizure characteristics via inferences on the time-varying parameters that characterize these latent components, and to relate differences in such characteristics across seizures to differences in the therapeutic effectiveness and cognitive side effects of those seizures. This article discusses the scientific context and problems, development of nonstationary time series models and new methods of decomposition to explore time-frequency structure, and aspects of model fitting and analysis. We include applied studies on two datasets from recent clinical ECT studies. One is an initial illustrative analysis of a single EEG trace, the second compares the EEG data recorded during two types of ECT treatment that differ in therapeutic effectiveness and cognitive side effects. The uses of these models and time series decomposition methods in extracting and contrasting key features of the seizure underlying the EEG signals are highlighted. Through the use of these models we have quantified, for the first time, decreases in the dominant frequencies of low-frequency EEG components during ECT seizures. We have also identified preliminary evidence that such decreases are enhanced under the more effective ECTs at higher electrical dosages, a finding consistent with prior reports and the hypothesis that more effective forms of ECT are more effective in eliciting neurophysiological inhibitory processes."], ["Adjusting for Nonignorable Drop-Out Using Semiparametric Nonresponse Models", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["A Weighted Estimating Equation for Missing Covariate Data with Properties Similar to Maximum Likelihood", "In regression analysis, missing covariate data occurs often. A recent approach to analyzing such data is weighted estimating equations. With weighted estimating equations, the contribution to the estimating equation from a complete observation is weighted by the inverse probability of being observed. In this article we propose a weighted estimating equation that is almost identical to the maximum likelihood estimating equations. As such, we propose an EM-type algorithm to solve these weighted estimating equations. Although the weighted estimating equations are a special case of those proposed earlier by Robins et al., our EM-type algorithm to solve them is new. Similar to Robins and Ritov, we give the result that to obtain a consistent estimate of the regression parameters, either the missing-data mechanism or the distribution of the missing data given the observed data must be correctly specified. We compare the weighted estimating equations to maximum likelihood via two examples, a simulation and an asymptotic study."], ["Association-Marginal Modeling of Multivariate Categorical Responses: A Maximum Likelihood Approach", "Generalized log-linear models can be used to describe the association structure and/or the marginal distributions of multivariate categorical responses. We simultaneously model the association structure and marginal distributions using association-marginal (AM) models, which are specially formulated generalized log-linear models that combine two models: an association (A) model, which describes the association among all the responses; and a marginal (M) model, which describes the marginal distributions of the responses. Because the model's composite link function is not required to be invertible, a large class of models can be entertained and model specification is typically straightforward. We propose a \u201cmixed freedom/constraint\u201d parameterization that exploits the special structure of an AM model. Using this parameterization, maximum likelihood fitting is straightforward and typically feasible for large, sparse tables. When a parsimonious association model is used, the size of the fitting problem is substantially reduced, and some of the problems associated with sampling O's are avoided. We compare the asymptotic behavior of AM model parameter estimators assuming product-multinomial and Poisson sampling. For computational convenience, the product-multinomial variances are obtained by adjusting the Poisson variances. We propose a conditional score statistic for AM model assessment. The proposed maximum likelihood methods are illustrated through an analysis of marijuana use data from five waves of the National Youth Survey."], ["A New Skewed Link Model for Dichotomous Quantal Response Data", null], ["Dimension Reduction in Binary Response Regression", "The idea of dimension reduction without loss of information can be quite helpful for guiding the construction of summary plots in regression without requiring a prespecified model. Focusing on the central subspace, we investigate such \u201csufficient\u201d dimension reduction in regressions with a binary response. Three existing methods\u2014sliced inverse regression, principal Hessian direction, and sliced average variance estimation\u2014and one new method\u2014difference of covariances\u2014are studied for their ability to estimate the central subspace and produce sufficient summary plots. Combining these numerical methods with the graphical methods proposed earlier by Cook leads to a novel paradigm for the analysis of binary response regressions."], ["Hypothesis Testing in Time Series via the Empirical Characteristic Function: A Generalized Spectral Density Approach", "The standardized spectral density completely describes serial dependence of a Gaussian process. For non-Gaussian processes, however, it may become an inappropriate analytic tool, because it misses the nonlinear processes with zero autocorrelation. By generalizing the concept of the standardized spectral density, I propose a new spectral tool suitable for both linear and nonlinear time series analysis. The generalized spectral density is indexed by frequency and a pair of auxiliary parameters. It is well defined for both continuous and discrete random variables, and requires no moment condition. Introduction of the auxiliary parameters renders the spectrum able to capture all pairwise dependencies, including those with zero autocorrelation. The standardized spectral density can be derived by properly differentiating the generalized spectral density with respect to the auxiliary parameters at the origin. The consistency of a class of Parzen's kernel-type estimators for the generalized spectral density is established, and their optimal convergence rates are derived using the integrated mean squared error criterion. A data-dependent asymptotically optimal bandwidth (or lag order) is introduced. The kernel estimators and their derivatives are applied to construct a class of asymptotically one-sided N(0, 1) tests for generic serial dependence and hypotheses on various specific aspects of serial dependence. The latter include serial uncorrelatedness, martingale, conditional homoscedasticity, conditional symmetry, and conditional homokurtosis. All of the proposed tests, which include Hong's spectral density test for serial correlation, can be derived from a unified framework. An empirical application to Deutschemark exchange rates highlights the approach."], ["Conditional Regression Analysis for Recurrence Time Data", "Recurrence time data can be regarded as a specific type of correlated survival data in which recurrent event times of a subject are stochastically ordered. Given the ordinal nature of recurrence times, this article focuses on conditional regression analysis. A semiparametric hazards model, including the structural and episode-specific parameters, is proposed for recurrence time data. In this model the order of episodes serves as the stratification variable. Estimation of the structural parameter can be constructed on the basis of all of the observed recurrence times. The structural parameter is estimated by the profile-likelihood approach. Although the structural parameter estimator is asymptotically normal, the episode-specific parameters may or may not be estimated consistently due to the sparseness of data for specific events. Examples are presented to illustrate the performance of the estimators of the structural and episode-specific parameters. An extension of the univariate recurrent events to the bivariate events, which occur repeatedly and sequentially, is discussed with an example."], ["An Improved Estimator of the Density Function at the Boundary", "We propose a new method of boundary correction for kernel density estimation. The technique is a kind of generalized reflection method involving reflecting a transformation of the data. The transformation depends on a pilot estimate of the logarithmic derivative of the density at the boundary. In simulations, the new method is seen to clearly outperform an earlier generalized reflection idea. It also has overall advantages over boundary kernel methods and a nonnegative adaptation thereof, although the latter are competitive in some situations. We also present the theory underlying the new methodology."], ["Bayes Factors and Approximations for Variance Component Models", null], ["Nonconjugate Bayesian Estimation of Covariance Matrices and its Use in Hierarchical Models", "The problem of estimating a covariance matrix in small samples has been considered by several authors following early work by Stein. This problem can be especially important in hierarchical models where the standard errors of fixed and random effects depend on estimation of the covariance matrix of the distribution of the random effects. We propose a set of hierarchical priors (HPs) for the covariance matrix that produce posterior shrinkage toward a specified structure\u2014here we examine shrinkage toward diagonality. We then address the computational difficulties raised by incorporating these priors, and nonconjugate priors in general, into hierarchical models. We apply a combination of approximation, Gibbs sampling (possibly with a Metropolis step), and importance reweighting to fit the models, and compare this hybrid approach to alternative Markov Chain Monte Carlo methods. Our investigation involves three alternative HPs. The first works with the spectral decomposition of the covariance matrix and produces both shrinkage of the eigenvalues toward each other and shrinkage of the rotation matrix toward the identity. The second produces shrinkage of the correlations toward 0, and the third uses a conjugate Wishart distribution to shrink toward diagonality. A simulation study shows that the first two HPs can be very effective in reducing small-sample risk, whereas the conjugate Wishart version sometimes performs very poorly. We evaluate the computational algorithm in the context of a normal nonlinear random-effects model and illustrate the methodology with a logistic random-effects model."], ["Parameter Expansion for Data Augmentation", "Viewing the observed data of a statistical model as incomplete and augmenting its missing parts are useful for clarifying concepts and central to the invention of two well-known statistical algorithms: expectation-maximization (EM) and data augmentation. Recently, Liu, Rubin, and Wu demonstrated that expanding the parameter space along with augmenting the missing data is useful for accelerating iterative computation in an EM algorithm. The main purpose of this article is to rigorously define a parameter expanded data augmentation (PX-DA) algorithm and to study its theoretical properties. The PX-DA is a special way of using auxiliary variables to accelerate Gibbs sampling algorithms and is closely related to reparameterization techniques. We obtain theoretical results concerning the convergence rate of the PX-DA algorithm and the choice of prior for the expansion parameter. To understand the role of the expansion parameter, we establish a new theory for iterative conditional sampling under the transformation group formulation, which generalizes the standard Gibbs sampler. Using the new theory, we show that the PX-DA algorithm with a Haar measure prior (often improper) for the expansion parameter is always proper and is optimal among a class of such algorithms including reparameterization."], ["On Single-Index Coefficient Regression Models", "In this article we investigate a class of single-index coefficient regression models under dependence. This includes many existing models, such as the smooth transition threshold autoregressive (STAR) model of Chan and Tong, the functional-coefficient autoregressive (FAR) model of Chen and Tsay, and the single-index model of Ichimura. Compared to the varying-coefficient model of Hastie and Tibshirani, our model can avoid the curse of dimensionality in multivariate nonparametric estimations. Another advantage of this model is that a threshold variable is chosen automatically. An estimation method is proposed, and the corresponding estimators are shown to be consistent and asymptotically normal. Some simulations and applications are also reported."], ["Regression Analysis, Nonlinear or Nonnormal: Simple and Accurate p Values from Likelihood Analysis", "We develop simple approximations for the p values to use with regression models having linear or nonlinear parameter structure and normal or nonnormal error distribution; computer iteration then gives confidence intervals. Both frequentist and Bayesian versions are given. The approximations are derived from recent developments in likelihood analysis and have third-order accuracy. Also, for very small and medium-sized samples, the accuracy can typically be high. The likelihood basis of the procedure seems to provide the grounds for this general accuracy. Examples are discussed, and simulations record the distributional accuracy."], ["Goodness of Fit and Related Inference Processes for Quantile Regression", null], ["1 \u2014 \u03b1 Equivariant Confidence Rules for Convex Alternatives are \u03b1/2\u2013Level Tests\u2014With Applications to the Multivariate Assessment of Bioequivalence", null], ["Discrimination with Many Variables", "Many statistical methods for discriminant analysis do not adapt well or easily to situations where the number of variables is large, possibly even exceeding the number of cases in the training set. We explore a variety of methods for providing robust identification of future samples in this situation. We develop a range of flexible Bayesian methods, and primarily a new hierarchical covariance compromise method, akin to regularized discriminant analysis. Although the methods are much more widely applicable, the motivating problem was that of discriminating between groups of samples on the basis of their near-infrared spectra. Here the ability of the Bayesian methods to take account of continuity of the spectra may be beneficial. The spectra may consist of absorbances or reflectances at as many as 1,000 wavelengths, and yet there may be only tens or hundreds of training samples in which both sample spectrum and group identity are known. Such problems arise in the food and pharmaceutical industries; for example, authentication of foods (e.g., detecting the adulteration of orange juice) and identification of pharmaceutical ingredients. Our illustrating example concerns the discrimination of 39 microbiological taxa and 8 aggregate genera. Simulations also illustrate the effectiveness of the hierarchical Bayes covariance method. We discuss a number of scoring rules, both local and global, for judging the fit of data to the Bayesian models, and adopt a cross-classificatory approach for estimating hyperparameters."], ["Classes of Nonseparable, Spatio-Temporal Stationary Covariance Functions", null], ["Detecting Common Signals in Multiple Time Series Using the Spectral Envelope", null], ["Rank-Based Autoregressive Order Identification", null], ["The Null Hypothesis Testing Controversy in Psychology", "A controversy concerning the usefulness of \u201cnull\u201d hypothesis tests in scientific inference has continued in articles within psychology since 1960 and has recently come to a head, with serious proposals offered for a test ban or something close to it. This article sketches some of the views of statistical theory and practice among different groups of psychologists, reviews a recent book offering multiple perspectives on null hypothesis tests, and argues that the debate within psychology is a symptom of serious incompleteness in the foundations of statistics."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["1999 Editorial Collaborators", null], ["Editorial Board Page", "This article has no abstract"], ["Bridging Different Eras in Sports", "This article addresses the problem of comparing abilities of players from different eras in professional sports. We study National Hockey League players, professional golfers, and Major League Baseball players from the perspectives of home run hitting and hitting for average. Within each sport, the careers of the players overlap to some extent. This network of overlaps, or bridges, is used to compare players whose careers took place in different eras. The goal is not to judge players relative to their contemporaries, but rather to compare all players directly. Hence the model that we use is a statistical time machine. We use additive models to estimate the innate ability of players, the effects of aging on performance, and the relative difficulty of each year within a sport. We measure each of these effects separated from the others. We use hierarchical models to model the distribution of players and specify separate distributions for each decade, thus allowing the \u201ctalent pool\u201d within each sport to change. We study the changing talent pool in each sport and address Gould's conjecture about the way in which populations change. Nonparametric aging functions allow us to estimate the league-wide average aging function. Hierarchical random curves allow for individuals to age differently from the average of athletes in that sport. We characterize players by their career profile rather than a one-number summary of their career."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Estimation of the Causal Effect of a Time-Varying Exposure on the Marginal Mean of a Repeated Binary Outcome", "We provide sufficient conditions for estimating from longitudinal data the causal effect of a time-dependent exposure or treatment on the marginal probability of response for a dichotomous outcome. We then show how one can estimate this effect under these conditions using the g-computation algorithm of Robins. We also derive the conditions under which some current approaches to the analysis of longitudinal data, such as the generalized estimating equations (GEE) approach of Zeger and Liang, the feedback model techniques of Liang and Zeger, and within-subject conditional methods, can provide valid tests and estimates of causal effects. We use our methods to estimate the causal effect of maternal stress on the marginal probability of a child's illness from the Mothers' Stress and Children's Morbidity data and compare our results with those previously obtained by Zeger and Liang using a GEE approach."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Retrospective Ascertainment of Recurrent Events: An Application to Time to Pregnancy", "Retrospectively ascertained data are common in many areas, including demography, epidemiology, and actuarial science. The main objective of this article is to study the effect of retrospective ascertainment on inference regarding recurrent events of time to pregnancy (TTP) data. For the particular TTP dataset that we consider, couples are included retrospectively based on their first pregnancy and then followed prospectively to a second pregnancy or to end of study. We consider a conditional model for the recurrent events data where the second TTP is included only if it is observed and a full model where the nonobserved second TTPs are included as suitably right censored. We furthermore consider two different approaches to modeling the dependencies of the recurrent events. A traditional frailty model, where the frailty enters the model as an unobserved covariate, and a marginal frailty model are applied. We find that efficiency is gained from including the second TTPs, with the full model being the most efficient. Further, the marginal frailty model is preferred over the traditional frailty model because estimates of covariate effects are easier to interpret and are more robust to changes in the frailty distribution."], ["Combining Data from Polymerase Chain Reaction DNA Typing Experiments: Applications to Sperm Typing Data", "The polymerase chain reaction (PCR) is a procedure by which the DNA in a single cell can be made to replicate many times in a test tube. By amplifying the DNA from individual sperm cells and typing the results, estimates of male recombination fractions can be made, which are valuable for creating genetic maps and locating regions of unusually intense crossover activity on the human genome. Because PCR typing results are subject to random error, stochastic models must be constructed to obtain accurate results. In practice, to obtain enough information to accurately estimate small recombination fractions, it is necessary to combine data from several PCR experiments. Stochastic models in common use assume that PCR error rates are constant across experiments. We show by analysis of a dataset that PCR error rates can vary considerably from experiment to experiment, and that models that fail to take this heterogeneity into account can produce biased estimators. We present two new estimators and show with simulation studies that they perform better than conventional methods under realistic conditions. These estimators may be appropriate whenever PCR data from several experiments are combined."], ["Pseudolikelihood Modeling of Multivariate Outcomes in Developmental Toxicology", "The primary goal of this article is to determine benchmark doses based on the ethylene glycol study, which comprises data from a developmental toxicity study in mice. Because the data involve a vector of malformation indicators, a flexible model for multivariate clustered data is required. An exponential family model is considered and pseudolikelihood-based inferential tools are proposed, hence avoiding excessive computational requirements."], ["A Signal Extraction Approach to Modeling Hormone Time Series with Pulses and a Changing Baseline", "Hormones serve as regulating signals for many biological processes. In recent years, it was determined that many hormones are secreted in a pulsatile manner and that the pulsatile secretion pattern, in addition to the absolute concentration level, is important in regulating biological processes. Consequently, it is necessary to characterize the latent secretion patterns from measurements of concentration levels. The characterization is complicated by the presence of a biological circadian rhythm. When hormone concentrations are plotted over time, the resultant time series usually exhibits occasional short rises superimposed on a slowly changing baseline. This is a result of a mixture of pulsatile secretions and a circadian rhythm. In this article we present a signal extraction approach to model simultaneously a slowly changing component and a pulsatile component of a time series. A smoothing spline is used to model the baseline, and a multiprocess dynamic linear model is used to model the pulsatile component. An additive structure is assumed, and both components are estimated simultaneously using a multiprocess Kalman filter. The unknown parameters are estimated by approximate maximum likelihood. The locations and amplitudes of the pulses are also estimated as posterior means via the multiprocess Kalman filter. Bayesian confidence intervals can be constructed for the baseline. This approach is found to be robust in simulated data and effective in modeling hormone time series."], ["Analyzing Multiple Emotions over Time by Autoregressive Negative Multinomial Regression Models", "This article presents an autoregressive random coefficient model with overdispersed negative multinomial marginal distributions for the analysis of heterogeneity and serial dependencies in multivariate longitudinal count data. The model structure consists of four components that take into account (a) individual difference effects, (b) random time effects, (c) multiple event categories, and (d) autodependencies. The last component is based on a stochastic integer-valued autoregressive process proposed by McKenzie. The model is applied to analyze count data from a panel diary study about the relationship between personality factors and emotion experiences. It is shown that there are large and stable individual personality differences in the incidence and duration of self-reported emotional experiences. Theoretical and clinical implications of this result are discussed."], ["Modeling Uncertainty in Latent Class Membership: A Case Study in Criminology", "Social scientists are commonly interested in relating a latent trait (e.g., criminal tendency) to measurable individual covariates (e.g., poor parenting) to understand what defines or perhaps causes the latent trait. In this article we develop an efficient and convenient method for answering such questions. The basic model presumes that two types of variables have been measured: Response variables (possibly longitudinal) that partially determine the latent class membership, and covariates or risk factors that we wish to relate to these latent class variables. The model assumes that these observable variables are conditionally independent, given the latent class variable. We use a mixture model for the joint distribution of the observables. We apply this model to a longitudinal dataset assembled as part of the Cambridge Study of Delinquent Development to test a fundamental theory of criminal development. This theory holds that crime is committed by two distinct groups within the population: Adolescent-limited offenders and life-course-persistent offenders. As these labels suggest, the two groups are distinguished by the longevity of their offending careers. The theory also predicts that life-course-persistent offenders are disproportionately comprised of individuals born with neurological deficits and reared by caregivers without the skills and resources to effectively socialize a difficult child."], ["Variable Selection and Function Estimation in Additive Nonparametric Regression Using a Data-Based Prior", "A hierarchical Bayesian approach is proposed for variable selection and function estimation in additive nonparametric Gaussian regression models and additive nonparametric binary regression models. The prior for each component function is an integrated Wiener process resulting in a posterior mean estimate that is a cubic smoothing spline. Each of the explanatory variables is allowed to be in or out of the model, and the regression functions are estimated by model averaging. To allow variable selection and model averaging, data-based priors are used for the smoothing parameter and the slope at 0 of each component function. A two-step Markov chain Monte Carlo method is used to efficiently obtain the data-based prior and to carry out variable selection and function estimation. It is shown by simulation that significant improvements in the function estimators can be obtained over an approach that estimates all the unknown functions simultaneously. The methodology is illustrated for a binary regression using heart attack data."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["SiZer for Exploration of Structures in Curves", "In the use of smoothing methods in data analysis, an important question is which observed features are \u201creally there,\u201d as opposed to being spurious sampling artifacts. An approach is described based on scale-space ideas originally developed in the computer vision literature. Assessment of Significant ZERo crossings of derivatives results in the SiZer map, a graphical device for display of significance of features with respect to both location and scale. Here \u201cscale\u201d means \u201clevel of resolution\u201d; that is, \u201cbandwidth.\u201d"], ["Nonparametric Methods for Doubly Truncated Data", "Truncated data play an important role in the statistical analysis of astronomical observations as well as in survival analysis. The motivating example for this article concerns a set of measurements on quasars in which there is double truncation. That is, the quasars are observed only if their luminosity occurs within a certain finite interval, bounded at both ends, with the interval varying for different observations. Nonparametric methods for the testing and estimation of doubly truncated data are developed. These methods extend some known techniques for data that are truncated only on one side, in particular Lynden-Bell's estimator and the truncated version of Kendall's tau statistic. However the kind of hazard function arguments that underlie the one-sided methods fail for two-sided truncation. Bootstrap and Markov Chain Monte Carlo techniques are used here in their place. Finally, we apply these techniques to the quasar data, answering a question about their long-term luminosity evolution."], ["Random Sieve Likelihood and General Regression Models", null], ["A Test for Global Maximum", "We give simple necessary and sufficient conditions for consistency and asymptotic optimality of a root to the likelihood equation. Based on the results, a large-sample test is proposed for detecting whether a given root is consistent and asymptotically efficient, a property often possessed by the global maximizer of the likelihood function. A number of examples, and the connection between the proposed test and the test of White for model misspecification, are discussed. Monte Carlo studies show that the test performs quite well when the sample size is large but may suffer the problem of overrejection with relatively small samples."], ["Yule's Association Paradox and Ignored Stratum Heterogeneity in Capture\u2014Recapture Studies", "This article explores a reparameterization of a 2 \u00d7 2 table. This reparameterization turns out to be useful in considering the consequences of amalgamating several 2 \u00d7 2 tables. A formal theorem is derived that permits characterization of Yule's association paradox. The same theorem also permits discussion of the likely direction of error incurred when using capture\u2013recapture methods to estimate the size of a population. This has implications for the \u201cwily trout\u201d argument in the census correction controversy."], ["Tests of Linear and Logarithmic Transformations for Integrated Processes", "This article proposes nonnested tests of linear and logarithmic transformations of integrated processes against each other, where the innovations of both series follow autoregressive processes. It is shown that the null distributions of the test statistics for both the linear and logarithmic transformations are nonstandard when the processes have no drift, whereas they are asymptotically normal when the processes have positive drift. Monte Carlo experiments and illustrative empirical examples are provided to show the practical usefulness of the tests in differentiating between linear and logarithmic transformations of a process in finite samples."], ["Testing the Fit of a Parametric Function", "General methods for testing the fit of a parametric function are proposed. The idea underlying each method is to \u201caccept\u201d the prescribed parametric model if and only if it is chosen by a model selection criterion. Several different selection criteria are considered, including one based on a modified version of the Akaike information criterion and others based on various score statistics. The tests have a connection with nonparametric smoothing because they use orthogonal series estimators to detect departures from a parametric model. An important aspect of the tests is that they can be applied in a wide variety of settings, including generalized linear models, spectral analysis, the goodness-of-fit problem, and longitudinal data analysis. Implementation using standard statistical software is straightforward. Asymptotic distribution theory for several test statistics is described, and the tests are shown to be consistent against essentially any alternative hypothesis. Simulations and a data example illustrate the usefulness of the tests."], ["On Testing of Individual Bioequivalence", null], ["Rank Estimation of Treatment Differences Based on Repeated Measurements Subject to Dependent Censoring", "In comparing the effectiveness of two treatments, suppose that for each patient repeated measurements of an outcome variable are taken at prespecified time points, but some observations may be missing due to the patient's dependent right censoring. In this article, a simple rank estimation procedure, constructed based on an artificial censoring technique, is proposed for the treatment differences over time without imposing a parametric structure on the dependence between the outcome measures and the censoring variable. Our method can be easily implemented and is illustrated by a dataset from an AIDS clinical trial."], ["Proportional Hazards Regression with Missing Covariates", "Nonparametric maximum likelihood (NPML) is used to estimate regression parameters in a proportional hazards regression model with missing covariates. The NPML estimator is shown to be consistent and asymptotically normally distributed under some conditions. EM type algorithms are applied to solve the maximization problem. Variance estimates of the regression parameters are obtained by a profile likelihood approach that uses EM-aided numerical differentiation. Simulation results indicate that the NPML estimates of the regression parameters are more efficient than the approximate partial likelihood estimates and estimates from complete-case analysis when missing covariates are missing completely at random, and that the proposed method corrects for bias when the missing covariates are missing at random."], ["A New Bayesian Model for Survival Data with a Surviving Fraction", "We consider Bayesian methods for right-censored survival data for populations with a surviving (cure) fraction. We propose a model that is quite different from the standard mixture model for cure rates. We provide a natural motivation and interpretation of the model and derive several novel properties of it. First, we show that the model has a proportional hazards structure, with the covariates depending naturally on the cure rate. Second, we derive several properties of the hazard function for the proposed model and establish mathematical relationships with the mixture model for cure rates. Prior elicitation is discussed in detail, and classes of noninformative and informative prior distributions are proposed. Several theoretical properties of the proposed priors and resulting posteriors are derived, and comparisons are made to the standard mixture model. A real dataset from a melanoma clinical trial is discussed in detail."], ["Bayesian Multiscale Models for Poisson Processes", "I introduce a class of Bayesian multiscale models (BMSM's) for one-dimensional inhomogeneous Poisson processes. The focus is on estimating the (discretized) intensity function underlying the process. Unlike the usual transform-based approach at the heart of most wavelet-based methods for Gaussian data, these BMSM's are constructed using recursive dyadic partitions (RDP's) within an entirely likelihood-based framework. Each RDP may be associated with a binary tree, and a new multiscale prior distribution is introduced for the unknown intensity through the placement of mixture distributions at each of the nodes of the tree. The concept of model mixing is then applied to a complete collection of such trees. In addition to allowing for the inclusion of full location/scale information in the model, this last step also is fundamental both in inducing stationarity in the prior distribution and in enabling a given intensity function to be approximated at the resolution of the data. Under squared-error loss, a closed-form recursive expression for the Bayes optimal estimator is derived, which makes computationally efficient implementation possible. The mixing parameters in the prior distribution can be interpreted as the \u201cfraction of homogeneity\u201d in the underlying intensity function at each scale, and I provide an empirical Bayes approach to eliciting their values, resulting in the ability to quantify multiscale structure in the data. The practical performance of the overall procedure is investigated through a series of simulations and illustrated using a real-data example from the field of high-energy astrophysics."], ["Inhomogeneous Prior Models for Image Reconstruction", "Over recent years, the use of homogeneous Gibbs prior models in image processing has become widely accepted. There has been, however, much discussion over precisely which models are most appropriate. For most applications, the simplest Gaussian model tends to oversmooth reconstructions, so it has been rejected in favor of various edge-preserving alternatives. We claim that the problem is not with the Gaussian family, but rather with the assumption of homogeneity. In this article we propose an inhomogeneous Gaussian random field as a general prior model for many image-processing applications. The simplicity of the Gaussian model allows rapid calculation, and the flexibility of the spatially varying prior parameter allows varying degrees of spatial smoothing. This approach is in the spirit of adaptive kernel density methods where only the choice of the variable window width is important. The analysis of real single-photon emission computed tomography data is used to illustrate the methods, and simulated data are used to demonstrate that the proposed procedures lead to more accurate reconstruction than edge-preserving homogeneous alternatives. The inhomogeneous model allows greater flexibility; small features are not masked by the smoothing, and constant regions obtain sufficient smoothing to remove the effects of noise."], ["The Masking Breakdown Point of Multivariate Outlier Identification Rules", null], [null, null], ["Nonparametric Two-Sample Comparisons of Changes on Ordinal Responses", null], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Data and Dogma in Public Policy", "Statistics play an important role in the affairs of state. The development over the last 70 years of national economic and product accounts, the creation in 1946 of a Council of Economic Advisers, and advancements in the collection and analysis of economic data have strengthened the ability of policy makers to understand the forces that affect the U.S. economy, anticipate economic changes, and take steps to minimize the adverse effects of market fluctuations. Although it is often correctly said that the business cycle has not been repealed, it is also true that in the last half-century measurement tools have helped produce a remarkable record of virtually uninterrupted economic growth in the United States. Yet problems remain, particularly because data in the social sciences are inherently more prone to error than data in the natural sciences. Perhaps most important, errors in measuring consumer prices can have enormous consequences for the economy and body politic, because the Consumer Price Index (CPI) is used to index Social Security benefits as well as a wide range of federal retirement payments and the tax code. Small differences in measurement have large consequences for federal finances. For example, the Advisory Commission to Study the Consumer Price Index established by the Senate Finance Committee in 1995 estimated that the CPI overstates changes in the cost of living by approximately 1.1 percentage points per year. Correcting this error would \u201csave\u201d the Federal government roughly $1 trillion over 12 years. This measurement error, along with other factors, has contributed to a huge transfer of resources from younger to older age cohorts. However, it is difficult to fully assess the consequences of this resource transfer, because data are not collected or well organized for this purpose. This article suggests that the creation of a Council of Social Advisers, akin to the President's Council of Economic Advisers, could improve the ability of the Federal government to better address the nation's social problems. The article also points out that despite repeated attempts over a period of many years to coordinate the collection, analysis, and use of statistical information by the U.S. government, the Federal statistical infrastructure remains poorly organized. The article also suggests that progress in addressing this continuing problem could be made through enactment of legislation to establish a 15-member commission to study the Federal Statistical System and make recommendations for its improvement. The author, along with other members of Congress, first introduced such legislation in Congress in 1996."], ["A Dynamic Model of Purchase Timing with Application to Direct Marketing", "Predicting changes in individual customer behavior is an important element for success in any direct marketing activity. In this article we develop a hierarchical Bayes model of customer interpurchase times based on the generalized gamma distribution. The model allows for both cross-sectional and temporal heterogeneity, with the latter introduced through the component mixture model dependent on lagged covariates. The model is applied to personal investment data to predict when and if a specific customer will likely increase time between purchases. This prediction can be used managerially as a signal for the firm to use some type of intervention to keep that customer."], ["Evaluation and Comparison of EEG Traces: Latent Structure in Nonstationary Time Series", "We explore and illustrate the use of time series decomposition methods for evaluating and comparing latent structure in nonstationary electroencephalographic (EEG) traces obtained from depressed patients during brain seizures induced as part of electroconvulsive therapy (ECT). Analysis of the patterns of change over time in the frequency structure of such EEG data provides insight into the neurophysiological mechanisms of action of this effective but poorly understood antidepressant treatment, and allows clinicians to modify ECT treatments to optimize therapeutic benefits while minimizing associated side effects. Our work has introduced new methods of time-frequency analysis of EEG series that identify the complete pattern of time evolution of frequency structure over the course of a seizure, and usefully assist in these scientific and clinical studies. New methods of decomposition of flexible dynamic models provide time domain decompositions of individual EEG series into collections of latent components in different frequency bands. This allows us to explore ECT seizure characteristics via inferences on the time-varying parameters that characterize these latent components, and to relate differences in such characteristics across seizures to differences in the therapeutic effectiveness and cognitive side effects of those seizures. This article discusses the scientific context and problems, development of nonstationary time series models and new methods of decomposition to explore time-frequency structure, and aspects of model fitting and analysis. We include applied studies on two datasets from recent clinical ECT studies. One is an initial illustrative analysis of a single EEG trace, the second compares the EEG data recorded during two types of ECT treatment that differ in therapeutic effectiveness and cognitive side effects. The uses of these models and time series decomposition methods in extracting and contrasting key features of the seizure underlying the EEG signals are highlighted. Through the use of these models we have quantified, for the first time, decreases in the dominant frequencies of low-frequency EEG components during ECT seizures. We have also identified preliminary evidence that such decreases are enhanced under the more effective ECTs at higher electrical dosages, a finding consistent with prior reports and the hypothesis that more effective forms of ECT are more effective in eliciting neurophysiological inhibitory processes."], ["Regression Depth", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["A Fast Procedure for Outlier Diagnostics in Large Regression Problems", "We propose a procedure for computing a fast approximation to regression estimates based on the minimization of a robust scale. The procedure can be applied with a large number of independent variables where the usual algorithms require an unfeasible or extremely costly computer time. Also, it can be incorporated in any high-breakdown estimation method and may improve it with just little additional computer time. The procedure minimizes the robust scale over a set of tentative parameter vectors estimated by least squares after eliminating a set of possible outliers, which are obtained as follows. We represent each observation by the vector of changes of the least squares forecasts of the observation when each of the data points is deleted. Then we obtain the sets of possible outliers as the extreme points in the principal components of these vectors, or as the set of points with large residuals. The good performance of the procedure allows identification of multiple outliers, avoiding masking effects. We investigate the procedure's efficiency for robust estimation and power as an outlier detection tool in a large real dataset and in a simulation study."], ["Prediction Intervals, Factor Analysis Models, and High-Dimensional Empirical Linear Prediction", null], ["Self-Consistency and Principal Component Analysis", null], ["Stepwise Confidence Intervals without Multiplicity Adjustment for Dose\u2014Response and Toxicity Studies", null], ["Exact Confidence Bounds for all Contrasts of Three or More Regression Lines", null], ["Multiple Confidence Sets Based on Stagewise Tests", "Recent years have seen suggested constructions of multiple confidence sets related to stagewise multiple tests by some authors. These methods are a type of mixture between test and confidence interval methods, because confidence interval statements are made only for some parameters, whereas test statements for fixed parameter values are made for the other parameters. In this article I define a concept\u2014confidence directional set\u2014giving a confidence bound for one parameter, which may depend on other parameters. Using this concept, one can construct multiple confidence sets, which are always confidence set statements and not test statements for fixed parameter values. The confidence sets correspond exactly to stagewise tests, which is theoretically appealing. Special examples of the general technique are given for the independent test statistic case and for comparison of a number of treatments to a control in the case of normally distributed observations with the same variance."], ["A Proportional Hazards Model for the Subdistribution of a Competing Risk", "With explanatory covariates, the standard analysis for competing risks data involves modeling the cause-specific hazard functions via a proportional hazards assumption. Unfortunately, the cause-specific hazard function does not have a direct interpretation in terms of survival probabilities for the particular failure type. In recent years many clinicians have begun using the cumulative incidence function, the marginal failure probabilities for a particular cause, which is intuitively appealing and more easily explained to the nonstatistician. The cumulative incidence is especially relevant in cost-effectiveness analyses in which the survival probabilities are needed to determine treatment utility. Previously, authors have considered methods for combining estimates of the cause-specific hazard functions under the proportional hazards formulation. However, these methods do not allow the analyst to directly assess the effect of a covariate on the marginal probability function. In this article we propose a novel semiparametric proportional hazards model for the subdistribution. Using the partial likelihood principle and weighting techniques, we derive estimation and inference procedures for the finite-dimensional regression parameter under a variety of censoring scenarios. We give a uniformly consistent estimator for the predicted cumulative incidence for an individual with certain covariates; confidence intervals and bands can be obtained analytically or with an easy-to-implement simulation technique. To contrast the two approaches, we analyze a dataset from a breast cancer clinical trial under both models."], ["A General Theory on Stochastic Curtailment for Censored Survival Data", "Stochastic curtailment is a valuable tool in monitoring long-term medical studies. Under this approach, one calculates the conditional power, which is the probability of rejecting the null hypothesis at the scheduled end of the study given the existing data at the interim analysis, along with certain speculation about the future data. The conditional power may be used to aid the decision to terminate a study prematurely or to extend a study beyond its originally planned duration. This article provides a formal and systematic investigation into the use of stochastic curtailment in the context of censored survival data. To enhance generality, we introduce a broad class of statistics that includes two-sample weighted log-rank statistics, as well as the partial likelihood score statistic for testing treatment difference with covariate adjustment under the proportional hazards model. We establish the weak convergence under both the null hypothesis and contiguous alternatives for this class of statistics when calculated repeatedly over the calendar time (i.e., time of interim analysis). Further, we derive the conditional distributions of these statistics calculated at the end of the study given all the data collected up to the interim look or given the statistics calculated at the interim look, and provide analytic expressions for the corresponding conditional powers. These results enable us to address several subtle issues involved in the definition and implementation of conditional power for censored survival data, especially when there is staggered patient entry with a potential time trend in the survival distribution, when the Gehan-type weight function is used, or when treatment is not independent of covariates. For randomized clinical trials, we show that very simple formulas can be used to calculate the conditional powers of the unweighted log-rank test (with or without covariate adjustment) under both the null and alternative hypotheses. Simulation studies demonstrate that the conditional powers for survival studies can be accurately evaluated through the proposed formulas even when the sample size is small. An illustration with data taken from a colon cancer study is provided."], ["Invariant Exponential Models Applied to Reliability Theory and Survival Analysis", "For a two-parameter exponential model with increasing failure rate (IFR) or decreasing failure rate (DFR) distributions necessary and sufficient conditions of the existence of a solution of the likelihood equations are given. Also, all of the scale-invariant two-parameter statistical models closed by raising to a power and by exponential tilting are introduced. The conditions of existence of a solution of the likelihood equations are studied for these invariant models, and the models are applied to obtain some uniformly most powerful unbiased tests of exponentially against alternatives in these models."], ["The Best Test of Exponentiality against Singly Truncated Normal Alternatives", "We show that the likelihood ratio test of exponentiality against singly truncated normal alternatives is the uniformly most powerful unbiased test and can be expressed in terms of the sampling coefficient of variation. This test is closely related to Greenwood's statistic for testing departures from the uniform distribution. We provide a way to approximate the critical points of the test, using saddlepoint methods, that gives a high degree of accuracy."], ["A Conditional Saddlepoint Approximation for Testing Problems", null], ["Default Bayes Factors for Nonnested Hypothesis Testing", null], ["Bayesian Morphology: Fast Unsupervised Bayesian Image Analysis", null], ["On Subsampling Estimators with Unknown Rate of Convergence", null], ["Iterated Transformation\u2013Kernel Density Estimation", "Transformation from a parametric family can improve the performance of kernel density estimation. In this article we give two data-driven estimators for the optimal transformation parameter. We demonstrate that multiple families of transformations can be used at the same time, and there can be benefits to iterating this process. The transformation scheme can be expected to first pick the right transformation family and then pick the optimal parameter. Insight as to the performance of the method comes from our analysis of a number of real datasets, two of which are included in this article. To illustrate the effectiveness and asymptotics of the transformation method, we also present results on one of the five target densities used in our simulation study. It is then proved that the Johnson family of transformations, when coupled with transformation-kernel density estimation, makes a wide variety of density shapes easier to estimate. The transformation method has overall better performance than the usual method and in many cases is much more effective."], ["Filtering via Simulation: Auxiliary Particle Filters", "This article analyses the recently suggested particle approach to filtering time series. We suggest that the algorithm is not robust to outliers for two reasons: The design of the simulators and the use of the discrete support to represent the sequentially updating prior distribution. Here we tackle the first of these problems."], ["Combining Classifiers via Discretization", "I consider a method for combining different classifiers to develop more effective classification rules. The proposed combined classifier, which turns out to be strongly consistent, is quite simple to use in real applications. It is also shown that this combined classifier is, (strongly) asymptotically, at least as good as any one of the individual classifiers. In addition, if one of the individual classifiers is already Bayes optimal (asymptotically), then so is the combined classifier."], ["Implementation of a Maximin Power Clustering Criterion to Select near Replicates for Regression Lack-of-Fit Tests", "In earlier work, we presented a maximin power clustering criterion to partition observations into groups of near replicates. Specifically, the criterion selects near replicate clusters for use with Christensen's tests for orthogonal between and within cluster lack of fit. This article further explores implementation of this clustering criterion. In particular, a methodology is developed to determine a collection of candidate groupings to which the maximin power criterion can be applied."], ["Pooled Mean Group Estimation of Dynamic Heterogeneous Panels", null], ["Extensions of Calibration Estimators in Survey Sampling", "Estimators from the family of calibration estimators are often used when auxiliary information about a population is available. By viewing calibration as an algebraic problem, this article extends the calibration technique to estimate population parameters other than totals and means, and also extends the technique to the case where there is no solution to the calibration equation. These extensions permit the development of estimators for small domains that have a synthetic component and yet good asymptotic properties. A new method to compute a calibration estimator that uses an arbitrary distance measure is developed. This method points to a new path for the investigation of the properties of the estimator. It is shown how through the Kronecker product the calibration method can be used to estimate variances and domain totals. Monte Carlo studies show that important improvements in the precision of variance estimators are possible with use of the calibration method. Necessary and sufficient conditions for the existence of weights that satisfy the calibration equation are also given."], ["Book Reviews", null], ["Telegraphic Reviews", null], ["Letters to the Editor", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["Markovian Structures in Biological Sequence Alignments", "The alignment of multiple homologous biopolymer sequences is crucial in research on protein modeling and engineering, molecular evolution, and prediction in terms of both gene function and gene product structure. In this article we provide a coherent view of the two recent models used for multiple sequence alignment\u2014the hidden Markov model (HMM) and the block-based motif model\u2014to develop a set of new algorithms that have both the sensitivity of the block-based model and the flexibility of the HMM. In particular, we decompose the standard HMM into two components: the insertion component, which is captured by the so-called \u201cpropagation model,\u201d and the deletion component, which is described by a deletion vector. Such a decomposition serves as a basis for rational compromise between biological specificity and model flexibility. Furthermore, we introduce a Bayesian model selection criterion that\u2014in combination with the propagation model, genetic algorithm, and other computational aspects\u2014forms the core of PROBE, a multiple alignment and database search methodology. The application of our method to a GTPase family of protein sequences yields an alignment that is confirmed by comparison with known tertiary structures."], ["Meta-Analysis of Migraine Headache Treatments: Combining Information from Heterogeneous Designs", "Migraine headache is a common condition in the United States for which a wide range of drug and nondrug treatments are available. There is wide disagreement about which treatments are most effective; meta-analysis of existing clinical trials can help to bring existing evidence to bear on this question. Conducting a meta-analysis is a challenging statistical problem because of the absence of a uniform accepted definition of headache syndromes, the diversity of treatments, and the heterogeneous and incomplete nature of published information. The results of studies are summarized in various ways; most studies report continuous treatment effects for each treatment, but some report only differences in effectiveness for pairs of treatments, and others report only 2 \u00d7 2 contingency tables for dichotomized responses. In this article we present a hierarchical Bayesian grouped random-effects model for synthesizing evidence from several clinical trials comparing the effectiveness of commonly recommended prophylactic treatments for migraine headaches. We incorporate explicitly the relationships among the different classes of treatments and introduce latent auxiliary variables to create a common scale for combining information from studies that report results in very different forms. This model permits us to synthesize this heterogeneous information and to make inferences about treatment effects and the relative ranks of treatment without understating uncertainty. Estimation, ranking, model validation, and sensitivity analysis are all implemented through simulation-based methods."], ["Hierarchical Generalized Linear Models in the Analysis of Variations in Health Care Utilization", "In recent years many studies have reported large differences in the use of medical treatments and procedures across geographic regions, hospitals, and individual health care providers. Beyond reporting on the extent of observed variations, these studies examine the role of contributing factors including patient, regional, and provider characteristics. In addition, they may assess the relation between health care processes and outcomes, such as patient mortality, morbidity, and functioning. Studies of variations in health care utilization and outcomes involve the analysis of multilevel clustered data; for example, data on patients clustered by hospital and/or geographic region. The goals of the analysis include the estimation of cluster-specific adjusted responses, covariate effects, and components of variance. The analytic strategy needs to account for correlations induced by clustering and to handle the presence of large variations in cluster size. In this article we formulate a broad class of hierarchical generalized linear models (HGLMs) and discuss their applications to the analysis of health care utilization data. The models can incorporate covariates at each level of the hierarchical data structure, can account for greater variation than what is allowed by the variance in a one-parameter exponential family, and permit the use of heavy-tailed distributions for the random effects. We develop a Bayesian approach to fitting HGLMs using Markov chain Monte Carlo methods and discuss several methods for model checking. The HGLM analysis is presented in the context of two examples of applications to the study of variations in the utilization of medical procedures for elderly Medicare beneficiaries who sustained a heart attack. The first example involves the analysis of clustered longitudinal data with binomial responses and examines geographic and temporal trends in the utilization of coronary angiography across the United States during the 4-year period 1987\u20131990. The second example involves the analysis of multilevel, clustered data with Poisson responses and examines hospital variations in the utilization of coronary artery bypass graft surgery in 1990. The HGLM analysis incorporates state-level and hospital-level covariates and makes it possible to estimate covariate effects and cluster-specific rates of utilization for both hospitals and states."], ["A Hierarchical Latent Variable Model for Ordinal Data from a Customer Satisfaction Survey with \u201cNo Answer\u201d Responses", "We propose an item response theory model for ordinal customer satisfaction data where the probability of each response is a function of latent person and question parameters and of cutoffs for the ordinal response categories. This structure was incorporated into a Bayesian hierarchical model by Albert and Chib. We extend this formulation by modeling item nonresponse, coded as \u201cno answer\u201d (NA), as due to either lack of a strong opinion or indifference to the entire question. Because the probability of an NA is related to the latent opinion, the missing-data model is nonignorable. In our hierarchical Bayesian framework, prior means for the person and item effects are related to observed covariates. This structure supports model inferences about satisfaction of individual customers and about associations between customer characteristics and satisfaction levels or propensity to respond. We contrast this with exploratory and standard regression analyses that do not fully support these inferences. Our motivating example, an analysis of a DuPont Corporation 1992 customer satisfaction survey, is described in detail. The nonconjugate likelihood and prior prevent closed-form posterior inference. We present a Markov chain Monte Carlo solution using data augmentation. We diagnose case influence and identify outliers by importance reweighting, and apply posterior predictive model checks. The methods illustrated have application in other situations in which categorical observations can be determined by several latent variables."], ["Estimating Multistate Transition Hazards from Last-Move Data", "Following United Nations recommendations, many countries collect or publish internal migration data in last-move form, despite continuing uncertainty among researchers about how to estimate transition rates and probabilities from such information. \u201cLast-move\u201d data are a form of retrospective event history in which the only available information for each observational unit are the state at the time of a survey (\u03c9), the last previous state (\u03c8), and the time at which the \u03c8 \u2192 \u03c9 transition occurred. The statistical literature has addressed special cases, but there is still no general method for estimating transition hazards from last-move data. In this article I propose such a method, analyze its performance in a Monte Carlo simulation study, and apply it to migration data from Brazil's 1980 census."], ["The Influence of Social Programs in Source Countries on Various Classes of U.S. Immigration", "This article uses a unique set of pooled cross-sectional and time series data to examine the annual rate of U.S. immigration during 1972\u20131991 from 60 source countries. One distinguishing feature of the article is that it breaks out and cross-classifies various classes of immigrants\u2014numerically limited versus numerically exempt and new immigrant versus adjustment of status. A second distinguishing feature is that it utilizes a unique vector of variables relating to the presence and characteristics of various social programs in source countries. The models developed here emphasize the importance of both differential economic advantage and the ease with which a prospective migrant can transfer skills to the U.S. labor market. Hausman\u2013Taylor instrumental variable estimates of the coefficients indicate that in addition to other factors, social programs in source countries are significant determinants of immigration to the U.S.A."], ["Comparing Predictions and Outcomes: Theory and Application to Income Changes", "Household surveys often elicit respondents' intentions or predictions of future outcomes. The survey questions may ask respondents to choose among a selection of (ordered) response categories. If panel data or repeated cross-sections are available, then predictions may be compared with realized outcomes. The categorical nature of the predictions data complicates this comparison, however. Generalizing previous findings on binary intentions data, we derive bounds on features of the empirical distribution of realized outcomes under the \u201cbest-case\u201d hypothesis that respondents form rational expectations and that reported expectations are best predictions of future outcomes. These bounds are shown to depend on the assumed model of how respondents form their \u201cbest prediction\u201d when forced to choose among (ordered) categories. An application to data on income change expectations and realizations illustrates how alternative response models may be used to test the best-case hypothesis."], ["Prediction of Spatial Cumulative Distribution Functions Using Subsampling", "The spatial cumulative distribution function (SCDF) is a random function that provides a statistical summary of a random field over a spatial domain of interest. In this article we develop a spatial subsampling method for predicting an SCDF based on observations made on a hexagonal grid, similar to the one used in the Environmental Monitoring and Assessment Program of the U.S. Environmental Protection Agency. We show that under quite general conditions, the proposed subsampling method provides accurate data-based approximations to the sampling distributions of various functionals of the SCDF predictor. In particular, it produces estimators of different population characteristics, such as the quantiles and weighted mean integrated squared errors of the empirical predictor. As an illustration, we apply the subsampling method to construct large-sample prediction bands for the SCDF of an ecological index for foliage condition of red maple trees in the state of Maine."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Semiparametric Regression Models for Repeated Events with Random Effects and Measurement Error", "Statistical methodology is presented for the regression analysis of multiple events in the presence of random effects and measurement error. Omitted covariates are modeled as random effects. Our approach to parameter estimation and significance testing is to start with a naive model of semiparametric Poisson process regression, and then to adjust for random effects and any possible covariate measurement error. We illustrate the techniques with data from a randomized clinical trial for the prevention of recurrent skin tumors."], ["Semiparametric Inference in the Proportional Odds Regression Model", null], ["Censored Median Regression Using Weighted Empirical Survival and Hazard Functions", "For median regression models that regress the median of the survival time or a transform thereof on the covariates, some semi-parametric estimators that include the intercept component are introduced when the survival time may be censored. These new median regression estimators do not require estimating the censoring distributions. They can be viewed as an extension of the sample median to the censored regression model. These estimators are based on some weighted empirical survival and hazard functions and are shown to be consistent and asymptotically normal. They performed very well in various numerical studies. The proposed procedures are illustrated in some real data examples."], ["Nonparametric Estimation of a Recurrent Survival Function", "Recurrent event data are frequently encountered in studies with longitudinal designs. Let the recurrence time be the time between two successive recurrent events. Recurrence times can be treated as a type of correlated survival data in statistical analysis. In general, because of the ordinal nature of recurrence times, statistical methods that are appropriate for standard correlated survival data in marginal models may not be applicable to recurrence time data. Specifically, for estimating the marginal survival function, the Kaplan\u2013Meier estimator derived from the pooled recurrence times serves as a consistent estimator for standard correlated survival data but not for recurrence time data. In this article we consider the problem of how to estimate the marginal survival function in nonparametric models. A class of nonparametric estimators is introduced. The appropriateness of the estimators is confirmed by statistical theory and simulations. Simulation and analysis from schizophrenia data are presented to illustrate the estimators' performance."], ["Methods for Estimating a Conditional Distribution Function", "Motivated by the problem of setting prediction intervals in time series analysis, we suggest two new methods for conditional distribution estimation. The first method is based on locally fitting a logistic model and is in the spirit of recent work on locally parametric techniques in density estimation. It produces distribution estimators that may be of arbitrarily high order but nevertheless always lie between 0 and 1. The second method involves an adjusted form of the Nadaraya\u2013Watson estimator. It preserves the bias and variance properties of a class of second-order estimators introduced by Yu and Jones but has the added advantage of always being a distribution itself. Our methods also have application outside the time series setting; for example, to quantile estimation for independent data. This problem motivated the work of Yu and Jones."], ["Improved Estimators in Nonparametric Regression Problems", "Linear estimators of multivariate means are considered. Generalizations of some well-known theorems about admissibility of linear estimators are given. The results then are applied to show that commonly used kernel-type estimators in nonparametric regression problems can be constructively improved in a simple way. An asymptotic result is described that gives a quantitative measure of the maximum improvement to be gained in certain situations. A theoretical bound shows that gains are achievable in the relative risk of up to 58.6% (rectangular kernel) or 29.2% (Epanechnikov kernel). Some examples of smaller sample size are also investigated, and these show relative risk gains ranging up to 18% in realistic settings."], ["A Class of Locally and Globally Robust Regression Estimates", null], ["Quasi-Linear Wavelet Estimation", "The main paradigm of the modern wavelet theory of spatial adaptation formulated by Donoho and Johnstone is that there is a divergence between the linear minimax adaptation theory and the heuristic guiding algorithm development that leads to the necessity of using strongly nonlinear adaptive thresholded methods. On the other hand, it is well known that linear adaptive estimates are the best whenever an estimated function is smooth. Is it possible to suggest a quasi-linear wavelet estimate, by adding to a linear adaptive estimate a minimal number of nonlinear terms on finest scales, that offers advantages of linear adaptive estimates and at the same time matches asymptotic properties of strongly nonlinear procedures like the benchmark SureShrink? The answer is \u201cyes,\u201d and we discuss quasi-linear estimation both theoretically and via a Monte Carlo study. In particular, I show that, asymptotically, a quasi-linear procedure not only matches properties of SureShrink over the Besov scale, but also allows us to relax familiar assumptions and solve a long-standing problem of rate and sharp optimal estimation of monotone functions. For the case of small sample sizes and functions that contain spiky/jumps parts and smooth parts, a quasi-linear estimate performs exceptionally well in terms of visual aesthetic appeal, approximation, and data compression."], ["High-Breakdown Rank Regression", null], ["On Estimation of Monotone and Concave Frontier Functions", null], ["Combining Conditional Log-Linear Structures", "Graphical models offer simple and intuitive interpretations in terms of conditional independence relationships, and these are especially valuable when large numbers of variables are involved. In some settings, restrictions on experiments and other forms of data collection may result in our being able to estimate only parts of a large graphical model; for example, when the data in a large contingency table are extremely sparse. In other settings, we might use a model building strategy that constructs component pieces first, and then tries to combine those pieces into a larger model. In this article we address this problem of combining component models in the context of cross-classified categorical data, and we show how to derive partial information about an underlying log-linear structure from its conditional log-linear structures and then how to use this information to choose a log-linear structure under the assumption that it is graphical. We illustrate the results using a simulated dataset based on a problem arising in cognitive psychology applied to learning."], ["Treatment Effects in a Logistic Model Involving the Box-Cox Transformation", null], ["Identifiability, Improper Priors, and Gibbs Sampling for Generalized Linear Models", "Markov chain Monte Carlo algorithms are widely used in the fitting of generalized linear models (GLMs). Such model fitting is somewhat of an art form, requiring suitable trickery and tuning to obtain results in which one can have confidence. A wide range of practical issues arise. The focus here is on parameter identifiability and posterior propriety. In particular, we clarify that nonidentifiability arises for usual GLMs and discuss its implications for simulation-based model fitting. Because often some part of the prior specification is vague, we consider whether the resulting posterior is proper, providing rather general and easily checked results for GLMs. We also show that if a Gibbs sampler is run with an improper posterior, then it may be possible to use the output to obtain meaningful inference for certain model unknowns."], ["Variance Estimation for Survey Data with Composite Imputation and Nonnegligible Sampling Fractions", "This article considers variance estimation for Horvitz\u2013Thompson\u2013type estimated totals based on survey data with imputed non-respondents and with nonnegligible sampling fractions. A method based on a variance decomposition is proposed. Our method can be applied to complicated situations where a composite of some deterministic and/or random imputation methods is used, including using imputed data in subsequent imputations. Although here linearization or Taylor expansion\u2013type techniques are adopted, replication methods such as the jackknife, balanced repeated replication, and random groups can also be used in applying our method to derive variance estimators. Using our method, variance estimators can be derived under either the customary design-based approach or the model-assisted approach, and are asymptotically unbiased and consistent. The Transportation Annual Survey conducted at the U.S. Census Bureau, in which nonrespondents are imputed using a composite of cold deck and ratio type imputation methods, is used as an example as well as the motivation for our study."], ["Order Statistic Properties, Random Generation, and Goodness-of-Fit Testing for a Minimal Repair Model", "We present properties of statistics arising from the Block, Borges, and Savits age-dependent minimal repair model. These properties are analogous to the order statistic properties in homogeneous Poisson processes. These properties are exploited to obtain an algorithm for generating a realization from the minimal repair model, and for testing hypothesis concerning the initial distribution function or hazard function of the repair model."], ["A Class of Permutation Tests of Bivariate Interchangeability", null], ["Data-Driven Rank Tests for Independence", "We introduce new rank tests for testing independence. The new testing procedures are sensitive not only for grade linear correlation, but also for grade correlations of higher-order polynomials. The number of polynomials involved is determined by the data. Model selection is combined with application of the score test in the selected model. Whereas well-known tests as Spearman's test or Hoeffding's test may completely break down for alternatives that are dependent but have low grade linear correlation, the new tests have greater power stability. Monte Carlo results clearly show this behavior. Theoretical support is obtained by proving consistency of the new tests."], ["Combining the Advantages of One-Sided and Two-Sided Test Procedures for Comparing Several Treatment Effects", null], ["Adjusted Score Tests of Homogeneity for Poisson Processes", "Tests of homogeneity are being increasingly used for the analysis of event time data, but relatively little attention has been paid to their distributional properties in settings with small to moderate sample sizes. Here we consider tests of homogeneity for recurrent event data in which the null model is a Poisson process and the alternative is a mixed Poisson process. We examine score and adjusted score statistics in the context of parametric and semiparametric regression models, where the adjustments correct for the bias induced by substituting the maximum likelihood estimates of the parameters into the test statistic. We report the results of simulation studies suggesting that the adjusted score statistics are superior in terms of size and power. We also find that the adjusted score test in the semiparametric regression model does not perform particularly well in small samples, but the adjusted score test based on a piecewise exponential model has satisfactory performance. The latter thus provides an attractive alternative in terms of robustness and frequency properties, and we recommend using this test statistic for datasets of small to moderate size. We illustrate and contrast the methods by application to a clinical trial."], ["The Versatility of Function-Indexed Weighted Log-Rank Statistics", null], ["Book Reviews", null], ["Letters to the Editor", null], ["Editorial Board Page", "This article has no abstract"]]}