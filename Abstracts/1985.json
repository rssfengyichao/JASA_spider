{"1985": [["Applications of Catastrophe Theory for Statistical Modeling in the Biosciences", "Although catastrophe theory has been applied with mixed success to many problems in the biosciences, very few of these applications have used any form of statistical modeling. We present examples of the applications of statistical catastrophe theory in the analysis of experimental data. These include examples of hysteresis effects, bifurcation effects, and the full cusp catastrophe model. The methods of statistical catastrophe theory draw upon the theories of parameter estimation for multiparameter exponential families, nonlinear time-series analysis, and stochastic differential equations. We discuss the application of these methods to both canonical and noncanonical catastrophe models."], ["Regularity and Structure of the Spatial Pattern of Blue Cones of Macaque Retina", "Models were developed for describing the regular pattern of blue cones in macaque retina. Two functional descriptions were applied to patterns, one based on the cdf of interpoint distances, the other on the cdf of the central angles of Voronoi regions. Disordered triangular and square lattices and hard balls failed to model the blue-cone point pattern. An elastic-ball model was developed whose spatial properties fit well those of the blue-cone pattern. This model employed a hard core and a soft surrounding shell and is proposed as a valid model for the blue-cone pattern."], ["Comment", null], ["Rejoinder", null], ["Statistical Aspects of Equitable Apportionment", null], ["Survival Models for Fertility Evaluation", "Two proportional hazards models for cohort fertility evaluation are constructed. Time-dependent covariates describe sources of heterogeneity between and within women regarding fertility characteristics. In the first model, U.S. birth rates specific to maternal age, race, parity, and birth cohort are used as underlying hazard rates. Covariate effects are estimated by maximizing the full likelihood. In the second model, covariate effects are estimated via Cox regression with stratified underlying hazard rates regarded as unknown nuisance parameters. The two models are illustrated with an evaluation of the fertility histories of the wives of workers at a manufacturing plant with potential for hazardous exposure. Adjustments to the U.S. birth rates for maternal age and parity zero experience are required with the first approach. Then, despite differences in the model-specific estimation procedures, the point estimates of the exposure effect and the estimated standard errors from the two models are practically equivalent."], ["Multivariate Gamma-Poisson Models", "This article extends the multivariate gamma-Poisson model of repeated events, developed by Arbous and Kerrich (1951) and Bates and Neyman (1952), by (a) compounding it with a Dirichlet distribution in the analysis of cross-sectional data and by (b) allowing individual rates to shift at the start of the second period in the analysis of two-period longitudinal data. The first extension allows individuals to specialize in particular types of events. The model is shown to describe multivariate distributions of work incapacities, prison infractions, and criminal victimizations. The second extension provides a framework for estimating changes in individual rates. It is used to describe the shift in victimization rates observed in longitudinal studies of the National Crime Survey."], ["Models for Memory Effects", null], ["Use of Geographically Classified Telephone Directory Lists in Multi-Mode Surveys", "Commercially available telephone directory lists, classified by block groups and enumeration districts, can be used in association with area frames for area household sampling. Some advantages and disadvantages of using such lists as sampling frames for multi-mode (telephone and in-person) surveys are discussed. Two recently completed studies that used such methodology are cited. Recommendations are presented concerning the use of such telephone directory lists in the design of area household surveys."], ["Weighted Normal Plots", null], ["Efficient Scores, Variance Decompositions, and Monte Carlo Swindles", null], ["The Analysis of Panel Data under a Markov Assumption", "Methods for the analysis of panel data under a continuous-time Markov model are proposed. We present procedures for obtaining maximum likelihood estimates and associated asymptotic covariance matrices for transition intensity parameters in time homogeneous models, and for other process characteristics such as mean sojourn times and equilibrium distributions. Generalizations to handle covariance analysis and to the fitting of certain nonhomogeneous models are presented, and an example based on a longitudinal study of the smoking habits of school children is discussed. Questions of embeddability and estimation are examined."], ["A Note on Parameter-Effects Curvature", "The parameter-effects curvature measure proposed by Bates and Watts (1980) is examined for a growth model and the Fieller-Creasy problem. Exact confidence regions are constructed and compared to linear approximation regions. For the growth model the agreement between the regions is good despite high curvature. In the Fieller\u2014Creasy problem it is shown that the agreement can be quite poor despite low curvature."], ["Errors in Inspection: Integer Parameter Maximum Likelihood in a Finite Population", "In a probability model for possible errors in the inspection of a finite lot containing defective items, it is shown how maximum likelihood estimation is affected by whether the sampling is carried out with or without replacement. These changes are examined by developing a method for simultaneous maximization over an integer and a scalar parameter. A new method for the bounding of the integer solution is introduced. The results show that the parameter estimates in the inspection model are generally stable under wide changes in the sampling framework. Conditions for instability of the solution are identified. Some apparently new relations for discrete distributions are derived in the process of the investigation."], ["A Useful Upper Bound for the Tail Probabilities of the Scan Statistic When the Sample Size is Large", "Although a number of approximations to the distribution (under uniformity) of the scan statistic are known, they are, when the sample size is large, either difficult to compute or numerically unstable. A simple upper bound for the upper tail of the distribution, based on a result of Hunter (1976), is derived. It is found that for large sample sizes and a reasonably wide range of window widths, this bound gives a numerically stable approximation that is sufficiently accurate for hypothesis testing purposes."], ["Surveys Combining Probability and Quota Methods of Sampling", "The problem of choosing between full probability and quota sampling is analyzed in terms of a Bayesian model of biased measurement. The model differs from previous models in its attempt to approximate the effects of the multistage cluster sampling used in actual national surveys. Allocation rules are developed that may involve devotion of all resources to quota sampling, to probability sampling, or to designs that combine both types of sampling. The sensitivity of the optimal allocation to the specification of the prior distribution and the cost parameters is investigated."], ["Bayesian Inference for Finite Population Parameters in Multistage Cluster Sampling", null], ["Applications of Transportation Theory to Statistical Problems", "The two-dimensional controlled selection problem and the problem of maximizing the overlap of old and new primary sampling units after restratification and change of selection probabilities have been studied for several decades but have never been completely solved until now. Using transportation theory, complete solutions are obtained here for these and other problems. The solution to the controlled selection problem is based on a specific transportation model that was originally developed, in a previous paper by Cox and Ernst (1982), to solve completely the controlled rounding problem, namely the problem of optimally rounding real-valued entries in a two-way tabular array to adjacent integer values in a manner that preserves the tabular (additive) structure of the array. This model is also applied to other statistical problems, such as raking and statistical disclosure for frequency count tabulations and microdata."], ["The Measure of \u201cSize\u201d Independent of \u201cShape\u201d for Multivariate Lognormal Populations", "The unique measure of \u201csize\u201d that is statistically independent of \u201cshape\u201d for random vectors of measurements following a multivariate lognormal distribution is derived. This size measure is a weighted geometric mean (possibly including negative weights) and generalizes work of Mosimann (1970) on size and shape variables. It is also closely related to White and Gould's (1965) measure of size differences for constant shape. Estimation of the size measure and an example of its application are discussed."], ["Analysis of Linear Combinations with Extreme Ratios of Variance", null], ["Estimating a Particular Function of the Multiple Correlation Coefficient", null], ["The Relationship among the Specification Tests of Hausman, Ramsey, and Chow", "This article describes a relationship between the tests for specification error in regression models introduced by Ramsey (1969) and Hausman (1978). It also shows that the Chow (1960) test for structural shift can be viewed as a special case of specification error tests. This article shows the common structure of these tests. The common basis is the comparison of alternative estimators of regression coefficients. Under the null hypothesis, one estimator is efficient and consistent and the other is consistent. The probability limits of the two estimators are different under the alternative hypothesis."], ["Comparison of Least Squares and Errors-in-Variables Regression, with Special Reference to Randomized Analysis of Covariance", "In an errors-in-variables regression model, the least squares estimate is generally inconsistent for the complete regression parameter but can be consistent for certain linear combinations of this parameter. We explore the conjecture that, when the least squares estimate is consistent for a linear combination of the regression parameter, it will be preferred to an errors-in-variables estimate, at least asymptotically. The conjecture is false, in general, but it is true for some important classes of problems. One such problem is a randomized two-group analysis of covariance, upon which we focus."], ["Data-Driven Choice of a Spectrum Estimate: Extending the Applicability of Cross-Validation Methods", null], ["Properties of Predictors in Misspecified Autoregressive Time Series Models", null], ["Comparison of Efficiencies of Several Estimators for Linear Regressions with Autocorrelated Errors", null], ["Exact Power of Goodness-of-Fit Tests of Kolmogorov Type for Discontinuous Distributions", null], ["The Number of Classes in Chi-Squared Goodness-of-Fit Tests", null], ["Computing an Exact Confidence Interval for the Common Odds Ratio in Several 2\u00d72 Contingency Tables", "A quadratic time network algorithm is provided for computing an exact confidence interval for the common odds ratio in several 2\u00d72 independent contingency tables. The algorithm is shown to be a considerable improvement on an existing algorithm developed by Thomas (1975), which relies on exhaustive enumeration. Problems that would formerly have consumed several CPU hours can now be solved in a few CPU seconds. The algorithm can easily handle sparse data sets where asymptotic results are suspect. The network approach, on which the algorithm is based, is also a powerful tool for exact statistical inference in other settings."], ["Efficient Sequential Designs with Binary Data", "A class of sequential designs for estimating the percentiles of a quantal response curve is proposed. Its updating rule is based on an efficient summary of all of the data available via a parametric model. The logit-MLE version of the proposed designs can be viewed as a natural analog of the Robbins\u2014Monro procedure in the case of binary data. It is shown to be asymptotically consistent, optimal, and nonparametric via its connection with the latter procedure. For certain choices of initial designs, the proposed method performs very well in a simulation study for sample sizes up to 35. A nonparametric sequential design, via the Spearman\u2014K\u00e4rber estimator, for estimating the median is also proposed."], ["Nearly Trend-Free Block Designs", null], ["Estimation of Latent Group Effects", "Conventional methods of multivariate normal analysis do not apply when the variables of interest are not observed directly but must be inferred from fallible or incomplete data. A method of estimating such effects by marginal maximum likelihood, implemented by means of an EM algorithm, is proposed. Asymptotic standard errors and likelihood ratio tests of fit are provided. The procedures are illustrated with data from the administration of the Armed Services Vocational Aptitude Battery to a probability sample of American youth."], ["Nonparametric Stepwise Multiple Comparison Procedures", "Nonparametric multiple comparisons are discussed, with particular emphasis given to stepwise procedures. Nonparametric analogs of the stepwise all-subset procedure of Einot and Gabriel are presented, along with an ad hoc nonparametric analog of the Newman\u2014Keuls procedure. These new procedures are compared among themselves and with nonstepwise procedures based on Type I error levels and comparisonwise power. It is shown that these stepwise nonparametric procedures control Type I error levels, and that they have superior pairwise power compared to the commonly used nonstepwise procedures."], ["A Smooth Nonparametric Estimator of a Quantile Function", "A smooth alternative to the conventional sample quantile function as a nonparametric estimator of a population quantile function is proposed. The proposed estimator is essentially a kernel type of estimator and has the same asymptotic distribution as the conventional sample quantile. The mean squared convergence rate of the proposed estimator is also estimated. Monte Carlo studies are conducted to compare the proposed estimator with the sample quantile and the estimator proposed by Kaigh and Lachenbruch. The feasibility of using bootstrap techniques to estimate the optimal window width for the proposed estimator is also considered."], [null, null], ["Masking Effect on Tests for Outliers in Exponential Models", "The masking effect in cases of tests for outlier(s) is defined and quantified by the loss in power due to the presence of more than the anticipated number of discordant observations in the sample. This effect is illustrated in cases of some commonly used outlier tests for exponential samples\u2014namely, Dixon-type tests and the Cochran test. A comparison between the performances of the modified Dixon-type test and the Cochran test is made. Tables for powers of these tests are presented for different sample sizes and different values of discordancy parameter. An illustrative example is presented to support the conclusion that Cochran and modified Dixon-type tests do not suffer from the masking effect in the presence of two outliers."], ["Bootstrap Prediction Intervals for Regression", "Bootstrap prediction intervals provide a nonparametric measure of the probable error of forecasts from a standard linear regression model. These intervals approximate the nominal probability content in small samples without requiring specific assumptions about the sampling distribution. Empirical measures of the prediction error rate motivate the choice of these intervals, which are calculated by an application of the bootstrap. The intervals are contrasted to other nonparametric procedures in several Monte Carlo experiments. Asymptotic invariance properties are also investigated."], ["Bounded-Influence Regression via Local Minimax Mean Squared Error", null], ["The Resistant Line and Related Regression Methods", null], ["Comment", null], ["Book Reviews", null], ["Corrections", null], ["Editorial Board Page", "This article has no abstract"], ["Graphics and Human Information Processing: A Review of Five Books", null], ["The Hierarchical Logistic Regression Model for Multilevel Analysis", "A hierarchical logistic regression model is proposed for studying data with group structure and a binary response variable. The group structure is defined by the presence of micro observations embedded within contexts (macro observations), and the specification is at both of these levels. At the first (micro) level, the usual logistic regression model is defined for each context. The same regressors are used in each context, but the micro regression coefficients are free to vary over contexts. At the second level, the micro coefficients are treated as functions of macro regressors. An empirical Bayes estimation procedure is proposed for estimating the micro and macro coefficients. Explicit formulas are provided that are computationally feasible for large-scale data analyses; these include an algorithm for finding the maximum likelihood estimates of the covariance components representing within\u2014 and between\u2014macro-equation error variability. The methodology is applied to World Fertility Survey data, with individuals viewed as micro observations and countries as macro observations."], ["A Time Series Model for Cohort Data", "Many forms of behavior are both age and cohort dependent. An appropriate way of disentangling these two kinds of effects is to examine changes between successive cohorts. Data on recent cohorts are always incomplete. A univariate time series model is presented for projecting age- or duration-specific data of recent cohorts. The model is also capable of taking period effects into account. By way of illustration, the model is applied to Dutch marital fertility rates."], ["Errors in Variables and Seasonal Adjustment Procedures", "Seasonal adjustment procedures attempt to estimate the sample realizations of an unobservable economic time series in the presence of both seasonal and irregular factors. In this article, we consider a factor that has not been considered explicitly in previous treatments of seasonal adjustment: measurement error. Because of the sample design used in the Current Population Survey, measurement error will not be a white-noise process, but instead it will be characterized by serial correlation of a known form. We first consider what effect the serially correlated measurement error has on estimation of the nonseasonal component in seasonal adjustment models. We also consider the effect of measurement error on the widely used seasonal adjustment process X-11. Estimated unobserved-components models are used to estimate the precision (root mean squared error) of the official and optimal seasonally adjusted data."], ["Calculating the Variance of Seasonally Adjusted Series", "This article considers the use of the Kalman filter to perform the seasonal adjustment and to calculate the variance of the signal extraction error in model-based seasonal adjustment procedures. The steady-state filter covariance is seen to provide a convenient basis for obtaining the variances not only of the current adjustment but also of subsequent revisions. The method is applied to the unobserved-components model we have recently proposed as a justification of the X-11 method and to a real economic time series."], ["A Note on the Use of Local Maxima to Predict Turning Points in Related Series", "This article deals with the issue of to what extent local maxima of one time series can be used to predict the turning points of other series of interest. It is argued that the theory of high-level crossings is the appropriate framework to handle such issues. The article then defines precisely the notion of a turning point and introduces a test for investigating the predictive value of the U.S. leading-indicator series."], ["A One-Sided Goodness-of-Fit Test for a Multinomial Population", null], ["Comment", null], ["Optimal Data Quality", "Commonly accepted hypotheses guide practical determination of needed data quality; for example, as the probability that a decision uses data increases, the needed data quality increases, and the more rudimentary the uses of the data, the less data quality is needed. These hypotheses are formally defined and analyzed in some decision-theoretic models. Conditions under which the hypotheses hold and fail are examined. Particular attention is given to determining needed data quality when the users of the data behave nonoptimally."], ["Orthogonal Central Composite Designs of the Third Order in the Evaluation of Sensitivity and Plant Growth Simulation Models", "The U.S. Department of Agriculture has used response-surface techniques as applied by Baker and Bargmann (1981) to plant process simulation models as an aid in the identification of interrelationships among yield and various single-valued and functional parameters. Orthogonal cubic surfaces have provided insight into higher order relationships as well as a measure of the relative sensitivity of yield to experimentally determined parameter values. Several examples investigate the effectiveness of those higher order surfaces and illustrate how less precise (and less costly) measurements may be possible in building and using these simulation models."], ["Estimating Optimal Transformations for Multiple Regression and Correlation", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Inference from Stratified Samples: Second-Order Analysis of Three Methods for Nonlinear Statistics", "For stratified samples and nonlinear statistics that can be expressed as functions of estimated totals, second-order asymptotic expansions of the linearization, jackknife, and balanced repeated-replication variance estimators are obtained. Based on these, comparisons are made in terms of their biases. Some higher order asymptotic equivalence results are also established. The special case of a combined ratio estimator is investigated in detail. Some results on bias reduction achieved by the jack-knife and balanced repeated-replication estimators of a nonlinear function of totals are also given."], ["Nonlinear Prediction Theory and the Estimation of Proportions in a Finite Population", "The prediction approach to finite population inference is developed for a general nonlinear model. An estimator of the finite population total and an estimator of its variance are derived, and the asymptotic properties of both are obtained when the random variables in the model are independent. The theory is applied to the problem of estimating the total number of units that have a specified characteristic. An empirical study is presented, which confirms that a nonlinear Bernoulli model is potentially useful for that problem but also illustrates difficulties that may be encountered. Comparisons with the ratio and linear regression estimators are also included."], ["Mean Squared Error Properties of Empirical Bayes Estimators in a Multivariate Random Effects General Linear Model", null], ["Estimation of Response Probabilities from Augmented Retrospective Observations", "When augmented by suitable auxiliary information, retrospective data can identify response probabilities. The auxiliary information may take the form of data on marginal distributions or appropriate structural assumptions. When a combination of retrospective observation and auxiliary information suffices in principle to identify response probabilities, practical use of this fact requires that statistically sound and computationally tractable estimation methods be available. This article analyzes the problem of identification and presents the needed estimators."], ["Log-Linear Models for Doubly Sampled Categorical Data Fitted by the EM Algorithm", "Double-sampling experiments can be expressed as incomplete multiway contingency tables and analyzed by using techniques appropriate for fitting log-linear models. The framework described can be applied to experiments with any number of factors and measurement devices. Maximum likelihood estimation via the EM algorithm leads to straightforward expressions for covariances of estimates of the parameters and functions of the parameters."], ["An Improved Goodness-of-Fit Statistic for Sparse Multinomials", "A new goodness-of-fit statistic for sparse multinomials is proposed. It is assumed that the null distribution exhibits smoothness. The test statistic is based on the maximum posterior estimator probability estimates of Simonoff (1983). Computer simulations are used to estimate the null distribution, significance levels, and the power function of the test. It is shown that power of the test is a great improvement over that of the standard tests if the alternative distribution exhibits smoothness."], ["The Influence Curve and Goodness of Fit", "The influence curve introduced by Hampel (1968) is applied to goodness-of-fit statistics. The efficacy curve is then defined to be the square of the influence curve weighted by a constant that arises in the context of approximate Bahadur efficiency. For a number of goodness-of-fit statistics, the ratios of these curves are shown to be equal to asymptotic relative efficiencies in the Pitman sense when testing for point contamination. These efficacy curves graphically portray the sensitivities of certain goodness-of-fit statistics to minor perturbations in the assumed distribution."], ["Confidence Bands for Regression Functions", "A method for obtaining (conservative) confidence bands for regression functions in low dimensions is given. Of particular importance is the applicability of the method to nonparametric regression. Computations, examples, and comparisons indicate the effectiveness of the method."], ["Average-Width Optimality for Confidence Bands in Simple Linear Regression", "The problem of constructing optimal confidence bands for a simple linear regression over the whole real line is considered. Optimality is defined as minimization of the average width of the bands weighted by a normalized function. This weight function is presented as an indicator of experimental interest in the varying width of the band. A comparison between the commonly seen hyperbolic bands and segmented-line bands is presented."], ["Combining Experiments under Gauss-Markov Models", null], ["A Variational Approach to Optimal Two-Treatment Crossover Designs: Application to Carryover-Effect Models", null], ["Some New Results on Grubbs's Estimators", null], ["Robustness of Sequential Exponential Life-Testing Procedures", "Sequential life-testing procedures for the exponential distribution are often used when the underlying distribution of life lengths is not exponential. In this article, we investigate the robustness of these procedures with respect to the risks and the expected sample sizes, when the underlying distribution has a monotone failure rate. We also describe the regions in which the true operating characteristic curves of the misused tests lie. A main conclusion is that a producer could penalize a consumer by overloading the test bench with unsatisfactory items, whenever the latter have an increasing failure rate."], ["Aggregation, Structural Change, and Cross-Section Estimation", null], ["Tests for Equality between Sets of Coefficients in Two Linear Regressions under Heteroscedasticity", "Structural shift is a common problem in a relationship dealing with time series data. Chow (1960) developed a test to detect such a shift under the assumption that observations both before and after the shift have the same variance. Structural shifts, however, often accompany changes in variance as well, and the Chow test is not robust to such changes. Two relatively robust tests are proposed and are found to be highly powerful."], ["Robust Estimators of Scale: Finite-Sample Performance in Long-Tailed Symmetric Distributions", null], ["Asymptotic Properties of a Family of Minimum Quantile Distance Estimators", null], ["An Asymptotic Acceptance of Aligned Rank Tests under Alternatives of Contaminated Distributions in a Randomized-Blocks Design", null], ["Condition Numbers and Minimax Ridge Regression Estimators", "Ridge regression was originally formulated with two goals in mind: improvement in mean squared error and numerical stability of the coefficient estimates. Conditions are given under which a minimax ridge regression estimator can also improve numerical stability, a quantity that can be measured with the condition number of the matrix to be inverted. The consequences of trading numerical stability for minimaxity are also discussed."], ["Projection-Pursuit Approach to Robust Dispersion Matrices and Principal Components: Primary Theory and Monte Carlo", null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Estimating the Short-Run Income Elasticity of Demand for Electricity by Using Cross-Sectional Categorized Data", "This study uses the 1980 energy application survey conducted by Ontario Hydro to estimate a short-run or conditional demand for electricity model. Two-step inference procedures are developed to take account of the special nature of the data in which the income variable is recorded in categorical form. Comparisons of short-run income elasticities with other studies, using the complete information of income variable, are also made."], ["Are Productive People to Be Found? Robust Analysis of Sparse Two-Way Tables", "Data from individual measurements of production in a manufacturing environment may be thought of as a two-way table indexed by worker and task. If there are many tasks, such a table will normally be sparse. Adjustment for task effects is important if workers are to be compared to one another, since any two workers may have no tasks in common. Three techniques are presented for the analysis of such sparse tables: table-stripe regression, rectangle ratios, and network scores. These techniques are illustrated on data from a large industrial concern, where 6,846 person\u2014task\u2014weeks of data on 479 workers and 678 tasks were collected as part of an ongoing group incentive plan."], ["Forecasting Interstate Migration with Limited Data: A Demographic-Economic Approach", "The limitations of available migration data preclude a time-series approach to modeling interstate migration. The method presented here combines aspects of the demographic and economic approaches to forecasting migration in a manner compatible with existing data. Migration rates are modeled to change in response to changes in economic conditions. When applied to recently constructed data on migration based on income tax returns and then compared to standard demographic projections, the demographic\u2014economic approach has a 20% lower total error in forecasting net migration by state for cohorts of laborforce age."], ["Estimation of Allocation Rates in a Cluster Analysis Context", "A sample of multivariate observations is assumed to be drawn from a mixture of a given number of underlying populations. The mixture likelihood approach to clustering is used to allocate each individual in the sample to its population of origin on the basis of the estimated posterior probabilities of population membership. Estimation of the correct allocation rate is considered for individual populations as well as for the overall mixture by averaging functions of the maximum of these posterior probabilities. The estimates of the correct allocation rates provide a means of assessing the performance of the mixture approach to clustering. The bootstrap technique is investigated for its effectiveness in reducing the bias of the estimates so obtained. Results are reported for three real data sets and a simulation study. It is demonstrated that the proposed estimates generally provide useful information on the unobservable allocation rates of the mixture approach. Encouraging results are obtained for the bootstrap method of bias correction applied to the estimates of the individual and overall allocation rates."], ["Fitting H. F. Smith's Empirical Law to Cluster Variances for Use in Designing Multi-Stage Sample Surveys", null], ["Estimating the Herfindahl Index from Concentration Ratio Data", null], ["Uncertain Population Forecasting", "Errors in population forecasts arise from errors in the jump-off population and errors in the predictions of future vital rates. The propagation of these errors through the linear (\u201cLeslie\u201d) growth model is studied, and prediction intervals for future population are developed. For U.S. national forecasts, the prediction intervals are compared with the U.S. Census Bureau's high\u2014low intervals. To assess the accuracy of the predictions of future vital rates, we derive the predictions from a parametric statistical model and estimate the extent of model misspecification and errors in parameter estimates. Subjective, \u201cexpert\u201d opinion, so important in real forecasting, is incorporated with the technique of mixed estimation. A robust regression model is used to assess the effects of model misspecification."], ["Survival Analysis of Backward Recurrence Times", "Many surveys include questions that attempt to measure the time of the most recent occurrence of some event, for example, last visit to a physician. Although it is tempting to apply survival (failure-time) methods to such data, the conditions under which such applications are appropriate have not been apparent. In this article it is shown that standard methods may be applied when the data arise from certain well-known stochastic processes. Special procedures may be necessary if the models include duration dependence, however. The methods are illustrated by the estimation of regression models for data on residential mobility."], ["Exploring an Ozone Spatial Time Series in the Frequency Domain", "In 1974, atmospheric scientists (Molina and Rowland; Cicerone et al.) first postulated that man-made chemicals known as CFC's (e.g., freon) deplete the earth's ozone when released into the atmosphere. This article presents an exploratory data analysis (EDA) of a spatial time series of ozone measurements. Frequency domain techniques for the EDA are introduced and used to graphically display global, long-term trends relevant to the CFC issue. My intention is to illustrate the effectiveness of a frequency domain approach to exploring a spatial time series."], ["A Family of Bivariate Distributions Generated by the Bivariate Bernoulli Distribution", null], ["Self-Calibrating Priors Do Not Exist", "It is shown that self-calibrating priors in the sense of Dawid (1982a) do not exist."], ["Comment", "Oakes's argument shows that no statistical analysis, however complex, of sequential data can be guaranteed to provide asymptotically valid forecasts for every possible set of outcomes."], ["Comment", null], ["Multiple Probability Assessments by Dependent Experts", "When two or more information sources (\u201cexperts\u201d) provide a decision maker with information on two or more random variables, the decision maker using Bayes's rule has an opportunity to (a) update a prior about the random variables and (b) calibrate the experts. (Calibration is the process of adjusting the decision maker's likelihood about the experts' assessments.) This article presents a model for this two-way process and specializes to the case in which the experts' assessment errors have a multivariate normal density. In general, we find that variables which the decision maker and the experts regard as independent a priori will be dependent a posteriori because of dependence in the assessment errors. Formulas for posterior densities are given for the normal model. In this model the posterior density of the random variables depends on only a weighted average of the expert's means, with weights that depend on the experts' assessments of previously known quantities. I also present a special case of the model for which the mean of the posterior density is correctly given by a simple (unweighted) average of assessments."], ["Frequency Polygons: Theory and Application", null], ["Conditional Coverage Properties of Finite Population Confidence Intervals", null], ["Qualms about Bootstrap Confidence Intervals", "The percentile method and bias-corrected percentile method of Efron (1981, 1982) are discussed. When these methods are used to construct nonparametric confidence intervals for the variance of a normal distribution, the coverage probabilities are substantially below the nominal level for small to moderate samples. This is due to the inapplicability of assumptions underlying the methods. These assumptions are difficult or impossible to check in the complicated situations for which the bootstrap is intended. Therefore, bootstrap confidence intervals should be used with caution in complex problems."], ["A New Method for Testing Separate Families of Hypotheses", null], ["Hypothesis Testing in ARIMA(p, 1, q) Models", null], ["A Diagnostic Statistic for the Score Test", "This article provides exact and approximate expressions for the change in the score test on deletion of observations. Deletions of individuals and of entire risk sets from matched case-control and survival studies are discussed in detail. A simple formula is derived, which measures the contribution of individual strata to score tests based on combining several contingency tables. The diagnostic is illustrated by application to the Stanford heart-transplant data."], ["The Use of Maximin Efficiency Robust Tests in Combining Contingency Tables and Survival Analysis", null], ["Maximum Likelihood Regression of Rank-Censored Data", null], ["A Nonparametric Test for Linear Regression Based on Combining Kendall's Tau with the Sign Test", "A nonparametric test for hypotheses concerning linear regression parameters is based on the signs of the residuals and their Kendall correlation with the independent variable. The small-sample distribution of the test statistic is tabulated, asymptotic approximations are derived, and the corresponding confidence region is presented. Furthermore, the Pitman asymptotic relative efficiency of the test is calculated with respect to the classical test based on least squares and with respect to the Brown-Mood test."], ["The Asymptotics of Maximum Likelihood and Related Estimators Based on Type II Censored Data", "Some simple procedures are provided for establishing the asymptotic normality and uniform strong convergence of a class of functions that arise in the context of estimating parameters from a Type II censored sample. These lead to an elementary yet rigorous treatment of the asymptotic properties of maximum likelihood estimators based on Type II censored data. Further applications include the treatment of asymptotics of some modified maximum likelihood (MML) estimators. In particular, conditions are provided for the consistency and limiting normality of the MML estimators of Mehrotra and Nanda, and the asymptotic efficiencies of these estimators are evaluated."], ["Distribution-Free Methods of Estimating Location Difference with Censored Paired Data", "Nonparametric point and interval estimators of location difference in matched pairs data with arbitrary right censorship are proposed, based on a generalized Gehan\u2014Gilbert test (Wei 1980). The point estimator is shown to be asymptotically normal and strongly consistent. The limits of the corresponding interval estimator are readily computable. An example is also provided to illustrate the techniques to problems in survival data analysis."], ["Testing Goodness of Fit for the Poisson Assumption When Observations are Not Identically Distributed", "Tests for detecting negative binomial departures from a Poisson model are studied for the one-way-layout and regression-through-the-origin cases. Approximations to the null and alternative distributions of these test statistics are presented. Locally optimal tests and tests suggested in the literature are compared in terms of asymptotic relative efficiency. Small sample comparisons are included."], ["Tests for an Increasing Trend in the Intensity of a Poisson Process: A Power Study", null], [null, null], ["Measures of Location of Skew Distributions Obtained through Box-Cox Transformations", "Box\u2014Cox power transformations are used to transform a set of observations to symmetry. A measure of location of the original data is obtained by applying the inverse transformation to the center of the transformed data. Its properties are discussed, and it is compared with other measures of location."], ["Multiple Comparisons and Type III Errors", "In multiple comparison procedures often the ordering of populations is of more interest than the testing of hypotheses. In such a situation it is important to control the probability of a Type III error (introduced by Harter 1957 to indicate that one population is concluded to be better than another when actually it is worse). The studentized differences between means may be used to give ordering conclusions on all pairs of populations under consideration. Although this may be done by setting up confidence intervals using Tukey's honest significant difference, a more efficient method (when the confidence intervals themselves are not of interest) is suggested here, and tables are provided for its implementation."], ["Exact Simultaneous Confidence Intervals for Pairwise Comparisons of Three Normal Means", null], ["Discrimination with Polychotomous Predictor Variables Using Orthogonal Functions", "Three discriminant functions proposed by Bahadur (1961 a, b), Martin and Bradley (1972), and Ott and Kronmal (1976) are generalized to accommodate polychotomous predictor variables. The discrete Fourier orthogonal functions used in the generalization provide computational advantages with the use of the fast Fourier transform. Data-dependent inclusion rules for each expansion's coefficients are developed to \u201csmooth\u201d observed frequencies and improve discrimination. The performances of these methods are compared in a simulation study, and populations in which these rules are expected to perform well are identified."], ["Allocation Schemes for Estimating the Product of Positive Parameters", null], ["Some Sequential Procedures for Selecting the Better Bernoulli Treatment by Using a Matched Samples Design", null], ["A Best-Choice Problem with Linear Travel Cost", "The concepts of sampling cost and recall of previously seen applicants are here combined in a natural way: The best, second best, and so forth, of infinitely many applicants are located at points independently and uniformly distributed on the unit interval. A traveler, observing relative ranks, hopes to select the best applicant, and incurs a cost proportional to the total distance traveled, plus a unit loss if the applicant selected is not overall best. The optimal policy and its risk are derived. A finite version of the problem is also solved."], ["Bayes and Admissible Set Estimation", null], ["A Useful Inequality on Ratios of Integrals, with Application to Maximum Likelihood Estimation", null], ["A Nonlinear Version of the Gauss-Markov Theorem", null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Hard-Soft Problems", null], ["A Statistical Model for Positron Emission Tomography", "Positron emission tomography (PET)\u2014still in its research stages\u2014is a technique that promises to open new medical frontiers by enabling physicians to study the metabolic activity of the body in a pictorial manner. Much as in X-ray transmission tomography and other modes of computerized tomography, the quality of the reconstructed image in PET is very sensitive to the mathematical algorithm to be used for reconstruction. In this article, we tailor a mathematical model to the physics of positron emissions, and we use the model to describe the basic image reconstruction problem of PET as a standard problem in statistical estimation from incomplete data. We describe various estimation procedures, such as the maximum likelihood (ML) method (using the EM algorithm), the method of moments, and the least squares method. A computer simulation of a PET experiment is then used to demonstrate the ML and the least squares reconstructions. The main purposes of this article are to report on what we believe is an important contribution of statistics to PET and to familiarize statisticians with this exciting field that can benefit from further statistical methodologies to be developed with PET problems in mind. Thus no background in physics or previous knowledge of computerized tomography is assumed. The emphasis is on the basic PET model and the statistical methodology needed for it."], ["Comment: The EM Parametric Image Reconstruction Algorithm", null], ["Comment", null], ["Comment: Practical Considerations", null], ["Comment", null], ["Comment: EM for PET", null], ["Comment", null], ["Rejoinder", null], ["The Measurement of Early Retirement", null], ["Forecasting Records", "We address the problem of forecasting the future records for an athletic event on the basis of the observed past records in that event. The records are viewed as realizations of a random process. The bivariate distribution and covariance of the record in any two periods is derived by a simple extension of the theory of order statistics. In the special case of a uniform, normal, or extreme-value parent, minimum-variance-linear-unbiased (MVLU) estimates of the parameters and \u201cbest\u201d forecasts of future records may be obtained by generalized least squares. As an illustration, forecasts of the world record in six major running events are calculated for a 15-year period."], ["Statistical Analysis of Multiple Sociometric Relations", "Loglinear models are adapted for the analysis of multivariate social networks, a set of sociometric relations among a group of actors. Models that focus on the similarities and differences between the relations and models that concentrate on individual actors are discussed. This approach allows for the partitioning of the actors into blocks or subgroups. Some ideas for combining these models are described, and the various models and computational methods are applied to the analysis of data for a corporate interlock network of the 25 largest organizations in Minneapolis/St. Paul and for a classic network of 18 monks in a cloister. The computational techniques all involve variations on the standard iterative proportional-fitting algorithm used extensively in the analysis of multidimensional contingency tables."], ["Nonparametric Estimation of Lifetime Distributions from a Record of Failures and Follow-Ups", "This article deals with the estimation problem of the survival function of an industrial product in which the real operating time is different from its actual calendar time. In an observational study of the product, sometimes only the time until failure is observable from the repair requests made by the owner, but nonfailure times themselves are not. The generalized maximum likelihood estimator of the survival function is presented, and its consistency, asymptotic normality, and variance are given. Operating times of construction machines and lifetimes of Channing House men are used to illustrate the method."], ["Dynamic Generalized Linear Models and Bayesian Forecasting", "Dynamic Bayesian models are developed for application in nonlinear, non-normal time series and regression problems, providing dynamic extensions of standard generalized linear models. A key feature of the analysis is the use of conjugate prior and posterior distributions for the exponential family parameters. This leads to the calculation of closed, standard-form predictive distributions for forecasting and model criticism. The structure of the models depends on the time evolution of underlying state variables, and the feedback of observational information to these variables is achieved using linear Bayesian prediction methods. Data analytic aspects of the models concerning scale parameters and outliers are discussed, and some applications are provided."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment Bayesian Model Building and Forecasting", null], ["Rejoinder", null], ["Estimating the Population in a Census Year 1980 and beyond", "The New York case also went to trial in 1980; it was decided in favor of New York but was remanded for a new trial by the appellate court because of procedures used in the trial. The case went to trial again in January 1984 and has not yet been decided. This article by Ericksen and Kadane was an exhibit in that trial. The methodology was put forward by Ericksen and Kadane as feasible methodology to use for adjusting the 1980 census. Statisticians who testified for New York were Eugene Ericksen, Samuel Preston, John Tukey, Franklin Fisher, Joseph Kadane, Philip Hauser, Karl Taeuber, and Charles Keely. Those who testified for the government were James Trussell, Michael Stoto, Ansley Coale, Kenneth Wachter, Jacob Siegel, Richard Nathan, Vincent Barabba, Leon Gilford, Nathan Keyfitz, Gary Koch, David Freedman, Charles Cowan, Jeffrey Passel, Kirk Wolter, Robert Fay, and Barbara Bailar."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Decomposition of Prediction Error", null], ["A Diagnostic for Cox Regression and General Conditional Likelihoods", "Diagnostics for changes in the maximum likelihood estimate of \u03b2 due to deletion of single observations in linear and logistic regression models are equivalent to diagnostics obtained from fitting augmented regression models. For the Cox proportional hazards model and in conditional models for arbitrarily matched or stratified data, diagnostics derived from augmented regression models are again useful and easy to compute. These diagnostics are applicable to different formulations for the multiplicative covariable effects (over time or over strata), an exact or approximate treatment of tied failure times or multiple-case matching, and time-dependent covariates."], ["A Jackknifed Chi-Squared Test for Complex Samples", null], ["Optimal Design of Interviewer Variance Experiments in Complex Surveys", null], ["Bayesian Methods for Binomial Data with Applications to a Nonresponse Problem", "In this article, Bayesian estimation methods for several binomial probabilities are studied by using a mixture of a product of beta prior distributions. Approximations to the posterior means and credible regions are derived. The results obtained are applied to a sample survey problem in which there is significant nonresponse."], ["Modeling Agreement among Raters", null], ["Noncentrality Parameters in Chi-Squared Goodness-of-Fit Analyses with an Application to Log-Linear Procedures", null], ["Specifying a Prior Distribution in Structured Regression Problems", "The problem of assessing the prior mean and covariance matrix of the vector of regression coefficients in the normal multiple linear regression model is considered. Arguments are advanced for using expectations regarding the form of the design matrix in specifying the prior for problems with a structure that can aid in specifying these expectations. This is illustrated with models having second-order terms. An example is given."], ["Exact Simultaneous Confidence Intervals for Multiple Comparisons among Three or Four Mean Values", null], ["Asymptotic Distribution of the Hildreth\u2014Houck Estimator", "Hildreth and Houck (1968) discussed a random-coefficients regression model and proposed several weighted least squares estimators with estimated weights. They conjectured that the estimators are asymptotically equivalent to the weighted least squares estimator with known weights. In this article the conjecture is verified, providing the first correct proof that Hildreth\u2014Houck estimators are asymptotically normal. One of Hildreth and Houck's conditions is eliminated. An easily applied version of a broadly applicable theorem of Carroll and Ruppert (1982) is stated."], ["On the Bounded-Influence Regression Estimator of Krasker and Welsch", "Recently, Krasker and Welsch (1982) considered a class of bounded-influence regression estimators. They showed that within this class the so-called Krasker\u2014Welsch estimator is the only solution to a first-order necessary condition for strong optimality, that is, for minimizing, in the sense of positive definiteness, the asymptotic covariance matrix. However, whether any strongly optimal estimator in fact exists remained an open question. In this article, an example is given where no strongly optimal estimator exists. Moreover, the practical significance of the lack of a strongly optimal estimator is discussed."], ["Oversmoothed Nonparametric Density Estimates", null], ["Nonparametric Density Estimation, Prediction, and Regression for Markov Sequences", null], ["Estimating the Number of Faults in a System", null], [null, null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"]]}