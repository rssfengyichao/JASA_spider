{"2006": [["Hidden Markov Models for Microarray Time Course Data in Multiple Biological Conditions", "Among the first microarray experiments were those measuring expression over time, and time course experiments remain common. Most methods to analyze time course data attempt to group genes sharing similar temporal profiles within a single biological condition. However, with time course data in multiple conditions, a main goal is to identify differential expression patterns over time. An intuitive approach to this problem would be to apply at each time point any of the many methods for identifying differentially expressed genes across biological conditions and then somehow combine the results of the repeated marginal analyses. But considering each time point in isolation is inefficient, because it does not use the information contained in the dependence structure of the time course data. This problem is exacerbated in microarray studies, where low sensitivity is a problematic feature of many methods. Furthermore, a gene's expression pattern over time might not be identified by simply combining results from repeated marginal analyses. We propose a hidden Markov modeling approach developed to efficiently identify differentially expressed genes in time course microarray experiments and classify genes based on their temporal expression patterns. Simulation studies demonstrate a substantial increase in sensitivity, with little increase in the false discovery rate, compared with a marginal analysis at each time point. This increase is also observed in data from a case study of the effects of aging on stress response in heart tissue, where a significantly larger number of genes are identified using the proposed approach."], ["Discussion", null], ["Discussion", null], ["Rejoinder", null], ["A Bayesian Approach for Incorporating Variable Rates of Heterogeneity in Linkage Analysis", "A widely used approach for dealing with locus heterogeneity in linkage analysis is based on mixture likelihood, in which a single mixing (heterogeneity) parameter represents the probability that each family is of linked type. However, in general, different types of families exhibit different heterogeneity levels. To incorporate this variability, we propose a new approach wherein each family has its own heterogeneity parameter representing the probability that it is of linked type. These parameters are nuisance parameters, whereas the main parameter of interest is the location of the disease gene, if there is any. We model the problem in the Bayesian framework and implement it using the Markov chain Monte Carlo (MCMC) methodology. In particular, we use the reversible-jump MCMC sampler to move between the two models: linkage and no linkage. We first estimate the posterior probability of linkage on a chromosome and the corresponding Bayes factor. If linkage is inferred, then the location of the disease gene along with its credible set is estimated. The asymptotic joint distribution of the estimators is derived. We show that this approach is more powerful than the currently used approach in detecting linkage, whereas the two approaches have comparable false-positive rates. The proposed method was applied to a lung cancer dataset of Genetic Epidemiology of Lung Cancer Consortium and an asthma dataset consisting of three samples from Genetic Analysis Workshop 12. Since both lung cancer and asthma are complex traits with heterogeneous genetic predisposition, they provide suitable applications for the proposed method."], ["Using Wavelet-Based Functional Mixed Models to Characterize Population Heterogeneity in Accelerometer Profiles", "We present a case study illustrating the challenges of analyzing accelerometer data taken from a sample of children participating in an intervention study designed to increase physical activity. An accelerometer is a small device worn on the hip that records the minute-by-minute activity levels throughout the day for each day it is worn. The resulting data are irregular functions characterized by many peaks representing short bursts of intense activity. We model these data using the wavelet-based functional mixed model. This approach incorporates multiple fixed-effects and random-effects functions of arbitrary form, the estimates of which are adaptively regularized using wavelet shrinkage. The method yields posterior samples for all functional quantities of the model, which can be used to perform various types of Bayesian inference and prediction. In our case study, a high proportion of the daily activity profiles are incomplete (i.e., have some portion of the profile missing), and thus cannot be modeled directly using the previously described method. We present a new method for stochastically imputing the missing data that allows us to incorporate these incomplete profiles in our analysis. Our approach borrows strength from both the observed measurements within the incomplete profiles and from other profiles, from the same child as well as from other children with similar covariate levels, while appropriately propagating the uncertainty of the imputation throughout all subsequent inference. We apply this method to our case study, revealing some interesting insights into children's activity patterns. We point out some strengths and limitations of using this approach to analyze accelerometer data."], ["A Nonstationary Negative Binomial Time Series With Time-Dependent Covariates", "Boston Harbor has a history of poor water quality, including contamination by enteric pathogens. We conduct a statistical analysis of data collected by the Massachusetts Water Resources Authority (MWRA) between 1996 and 2002 to evaluate the effects of court-mandated improvements in sewage treatment. Motivated by the ineffectiveness of standard Poisson mixture models and their zero-inflated counterparts, we propose a new negative binomial model for time series of Enterococcus counts in Boston Harbor, where nonstationarity and autocorrelation are modeled using a nonparametric smooth function of time in the predictor. Without further restrictions, this function is not identifiable in the presence of time-dependent covariates; consequently, we use a basis orthogonal to the space spanned by the covariates and use penalized quasi-likelihood (PQL) for estimation. We conclude that Enterococcus counts were greatly reduced near the Nut Island Treatment Plant (NITP) outfalls following the transfer of wastewaters from NITP to the Deer Island Treatment Plant (DITP) and that the transfer of wastewaters from Boston Harbor to the offshore diffusers in Massachusetts Bay reduced the Enterococcus counts near the DITP outfalls."], ["Nonparametric Modeling of the Left Censorship of Analytical Data in Food Risk Assessment", "Contaminants and natural toxicants such as mycotoxins may be present in various food items that may be considered dangerous to human health if the cumulative intake remains above the toxicologic safe references. This intake or exposure can be estimated using both consumption surveys and analytical data that record the contamination levels of food. Analytical data often present some left censorship, that is, data below some limit of detection or quantification. This article proposes the integration of a nonparametric modeling of the left censorship of analytical data in a model aiming at giving a quantitative evaluation of the risk due to the presence of some particular contaminants in food. We focus on the estimation of the \u201crisk,\u201d defined as the probability for exposure to exceed the so-called \u201cprovisional tolerable weekly intake\u201d (PTWI), when both consumption data and contamination data are independently available. To account for the left censorship of the contamination data (due to the existence of detection/quantification limits), we propose using a Kaplan\u2013Meier estimator instead of the empirical cumulative distribution function generally used in nonparametric procedures. We give the asymptotic behavior of our estimator and derive the asymptotic properties of the associated risk estimator. Several confidence intervals are obtained using a double-bootstrap procedure. A detailed algorithm is proposed. As an illustration, we present an evaluation of the risk exposure to ochratoxin A in France and use our risk estimator to show that children under age 10 years are a population at particular risk. Imposing some maximum limits on particular food items, namely cereals and wine, would not significantly reduce the risk."], ["Small-Area Estimation With State\u2013Space Models Subject to Benchmark Constraints", "This article shows how to benchmark small-area estimators, produced by fitting separate state\u2013space models within the areas, to aggregates of the survey direct estimators within a group of areas. State\u2013space models are used by the U.S. Bureau of Labor Statistics (BLS) for the production of all of the monthly employment and unemployment estimates in census divisions and the states. Computation of the benchmarked estimators and their variances is accomplished by incorporating the benchmark constraints within a joint model for the direct estimators in the different areas, which requires the development of a new filtering algorithm for state\u2013space models with correlated measurement errors. The new filter coincides with the familiar Kalman filter when the measurement errors are uncorrelated. The properties and implications of the use of the benchmarked estimators are discussed and illustrated using BLS unemployment series. The problem of small-area estimation is how to produce reliable estimates of area (domain) characteristics and compute their variances when the sample sizes within the areas are too small to warrant the use of traditional direct survey estimates. This problem is commonly handled by borrowing strength from either neighboring areas and/or from previous surveys, using appropriate cross-sectional/time series models. To protect against possible model breakdowns and for consistency in publication, the area model\u2013dependent estimates often must be benchmarked to an estimate for a group of the areas, which it is sufficiently accurate. The latter estimate is a weighted sum of the direct survey estimates in the various areas, so that the benchmarking process defines another way of borrowing strength across the areas."], ["What Do Randomized Studies of Housing Mobility Demonstrate?", "During the past 20 years, social scientists using observational studies have generated a large and inconclusive literature on neighborhood effects. Recent workers have argued that estimates of neighborhood effects based on randomized studies of housing mobility, such as the \u201cMoving to Opportunity\u201d (MTO) demonstration, are more credible. These estimates are based on the implicit assumption of no interference between units; that is, a subject's value on the response depends only on the treatment to which that subject is assigned, not on the treatment assignments of other subjects. For the MTO studies, this assumption is not reasonable. Although little work has been done on the definition and estimation of treatment effects when interference is present, interference is common in studies of neighborhood effects and in many other social settings (e.g., schools and networks), and when data from such studies are analyzed under the \u201cno-interference assumption,\u201d very misleading inferences can result. Furthermore, the consequences of interference (e.g., spillovers) should often be of great substantive interest, even though little attention has been paid to this. Using the MTO demonstration as a concrete context, this article develops a frame-work for causal inference when interference is present and defines a number of causal estimands of interest. The properties of the usual estimators of treatment effects, which are unbiased and/or consistent in randomized studies without interference, are also characterized. When interference is present, the difference between a treatment group mean and a control group mean (unadjusted or adjusted for covariates) estimates not an average treatment effect, but rather the difference between two effects defined on two distinct subpopulations. This result is of great importance, for a researcher who fails to recognize this could easily infer that a treatment is beneficial when in fact it is universally harmful."], ["Exceedance Control of the False Discovery Proportion", "Multiple testing methods to control the false discovery rate, the expected proportion of falsely rejected null hypotheses among all rejections, have received much attention. It can be valuable to control not the mean of this false discovery proportion (FDP), but rather the probability that the FDP exceeds a specified bound. In this article we construct a general class of methods for exceedance control of FDP based on inverting tests of uniformity. The method also produces a confidence envelope for the FDP as a function of rejection threshold. We discuss how to select a procedure with good power."], ["The Adaptive Lasso and Its Oracle Properties", null], ["Estimating Network Loss Rates Using Active Tomography", "Active network tomography refers to an interesting class of large-scale inverse problems that arise in estimating the quality of service parameters of computer and communications networks. This article focuses on estimation of loss rates of the internal links of a network using end-to-end measurements of nodes located on the periphery. A class of flexible experiments for actively probing the network is introduced, and conditions under which all of the link-level information is estimable are obtained. Maximum likelihood estimation using the EM algorithm, the structure of the algorithm, and the properties of the maximum likelihood estimators are investigated. This includes simulation studies using the ns (network simulator) to obtain realistic network traffic. The optimal design of probing experiments is also studied. Finally, application of the results to network monitoring is briefly illustrated."], ["Focused Information Criteria and Model Averaging for the Cox Hazard Regression Model", null], ["Locally Efficient Estimators for Semiparametric Models With Measurement Error", "We derive constructive locally efficient estimators in semiparametric measurement error models. The setting is one in which the likelihood function depends on variables measured with and without error, where the variables measured without error can be modeled nonparametrically. The algorithm is based on backfitting. We show that if one adopts a parametric model for the latent variable measured with error and if this model is correct, then the estimator is semiparametric efficient; if the latent variable model is misspecified, then our methods lead to a consistent and asymptotically normal estimator. Our method further produces an estimator of the nonparametric function that achieves the standard bias and variance property. We extend the methodology to allow estimation of parameters in the measurement error model by additional data in the form of replicates or instrumental variables. The methods are illustrated through a simulation study and a data example, where the putative latent variable distribution is a shifted lognormal, but concerns about the effects of misspecification of this assumption and the linear assumption of another covariate demand a more model-robust approach. A special case of wide interest is the partial linear measurement error model. If one assumes that the model error and the measurement error are both normally distributed, then our estimator has a closed form. When a normal model for the unobservable variable is also posited, our estimator becomes consistent and asymptotically normally distributed for the general partially linear measurement error model, even without any of the normality assumptions under which the estimator is originally derived. We show that the method in fact reduces to a same estimator as that of Liang et al., thus demonstrating a previously unknown optimality property of their method."], ["Robust Estimation of Mixture Complexity", "In many applications, it is important to find the mixture with fewest number of components, known as the mixture complexity, that provides a satisfactory fit to the data. This article focuses on developing an estimator of mixture complexity that is consistent when the form of component densities are unknown but are postulated to be members of some parametric family and is simultaneously robust against model misspecification. We treat the estimation of mixture complexity as a model selection problem and construct an estimator of mixture complexity as a byproduct of minimizing a Hellinger information criterion. This estimator is shown to be consistent for any parametric family of mixtures. When the model is correctly specified, Monte Carlo simulations for a wide variety of normal mixtures show that our estimator is very competitive with several others in the literature in correctly identifying the true mixture complexity. The basic construction, being firmly rooted in the minimum Hellinger distance approach, enables our estimator to naturally inherit the property of robustness, which is examined, through simulations, under symmetric departures from postulated component normality. In terms of correctly identifying the mixture complexity under model misspecification, our estimator performs much better than an estimator based on the Kullback\u2013Leibler distance due to James, Priebe, and Marchette. An example concerning hypertension is revisited to further illustrate the performance of our estimator."], ["Bayesian Wombling", "Large-scale inference for random spatial surfaces over a region using spatial process models has been well studied. Under such models, local analysis of the surface (e.g., gradients at given points) has received recent attention. A more ambitious objective is to move from points to curves, to attempt to assign a meaningful gradient to a curve. For a point, if the gradient in a particular direction is large (positive or negative), then the surface is rapidly increasing or decreasing in that direction. For a curve, if the gradients in the direction orthogonal to the curve tend to be large, then the curve tracks a path through the region where the surface is rapidly changing. In the literature, learning about where the surface exhibits rapid change is called wombling, and a curve such as we have described is called a wombling boundary. Existing wombling methods have focused mostly on identifying points and then connecting these points using an ad hoc algorithm to create curvilinear wombling boundaries. Such methods are not easily incorporated into a statistical modeling setting. The contribution of this article is to formalize the notion of a curvilinear wombling boundary in a vector analytic framework using parametric curves and to develop a comprehensive statistical framework for curvilinear boundary analysis based on spatial process models for point-referenced data. For a given curve that may represent a natural feature (e.g., a mountain, a river, or a political boundary), we address the issue of testing or assessing whether it is a wombling boundary. Our approach is applicable to both spatial response surfaces and, often more appropriately, spatial residual surfaces. We illustrate our methodology with a simulation study, a weather dataset for the state of Colorado, and a species presence/absence dataset from Connecticut."], ["A Composite Likelihood Approach in Fitting Spatial Point Process Models", "We propose a new likelihood-based approach in fitting spatial point process models. A composite likelihood is first formed by adding some pairwise composite likelihood functions that are defined in terms of the second-order intensity function of the underlying process, and then used for estimating the unknown parameters. The estimation procedure is computationally simple and yields consistent and asymptotically normal estimators under some mild conditions. We demonstrate through a simulation study and applications to two real data examples that the proposed approach may lead to improved estimations compared with the commonly used \u201cminimum contrast estimation\u201d approach."], ["Nonparametric Analysis of Factorial Designs With Random Missingness", "We propose a nonparametric approach to the analysis of factorial designs where each subject is observed at two time points and both observations are subject to missingness. The procedures are fully nonparametric in that they do not require continuity, and do not impose models to describe the relation of the response distribution in different factor-level combinations. The approach for estimating and testing treatment and time effects is based on a method, which we introduce, for estimating a distribution function. The method requires a pattern-mixture\u2013type assumption on the missingness mechanism, which is weaker than the missing-completely-at-random assumption but neither weaker nor stronger than the missing-at-random assumption. This missingness assumption is the minimal requirement for nonparametric analysis. Comparisons with normal-based likelihood ratio tests indicate that the proposed tests fare well when the data are normal and homoscedastic, and outperform them in many other cases. Simulations also confirm that the proposed method has higher power than common nonparametric complete-pairs tests for observations missing completely at random. Finally, a dataset on the delinquent values of boys released from correctional institutions is analyzed and discussed."], ["Empirical Bayesian Analysis for Computer Experiments Involving Finite-Difference Codes", null], ["Fixed-Width Output Analysis for Markov Chain Monte Carlo", "Markov chain Monte Carlo is a method of producing a correlated sample to estimate features of a target distribution through ergodic averages. A fundamental question is when sampling should stop; that is, at what point the ergodic averages are good estimates of the desired quantities. We consider a method that stops the simulation when the width of a confidence interval based on an ergodic average is less than a user-specified value. Hence calculating a Monte Carlo standard error is a critical step in assessing the simulation output. We consider the regenerative simulation and batch means methods of estimating the variance of the asymptotic normal distribution. We give sufficient conditions for the strong consistency of both methods and investigate their finite-sample properties in various examples."], ["Inference for Mixtures of Finite Polya Tree Models", "Mixtures of Polya tree models provide a flexible alternative when a parametric model may only hold approximately. I provide computational strategies for obtaining full semiparametric inference for mixtures of finite Polya tree models given a standard parameterization, including models that would be troublesome to fit using Dirichlet process mixtures. Recommendations are put forth on choosing the level of a finite Polya tree, and model comparison is discussed. Several examples demonstrate the utility of finite Polya tree modeling, including data fit to generalized linear mixed models and several survival models."], ["Hierarchical Dirichlet Processes", "We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the \u201cChinese restaurant franchise.\u201d We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling."], ["A Monte Carlo Approach to Filtering for a Class of Marked Doubly Stochastic Poisson Processes", "Marked doubly stochastic Poisson processes are a particular type of marked point processes that are characterized by the number of events in any time interval as being conditionally Poisson distributed, given another positive stochastic process called intensity. Here we consider a subclass of these processes in which the intensity is assumed to be a deterministic function of another nonexplosive marked point process. In particular, we will investigate an intensity jump process with an exponential decay having an analytic form for the distribution of the times and sizes of the jumps, which can be seen as a generalization of the classical shot noise process. Assuming that the intensity is unobservable, interest here is in its filtering, that is, in the computation of its conditional distribution, over a whole time interval, given an observed trajectory of realized events. Because, in general, this computation cannot be performed analytically, we propose a simulation method that provides an approximate solution, which relies on the reversible-jump Markov chain Monte Carlo algorithm. Interestingly, the proposed filtering algorithm also allows the setup of a likelihood-based procedure for the estimation of the parameters of the model based on stochastic versions of the expectation\u2013maximization (EM) algorithm. The potential of the filtering and estimation methods proposed are illustrated through some simulation experiments as well as on a financial ultra-high-frequency dataset of intraday S&P500 futures prices."], ["Generalized Exponential Predictors for Time Series Forecasting", "We consider the problem of prediction for stationary and nonstationary univariate time series using a modification suggested by the usual exponentially weighted moving average method. The modification leads to a class of general exponential predictors that can improve on the usual finite approximations to an infinite autoregressive process. We provide the theoretical justifications and suggest a class of predictors that covers modified and finite autoregressive fits as special cases. Two examples involving sample data show how the method is competitive with autoregressive integrated moving average (ARIMA) when applied to a U.S. energy use series and improves on ARIMA when applied to a global temperature series. A simulation indicates that considerable improvements are possible for infinite autoregressive (ARIMA) processes exhibiting certain special patterns of long-range dependence."], ["Regression and Weighting Methods for Causal Inference Using Instrumental Variables", "Recent researches in econometrics and statistics have gained considerable insights into the use of instrumental variables (IVs) for causal inference. A basic idea is that IVs serve as an experimental handle, the turning of which may change each individual's treatment status and, through and only through this effect, also change observed outcome. The average difference in observed outcome relative to that in treatment status gives the average treatment effect for those whose treatment status is changed in this hypothetical experiment. We build on the modern IV framework and develop two estimation methods in parallel to regression adjustment and propensity score weighting in the case of treatment selection based on covariates. The IV assumptions are made explicitly conditional on covariates to allow for the fact that instruments can be related to these background variables. The regression method focuses on the relationship between responses (observed outcome and treatment status jointly) and instruments adjusted for covariates. The weighting method focuses on the relationship between instruments and covariates to balance different instrument groups with respect to covariates. For both methods, modeling assumptions are made directly on observed data and separated from the IV assumptions, whereas causal effects are inferred by combining observed data models with the IV assumptions through identification results. This approach is straightforward and flexible enough to host various parametric and semiparametric techniques that attempt to learn associational relationships from observed data. We illustrate the methods by an application to estimating returns to education."], ["A Distributional Approach for Causal Inference Using Propensity Scores", "Drawing inferences about the effects of treatments and actions is a common challenge in economics, epidemiology, and other fields. We adopt Rubin's potential outcomes framework for causal inference and propose two methods serving complementary purposes. One can be used to estimate average causal effects, assuming no confounding given measured covariates. The other can be used to assess how the estimates might change under various departures from no confounding. Both methods are developed from a nonparametric likelihood perspective. The propensity score plays a central role and is estimated through a parametric model. Under the assumption of no confounding, the joint distribution of covariates and each potential outcome is estimated as a weighted empirical distribution. Expectations from the joint distribution are estimated as weighted averages or, equivalently to first order, regression estimates. The likelihood estimator is at least as efficient and the regression estimator is at least as efficient and robust as existing estimators. Regardless of the no-confounding assumption, the marginal distribution of covariates times the conditional distribution of observed outcome given each treatment assignment and covariates is estimated. For a fixed bound on unmeasured confounding, the marginal distribution of covariates times the conditional distribution of counterfactual outcome given each treatment assignment and covariates is explored to the extreme and then compared with the composite distribution corresponding to observed outcome given the same treatment assignment and covariates. We illustrate the methods by analyzing the data from an observational study on right heart catheterization."], ["Fourier Methods for Estimating the Central Subspace and the Central Mean Subspace in Regression", "In regression with a high-dimensional predictor vector, it is important to estimate the central and central mean subspaces that preserve sufficient information about the response and the mean response. Using the Fourier transform, we have derived the candidate matrices whose column spaces recover the central and central mean subspaces exhaustively. Under the normality assumption of the predictors, explicit estimates of the central and central mean subspaces are derived. Bootstrap procedures are used for determining dimensionality and choosing tuning parameters. Simulation results and an application to a real data are reported. Our methods demonstrate competitive performance compared with SIR, SAVE, and other existing methods. The approach proposed in the article provides a novel view on sufficient dimension reduction and may lead to more powerful tools in the future."], ["Families of Multivariate Distributions Involving the Rosenblatt Construction", null], ["Inference on the Number of Species Through Geometric Lower Bounds", "Estimating the number of species in a population from a sample of individuals is investigated in a nonparametric Poisson mixture model. A sequence of lower bounds to the odds that a species is unseen in the sample are proposed from a geometric perspective. A lower bound and its representing mixing distribution can be computed by linear programming with guaranteed convergence. These lower bounds can be estimated by the maximum likelihood method and used to construct lower confidence limits for the number of species by the bootstrap method. Computing the nonparametric maximum likelihood estimator is discussed. Simulation is used to assess the performance of estimated lower bounds and compare them with several existing estimators. A genomic application is investigated."], ["Optimizing the Expected Overlap of Survey Samples via the Northwest Corner Rule", "In survey sampling there is often a need to coordinate the selection of pairs of samples drawn from two overlapping populations so as to maximize or minimize their expected overlap, subject to constraints on the marginal probabilities determined by the respective designs. For instance, maximizing the expected overlap between repeated samples can stabilize the resulting estimates of change and reduce the costs of first contacts; minimizing the expected overlap can avoid overburdening respondents with multiple surveys. We focus on the important special case in which both samples are selected by simple random sampling without replacement (SRSWOR) conducted independently within each stratum. Optimizing the expected sample overlap can be formulated as a linear programming problem known as a transportation problem (TP). We show that by appropriately grouping and ordering the possible samples in each survey, one can reduce the initial TP to a much smaller TP amenable to solution by an algorithm known as the Northwest Corner Rule (NWCR). The proposed NWCR method proceeds in two easily implemented steps: first selecting the numbers of births (new units) and deaths (deleted units) by a random selection from a hypergeometric distribution, and then selecting the births and deaths by SRSWOR. We formally prove properties of the NWCR solutions, including a minimal variance property of the minimal overlap solution. In a simulation study, the NWCR method compares favorably with a popular method based on assignment of permanent random numbers to each sampling unit."], ["Algorithms for Constructing Combined Strata Variance Estimators", "A jackknife or balanced repeated replication variance estimator in a large survey typically requires a large number of replicates and replicate weights. Reducing the number of replicates may have important advantages for computations and for limiting the risk of data disclosure from public use data files. This article proposes algorithms adapted from scheduling theory to combine variance strata and, thus, reduce the number of replicates. The algorithms are simple and efficient and can be adapted to easily account for vector characteristics and analytic domains. An important concern with combining strata is that the resulting variance estimators may be inconsistent. We establish conditions for the consistency of the combined variance estimator and show that the proposed algorithms ensure they are met. We also derive bounds on the degrees of freedom that the algorithms will assure. The algorithms are applied both to a real sample survey and to samples from simulated populations, and the algorithms perform very well, attaining variance estimators with precision levels close to the upper bounds."], ["Concomitants of Multivariate Order Statistics With Application to Judgment Poststratification", "We generalize the definition of a concomitant of an order statistic in the multivariate case, develop general expressions for its density, and establish related properties. We study the concomitant of a normal random vector in detail and discuss methods for calculating its moments. Furthermore, we apply the theory to develop new estimators of the mean from a judgment poststratified sample, where poststrata are formed by rank classes of auxiliary variables. Our estimators are shown to be more efficient than existing ones and robust against violations of the normality assumption. They are also well suited to applications requiring cost efficiency."], ["The Hazard Potential", "This is an expository article directed at reliability theorists, survival analysts, and others interested in looking at life history and event data. Here we introduce the notion of a hazard potential as an unknown resource that an item is endowed with at inception. The item fails when this resource becomes depleted. The cumulative hazard is a proxy for the amount of resource consumed, and the hazard function is a proxy for the rate at which this resource is consumed. With this conceptualization of the failure process, we are able to characterize accelerated, decelerated, and normal tests and are also able to provide a perspective on the cause of interdependent lifetimes. Specifically, we show that dependent life lengths are the result of dependent hazard potentials. Consequently, we are able to generate new families of multivariate life distributions using dependent hazard potentials as a seed. For an item that operates in a dynamic environment, we argue that its lifetime is the killing time of a continuously increasing stochastic process by a random barrier, and this barrier is the item's hazard potential. The killing time perspective enables us to see competing risks from a process standpoint and to propose a framework for the joint modeling of degradation or cumulative damage and its markers. The notion of the hazard potential generalizes to the multivariate case. This generalization enables us to replace a collection of dependent random variables by a collection of independent exponentially distributed random variables, each having a different time scale."], ["Book Reviews", null], ["Introduction to Statistical Thought", null], ["The Grammar of Graphics", null], ["Statistics for Experimenters: Design, Innovation, and Discovery", null], ["Kendall's Advanced Theory of Statistics, Vol. 1: Distribution Theory, Kendall's Advanced Theory of Statistics, Vol. 2A: Classical Inference and the Linear Model", null], ["Introduction to Nonparametric Regression", null], ["Semiparametric Regression", null], ["Quantile Regression", null], ["Image Processing and Jump Regression Analysis", null], ["Generalized, Linear, and Mixed Models", null], ["Mixed Models: Theory and Applications", null], ["Modeling Longitudinal Data", null], ["Linear Models for Optimal Test Design", null], ["Applied Adaptive Statistical Methods: Tests of Significance and Confidence Intervals", null], ["Functional Analysis for Probability and Stochastic Processes: An Introduction. Adam Bobrowski", null], ["SAS for Monte Carlo Studies: A Guide for Quantitative Researchers", null], ["Missing Data and Small-Area Estimation: Modern Analytical Equipment for the Survey Statistician", null], ["Kernel Methods for Pattern Analysis", null], ["Introductory Econometrics: Using Monte Carlo Simulation With Microsoft Excel", null], ["Binomial Models in Finance", null], ["Quantitative Risk Management: Concepts, Techniques, and Tools", null], ["Forecasting: Practice and Process for Demand Management", null], ["Bayesian Statistics and Marketing", null], ["Statistical Modelling in Glim4", null], ["Statistical Monitoring of Clinical Trials: Fundamentals for Investigators", null], ["Handbook of Exponential and Related Distributions for Engineers and Scientists", null], ["Telegraphic Reviews", null], ["2006 Editorial Collaborators", null], ["Recidivism and Social Interactions", "Using a national sample, this article identifies the risk factors for recidivism among female, male, black, white, and Hispanic felony probationers. The individual hazard function is assumed to depend on individual and neighborhood characteristics, as well as on social interactions among probationers. In selecting the covariates from a set of potential candidates, Bayesian model averaging is used to account for both model uncertainty and the subsequent inference. The results point to social interactions as one of the most significant factors affecting recidivism among all gender, ethnicity, and race groups. When a frailty parameter is added to account for the possibility of unobserved risk factors shared by probationers within neighborhoods, the empirical results remain robust indicating negligible unobserved neighborhood-level heterogeneity."], ["Testing for Racial Profiling in Traffic Stops From Behind a Veil of Darkness", "The key problem in testing for racial profiling in traffic stops is estimating the risk set, or \u201cbenchmark,\u201d against which to compare the race distribution of stopped drivers. To date, the two most common approaches have been to use residential population data or to conduct traffic surveys in which observers tally the race distribution of drivers at a certain location. It is widely recognized that residential population data provide poor estimates of the population at risk of a traffic stop; at the same time, traffic surveys have limitations and are more costly to carry out than the alternative that we propose herein. In this article we propose a test for racial profiling that does not require explicit, external estimates of the risk set. Rather, our approach makes use of what we call the \u201cveil of darkness\u201d hypothesis, which asserts that police are less likely to know the race of a motorist before making a stop after dark than they are during daylight. If we assume that racial differences in traffic patterns, driving behavior, and exposure to law enforcement do not vary between daylight and darkness, then we can test for racial profiling by comparing the race distribution of stops made during daylight to the race distribution of stops made after dark. We propose a means of weakening this assumption by restricting the sample to stops made during the evening hours and controlling for clock time while estimating daylight/darkness contrasts in the race distribution of stopped drivers. We provide conditions under which our estimates are robust to a substantial nonreporting problem present in our data and in many other studies of racial profiling. We propose an approach to assess the sensitivity of our results to departures from our maintained assumptions. Finally, we apply our method to data from Oakland, California and find that in this example the data yield little evidence of racial profiling in traffic stops."], ["Randomization Inference With Natural Experiments", null], ["Evaluating Kindergarten Retention Policy", null], ["A Hierarchical Multivariate Two-Part Model for Profiling Providers' Effects on Health Care Charges", "Procedures for analyzing and comparing health care providers' effects on health services delivery and outcomes have been referred to as provider profiling. In a typical profiling procedure, patient-level responses are measured for clusters of patients treated by providers that in turn can be considered statistically exchangeable. Thus a hierarchical model naturally represents the structure of the data. When provider effects on multiple responses are profiled, a multivariate model rather than a series of univariate models can capture associations among responses at both the provider and patient levels. When responses are in the form of charges for health care services and sampled patients include nonusers of services, charge variables are a mix of 0's and highly skewed positive values that present a modeling challenge. For analysis of covariate effects on charges for a single service, a frequently used approach is a two-part model that combines logistic or probit regression on any use of the service and linear regression on log-positive charges given use of the service. Here we extend the two-part model to the case of charges for multiple services, using a log-linear model and a general multivariate lognormal model, and use the resultant multivariate two-part model as the within-provider component of a hierarchical model. The log-linear likelihood is reparameterized as proposed by Fitzmaurice and Laird, so that covariate effects on any use of each service are marginal with respect to any use of other services. The general multivariate lognormal likelihood is structured in such a way that the variance of log-positive charges for each service is provider-specific but correlations among logs of positive charges for different services are uniform across providers. A data augmentation step is included in the Gibbs sampler used to fit the hierarchical model to accommodate the fact that values of log-positive charges are undefined for unused services. We apply this hierarchical, multivariate, two-part model to analyze the effects of primary care physicians on their patients' annual charges for two services, primary care and specialty care. We also demonstrate an approach for incorporating prior information about the effects of patient morbidity on response variables, to improve the accuracy of provider profiles based on patient samples of limited size."], ["Multiple Imputation of Missing Income Data in the National Health Interview Survey", null], ["Bayesian Inference for a Two-Part Hierarchical Model", "Profiling is currently an important, and hotly debated, topic in health care and other industries looking for ways to control costs, increase profitability, and increase service quality. Managed care in particular has seen a proliferation in the use of statistical profiling methodology, particularly with regard to monitoring expenditure data. This article focuses on the specific problem of developing statistical methods appropriate for profiling physician contributions to patient pharmacy expenditures incurred in a managed care setting. The two-part hierarchical model with a correlated random-effects structure considered here accounts for both the skewed, zero-inflated nature of pharmacy expenditure data and the fact that patient pharmacy expenditures are correlated within physicians. The random-effects structure has an attractive interpretation in terms of a conceptual model for physician prescribing patterns. Using this model, we propose to rank physicians based on an appropriately constructed provider-level performance measure. This information is subsequently used to develop a novel financial incentive scheme. Inference is conducted in a Bayesian framework using Markov chain Monte Carlo."], ["Investigating Heterogeneity in Pneumococcal Transmission", null], ["Trees for Correlated Survival Data by Goodness of Split, With Applications to Tooth Prognosis", "In this article the regression tree method is extended to correlated survival data and applied to the problem of developing objective prognostic classification rules in periodontal research. The robust logrank statistic is used as the splitting statistic to measure the between-node difference in survival, while adjusting for correlation among failure times from the same patient. The partition-based survival function estimator is shown to converge to the true conditional survival function. Tooth loss data from 100 periodontal patients (2,509 teeth) was analyzed using the proposed method. The goal is to assign each tooth to one of the five prognosis categories (good, fair, poor, questionable, or hopeless). After the best-sized tree was identified, an amalgamation procedure was used to form five prognostic groups. The prognostic rules established here may be used by periodontists, general dentists, and insurance companies in devising appropriate treatment plans for periodontal patients."], ["Calibrated Probabilistic Forecasting at the Stateline Wind Energy Center", "With the global proliferation of wind power, the need for accurate short-term forecasts of wind resources at wind energy sites is becoming paramount. Regime-switching space\u2013time (RST) models merge meteorological and statistical expertise to obtain accurate and calibrated, fully probabilistic forecasts of wind speed and wind power. The model formulation is parsimonious, yet takes into account all of the salient features of wind speed: alternating atmospheric regimes, temporal and spatial correlation, diurnal and seasonal nonstationarity, conditional heteroscedasticity, and non-Gaussianity. The RST method identifies forecast regimes at a wind energy site and fits a conditional predictive model for each regime. Geographically dispersed meteorological observations in the vicinity of the wind farm are used as off-site predictors. The RST technique was applied to 2-hour-ahead forecasts of hourly average wind speed near the Stateline wind energy center in the U. S. Pacific Northwest. The RST point forecasts and distributional forecasts were accurate, calibrated, and sharp, and they compared favorably with predictions based on state-of-the-art time series techniques. This suggests that quality meteorological data from sites upwind of wind farms can be efficiently used to improve short-term forecasts of wind resources."], ["Quantile Autoregression", "We consider quantile autoregression (QAR) models in which the autoregressive coefficients can be expressed as monotone functions of a single, scalar random variable. The models can capture systematic influences of conditioning variables on the location, scale, and shape of the conditional distribution of the response, and thus constitute a significant extension of classical constant coefficient linear time series models in which the effect of conditioning is confined to a location shift. The models may be interpreted as a special case of the general random-coefficient autoregression model with strongly dependent coefficients. Statistical properties of the proposed model and associated estimators are studied. The limiting distributions of the autoregression quantile process are derived. QAR inference methods are also investigated. Empirical applications of the model to the U.S. unemployment rate, short-term interest rate, and gasoline prices highlight the model's potential."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Functional Variance Processes", "We introduce the notion of a functional variance process to quantify variation in functional data. The functional data are modeled as samples of smooth random trajectories observed under additive noise. The noise is assumed to be composed of white noise and a smooth random process\u2014the functional variance process\u2014which gives rise to smooth random trajectories of variance. The functional variance process is a tool for analyzing stochastic time trends in noise variance. As a smooth random process, it can be characterized by the eigenfunctions and eigenvalues of its autocovariance operator. We develop methods to estimate these characteristics from the data, applying concepts from functional data analysis to the residuals obtained after an initial smoothing step. Asymptotic justifications for the proposed estimates are provided. The proposed functional variance process extends the concept of a variance function, an established tool in nonparametric and semiparametric regression analysis, to the case of functional data. We demonstrate that functional variance processes offer a novel data analysis technique that leads to relevant findings in applications, ranging from a seismic discrimination problem to the analysis of noisy reproductive trajectories in evolutionary biology."], ["Estimation in Multiple-Frame Surveys", "Multiple-frame surveys are commonly used to decrease costs of sampling or to reduce undercoverage that could occur if only one sampling frame were used. We describe potential uses and examples of multiple-frame surveys. We then derive optimal linear estimators and pseudo\u2013maximum likelihood estimators for the population total when samples are taken independently from each frame using probability sampling designs. We explore the properties of these estimators theoretically and through a simulation study. We also derive variance estimators and discuss some practical problems that may be encountered in multiple-frame surveys."], ["Optimal and Efficient Crossover Designs When Subject Effects Are Random", "Most studies on optimal crossover designs are based on models that assume subject effects to be fixed effects. In this article we identify and study optimal and efficient designs for a model with random subject effects. With the number of periods not exceeding the number of treatments, we find that totally balanced designs are universally optimal for treatment effects in a large subclass of competing designs. However, in the entire class of designs, totally balanced designs are in general not optimal, and their efficiency depends on the ratio of the subject effects variance and the error variance. We develop tools to study the efficiency of totally balanced designs and to identify designs with higher efficiency."], ["On Lower Tolerance Limits With Accurate Coverage Probabilities for the Normal Random Effects Model", "A method for constructing accurate lower tolerance limits for the balanced one-way normal random-effects model is derived by conditioning on an estimator of the unknown expected mean square ratio. Simulation studies indicate that the present procedure is less conservative than that of several existing methods and gives more accurate coverage rates and smaller standard deviations. Numerical examples are given to illustrate the use of the new procedure. Tables needed to implement the procedure are also included."], ["Statistical Inference for the Difference Between the Best Treatment Mean and a Control Mean", "In many experiments, researchers are interested in comparing several treatment means with a control mean. When there are some treatments significantly better than the control, it is often of interest to evaluate the difference between the best treatment mean and the control mean and to identify the best treatment. In this article we derive lower confidence bounds for the aforementioned difference for the case that treatments are at least as effective as the control and for the case that no restriction is placed on the treatment means and the control mean. The evaluation of the lower confidence bound for the difference between the best treatment mean and the control mean is a concave programming problem subject to homogeneous linear inequality constraints. We propose two efficient computation algorithms and discuss the connection between our procedures and Gupta's subset selection procedure. We compare the expected lower confidence bounds of the two procedures with that of Dunnett's procedure. An application to a real-life data is included."], ["Estimation in Linear Models Based on Observations With Unknown and Possibly Unequal Scaling", null], ["Geoadditive Survival Models", "Survival data often contain small-area geographical or spatial information, such as the residence of individuals. In many cases, the impact of such spatial effects on hazard rates is of considerable substantive interest. Therefore, extensions of known survival or hazard rate models to spatial models have been suggested. Mostly, a spatial component is added to the usual linear predictor of the Cox model. In this article flexible continuous-time geoadditive models are proposed, extending the Cox model with respect to several aspects often needed in applications. The common linear predictor is generalized to an additive predictor, including nonparametric components for the log-baseline hazard, time-varying effects, and possibly nonlinear effects of continuous covariates or further time scales, and a spatial component for geographical effects. In addition, uncorrelated frailty effects or nonlinear two-way interactions can be incorporated. Inference is developed within a unified fully Bayesian framework. Penalized regression splines and Markov random fields are suggested as basic building blocks, and geostatistical (kriging) models are also considered. Posterior analysis uses computationally efficient Markov chain Monte Carlo sampling schemes. Smoothing parameters are an integral part of the model and are estimated automatically. Propriety of posteriors is shown under fairly general conditions, and practical performance is investigated through simulation studies. Our approach is applied to data from a case study in London and Essex that aims to estimate the effect of area of residence and further covariates on waiting times to coronary artery bypass grafting. Results provide clear evidence of nonlinear time-varying effects, and considerable spatial variability of waiting times to bypass grafting."], ["Locally Efficient Estimation With Bivariate Right-Censored Data", null], ["Rank Estimation of Accelerated Lifetime Models With Dependent Censoring", "Under independent censoring, estimation of the covariate effects in the accelerated lifetime model may be based on censored data rank tests. Similar rank methodology has been developed with bivariate accelerated lifetime models for dependent censoring but uses artificial censoring, which may lead to substantial information loss. We present a new artificial censoring technique using pairwise ranking and establish the asymptotic properties of a pairwise rank estimator. Simulations show that the pairwise approach achieves large reductions in artificial censoring and large efficiency gains over the existing rank estimator. The simulations evidence moderate efficiency gains under independent censoring over a rank estimator that is semiparametric efficient under independent censoring. An AIDS data analysis illustrates the practical utility of the inferential procedures."], ["Estimating a Unimodal Distribution From Interval-Censored Data", "In this article we consider three nonparametric maximum likelihood estimators based on mixed-case interval-censored data. Apart from the unrestricted estimator, we consider estimators under the assumption that the underlying distribution function of event times is concave or unimodal. Characterizations of the estimates are derived, and algorithms are proposed for their computation. The estimators are shown to be asymptotically consistent, and the benefits of additional constraints are illustrated through simulations. Finally, the estimators are used as an ingredient in a nonparametric comparison of two samples."], ["Nonparametric Two-Sample Methods for Ranked-Set Sample Data", "A new collection of procedures is developed for the analysis of two-sample, ranked-set samples, providing an alternative to the Bohn\u2013Wolfe procedure. These procedures split the data based on the ranks in the ranked-set sample and lead to tests for the centers of distributions, confidence intervals, and point estimators. The advantages of the new tests are that they require essentially no assumptions about the mechanism by which rankings are imperfect, that they maintain their level whether rankings are perfect or imperfect, that they lead to generalizations of the Bohn\u2013Wolfe procedure that can be used to increase power in the case of perfect rankings, and that they allow one to analyze both balanced and unbalanced ranked-set samples. A new class of imperfect ranking models is proposed, and the performance of the procedure is investigated under these models. When rankings are random, a theorem is presented which characterizes efficient data splits. Because random rankings are equivalent to iid samples, this theorem applies to a wide class of statistics and has implications for a variety of computationally intensive methods."], ["A Statistical Measure of Regularity for the Study of Wind-Generated Wave Field Images", "The study of water waves has generated a wealth of sophisticated modeling developments in applied mathematics. Empirical observation capabilities have created a need for novel data analysis tools. This article is motivated by consideration of wind-generated wave field image data from a wave tank facility. A quantitative measure of wave field regularity is developed. The methodology is based on decomposition of the wave field into simple plane waves (an adaptation of projection pursuit). The percent variance explained as a function of the number of terms in the plane wave representation is used to define a variogram, and regularity of the wave field is assessed in terms of the weighted difference between the observed variogram and the expected variogram for a completely random field. The proposed regularity measure is illustrated by application to image data from wind-generated waves. The results suggest that the regularity measure is a function of parameters describing the generation of the wave field (wind speed and evolution). An analysis of the statistical behavior of the regularity measure as a function of sample size (image resolution) is carried out. Even though the regularity measure is based on the nonparametric estimation of functions, provided that this estimation is carried out in a consistent fashion, the error in estimation of regularity has a parametric dependence on image resolution."], ["Bayes Linear Calibrated Prediction for Complex Systems", "A calibration-based approach is developed for predicting the behavior of a physical system that is modeled by a computer simulator. The approach is based on Bayes linear adjustment using both system observations and evaluations of the simulator at parameterizations that appear to give good matches to those observations. This approach can be applied to complex high-dimensional systems with expensive simulators, where a fully Bayesian approach would be impractical. It is illustrated with an example concerning the collapse of the thermohaline circulation (THC) in the Atlantic Ocean."], ["Likelihood Subgradient Densities", null], [null, null], ["Generalized Poststratification and Importance Sampling for Subsampled Markov Chain Monte Carlo Estimation", "Benchmark estimation is motivated by the goal of producing an approximation to a posterior distribution that is better than the empirical distribution function. This is accomplished by incorporating additional information into the construction of the approximation. We focus here on generalized poststratification, the most successful implementation of benchmark estimation in our experience. We develop generalized poststratification for settings where the source of the simulation differs from the posterior that is to be approximated. This allows us to use the techniques in settings where it is advantageous to draw from a distribution different than the posterior, whether for exploration of the data and/or model, for algorithmic simplicity, for improved convergence of the simulation, or for improved estimation of selected features of the posterior. We develop an asymptotic (in simulation size) theory for the estimators, providing conditions under which central limit theorems hold. The central limit theorems apply both to an importance sampling context and to direct sampling from the posterior distribution. The asymptotic results, coupled with large-sample (size of data) approximation results provide guidance on how to implement generalized poststratification. The theoretical results also explain the gains associated with generalized poststratification and the empirically observed robustness to cutpoints for the strata. We note that the results apply well beyond the setting of Markov chain Monte Carlo simulation. The technique is illustrated with an infinite-dimensional semiparametric Bayesian regression model and a low-dimensional, overdispersed hierarchical Bayesian model. In both cases, the technique shows substantial benefits."], [null, null], ["Principal Components Analysis Based on Multivariate MM Estimators With Fast and Robust Bootstrap", "We consider robust principal components analysis (PCA) based on multivariate MM estimators. We first study the robustness and efficiency of these estimators, particularly in terms of eigenvalues and eigenvectors. We then focus on inference procedures based on a fast and robust bootstrap for MM estimators. This method is an alternative to the approach based on the asymptotic distribution of the estimators and can also be used to assess the stability of the principal components. A formal consistency proof for the bootstrap method is given, and its finite-sample performance is investigated through simulations. We illustrate the use of the robust PCA and the bootstrap inference on a real dataset."], ["Estimation and Testing for Varying Coefficients in Additive Models With Marginal Integration", null], ["Efficient Estimation of Semiparametric Multivariate Copula Models", "We propose a sieve maximum likelihood estimation procedure for a broad class of semiparametric multivariate distributions. A joint distribution in this class is characterized by a parametric copula function evaluated at nonparametric marginal distributions. This class of distributions has gained popularity in diverse fields due to its flexibility in separately modeling the dependence structure and the marginal behaviors of a multivariate random variable, and its circumvention of the \u201ccurse of dimensionality\u201d associated with purely nonparametric multivariate distributions. We show that the plug-in sieve maximum likelihood estimators (MLEs) of all smooth functionals, including the finite-dimensional copula parameters and the unknown marginal distributions, are semiparametrically efficient, and that their asymptotic variances can be estimated consistently. Moreover, prior restrictions on the marginal distributions can be easily incorporated into the sieve maximum likelihood estimation procedure to achieve further efficiency gains. Two such cases are studied: (a) the marginal distributions are equal but otherwise unspecified, and (b) some but not all marginal distributions are parametric. Monte Carlo studies indicate that the sieve MLEs perform well in finite samples, especially when prior information on the marginal distributions is incorporated."], ["Modeling Marked Point Processes via Bivariate Mixture Transition Distribution Models", null], ["Distribution of Runs and Longest Runs", "Exact distributions of run statistics are traditionally obtained using combinatorial methods, which, under certain situations, become very tedious. Run distributions of multiple object systems, although appearing frequently in applications from various fields, such as computational biology, are not commonly used, due in part to the lack of easy-to-use formulas. In this article, a method for evaluating partition functions of lattice models in the field of statistical mechanics is used to develop a systematic method to study various run statistics in multiple object systems. By using particular generating functions for the specified situation under study, many new distributions can be obtained in a unified and coherent way. The method makes it possible to manipulate formulas of run statistics by using binomial identities to obtain more general, yet simpler formulas. To illustrate the applications of the general method, the distributions of the total number of runs and the longest runs are investigated. Novel and general explicit formulas are derived for the distribution and moments of the total number of runs, and simple explicit formulas are derived for the distributions of the longest runs. In addition, some classical run statistics are recovered and generalized in the same unified way. As examples of applications to biological sequence analysis, the run statistics developed using the general method are applied to several protein sequences to examine their global and local features."], [null, null], ["Doubly Robust Estimation of the Area Under the Receiver-Operating Characteristic Curve in the Presence of Verification Bias", "The area under the receiver operating characteristic curve (AUC) is a popular summary measure of the efficacy of a medical diagnostic test to discriminate between healthy and diseased subjects. A frequently encountered problem in studies that evaluate a new diagnostic test is that not all patients undergo disease verification because the verification test is expensive, invasive, or both. Furthermore, the decision to send patients to verification often depends on the new test and on other predictors of true disease status. In such cases, usual estimators of the AUC based on verified patients only are biased. In this article we develop estimators of the AUC of markers measured on any scale that adjust for selection to verification. These estimators adjust for measured patient covariates and diagnostic test results and also for an assumed degree of residual selection bias. They can then be used in a sensitivity analysis to examine how the AUC estimates change when different plausible degrees of residual association are assumed. As with other missing-data problems, due to the curse of dimensionality, a model for disease or a model for selection is needed to obtain well-behaved estimators of the AUC when the marker and/or the measured covariates are continuous. We describe a doubly robust estimator that has the attractive feature of being consistent and asymptotically normal if either the disease or the selection model (but not necessarily both) is correct."], ["Tests of Spatial Randomness Adjusted for an Inhomogeneity", "In many applications, it is of interest to test whether a spatial point pattern is randomly generated after adjusting for an underlying spatial inhomogeneity. A great variety of different test statistics have been proposed for this purpose by scientists in different fields; these are reviewed in this article. Despite apparent dissimilarities in terms of their original formulations, most of these statistics can be placed into one general framework of which they are special cases. This makes it easier to see exactly how they relate to one another and also to determine which test to use for a particular application. The general framework can also be used for proposing new tests by combining properties of existing tests, for developing theoretical foundations for these types of test statistics, for doing structured comparative evaluations, and for software development."], ["Book Reviews", null], ["Models for Discrete Longitudinal Data", null], ["Fixed Effects Regression Methods for Longitudinal Data Using SAS", null], ["Nonparametric and Semiparametric Models", null], ["Generalized Method of Moments", null], ["Analysis of Variance for Random Models: Theory, Methods, Applications, and Data Analysis, Vol. 1: Balanced Data, Analysis of Variance for Random Models: Theory, Methods, Applications, and Data Analysis, Vol. 2: Unbalanced Data.", null], ["Analysis of Variance for Random Models: Theory, Methods, Applications, and Data Analysis, Vol. 1: Balanced Data,  Analysis of Variance for Random Models: Theory, Methods, Applications, and Data Analysis, Vol. 2: Unbalanced Data", null], ["Survey Methodology", null], ["Environmental Statistics: Methods and Applications", null], ["Analysis and Modelling of Spatial Environmental Data", null], ["Statistical Methods in Molecular Evolution", null], ["Bayesian Nonparametrics via Neural Networks", null], ["Contemporary Bayesian Econometrics and Statistics", null], ["Multivariate Bayesian Statistics: Models for Source Separation and Signal Unmixing", null], ["Financial Modelling With Jump Processes", null], ["Insurance Risk and Ruin", null], ["Flowgraph Models for Multistate Time-to-Event Data", null], ["An Introduction to Statistical Signal Processing", null], ["Random Graphs for Statistical Pattern Recognition", null], ["Applied Data Mining: Statistical Methods for Business and Industry", null], ["Telegraphic Reviews", null], ["Correction", null], ["How Many People Do You Know in Prison?", "Networks\u2014sets of objects connected by relationships\u2014are important in a number of fields. The study of networks has long been central to sociology, where researchers have attempted to understand the causes and consequences of the structure of relationships in large groups of people. Using insight from previous network research, Killworth et al. and McCarty et al. have developed and evaluated a method for estimating the sizes of hard-to-count populations using network data collected from a simple random sample of Americans. In this article we show how, using a multilevel overdispersed Poisson regression model, these data also can be used to estimate aspects of social structure in the population. Our work goes beyond most previous research on networks by using variation, as well as average responses, as a source of information. We apply our method to the data of McCarty et al. and find that Americans vary greatly in their number of acquaintances. Further, Americans show great variation in propensity to form ties to people in some groups (e.g., males in prison, the homeless, and American Indians), but little variation for other groups (e.g., twins, people named Michael or Nicole). We also explore other features of these data and consider ways in which survey data can be used to estimate network structure."], ["Conditional Inference Methods for Incomplete Poisson Data With Endogenous Time-Varying Covariates", "We investigate the effect of protease inhibitors (PIs) on the rate of emergency room (ER) visits among HIV-infected women from a longitudinal cohort study. One strategy for accounting for serial correlation in longitudinal studies is to assume that observations are independent, conditional on unit-specific nuisance parameters. It is possible to estimate these models using unconditional maximum likelihood, where the nuisance parameters are assigned a parametric distribution and integrated out of the likelihood. Alternatively, we can proceed using conditional inference, where we eliminate the nuisance parameters from the likelihood by conditioning on a sufficient statistic for these parameters. An advantage of conditional inference methods over parametric random-effects models is that all patient-level time-invariant factors (both measured and unmeasured) are accounted for in the analysis. A limitation is that standard conditional inference methods assume that missing data are missing completely at random and do not allow endogenous time-varying covariates (i.e., past ER visits cannot predict future PI use). Both assumptions are unlikely to be met for these data, because one would expect \u201csicker\u201d patients would be more likely to receive treatment and/or drop out of the study. We develop new estimation strategies that allow endogenous time-varying covariates and missing-at-random dropouts. The analysis shows that PI use reduces the rate of ER visits in patients whose CD4 cell count was <200 cells/ml at baseline. The size of the effect is substantially smaller than that estimated using a random-effects approach."], ["A Bayesian Approach for Clustered Longitudinal Ordinal Outcome With Nonignorable Missing Data", "Asthma, a chronic inflammatory disease of the airways, affects an estimated 6.3 million children under age 18 in the United States. A key to successful asthma management, and hence improved quality of life (QOL), calls for an active partnership between asthma patients and their health care providers. To foster this partnership, an intervention program was designed and evaluated using a randomized longitudinal study. The study focused on several outcomes where typically missing data remained a pervasive problem. We suspected that the underlying missing-data mechanism may not be ignorable. Thus here we present a method for analyzing clustered longitudinal data with missing values resulting from a nonignorable missing-data mechanism. The transition Markov model with random effects was used to investigate changes in ordinal outcomes over time. A Bayesian pattern-mixture model with the flexibility to incorporate models for missing data in both outcome and time-varying covariates was used to model the nonignorable missing-data mechanism. The pattern-mixture model uses easy-to-understand parameters\u2014namely, ratios of the cumulative odds across patterns with the complete-data pattern\u2014as the reference pattern. Sensitivity analysis was performed using different prior distributions for the parameters. A fully Bayesian approach was derived by integrating over a class of prior distributions. The data from the Asthma Intervention Study were analyzed to explore the effect of the intervention program on improving QOL."], ["Assessing Evidence Inconsistency in Mixed Treatment Comparisons", "Randomized comparisons among several treatments give rise to an incomplete-blocks structure known as mixed treatment comparisons (MTCs). To analyze such data structures, it is crucial to assess whether the disparate evidence sources provide consistent information about the treatment contrasts. In this article we propose a general method for assessing evidence inconsistency in the framework of Bayesian hierarchical models. We begin with the distinction between basic parameters, which have prior distributions, and functional parameters, which are defined in terms of basic parameters. Based on a graphical analysis of MTC structures, evidence inconsistency is defined as a relation between a functional parameter and at least two basic parameters, supported by at least three evidence sources. The inconsistency degrees of freedom (ICDF) is the number of such inconsistencies. We represent evidence consistency as a set of linear relations between effect parameters on the log odds ratio scale, then relax these relations to allow for inconsistency by adding to the model random inconsistency factors (ICFs). The number of ICFs is determined by the ICDF. The overall consistency between evidence sources can be assessed by comparing models with and without ICFs, whereas their posterior distribution reflects the extent of inconsistency in particular evidence cycles. The methods are elucidated using two published datasets, implemented with standard Markov chain Monte Carlo software."], ["Specifying and Implementing Nonparametric and Semiparametric Survival Estimators in Two-Stage (Nested) Cohort Studies With Missing Case Data", null], ["Forecasting Cause-Age Specific Mortality Using Two Random Processes", "Mortality forecasts are critical information for assessing the health of a population and are necessary for making informed decisions about how best to direct health-related resources and activities. Timeliness in making health statistics available is crucial to identify and address current health problems. Being motivated to meet these needs, we propose a method to forecast the number of cause-age specific deaths through a two random processes model. Unlike the previous methods, the new method incorporates both cross-sectional and longitudinal correlations into our model without a high-dimensional problem. A bootstrap confidence interval is presented to measure the validity of our model and to detect an unusual occurrence of deaths. Our data analysis demonstrates that our method gives promising results compared with the true final counts."], ["Advanced Distribution Theory for SiZer", "SiZer is a powerful method for exploratory data analysis. In this article approximations to the distributions underlying the simultaneous statistical inference are investigated, and large improvements are made in the approximation using extreme value theory. This results in improved size, and also in an improved global inference version of SiZer. The main points are illustrated with real data and simulated examples."], [null, null], ["Analysis of Failure Time Data Arising From Studies With Alternating Treatment Schedules", "We develop statistical methods for designing and analyzing studies in which treatments are deliberately interrupted and reinitiated, but where interest lies in making inferences about continuous treatment use. We refer to such designs as alternating designs, because subjects alternate between periods in which they are taking the treatment of interest and periods when they are not. Our goals are to determine how to estimate the distribution of time to an event if the treatment were given continuously, to compare the distributions of two such continuously given treatments, and to assess the effects of covariates on the distribution of a continuously given treatment. We examine a nonparametric estimator of the cumulative hazard function for continuous treatment using data from an alternating design and show it to be uniformly consistent and asymptotically normal under certain conditions relating to the effects of interrupting the treatment. We then introduce nonparametric tests for comparing the distributions corresponding to two such continuously given treatments and derive their asymptotic properties under general alternatives to the null and under various conditions related to the interruption of treatment. We compare the properties of the alternating treatment design and the classical parallel group design and present results from a simulation study that assesses the size and power of the test procedures introduced. Finally, we examine partial likelihood methods for assessing the effects of covariates and continuous treatment on time until an event under a proportional hazards model. We illustrate the proposed methods using the results from a recent study in which subjects alternate between taking an active drug and placebo on an annual basis."], ["Bootstrap Approximations in Model Checks for Binary Data", "Consider a binary regression model in which the conditional expectation of a binary variable given an explanatory variable belongs to a parametric family. To check whether a sequence of independent and identically distributed observations belongs to such a parametric family, we use Kolmogorov\u2013Smirnov and Cram\u00e9r\u2013von Mises type tests based on a marked empirical process introduced by Stute. We propose and study a new resampling scheme for a bootstrap in this setup to approximate critical values for these tests. We also apply this approach to simulated and real data. In the latter case we check some parametric models that are used to analyze right-censored lifetime data under a semiparametric random censorship model."], ["Goodness-of-Fit Tests for Linear and Nonlinear Time Series Models", null], ["Bent-Cable Regression Theory and Applications", "We use the so-called \u201cbent-cable\u201d model to describe natural phenomena that exhibit a potentially sharp change in slope. The model comprises two linear segments, joined smoothly by a quadratic bend. The class of bent cables includes, as a limiting case, the popular piecewise-linear model (with a sharp kink), otherwise known as the broken stick. Associated with bent-cable regression is the estimation of the bend-width parameter, through which the abruptness of the underlying transition may be assessed. We present worked examples and simulations to demonstrate the regularity and irregularity of bent-cable regression encountered in finite-sample settings. We also extend existing bent-cable asymptotics that previously were limited to the basic model with known linear slopes of 0 and 1. Practical conditions on the design are given to ensure regularity of the full bent-cable estimation problem if the underlying bend segment has nonzero width. Under such conditions, the least-squares estimators are shown to be consistent and to asymptotically follow a multivariate normal distribution. Furthermore, the deviance statistic (or the likelihood ratio statistic, if the random errors are normally distributed) is shown to have an asymptotic chi-squared distribution."], ["Optimal Model Assessment, Selection, and Combination", "Central to statistical theory and application is statistical modeling, which typically involves choosing a single model or combining a number of models of different sizes and from different sources. Whereas model selection seeks a single best modeling procedure, model combination combines the strength of different modeling procedures. In this article we look at several key issues and argue that model assessment is the key to model selection and combination. Most important, we introduce a general technique of optimal model assessment based on data perturbation, thus yielding optimal selection, in particular model selection and combination. From a frequentist perspective, we advocate model combination over a selected subset of modeling procedures, because it controls bias while reducing variability, hence yielding better performance in terms of the accuracy of estimation and prediction. To realize the potential of model combination, we develop methodologies for determining the optimal tuning parameter, such as weights and subsets for combining via optimal model assessment. We present simulated and real data examples to illustrate main aspects."], ["Incorporating Additional Information to Normal Linear Discriminant Rules", null], ["Random Forests and Adaptive Nearest Neighbors", null], ["Semiparametric Normal Transformation Models for Spatially Correlated Survival Data", "There is an emerging interest in modeling spatially correlated survival data in biomedical and epidemiologic studies. In this article we propose a new class of semiparametric normal transformation models for right-censored spatially correlated survival data. This class of models assumes that survival outcomes marginally follow a Cox proportional hazard model with unspecified baseline hazard, and their joint distribution is obtained by transforming survival outcomes to normal random variables, whose joint distribution is assumed to be multivariate normal with a spatial correlation structure. A key feature of the class of semiparametric normal transformation models is that it provides a rich class of spatial survival models where regression coefficients have population average interpretation and the spatial dependence of survival times is conveniently modeled using the transformed variables by flexible normal random fields. We study the relationship of the spatial correlation structure of the transformed normal variables and the dependence measures of the original survival times. Direct nonparametric maximum likelihood estimation in such models is practically prohibited due to the high-dimensional intractable integration of the likelihood function and the infinite-dimensional nuisance baseline hazard parameter. We hence develop a class of spatial semiparametric estimating equations, which conveniently estimate the population-level regression coefficients and the dependence parameters simultaneously. We study the asymptotic properties of the proposed estimators and show that they are consistent and asymptotically normal. The proposed method is illustrated with an analysis of data from the East Boston Asthma Study, and its performance is evaluated using simulations."], ["Non-Gaussian Bayesian Geostatistical Modeling", "Sampling models for geostatistical data are usually based on Gaussian processes. However, real data often display non-Gaussian features, such as heavy tails. In this article we propose a more flexible class of sampling models. We start from the spatial linear model that has a spatial trend plus a stationary Gaussian error process. We extend the sampling model to non-Gaussianity by including a scale parameter at each location. We make sure that we obtain a valid stochastic process. The scale parameters are spatially correlated to ensure that the process is mean square continuous. We derive expressions for the moments and the kurtosis of the process. This more general stochastic process allows us to accommodate and identify observations that would be outliers under a Gaussian sampling process. For the spatial correlation structure, we adopt the flexible Mat\u00e8rn class with unknown smoothness parameter. Furthermore, a nugget effect is included in the model. Bayesian inference (posterior and predictive) is performed using a Markov chain Monte Carlo algorithm. The choice of the prior distribution is discussed and its importance assessed in a sensitivity analysis. We also examine identifiability of the parameters. Our methods are illustrated with two datasets."], ["On the Large-Sample Minimal Coverage Probability of Confidence Intervals After Model Selection", "We give a large-sample analysis of the minimal coverage probability of the usual confidence intervals for regression parameters when the underlying model is chosen by a \u201cconservative\u201d (or \u201coverconsistent\u201d) model selection procedure. We derive an upper bound for the large-sample limit minimal coverage probability of such intervals that applies to a large class of model selection procedures including the Akaike information criterion as well as various pretesting procedures. This upper bound can be used as a safeguard to identify situations where the actual coverage probability can be far below the nominal level. We illustrate that the (asymptotic) upper bound can be statistically meaningful even in rather small samples."], ["On Sliced Inverse Regression With High-Dimensional Covariates", "Sliced inverse regression is a promising method for the estimation of the central dimension-reduction subspace (CDR space) in semiparametric regression models. It is particularly useful in tackling cases with high-dimensional covariates. In this article we study the asymptotic behavior of the estimate of the CDR space with high-dimensional covariates, that is, when the dimension of the covariates goes to infinity as the sample size goes to infinity. Strong and weak convergence are obtained. We also suggest an estimation procedure of the Bayes information criterion type to ascertain the dimension of the CDR space and derive the consistency. A simulation study is conducted."], ["Bounded-Influence Robust Estimation in Generalized Linear Latent Variable Models", "Latent variable models are used for analyzing multivariate data. Recently, generalized linear latent variable models for categorical, metric, and mixed-type responses estimated via maximum likelihood (ML) have been proposed. Model deviations, such as data contamination, are shown analytically, using the influence function and through a simulation study, to seriously affect ML estimation. This article proposes a robust estimator that is made consistent using the basic principle of indirect inference and can be easily numerically implemented. The performance of the robust estimator is significantly better than that of the ML estimators in terms of both bias and variance. A real example from a consumption survey is used to highlight the consequences in practice of the choice of the estimator."], ["Outlier Detection in Multivariate Time Series by Projection Pursuit", "In this article we use projection pursuit methods to develop a procedure for detecting outliers in a multivariate time series. We show that testing for outliers in some projection directions can be more powerful than testing the multivariate series directly. The optimal directions for detecting outliers are found by numerical optimization of the kurtosis coefficient of the projected series. We propose an iterative procedure to detect and handle multiple outliers based on a univariate search in these optimal directions. In contrast with the existing methods, the proposed procedure can identify outliers without prespecifying a vector ARMA model for the data. The good performance of the proposed method is illustrated in a Monte Carlo study and in a real data analysis."], ["Semiparametric Transformation Models for Survival Data With a Cure Fraction", "We propose a class of transformation models for survival data with a cure fraction. The class of transformation models is motivated by biological considerations and includes both the proportional hazards and the proportional odds cure models as two special cases. An efficient recursive algorithm is proposed to calculate the maximum likelihood estimators (MLEs). Furthermore, the MLEs for the regression coefficients are shown to be consistent and asymptotically normal, and their asymptotic variances attain the semiparametric efficiency bound. Simulation studies are conducted to examine the finite-sample properties of the proposed estimators. The method is illustrated on data from a clinical trial involving the treatment of melanoma."], ["Inference in Semiparametric Dynamic Models for Binary Longitudinal Data", null], ["Nonparametric Density Estimation From Covariate Information", null], ["Estimating Mean Dimensionality of Analysis of Variance Decompositions", null], ["Testing for Covariate Effects in the Fully Nonparametric Analysis of Covariance Model", "Traditional inference questions in the analysis of covariance mainly focus on comparing different factor levels by adjusting for the continuous covariates, which are believed to also exert a significant effect on the outcome variable. Testing hypotheses about the covariate effects, although of substantial interest in many applications, has received relatively limited study in the semiparametric/nonparametric setting. In the context of the fully nonparametric analysis of covariance model of Akritas et al., we propose methods to test for covariate main effects and covariate\u2013factor interaction effects. The idea underlying the proposed procedures is that covariates can be thought of as factors with many levels. The test statistics are closely related to some recent developments in the asymptotic theory for analysis of variance when the number of factor levels is large. The limiting normal distributions are established under the null hypotheses and local alternatives by asymptotically approximating a new class of quadratic forms. The test statistics bear similar forms to the classical F-test statistics and thus are convenient for computation. We demonstrate the methods and their properties on simulated and real data."], [null, null], ["Optimal Designs for Dose\u2013Response Models With Restricted Design Spaces", null], ["Bayesian Sample Size Determination for Case-Control Studies", "Case-control studies are among the most commonly used means of assessing association between exposure and outcome. Sample size determination and the optimal control-to-case ratio are vital to the design of such studies. In this article we investigate Bayesian sample size determination and the control-to-case ratio for case-control studies, when interval estimation is the goal of the eventual statistical analysis. In certain cases we are able to derive approximate closed-form sample size formulas. We also describe two Monte Carlo methods, each of which provides a unified approach to the sample size problem, because they may be applied to a wide range of interval-based criteria. We compare the accuracy of the different methods. We also extend our methods to include cross-sectional designs and designs for gene\u2013environment interaction studies."], ["Bayesian-Optimal Design via Interacting Particle Systems", "We propose a new stochastic algorithm for Bayesian-optimal design in nonlinear and high-dimensional contexts. Following Peter M\u00fcller, we solve an optimization problem by exploring the expected utility surface through Markov chain Monte Carlo simulations. The optimal design is the mode of this surface considered a probability distribution. Our algorithm relies on a \u201cparticle\u201d method to efficiently explore high-dimensional multimodal surfaces, with simulated annealing to concentrate the samples near the modes. We first test the method on an optimal allocation problem for which the explicit solution is available, to compare its efficiency with a simpler algorithm. We then apply our method to a challenging medical case study in which an optimal protocol treatment needs to be determined. For this case, we propose a formalization of the problem in the framework of Bayesian decision theory, taking into account physicians' knowledge and motivations. We also briefly review further improvements and alternatives."], ["A Class of Latent Marginal Models for Capture\u2013Recapture Data With Continuous Covariates", "We introduce a new family of latent class models for the analysis of capture\u2013recapture data where continuous covariates are available. The present approach exploits recent advances in marginal parameterizations to model simultaneously, and conditionally on individual covariates, the size of the latent classes, the marginal probabilities of being captured by each list given the latent, and possible higher-order marginal interactions among lists conditionally on the latent. An EM algorithm for maximum likelihood estimation is described, and an expression for the expected information matrix is derived. In addition, a new method for computing confidence intervals for the size of the population having given covariate configurations is proposed and its asymptotic properties are derived. Applications to data on patients with human immunodeficiency virus, in the region of Veneto, Italy, and to new cases of cancer in Tuscany are discussed."], ["Large-Sample Joint Posterior Approximations When Full Conditionals Are Approximately Normal", null], ["On the Correlation Matrix of the Discrete Fourier Transform and the Fast Solution of Large Toeplitz Systems for Long-Memory Time Series", null], ["A Constructive Representation of Univariate Skewed Distributions", "We introduce a general perspective on the introduction of skewness into symmetric distributions. Through inverse probability integral transformations we provide a constructive representation of skewed distributions, where the skewing mechanism and the original symmetric distributions are specified separately. We study the effects of the skewing mechanism on, e.g., modality, tail behavior and the amount of skewness generated. The representation is used to introduce novel classes of skewed distributions, where we induce certain prespecified characteristics through particular choices of the skewing mechanism. Finally, we use a Bayesian linear regression framework to compare the new classes with some existing distributions in the context of two empirical examples."], ["On Convergence and Bias Correction of a Joint Estimation Algorithm for Multiple Sinusoidal Frequencies", "Twenty years ago, Kay proposed an iterative filtering algorithm (IFA) for jointly estimating the frequencies of multiple complex sinusoids from noisy observations. IFA is based on the fact that the noiseless signal is an autoregressive (AR) process, so the frequency estimation problem can be reformulated as the problem of estimating the AR coefficients. By iterating the cycle of AR coefficient estimation and AR filtering, IFA provides a computationally simple procedure yet capable of accurate frequency estimation especially at low signal-to-noise ratio (SNR). However, the convergence of IFA has not been established beyond simulation and a very special case of a single frequency and infinite sample size. This article provides a statistical analysis of the algorithm and makes several important contributions. It shows that the poles of the AR filter must be reduced by an extra shrinkage parameter to accommodate poor initial values and avoid being trapped into false solutions. It also shows that the AR estimates in each iteration must be bias-corrected to produce a more accurate frequency estimator; a closed-form expression is provided for bias correction. Finally, it shows that for a sufficiently large sample size, the resulting algorithm, called new IFA (NIFA), converges to the desired fixed point, which constitutes a consistent frequency estimator. Numerical examples, including a real data example in radar applications, are provided to demonstrate the findings. It is shown in particular that the shrinkage parameter not only controls the estimation accuracy, but also determines the requirements of initial values. It is also shown that the proposed bias-correction method considerably improves the estimation accuracy, especially for high SNR."], ["Book Reviews", null], ["Encyclopedia of Biostatistics", null], ["Bayesian Models for Categorical Data", null], ["Probability: A Graduate Course", null], ["Stochastic Inequalities and Applications", null], ["Statistical Inference for Ergodic Diffusion Processes", null], ["Testing Statistical Hypotheses", null], ["Modeling and Inverse Problems in Image Analysis", null], ["Inference for Change Point and Post Change Means After a CUSUM Test", null], ["The Analysis of Means: A Graphical Method for Comparing Means, Rates, and Proportions", null], ["Statistics and Finance: An Introduction", null], ["Theory of Financial Risk and Derivative Pricing: From Statistical Physics to Risk Management", null], ["Nonparametric Statistical Methods for Complete and Censored Data", null], ["Design and Analysis of Experiments, Vol. 2: Advanced Experimental Design", null], ["Analyzing Rater Agreement: Manifest Variable Methods", null], ["Estimation in Surveys With Nonresponse", null], ["Clustering for Data Mining: A Data Recovery Approach", null], ["An Introduction to Bioinformatics Algorithms", null], ["Statistics for Microarrays: Design, Analysis and Inference", null], ["Computational Statistics", null], ["Developing Statistical Software in Fortran 95", null], [null, null], [null, null], ["Telegraphic Reviews", null], ["Correction to Censored Regression Quantiles by S. Portnoy, 98 (2003), 1001\u20131012", null], ["Inverse Decision Theory", "Identifying an optimal decision rule using Bayesian decision theory requires priors, likelihoods, and losses. In many medical settings, we can develop priors and likelihoods, but specifying losses can be difficult, especially when considering both patient outcomes and economic costs. If there is a widely accepted treatment strategy, then we can consider the inverse problem and find a region in the space of losses where the procedure is optimal. We call this approach inverse decision theory (IDT). We apply IDT to the standard of care for diagnosis and treatment of precancerous lesions of the cervix, and consider an alternative procedure that has been proposed. We use a Bayesian approach to estimate the probabilities associated with the diagnostic tests and make inferences about the region in loss space where these medical procedures are optimal. In particular, we find evidence supporting the current standard of care."], ["Bayesian Model Averaging With Applications to Benchmark Dose Estimation for Arsenic in Drinking Water", "An important component of quantitative risk assessment involves characterizing the dose-response relationship between an environmental exposure and adverse health outcome and then computing a benchmark dose, or the exposure level that yields a suitably low risk. This task is often complicated by model choice considerations, because risk estimates depend on the model parameters. We propose using Bayesian methods to address the problem of model selection and derive a model-averaged version of the benchmark dose. We illustrate the methods through application to data on arsenic-induced lung cancer from Taiwan."], ["A Quantitative Study of Gene Regulation Involved in the Immune Response of Anopheline Mosquitoes", null], ["Quality Control and Robust Estimation for cDNA Microarrays With Replicates", null], ["Estimation of Expression Indexes for Oligonucleotide Arrays Using the Singular Value Decomposition", "Multiprobe oligonucleotide arrays are a widely used type of expression microarray with the attractive feature that numerous probes are used to represent each transcript. An \u201cexpression index\u201d is a statistic used to represent expression level for a particular gene that is estimated from the probe hybridization intensities. We show that a popular model-based expression index proposed by Li and Wong has an interpretation as a component of the singular value decomposition (SVD) of the probe intensity matrix. Following this observation, we propose a new SVD model that accounts for differing variance structure across probes. We also propose that nonlinearity in intensity response to expression can be corrected to some extent through a data transformation, which is guided by an SVD entropy measure. The methods are demonstrated with simulation and applications to two real datasets."], ["Causal Vaccine Effects on Binary Postinfection Outcomes", "The effects of vaccine on postinfection outcomes, such as disease, death, and secondary transmission to others, are important scientific and public health aspects of prophylactic vaccination. As a result, evaluation of many vaccine effects condition on being infected. Conditioning on an event that occurs posttreatment (in our case, infection subsequent to assignment to vaccine or control) can result in selection bias. Moreover, because the set of individuals who would become infected if vaccinated is likely not identical to the set of those who would become infected if given control, comparisons that condition on infection do not have a causal interpretation. In this article we consider identifiability and estimation of causal vaccine effects on binary postinfection outcomes. Using the principal stratification framework, we define a postinfection causal vaccine efficacy estimand in individuals who would be infected regardless of treatment assignment. The estimand is shown to be not identifiable under the standard assumptions of the stable unit treatment value, monotonicity, and independence of treatment assignment. Thus selection models are proposed that identify the causal estimand. Closed-form maximum likelihood estimators (MLEs) are then derived under these models, including those assuming maximum possible levels of positive and negative selection bias. These results show the relations between the MLE of the causal estimand and two commonly used estimators for vaccine effects on postinfection outcomes. For example, the usual intent-to-treat estimator is shown to be an upper bound on the postinfection causal vaccine effect provided that the magnitude of protection against infection is not too large. The methods are used to evaluate postinfection vaccine effects in a clinical trial of a rotavirus vaccine candidate and in a field study of a pertussis vaccine. Our results show that pertussis vaccination has a significant causal effect in reducing disease severity."], ["Piecewise Constant Cross-Ratio Estimation for Association of Age at a Marker Event and Age at Menopause", "A question of significant interest in female reproductive aging is to identify bleeding criteria for menopausal transition. Although various bleeding criteria, or markers, have been proposed for menopausal transition, their validity has not been adequately examined. The Tremin Trust data were collected from a long-term cohort study that followed a group of women throughout their whole reproductive life. Such data provide a unique opportunity for evaluating the utility of a bleeding criterion-based marker event by assessing the association between age at onset of the bleeding marker and age at onset of menopause. Formal statistical analysis of this dependence is challenged by the facts that both the marker event and menopause are subject to right-censoring and that their association depends on age at the marker event. We propose using the cross-ratio to measure their dependence by assuming the cross-ratio to be a piecewise constant function of age at onset of the marker event. We propose two estimation procedures, the direct two-stage method and the sequential two-stage method, extending the latter to allow for covariates in marginal survival functions. We apply the proposed methods to the analysis of the Tremin Trust data and evaluate their performance using simulations."], ["Adaptive Thresholds", null], ["Likelihood-Based Inference on Haplotype Effects in Genetic Association Studies", "A haplotype is a specific sequence of nucleotides on a single chromosome. The population associations between haplotypes and disease phenotypes provide critical information about the genetic basis of complex human diseases. Standard genotyping techniques cannot distinguish the two homologous chromosomes of an individual, so only the unphased genotype (i.e., the combination of the two homologous haplotypes) is directly observable. Statistical inference about haplotype\u2013phenotype associations based on unphased genotype data presents an intriguing missing-data problem, especially when the sampling depends on the disease status. The objective of this article is to provide a systematic and rigorous treatment of this problem. All commonly used study designs, including cross-sectional, case-control, and cohort studies, are considered. The phenotype can be a disease indicator, a quantitative trait, or a potentially censored time-to-disease variable. The effects of haplotypes on the phenotype are formulated through flexible regression models, which can accommodate various genetic mechanisms and gene\u2013environment interactions. Appropriate likelihoods are constructed that may involve high-dimensional parameters. The identifiability of the parameters and the consistency, asymptotic normality, and efficiency of the maximum likelihood estimators are established. Efficient and reliable numerical algorithms are developed. Simulation studies show that the likelihood-based procedures perform well in practical settings. An application to the Finland\u2013United States Investigation of NIDDM Genetics Study is provided. Areas in need of further development are discussed."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Prediction by Supervised Principal Components", "In regression problems where the number of predictors greatly exceeds the number of observations, conventional regression techniques may produce unsatisfactory results. We describe a technique called supervised principal components that can be applied to this type of problem. Supervised principal components is similar to conventional principal components analysis except that it uses a subset of the predictors selected based on their association with the outcome. Supervised principal components can be applied to regression and generalized regression problems, such as survival analysis. It compares favorably to other techniques for this type of problem, and can also account for the effects of other covariates and help identify which predictor variables are most important. We also provide asymptotic consistency results to help support our empirical findings. These methods could become important tools for DNA microarray data, where they may be used to more accurately diagnose and treat cancer."], ["Convexity, Classification, and Risk Bounds", "Many of the classification algorithms developed in the machine learning literature, including the support vector machine and boosting, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0\u20131 loss function. The convexity makes these algorithms computationally efficient. The use of a surrogate, however, has statistical consequences that must be balanced against the computational virtues of convexity. To study these issues, we provide a general quantitative relationship between the risk as assessed using the 0\u20131 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function\u2014that it satisfies a pointwise form of Fisher consistency for classification. The relationship is based on a simple variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise, and show that in this case, strictly convex loss functions lead to faster rates of convergence of the risk than would be implied by standard uniform convergence arguments. Finally, we present applications of our results to the estimation of convergence rates in function classes that are scaled convex hulls of a finite-dimensional base class, with a variety of commonly used loss functions."], ["Objective Bayesian Variable Selection", "A novel fully automatic Bayesian procedure for variable selection in normal regression models is proposed. The procedure uses the posterior probabilities of the models to drive a stochastic search. The posterior probabilities are computed using intrinsic priors, which can be considered default priors for model selection problems; that is, they are derived from the model structure and are free from tuning parameters. Thus they can be seen as objective priors for variable selection. The stochastic search is based on a Metropolis\u2013Hastings algorithm with a stationary distribution proportional to the model posterior probabilities. The procedure is illustrated on both simulated and real examples."], ["Variable Selection for Model-Based Clustering", "We consider the problem of variable or feature selection for model-based clustering. The problem of comparing two nested subsets of variables is recast as a model comparison problem and addressed using approximate Bayes factors. A greedy search algorithm is proposed for finding a local optimum in model space. The resulting method selects variables (or features), the number of clusters, and the clustering model simultaneously. We applied the method to several simulated and real examples and found that removing irrelevant variables often improved performance. Compared with methods based on all of the variables, our variable selection method consistently yielded more accurate estimates of the number of groups and lower classification error rates, as well as more parsimonious clustering models and easier visualization of results."], ["Order-Based Dependent Dirichlet Processes", "In this article we propose a new framework for Bayesian nonparametric modeling with continuous covariates. In particular, we allow the nonparametric distribution to depend on covariates through ordering the random variables building the weights in the stick-breaking representation. We focus mostly on the class of random distributions that induces a Dirichlet process at each covariate value. We derive the correlation between distributions at different covariate values and use a point process to implement a practically useful type of ordering. Two main constructions with analytically known correlation structures are proposed. Practical and efficient computational methods are introduced. We apply our framework, through mixtures of these processes, to regression modeling, the modeling of stochastic volatility in time series data, and spatial geostatistical modeling."], ["Partial Linear Regression Models for Clustered Data", null], ["A Note on Lack-of-Fit Tests for Linear Models Without Replication", "A class of three tests\u2014overall lack-of-fit test, between-cluster lack-of-fit test, and within-cluster lack-of-fit test\u2014are proposed for testing the lack of fit of a linear regression model applied to experiments without replicates. The power of the proposed tests is significantly higher than those of the known tests under the situations considered here. The proposed tests are capable of detecting which type of lack of fit is dominant when both between-cluster and within-cluster lack of fit are present."], ["Improved Estimation of Dissimilarities by Presmoothing Functional Data", "We examine the effect of presmoothing functional data on estimating the dissimilarities among objects in a dataset, with applications to cluster analysis and other distance methods, such as multidimensional scaling and statistical matching. We prove that a shrinkage method of smoothing results in a better estimator of the dissimilarities among a set of noisy curves. For a model with independent noise structure, the smoothed-data dissimilarity estimator dominates the observed-data estimator. For a dependent-error model\u2014often applicable when the functional data are measured nearly continuously over some domain\u2014an asymptotic domination result is given for the smoothed-data estimator. A simulation study indicates the magnitude of improvement provided by the shrinkage estimator and examines its behavior for heavy-tailed noise structure. The shrinkage estimator presented here combines Stein estimation and basis function-based linear smoothers in a novel manner. Statisticians increasingly analyze sizable sets of functional data, and the results in this article are a useful contribution to the theory of the effect of presmoothing on functional data analysis."], ["Structural Break Estimation for Nonstationary Time Series Models", "This article considers the problem of modeling a class of nonstationary time series using piecewise autoregressive (AR) processes. The number and locations of the piecewise AR segments, as well as the orders of the respective AR processes, are assumed unknown. The minimum description length principle is applied to compare various segmented AR fits to the data. The goal is to find the \u201cbest\u201d combination of the number of segments, the lengths of the segments, and the orders of the piecewise AR processes. Such a \u201cbest\u201d combination is implicitly defined as the optimizer of an objective function, and a genetic algorithm is implemented to solve this difficult optimization problem. Numerical results from simulation experiments and real data analyses show that the procedure has excellent empirical properties. The segmentation of multivariate time series is also considered. Assuming that the true underlying model is a segmented autoregression, this procedure is shown to be consistent for estimating the location of the breaks."], ["Discrimination of Locally Stationary Time Series Based on the Excess Mass Functional", "Discrimination of time series is an important practical problem with applications in various scientific fields. We propose and study a novel approach to this problem. Our approach is applicable to cases where time series in different categories have a different \u201cshape.\u201d Although based on the idea of feature extraction, our method is not distance-based, and as such does not require aligning the time series. Instead, features are measured for each time series, and discrimination is based on these individual measures. An AR process with a time-varying variance is used as an underlying model. Our method then uses shape measures or, better, measures of concentration of the variance function, as a criterion for discrimination. It is this concentration aspect or shape aspect that makes the approach intuitively appealing. We provide some mathematical justification for our proposed methodology, as well as a simulation study and an application to the problem of discriminating earthquakes and explosions."], ["Fiducial Generalized Confidence Intervals", null], ["Parameter Estimation for the Truncated Pareto Distribution", "The Pareto distribution is a simple model for nonnegative data with a power law probability tail. In many practical applications, there is a natural upper bound that truncates the probability tail. This article derives estimators for the truncated Pareto distribution, investigates their properties, and illustrates a way to check for fit. These methods are illustrated with applications from finance, hydrology, and atmospheric science."], ["Sample Size Determination for Robust Bayesian Analysis", null], ["High-Breakdown Inference for Mixed Linear Models", null], ["Estimation of Finite Population Domain Means", "In this article we introduce a general methodology for producing a model-assisted empirical best predictor (EBP) of a finite population domain mean using data from a complex survey. Our method improves on the commonly used design-consistent survey estimator by using a suitable mixed model. Such a model combines information from related sources, such as census and administrative data. Unlike a purely model-based EBP, the proposed model-assisted EBP converges in probability to the customary design-consistent estimator as the domain and sample sizes increase. The convergence in probability is shown to hold with respect to the sampling design, irrespective of the assumed mixed model, a property commonly known as design consistency. This property ensures robustness of the proposed predictor against possible model failures. In addition, the convergence in probability is shown to be valid with respect to the assumed mixed model (model consistency). A new mean squared prediction error (MSPE) estimator is proposed. Unlike earlier MSPE estimators, our MSPE estimator is second-order unbiased. Our simulation results demonstrate the robustness properties of our proposed model-assisted predictor and the usefulness of the second-order unbiased MSPE estimator."], ["Replication Variance Estimation for Two-Phase Stratified Sampling", "In two-phase sampling, the second-phase sample is often a stratified sample based on the information observed in the first-phase sample. For the total of a population characteristic, either the double-expansion estimator or the reweighted expansion estimator can be used. Given a consistent first-phase replication variance estimator, we propose a consistent variance estimator that is applicable to both the double-expansion estimator and the reweighted expansion estimator. The proposed method can be extended to multiphase sampling."], ["Distribution of the Length of the Longest Significance Run on a Bernoulli Net and Its Applications", null], ["Count Data Distributions", "In this article we characterize all two-parameter count distributions (satisfying very general conditions) that are partially closed under addition. We also find those for which the maximum likelihood estimator of the population mean is the sample mean. Mixed Poisson models satisfying these properties are completely determined. Among these models are the negative binomial, Poisson-inverse Gaussian, and other known distributions. New count distributions can also be constructed using these characterizations. Three examples of application are given."], ["Global Validation of Linear Model Assumptions", "An easy-to-implement global procedure for testing the four assumptions of the linear model is proposed. The test can be viewed as a Neyman smooth test and relies only on the standardized residual vector. If the global procedure indicates a violation of at least one of the assumptions, then the components of the global test statistic can be used to gain insight into which assumptions have been violated. The procedure can also be used in conjunction with associated deletion statistics to detect unusual observations. Simulation results are presented indicating the sensitivity of the procedure in detecting model violations under a variety of situations, and its performance is compared with three potential competitors, including a procedure based on the Box\u2013Cox power transformation. The procedure is demonstrated by applying it to a new car mileage dataset and a water salinity dataset that has been used earlier to illustrate model diagnostics."], ["Clustering Categorical Data Based on Distance Vectors", null], ["A Reference-Free Cuscore Chart for Dynamic Mean Change Detection and a Unified Framework for Charting Performance Comparison", "To detect and estimate nonconstant, time-varying mean shifts, statistical process control (SPC) tools, such as the cumulative score (Cuscore) and generalized likelihood ratio test (GLRT) charts, have recently been proposed. However, their efficiency is based on previous and exact knowledge of a reference pattern. In this article a reference-free Cuscore (RFCuscore) chart is proposed that can trace and detect dynamic mean changes quickly without knowing the reference pattern. In addition, a unified framework that contains most of the control charts is presented and applied for a theoretical comparison of the RFCuscore, Cuscore, GLRT, and CUSUM charts in detecting dynamic mean changes. Moreover, numerical simulations and a real example are used to illustrate and verify the results. Both theoretical analysis and numerical results show that the RFCuscore chart performs not only robustly, but also quickly in detecting both small and large dynamic mean changes."], ["Book Reviews", null], ["The Theory of Measures and Integration", null], ["Gaussian Markov Random Fields: Theory and Applications", null], ["Statistical Methods for Spatial Data Analysis", null], [null, null], ["Probability Matching Priors: Higher Order Asymptotics", null], ["Constrained Statistical Inference: Inequality, Order, and Shape Restrictions", null], ["Measurement Error and Misclassification in Statistics and Epidemiology", null], ["Statistical Methods in Bioinformatics: An Introduction", null], ["A Modern Introduction to Probability and Statistics: Understanding Why and How", null], ["Mathematical Statistics With Applications", null], ["Probability and Computing: Randomized Algorithms and Probabilistic Analysis", null], ["Probability and Probabilistic Reasoning for Electrical Engineering", null], ["Statistical Design of Experiments With Engineering Applications", null], ["Evolution of Biological Systems in Random Media: Limit Theorems and Stability", null], ["Semiparametric Regression for the Applied Econometrician", null], ["Exploring Multivariate Data With the Forward Search", null], ["Sensitivity Analysis in Practice: A Guide to Assessing Scientific Models", null], ["Exploratory Data Mining and Data Cleaning", null], ["Exploratory Data Analysis With MATLAB", null], ["Categorical Data Analysis With SAS and SPSS Applications, Categorical Data Analysis Using the SAS System", null], ["Handling Missing Data: Applications to Environmental Analysis", null], ["Nondetects and Data Analysis: Statistics for Censored Environmental Data", null], ["Analysis of Survey Data", null], ["Elementary Statistical Quality Control", null], ["Longitudinal and Panel Data: Analysis and Applications in the Social Sciences", null], ["Handbook of Epidemiology", null], ["Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models", null], ["Statistical Concepts and Applications in Clinical Medicine", null], ["Epidemiology: Study Design and Data Analysis", null], ["Telegraphic Reviews", null], ["Correction", null]]}