{"2011": [["Statistics: An All-Encompassing Discipline", null], ["A Likelihood Ratio Test Based Method for Signal Detection With Application to FDA\u2019s Drug Safety Data", " Several statistical methods that are available in the literature to analyze postmarket safety databases, such as the U.S. Federal Drug Administration\u2019s (FDA) adverse event reporting system (AERS), for identifying drug-event combinations with disproportionately high frequencies, are subject to high false discovery rates. Here, we propose a likelihood ratio test (LRT) based method and show, via an extensive simulation study, that the proposed method while retaining good power and sensitivity for identifying signals, controls both the Type I error and false discovery rates. The application of the LRT method to the AERS database is illustrated using two datasets; a small dataset consisting of suicidal behavior and mood change-related AE cases for the drug Montelukast, and a large dataset consisting of all possible AE cases reported to FDA during 2004\u20132008 for the drug Heparin. This article has supplementary material online. "], ["High-Dimensional ODEs Coupled With Mixed-Effects Modeling Techniques for Dynamic Gene Regulatory Network Identification", " Gene regulation is a complicated process. The interaction of many genes and their products forms an intricate biological network. Identification of this dynamic network will help us understand the biological processes in a systematic way. However, the construction of a dynamic network is very challenging for a high-dimensional system. In this article we propose to use a set of ordinary differential equations (ODE), coupled with dimensional reduction by clustering and mixed-effects modeling techniques, to model the dynamic gene regulatory network (GRN). The ODE models allow us to quantify both positive and negative gene regulation as well as feedback effects of genes in a functional module on the dynamic expression changes of genes in another functional module, which results in a directed graph network. A five-step procedure\u2014clustering, smoothing, regulation identification, parameter estimates refining, and function enrichment analysis (CSIEF)\u2014is developed to identify the ODE-based dynamic GRN. In the proposed CSIEF procedure, a series of cutting-edge statistical methods and techniques are employed, that include nonparametric mixed-effects models with a mixture distribution for clustering, nonparametric mixed-effects smoothing-based methods for ODE models, the smoothly clipped absolute deviation (SCAD)-based variable selection, and stochastic approximation EM (SAEM) approach for mixed-effects ODE model parameter estimation. The key step, the SCAD-based variable selection, is justified by investigating its asymptotic properties and validated by Monte Carlo simulations. We apply the proposed method to identify the dynamic GRN for yeast cell cycle progression data. We are able to annotate the identified modules through function enrichment analyses. Some interesting biological findings are discussed. The proposed procedure is a promising tool for constructing a general dynamic GRN and more complicated dynamic networks. This article has supplementary material online. "], ["Predicting Viral Infection From High-Dimensional Biomarker Trajectories", " There is often interest in predicting an individual\u2019s latent health status based on high-dimensional biomarkers that vary over time. Motivated by time-course gene expression array data that we have collected in two influenza challenge studies performed with healthy human volunteers, we develop a novel time-aligned Bayesian dynamic factor analysis methodology. The time course trajectories in the gene expressions are related to a relatively low-dimensional vector of latent factors, which vary dynamically starting at the latent initiation time of infection. Using a nonparametric cure rate model for the latent initiation times, we allow selection of the genes in the viral response pathway, variability among individuals in infection times, and a subset of individuals who are not infected. As we demonstrate using held-out data, this statistical framework allows accurate predictions of infected individuals in advance of the development of clinical symptoms, without labeled data and even when the number of biomarkers vastly exceeds the number of individuals under study. Biological interpretation of several of the inferred pathways (factors) is provided. "], ["Semiparametric Bayesian Modeling of Income Volatility Heterogeneity", " Research on income risk typically treats its proxy\u2014income volatility, the expected magnitude of income changes\u2014as if it were unchanged for an individual over time, the same for everyone at a point in time, or both. In reality, income risk evolves over time, and some people face more of it than others. To model heterogeneity and dynamics in (unobserved) income volatility, we develop a novel semiparametric Bayesian stochastic volatility model. Our Markovian hierarchical Dirichlet process (MHDP) prior augments the recently developed hierarchical Dirichlet process (HDP) prior to accommodate the serial dependence of panel data. We document dynamics and substantial heterogeneity in income volatility. "], ["Geostatistical Model Averaging for Locally Calibrated Probabilistic Quantitative Precipitation Forecasting", " Accurate weather benefit many key societal functions and activities, including agriculture, transportation, recreation, and basic human and infrastructural safety. Over the past two decades, ensembles of numerical weather prediction models have been developed, in which multiple estimates of the current state of the atmosphere are used to generate probabilistic forecasts for future weather events. However, ensemble systems are uncalibrated and biased, and thus need to be statistically postprocessed. Bayesian model averaging (BMA) is a preferred way of doing this. Particularly for quantitative precipitation, biases and calibration errors depend critically on local terrain features. We introduce a geostatistical approach to modeling locally varying BMA parameters, as opposed to the extant method that holds parameters constant across the forecast domain. Degeneracies caused by enduring dry periods are overcome by Bayesian regularization and Laplace approximations. The new approach, called geostatistical model averaging (GMA), was applied to 48-hour-ahead forecasts of daily precipitation accumulation over the North American Pacific Northwest, using the eight-member University of Washington Mesoscale Ensemble. GMA had better aggregate and local calibration than the extant technique, and was sharper on average. "], ["Rasch Model and Its Extensions for Analysis of Aphasic Deficits in Syntactic Comprehension", " Aphasia is the loss of the ability to produce and/or comprehend language, due to injury to brain areas responsible for these functions. Aphasic patients\u2019 performance on comprehension tests has traditionally been related both to the patient\u2019s individual ability and to the difficulty of the test questions. The natural choice for analysis of these test results is the Rasch model. It assumes that the probability of a patient responding correctly to a question is the inverse-logit function of the difference between the individual patient\u2019s ability and the difficulty of the test question. This study first modeled the way aphasic patients process different sentence types, as well as their ability to accomplish tasks using Rasch models. However, several scientifically important features of the data, such as the correlation of correct responses between two different comprehension tasks, and the association between response patterns in control sentences and response patterns in experimental sentences, were found to be inadequately captured by such models. Alternatively, we used a full Bayesian approach, exploring a mixture of generalized linear mixed models that clustered patients into similar response patterns and abilities. The mixture model was found to better describe the experimental results than any other model examined. The mixture model also expresses the hypothesis that aphasic patients can be classified into different ability and response profile groups, and that patients utilize different cognitive resources in different comprehension tasks. These results are scientifically important and could not have been discovered by using the simple Rasch model. This article has supplementary material online. "], ["Multi-Domain Sampling With Applications to Structural Inference of Bayesian Networks", " When a posterior distribution has multiple modes, unconditional expectations, such as the posterior mean, may not offer informative summaries of the distribution. Motivated by this problem, we propose to decompose the sample space of a multimodal distribution into domains of attraction of local modes. Domain-based representations are defined to summarize the probability masses of and conditional expectations on domains of attraction, which are much more informative than the mean and other unconditional expectations. A computational method, the multi-domain sampler, is developed to construct domain-based representations for an arbitrary multimodal distribution. The multi-domain sampler is applied to structural learning of protein-signaling networks from high-throughput single-cell data, where a signaling network is modeled as a causal Bayesian network. Not only does our method provide a detailed landscape of the posterior distribution but also improves the accuracy and the predictive power of estimated networks. This article has supplementary material online. "], ["A Bayesian Semiparametric Approach to Intermediate Variables in Causal Inference", " In causal inference studies, treatment comparisons often need to be adjusted for confounded post-treatment variables. Principal stratification (PS) is a framework to deal with such variables within the potential outcome approach to causal inference. Continuous intermediate variables introduce inferential challenges to PS analysis. Existing methods either dichotomize the intermediate variable, or assume a fully parametric model for the joint distribution of the potential intermediate variables. However, the former is subject to information loss and arbitrary choice of the cutoff point and the latter is often inadequate to represent complex distributional and clustering features. We propose a Bayesian semiparametric approach that consists of a flexible parametric model for the potential outcomes and a Bayesian nonparametric model for the potential intermediate outcomes using a Dirichlet process mixture (DPM) model. The DPM approach provides flexibility in modeling the possibly complex joint distribution of the potential intermediate outcomes and offers better interpretability of results through its clustering feature. Gibbs sampling based posterior inference is developed. We illustrate the method by two applications: one concerning partial compliance in a randomized clinical trial, and one concerning the causal mechanism between physical activity, body mass index, and cardiovascular disease in the observational Swedish National March Cohort study. "], [null, null], ["Instability, Sensitivity, and Degeneracy of Discrete Exponential Families", " A number of discrete exponential family models for dependent data, first and foremost relational data, have turned out to be near-degenerate and problematic in terms of Markov chain Monte Carlo (MCMC) simulation and statistical inference. I introduce the notion of instability with an eye to characterize, detect, and penalize discrete exponential family models that are near-degenerate and problematic in terms of MCMC simulation and statistical inference. I show that unstable discrete exponential family models are characterized by excessive sensitivity and near-degeneracy. In special cases, the subset of the natural parameter space corresponding to nondegenerate distributions and mean-value parameters far from the boundary of the mean-value parameter space turns out to be a lower-dimensional subspace of the natural parameter space. These characteristics of unstable discrete exponential family models tend to obstruct MCMC simulation and statistical inference. In applications to relational data, I show that discrete exponential family models with Markov dependence tend to be unstable, and that the parameter space of some curved exponential families contains unstable subsets. "], ["A Perturbation Method for Inference on Regularized Regression Estimates", " Analysis of high-dimensional data often seeks to identify a subset of important features and to assess the effects of these features on outcomes. Traditional statistical inference procedures based on standard regression methods often fail in the presence of high-dimensional features. In recent years, regularization methods have emerged as promising tools for analyzing high-dimensional data. These methods simultaneously select important features and provide stable estimation of their effects. Adaptive LASSO and SCAD, for instance, give consistent and asymptotically normal estimates with oracle properties. However, in finite samples, it remains difficult to obtain interval estimators for the regression parameters. In this article, we propose perturbation resampling-based procedures to approximate the distribution of a general class of penalized parameter estimates. Our proposal, justified by asymptotic theory, provides a simple way to estimate the covariance matrix and confidence regions. Through finite-sample simulations, we verify the ability of this method to give accurate inference and compare it with other widely used standard deviation and confidence interval estimates. We also illustrate our proposals with a dataset used to study the association of HIV drug resistance and a large number of genetic mutations. "], ["Elastic Net Regression Modeling With the Orthant Normal Prior", null], ["Order-Restricted Inference for Multivariate Binary Data With Application to Toxicology", null], ["Semiparametric Approach to a Random Effects Quantile Regression Model", " We consider a random effects quantile regression analysis of clustered data and propose a semiparametric approach using empirical likelihood. The random regression coefficients are assumed independent with a common mean, following parametrically specified distributions. The common mean corresponds to the population-average effects of explanatory variables on the conditional quantile of interest, whereas the random coefficients represent cluster-specific deviations in the covariate effects. We formulate the estimation of the random coefficients as an estimating equations problem and use empirical likelihood to incorporate the parametric likelihood of the random coefficients. A likelihood-like statistical criterion function is proposed that, which we show is asymptotically concave in a neighborhood of the true parameter value and motivates its maximizer as a natural estimator. We use Markov chain Monte Carlo samplers in the Bayesian framework, and propose the resulting quasi-posterior mean as an estimator. We show that the proposed estimator of the population-level parameter is asymptotically normal, and that the estimators of the random coefficients are shrunk toward the population-level parameter in the first-order asymptotic sense. These asymptotic results do not require Gaussian random effects, and the empirical likelihood-based likelihood-like criterion function is free of parameters related to the error densities. This makes the proposed approach both flexible and computationally simple. We illustrate the methodology with two real data examples. "], ["Bayesian Inference for General Gaussian Graphical Models With Application to Multivariate Lattice Data", " We introduce efficient Markov chain Monte Carlo methods for inference and model determination in multivariate and matrix-variate Gaussian graphical models. Our framework is based on the G-Wishart prior for the precision matrix associated with graphs that can be decomposable or non-decomposable. We extend our sampling algorithms to a novel class of conditionally autoregressive models for sparse estimation in multivariate lattice data, with a special emphasis on the analysis of spatial data. These models embed a great deal of flexibility in estimating both the correlation structure across outcomes and the spatial correlation structure, thereby allowing for adaptive smoothing and spatial autocorrelation parameters. Our methods are illustrated using a simulated example and a real-world application which concerns cancer mortality surveillance. Supplementary materials with computer code and the datasets needed to replicate our numerical results together with additional tables of results are available online. "], ["Maximum Likelihood Estimations and EM Algorithms With Length-Biased Data", " Length-biased sampling has been well recognized in economics, industrial reliability, etiology applications, and epidemiological, genetic, and cancer screening studies. Length-biased right-censored data have a unique data structure different from traditional survival data. The nonparametric and semiparametric estimation and inference methods for traditional survival data are not directly applicable for length-biased right-censored data. We propose new expectation-maximization algorithms for estimations based on full likelihoods involving infinite-dimensional parameters under three settings for length-biased data: estimating nonparametric distribution function, estimating nonparametric hazard function under an increasing failure rate constraint, and jointly estimating baseline hazards function and the covariate coefficients under the Cox proportional hazards model. Extensive empirical simulation studies show that the maximum likelihood estimators perform well with moderate sample sizes and lead to more efficient estimators compared to the estimating equation approaches. The proposed estimates are also more robust to various right-censoring mechanisms. We prove the strong consistency properties of the estimators, and establish the asymptotic normality of the semiparametric maximum likelihood estimators under the Cox model using modern empirical processes theory. We apply the proposed methods to a prevalent cohort medical study. Supplemental materials are available online. "], ["Dynamic Orthogonal Components for Multivariate Time Series", " We introduce dynamic orthogonal components (DOC) for multivariate time series and propose a procedure for estimating and testing the existence of DOCs for a given time series. We estimate the dynamic orthogonal components via a generalized decorrelation method that minimizes the linear and quadratic dependence across components and across time. We then use Ljung\u2013Box type statistics to test the existence of dynamic orthogonal components. When DOCs exist, univariate analysis can be applied to build a model for each component. Those univariate models are then combined to obtain a multivariate model for the original time series. We demonstrate the usefulness of dynamic orthogonal components with two real examples and compare the proposed modeling method with other dimension-reduction methods available in the literature, including principal component and independent component analyses. We also prove consistency and asymptotic normality of the proposed estimator under some regularity conditions. We provide some technical details in online Supplementary Materials. "], ["Model-Free Feature Screening for Ultrahigh-Dimensional Data", " With the recent explosion of scientific data of unprecedented size and complexity, feature ranking and screening are playing an increasingly important role in many scientific studies. In this article, we propose a novel feature screening procedure under a unified model framework, which covers a wide variety of commonly used parametric and semiparametric models. The new method does not require imposing a specific model structure on regression functions, and thus is particularly appealing to ultrahigh-dimensional regressions, where there are a huge number of candidate predictors but little information about the actual model forms. We demonstrate that, with the number of predictors growing at an exponential rate of the sample size, the proposed procedure possesses consistency in ranking, which is both useful in its own right and can lead to consistency in selection. The new procedure is computationally efficient and simple, and exhibits a competent empirical performance in our intensive simulations and real data analysis. "], ["A New Nuisance-Parameter Elimination Method With Application to the Unordered Homologous Chromosome Pairs Problem", " Motivated by applications of the case\u2013control model or exponential tilting model in the unordered homologous chromosome pairs problem in genetic studies and in the interim analysis in double-blinded clinical trials, we develop a new nuisance-parameter elimination method based on the empirical Shannon\u2019s mutual information. The asymptotic behaviors of the maximum empirical Shannon\u2019s mutual information estimation and the empirical Shannon\u2019s mutual information test are similar to those of the maximum likelihood estimation and the likelihood ratio test, respectively. Interestingly, we have found a connection between the empirical Shannon\u2019s mutual information and the profile empirical likelihood (Owen 1988) under some constraints. In the test of the null hypothesis that the unordered pairs come from the same distribution, the maximum Shannon\u2019s mutual information estimation has a degenerate information matrix. As a result, we have to expand the empirical Shannon\u2019s mutual information test statistic up to the fourth order to find the limiting distribution of the mixture of a distribution with point mass at zero and a chi-squared distribution with one degree of freedom. A real genetic dataset is employed for illustration. We also outline another application of Shannon\u2019s mutual information in general genetic mixture models. "], ["Semiparametric Stochastic Modeling of the Rate Function in Longitudinal Studies", " In longitudinal biomedical studies, there is often interest in the rate functions, which describe the functional rates of change of biomarker profiles. This article proposes a semiparametric approach to model these functions as the realizations of stochastic processes defined by stochastic differential equations. These processes are dependent on the covariates of interest and vary around a specified parametric function. An efficient Markov chain Monte Carlo algorithm is developed for inference. The proposed method is compared with several existing methods in terms of goodness of fit and more importantly the ability to forecast future functional data in a simulation study. The proposed methodology is applied to prostate-specific antigen profiles for illustration. Supplementary materials for this article are available online. "], ["Predictive Inference for Integrated Volatility", " Numerous volatility-based derivative products have been engineered in recent years. This has led to interest in constructing conditional predictive densities and confidence intervals for integrated volatility. In this article we propose nonparametric estimators of the aforementioned quantities, based on model-free volatility estimators. We establish consistency and asymptotic normality for the feasible estimators and study their finite-sample properties through a Monte Carlo experiment. Finally, using data from the New York Stock Exchange, we provide an empirical application to volatility directional predictability. "], ["Forecasting Time Series With Complex Seasonal Patterns Using Exponential Smoothing", " An innovations state space modeling framework is introduced for forecasting complex seasonal time series such as those with multiple seasonal periods, high-frequency seasonality, non-integer seasonality, and dual-calendar effects. The new framework incorporates Box\u2013Cox transformations, Fourier representations with time varying coefficients, and ARMA error correction. Likelihood evaluation and analytical expressions for point forecasts and interval predictions under the assumption of Gaussian errors are derived, leading to a simple, comprehensive approach to forecasting complex seasonal time series. A key feature of the framework is that it relies on a new method that greatly reduces the computational burden in the maximum likelihood estimation. The modeling framework is useful for a broad range of applications, its versatility being illustrated in three empirical studies. In addition, the proposed trigonometric formulation is presented as a means of decomposing complex seasonal time series, and it is shown that this decomposition leads to the identification and extraction of seasonal components which are otherwise not apparent in the time series plot itself. "], ["Bayesian Kernel Mixtures for Counts", " Although Bayesian nonparametric mixture models for continuous data are well developed, the literature on related approaches for count data is limited. A common strategy is to use a mixture of Poissons, which unfortunately is quite restrictive in not accounting for distributions with variance less than the mean. Other approaches include mixing multinomials, which requires finite support, and using a Dirichlet process prior with a Poisson base measure, which does not allow for smooth deviations from the Poisson. We propose broad class of alternative models, nonparametric mixtures of rounded continuous kernels. We develop an efficient Gibbs sampler for posterior computation, and perform a simulation study to assess performance. Focusing on the rounded Gaussian case, we generalize the modeling framework to account for multivariate count data, joint modeling with continuous and categorical variables, and other complications. We illustrate our methods through applications to a developmental toxicity study and marketing data. Supplemental material is available online. "], ["Large-Scale Correlation Screening", null], ["Coupling Optional P\u00f3lya Trees and the Two Sample Problem", " Testing and characterizing the difference between two data samples is of fundamental interest in statistics. Existing methods such as Kolmogorov\u2013Smirnov and Cramer\u2013von Mises tests do not scale well as the dimensionality increases and provide no easy way to characterize the difference should it exist. In this work, we propose a theoretical framework for inference that addresses these challenges in the form of a prior for Bayesian nonparametric analysis. The new prior is constructed based on a random-partition-and-assignment procedure similar to the one that defines the standard optional P\u00f3lya tree distribution, but has the ability to generate multiple random distributions jointly. These random probability distributions are allowed to \u201ccouple,\u201d that is, to have the same conditional distribution, on subsets of the sample space. We show that this \u201ccoupling optional P\u00f3lya tree\u201d prior provides a convenient and effective way for both the testing of two sample difference and the learning of the underlying structure of the difference. In addition, we discuss some practical issues in the computational implementation of this prior and provide several numerical examples to demonstrate its work. Supplementary materials for this article are available online. "], ["A Direct Estimation Approach to Sparse Linear Discriminant Analysis", null], ["Identifiability and Estimation of Causal Effects by Principal Stratification With Outcomes Truncated by Death", " In medical studies, there are many situations where the final outcomes are truncated by death, in which patients die before outcomes of interest are measured. In this article we consider identifiability and estimation of causal effects by principal stratification when some outcomes are truncated by death. Previous studies mostly focused on large sample bounds, Bayesian analysis, sensitivity analysis. In this article, we propose a new method for identifying the causal parameter of interest under a nonparametric and semiparametric model. We show that the causal parameter of interest is identifiable under some regularity assumptions and the assumption that there exists a pretreatment covariate whose conditional distributions among two principal strata are not the same, but our approach does not need the assumption of a mixture normal distribution for outcomes as required by Zhang, Rubin, and Mealli (2009). Hence, the proposed method is applicable not only to a continuous outcome but also to a binary outcome. When some of the assumptions are violated, we discuss biases of estimators and propose methods to reduce these biases. We conduct several simulation studies to evaluate the finite-sample performance of the proposed approach. Finally, we apply the proposed approach to a real dataset from a Southwest Oncology Group (SWOG) clinical trial. "], ["A Framework for Assessing Broad Sense Agreement Between Ordinal and Continuous Measurements", " Conventional agreement studies have been confined to addressing the sense of reproducibility, and therefore are limited to assessing measurements on the same scale. In this work, we propose a new concept, called \u201cbroad sense agreement,\u201d which extends the classical framework of agreement to evaluate the capability of interpreting a continuous measurement in an ordinal scale. We present a natural measure for broad sense agreement. Nonparametric estimation and inference procedures are developed for the proposed measure along with theoretical justifications. We also consider longitudinal settings which involve agreement assessments at multiple time points. Simulation studies have demonstrated good performance of the proposed method with small sample sizes. We illustrate our methods via an application to a mental health study. "], ["Tweedie\u2019s Formula and Selection Bias", null], ["Nonparametric Tests for Homogeneity Based on Non-Bipartite Matching", " Given a sequence of observations, has a change occurred in the underlying probability distribution with respect to observation order? This problem of detecting change points arises in a variety of applications including health prognostics for mechanical systems, syndromic disease surveillance in geographically dispersed populations, anomaly detection in information networks, and multivariate process control in general. Detecting change points in high-dimensional settings is challenging, and most change-point methods for multidimensional problems rely upon distributional assumptions or the use of observation history to model probability distributions. We present three new nonparametric statistical tests for heterogeneity based on the combinatorial properties of minimum non-bipartite matching (MNBM). The key idea underlying each of these tests is that if a sequence of independent random observations undergoes a change in distribution\u2014either an abrupt \u201cshift\u201d or a gradual \u201cdrift\u201d\u2014a MNBM based on inter-point distances tends to produce pairings that are closer in the sequence labeling than would be the case if the observations were drawn from the same distribution. Our tests follow on the work of Rosenbaum (2005) who used MNBM to derive a simple cross-match test statistic for the two-sample problem based on this idea. Similar ideas are present in the minimum spanning tree (MST) test derived by Friedman and Rafsky (1979, 1981). We extend these approaches by utilizing ensembles of orthogonal MNBMs which greatly increase information extraction from the data, leading to tests that compare favorably to parametric procedures while maintaining level and good power properties across distributions. "], ["Building Consistent Regression Trees From Complex Sample Data", null], ["Book Reviews", null], ["2011 Editorial Collaborators", null], ["Index to Volume 106 (2011)", null], ["Population Value Decomposition, a Framework for the Analysis of Image Populations", " Images, often stored in multidimensional arrays, are fast becoming ubiquitous in medical and public health research. Analyzing populations of images is a statistical problem that raises a host of daunting challenges. The most significant challenge is the massive size of the datasets incorporating images recorded for hundreds or thousands of subjects at multiple visits. We introduce the population value decomposition (PVD), a general method for simultaneous dimensionality reduction of large populations of massive images. We show how PVD can be seamlessly incorporated into statistical modeling, leading to a new, transparent, and rapid inferential framework. Our PVD methodology was motivated by and applied to the Sleep Heart Health Study, the largest community-based cohort study of sleep containing more than 85 billion observations on thousands of subjects at two visits. This article has supplementary material online. "], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Nonparametric Bayes Stochastically Ordered Latent Class Models", " Latent class models (LCMs) are used increasingly for addressing a broad variety of problems, including sparse modeling of multivariate and longitudinal data, model-based clustering, and flexible inferences on predictor effects. Typical frequentist LCMs require estimation of a single finite number of classes, which does not increase with the sample size, and have a well-known sensitivity to parametric assumptions on the distributions within a class. Bayesian nonparametric methods have been developed to allow an infinite number of classes in the general population, with the number represented in a sample increasing with sample size. In this article, we propose a new nonparametric Bayes model that allows predictors to flexibly impact the allocation to latent classes, while limiting sensitivity to parametric assumptions by allowing class-specific distributions to be unknown subject to a stochastic ordering constraint. An efficient MCMC algorithm is developed for posterior computation. The methods are validated using simulation studies and applied to the problem of ranking medical procedures in terms of the distribution of patient morbidity. "], ["Robust EM Continual Reassessment Method in Oncology Dose Finding", " The continual reassessment method (CRM) is a commonly used dose-finding design for phase I clinical trials. Practical applications of this method have been restricted by two limitations: (1) the requirement that the toxicity outcome needs to be observed shortly after the initiation of the treatment; and (2) the potential sensitivity to the prespecified toxicity probability at each dose. To overcome these limitations, we naturally treat the unobserved toxicity outcomes as missing data, and use the expectation-maximization (EM) algorithm to estimate the dose toxicity probabilities based on the incomplete data to direct dose assignment. To enhance the robustness of the design, we propose prespecifying multiple sets of toxicity probabilities, each set corresponding to an individual CRM model. We carry out these multiple CRMs in parallel, across which model selection and model averaging procedures are used to make more robust inference. We evaluate the operating characteristics of the proposed robust EM-CRM designs through simulation studies and show that the proposed methods satisfactorily resolve both limitations of the CRM. Besides improving the MTD selection percentage, the new designs dramatically shorten the duration of the trial, and are robust to the prespecification of the toxicity probabilities. "], ["Analysis of Long Period Variable Stars With Nonparametric Tests for Trend Detection", null], ["Fast and Accurate Approximation to Significance Tests in Genome-Wide Association Studies", null], ["Functional Principal Component Analysis of Density Families With Categorical and Continuous Data on Canadian Entrant Manufacturing Firms", " This article investigates the evolution of firm distributions for entrant manufacturing firms in Canada using functional principal components analysis. This methodology describes the dynamics of firms by examining production variables, size, and labor productivity, and a financial variable, leverage (debt-to-asset ratio). We adapt the canonical functional principal components analysis to allow for the inclusion of qualitative information in the form of discrete variables, industry, and region, to capture market structure differences, which is shown to change the dynamics of firm size and labor productivity distributions only. We also perform various tests with the null hypothesis that the distributions are equal across time. When accounting for industry and regional categories, there is a substantial fall in the number of rejections of the null hypothesis of equality for size and labor productivity, which is not the case for leverage. These results show the importance of including qualitative information to account for potential heterogeneity when applying functional principal component analysis to firm-level data. Finally, the methodology finds a correlation between the evolution of variable distributions and macroeconomic factors. This article has supplementary material online. "], ["Distinct Counting With a Self-Learning Bitmap", null], ["A Statistical Framework for the Analysis of ChIP-Seq Data", " We study data from a naked DNA sequencing experiment, which sequences noncross-linked DNA after deproteinizing and shearing, to understand factors affecting background distribution of data generated in a ChIP-Seq experiment. We introduce a background model that accounts for apparent sources of biases such as mappability and GC content and develop a flexible mixture model named MOSAiCS for detecting peaks in both one- and two-sample analyses of ChIP-Seq data. We illustrate that our model fits observed ChIP-Seq data well and further demonstrate advantages of MOSAiCS over commonly used tools for ChIP-Seq data analysis with several case studies. This article has supplementary material online. "], ["Adaptive Confidence Intervals for the Test Error in Classification", " The estimated test error of a learned classifier is the most commonly reported measure of classifier performance. However, constructing a high-quality point estimator of the test error has proved to be very difficult. Furthermore, common interval estimators (e.g., confidence intervals) are based on the point estimator of the test error and thus inherit all the difficulties associated with the point estimation problem. As a result, these confidence intervals do not reliably deliver nominal coverage. In contrast, we directly construct the confidence interval by using smooth data-dependent upper and lower bounds on the test error. We prove that, for linear classifiers, the proposed confidence interval automatically adapts to the nonsmoothness of the test error, is consistent under fixed and local alternatives, and does not require that the Bayes classifier be linear. Moreover, the method provides nominal coverage on a suite of test problems using a range of classification algorithms and sample sizes. This article has supplementary material online. "], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["An Asymptotically Pivotal Transform of the Residuals Sample Autocorrelations With Application to Model Checking", null], ["Variational Bayesian Inference for Parametric and Nonparametric Regression With Missing Data", " Bayesian hierarchical models are attractive structures for conducting regression analyses when the data are subject to missingness. However, the requisite probability calculus is challenging and Monte Carlo methods typically are employed. We develop an alternative approach based on deterministic variational Bayes approximations. Both parametric and nonparametric regression are considered. Attention is restricted to the more challenging case of missing predictor data. We demonstrate that variational Bayes can achieve good accuracy, but with considerably less computational overhead. The main ramification is fast approximate Bayesian inference in parametric and nonparametric regression models with missing data. Supplemental materials accompany the online version of this article. "], ["Bayesian Inference for the Spatial Random Effects Model", " Spatial statistical analysis of massive amounts of spatial data can be challenging because computation of optimal procedures can break down. The Spatial Random Effects (SRE) model uses a fixed number of known but not necessarily orthogonal (multiresolutional) spatial basis functions, which gives a flexible family of nonstationary covariance functions, results in dimension reduction, and yields optimal spatial predictors whose computations are scalable. By modeling spatial data in a hierarchical manner with a process model that includes the SRE model, the choice is whether to estimate the SRE model\u2019s parameters or to take a Bayesian approach and put a prior distribution on them. In this article, we develop Bayesian inference for the SRE model when the spatial basis functions are multiresolutional. Then the covariance matrix of the random effects decomposes naturally in terms of Givens angles and eigenvalues, for which a new class of prior distributions is developed. This approach to prior specification of a spatial covariance matrix offers remarkable improvement over other types of priors used in the random-effects literature (e.g., Wishart priors), as demonstrated in a simulation experiment. Further, a large remote-sensing dataset of aerosol optical depth (AOD), from the Multi-angle Imaging SpectroRadiometer (MISR) instrument on the Terra satellite, is analyzed in a fully Bayesian framework, using the new prior, and compared to an empirical-Bayesian analysis. "], ["Posterior Probability of Discovery and Expected Rate of Discovery for Multiple Hypothesis Testing and High Throughput Assays", " Technologies of measuring millions of quantities at once have been rapidly developed and used in biological and biomedical research and other fields in the past decade. Yet a key issue remains unsettled, namely, how to control spurious findings for the ensuing massive number of hypothesis tests. An emerging consensus is to control false discovery rate (FDR), with FDR defined as the expected proportion of true nulls among discoveries, and a discovery the rejection of a null hypothesis. However, the very concept of counting true nulls, implicitly or explicitly, is problematic, for nulls are rarely true in reality. We propose an approach that is philosophically different from the FDR and other approaches. Taking advantage of the massive measurements, we can and should directly evaluate the reproducibility of a discovery by calculating the posterior probability of discovery (PPD) given observed data. A discovery with a great PPD is deemed to be highly reproducible, that is, not spurious. For a subset of hypotheses tested, mean PPD yields the expected rate of discovery (ERD), a measure useful for various applications such as subset enrichment analysis. We present here the rationale, theoretical basis, and an algorithm for computing PPDs and ERDs from data. Their validity, utility, and optimal performance are demonstrated using both simulated and real data. Supplementary material is available online. "], ["Correcting for Population Stratification in Genomewide Association Studies", null], ["Independent Component Analysis Involving Autocorrelated Sources With an Application to Functional Magnetic Resonance Imaging", " In this article, we consider a novel approach to ICA that fully exploits the correlation structures within the source signals. Specifically, we propose to estimate the spectral density functions of the source signals instead of their marginal density functions. This is made possible by virtue of the intrinsic relationship between the (unobserved) sources and the (observed) mixed signals. Our methodology is described and implemented using spectral density functions from frequently used time series models such as autoregressive moving average (ARMA) processes. The time series parameters and the mixing matrix are estimated via maximizing the Whittle likelihood function. We illustrate the performance of the proposed method through extensive simulation studies and a real fMRI application. The numerical results indicate that our approach outperforms several popular methods including the most widely used fastICA algorithm. This article has supplementary material online. "], ["Large Volatility Matrix Inference via Combining Low-Frequency and High-Frequency Approaches", " It is increasingly important in financial economics to estimate volatilities of asset returns. However, most of the available methods are not directly applicable when the number of assets involved is large, due to the lack of accuracy in estimating high-dimensional matrices. Therefore it is pertinent to reduce the effective size of volatility matrices in order to produce adequate estimates and forecasts. Furthermore, since high-frequency financial data for different assets are typically not recorded at the same time points, conventional dimension-reduction techniques are not directly applicable. To overcome those difficulties we explore a novel approach that combines high-frequency volatility matrix estimation together with low-frequency dynamic models. The proposed methodology consists of three steps: (i) estimate daily realized covolatility matrices directly based on high-frequency data, (ii) fit a matrix factor model to the estimated daily covolatility matrices, and (iii) fit a vector autoregressive model to the estimated volatility factors. We establish the asymptotic theory for the proposed methodology in the framework that allows sample size, number of assets, and number of days go to infinity together. Our theory shows that the relevant eigenvalues and eigenvectors can be consistently estimated. We illustrate the methodology with the high-frequency price data on several hundreds of stocks traded in Shenzhen and Shanghai Stock Exchanges over a period of 177 days in 2003. Our approach pools together the strengths of modeling and estimation at both intra-daily (high-frequency) and inter-daily (low-frequency) levels. "], ["Rao\u2013Blackwellization for Bayesian Variable Selection and Model Averaging in Linear and Binary Regression: A Novel Data Augmentation Approach", null], ["Optimal Weight Choice for Frequentist Model Average Estimators", " There has been increasing interest recently in model averaging within the frequentist paradigm. The main benefit of model averaging over model selection is that it incorporates rather than ignores the uncertainty inherent in the model selection process. One of the most important, yet challenging, aspects of model averaging is how to optimally combine estimates from different models. In this work, we suggest a procedure of weight choice for frequentist model average estimators that exhibits optimality properties with respect to the estimator\u2019s mean squared error (MSE). As a basis for demonstrating our idea, we consider averaging over a sequence of linear regression models. Building on this base, we develop a model weighting mechanism that involves minimizing the trace of an unbiased estimator of the model average estimator\u2019s MSE. We further obtain results that reflect the finite sample as well as asymptotic optimality of the proposed mechanism. A Monte Carlo study based on simulated and real data evaluates and compares the finite sample properties of this mechanism with those of existing methods. The extension of the proposed weight selection scheme to general likelihood models is also considered. This article has supplementary material online. "], ["Permutation Multiple Tests of Binary Features Do Not Uniformly Control Error Rates", " Multiple testing for significant association between predictors and responses has a wide array of applications. One such application is pharmacogenomics, where testing for association between responses and many genetic markers is of interest. Permuting response group labels to generate a reference distribution is often thought of as a convenient thresholding technique that automatically captures dependence in the data. In reality, nontrivial model assumptions are required for permutation testing to control multiple testing error rates. When binary predictors (such as genetic markers) are individually tested by standard tests such as Fisher\u2019s exact test, permutation multiple testing can give incorrect unconditional and, especially, conditional assessment of significances. Regardless of whether the sample sizes are equal, how misleading permutation assessment can be is primarily a function of the difference in the covariances among the genetic markers between the phenotype groups. "], ["Hierarchical Clustering With Prototypes via Minimax Linkage", null], ["Summarizing Insurance Scores Using a Gini Index", " We show that the ordered Lorenz curve has desirable properties. It can be expressed in terms of weighted distributions functions. In special cases, curves can be ranked through a partial ordering. We show how to estimate the Gini index and give pointwise consistency and asymptotic normality results. A simulation study and an example using homeowners insurance underscore the potential applications of these methods. "], ["Linear or Nonlinear? Automatic Structure Discovery for Partially Linear Models", " Partially linear models provide a useful class of tools for modeling complex data by naturally incorporating a combination of linear and nonlinear effects within one framework. One key question in partially linear models is the choice of model structure, that is, how to decide which covariates are linear and which are nonlinear. This is a fundamental, yet largely unsolved problem for partially linear models. In practice, one often assumes that the model structure is given or known and then makes estimation and inference based on that structure. Alternatively, there are two methods in common use for tackling the problem: hypotheses testing and visual screening based on the marginal fits. Both methods are quite useful in practice but have their drawbacks. First, it is difficult to construct a powerful procedure for testing multiple hypotheses of linear against nonlinear fits. Second, the screening procedure based on the scatterplots of individual covariate fits may provide an educated guess on the regression function form, but the procedure is ad hoc and lacks theoretical justifications. In this article, we propose a new approach to structure selection for partially linear models, called the LAND (Linear And Nonlinear Discoverer). The procedure is developed in an elegant mathematical framework and possesses desired theoretical and computational properties. Under certain regularity conditions, we show that the LAND estimator is able to identify the underlying true model structure correctly and at the same time estimate the multivariate regression function consistently. The convergence rate of the new estimator is established as well. We further propose an iterative algorithm to implement the procedure and illustrate its performance by simulated and real examples. Supplementary materials for this article are available online. "], ["A Measure of Stationarity in Locally Stationary Processes With Applications to Testing", null], [null, null], ["Multi-Layer Designs for Computer Experiments", " Space-filling designs such as Latin hypercube designs (LHDs) are widely used in computer experiments. However, finding an optimal LHD with good space-filling properties is computationally cumbersome. On the other hand, the well-established factorial designs in physical experiments are unsuitable for computer experiments owing to the redundancy of design points when projected onto a subset of factor space. In this work, we present a new class of space-filling designs developed by splitting two-level factorial designs into multiple layers. The method takes advantage of many available results in factorial design theory and therefore, the proposed multi-layer designs (MLDs) are easy to generate. Moreover, our numerical study shows that MLDs can have better space-filling properties than optimal LHDs. "], ["GLS Estimation of Dynamic Factor Models", " In this article a simple two-step estimation procedure of the dynamic factor model is proposed. The estimator allows for heteroscedastic and serially correlated errors. It turns out that the feasible two-step estimator has the same limiting distribution as the generalized least squares (GLS) estimator assuming that the covariance parameters are known. In a Monte Carlo study of the small sample properties, we find that the GLS estimators may be substantially more efficient than the usual estimator based on principal components. Furthermore, it turns out that the iterated version of the estimator may feature considerably improved properties in sample sizes usually encountered in practice. "], ["Robust, Adaptive Functional Regression in Functional Mixed Model Framework", " Functional data are increasingly encountered in scientific studies, and their high dimensionality and complexity lead to many analytical challenges. Various methods for functional data analysis have been developed, including functional response regression methods that involve regression of a functional response on univariate/multivariate predictors with nonparametrically represented functional coefficients. In existing methods, however, the functional regression can be sensitive to outlying curves and outlying regions of curves, so is not robust. In this article, we introduce a new Bayesian method, robust functional mixed models (R-FMM), for performing robust functional regression within the general functional mixed model framework, which includes multiple continuous or categorical predictors and random effect functions accommodating potential between-function correlation induced by the experimental design. The underlying model involves a hierarchical scale mixture model for the fixed effects, random effect, and residual error functions. These modeling assumptions across curves result in robust nonparametric estimators of the fixed and random effect functions which down-weight outlying curves and regions of curves, and produce statistics that can be used to flag global and local outliers. These assumptions also lead to distributions across wavelet coefficients that have outstanding sparsity and adaptive shrinkage properties, with great flexibility for the data to determine the sparsity and the heaviness of the tails. Together with the down-weighting of outliers, these within-curve properties lead to fixed and random effect function estimates that appear in our simulations to be remarkably adaptive in their ability to remove spurious features yet retain true features of the functions. We have developed general code to implement this fully Bayesian method that is automatic, requiring the user to only provide the functional data and design matrices. It is efficient enough to handle large datasets, and yields posterior samples of all model parameters that can be used to perform desired Bayesian estimation and inference. Although we present details for a specific implementation of the R-FMM using specific distributional choices in the hierarchical model, 1D functions, and wavelet transforms, the method can be applied more generally using other heavy-tailed distributions, higher dimensional functions (e.g., images), and using other invertible transformations as alternatives to wavelets. Supplementary materials for this article are available online. "], ["Density Estimation in Several Populations With Uncertain Population Membership", " We devise methods to estimate probability density functions of several populations using observations with uncertain population membership, meaning from which population an observation comes is unknown. The probability of an observation being sampled from any given population can be calculated. We develop general estimation procedures and bandwidth selection methods for our setting. We establish large-sample properties and study finite-sample performance using simulation studies. We illustrate our methods with data from a nutrition study. "], ["Fusion-Refinement Procedure for Dimension Reduction With Missing Response at Random", " Dimension reduction methods are useful for handling high-dimensional data. It is a common situation that responses of some subjects are not observed in practice. Generally, the missingness carries additional information about the central subspace. Here we propose a two-stage procedure known as the fusion-refinement (FR) procedure. In the first stage, we obtain a subspace including the central subspace by fusing information on regression and missingness. In the second stage, we refine the obtained subspace to recover the central subspace by imputation method. We use sliced inverse regression to illustrate the FR procedure. We conduct simulation studies, and suggest a data-driven procedure to choose from the complete-case analysis and the FR procedure for a purpose of a real application. A real data analysis is used to illustrate our methods. "], ["Predicting False Discovery Proportion Under Dependence", null], ["Book Reviews", null], ["Estimating the Term Structure With a Semiparametric Bayesian Hierarchical Model: An Application to Corporate Bonds", " The term structure of interest rates is used to price defaultable bonds and credit derivatives, as well as to infer the quality of bonds for risk management purposes. We introduce a model that jointly estimates term structures by means of a Bayesian hierarchical model with a prior probability model based on Dirichlet process mixtures. The modeling methodology borrows strength across term structures for purposes of estimation. The main advantage of our framework is its ability to produce reliable estimators at the company level even when there are only a few bonds per company. After describing the proposed model, we discuss an empirical application in which the term structure of 197 individual companies is estimated. The sample of 197 consists of 143 companies with only one or two bonds. In-sample and out-of-sample tests are used to quantify the improvement in accuracy that results from approximating the term structure of corporate bonds with estimators by company rather than by credit rating, the latter being a popular choice in the financial literature. A complete description of a Markov chain Monte Carlo (MCMC) scheme for the proposed model is available as Supplementary Material. "], ["An Approach to the Estimation of Chronic Air Pollution Effects Using Spatio-Temporal Information", null], ["Multivariate Regression Analysis for the Item Count Technique", " The item count technique is a survey methodology that is designed to elicit respondents\u2019 truthful answers to sensitive questions such as racial prejudice and drug use. The method is also known as the list experiment or the unmatched count technique and is an alternative to the commonly used randomized response method. In this article, I propose new nonlinear least squares and maximum likelihood estimators for efficient multivariate regression analysis with the item count technique. The two-step estimation procedure and the Expectation Maximization algorithm are developed to facilitate the computation. Enabling multivariate regression analysis is essential because researchers are typically interested in knowing how the probability of answering the sensitive question affirmatively varies as a function of respondents\u2019 characteristics. As an empirical illustration, the proposed methodology is applied to the 1991 National Race and Politics survey where the investigators used the item count technique to measure the degree of racial hatred in the United States. Small-scale simulation studies suggest that the maximum likelihood estimator can be substantially more efficient than alternative estimators. Statistical efficiency is an important concern for the item count technique because indirect questioning means loss of information. The open-source software is made available to implement the proposed methodology. "], ["Self-Controlled Case Series Analysis With Event-Dependent Observation Periods", " The self-controlled case series method may be used to study the association between a time-varying exposure and a health event. It is based only on cases, and it controls for fixed confounders. Exposure and event histories are collected for each case over a predefined observation period. The method requires that observation periods should be independent of event times. This requirement is violated when events increase the mortality rate, since censoring of the observation periods is then event dependent. In this article, the case series method for rare nonrecurrent events is extended to remove this independence assumption, thus introducing an additional term in the likelihood that depends on the censoring process. In order to remain within the case series framework in which only cases are sampled, the model is reparameterized so that this additional term becomes estimable from the distribution of intervals from event to end of observation. The exposure effect of primary interest may be estimated unbiasedly. The age effect, however, takes on a new interpretation, incorporating the effect of censoring. The model may be fitted in standard loglinear modeling software; this yields conservative standard errors. We describe a detailed application to the study of antipsychotics and stroke. The estimates obtained from the standard case series model are shown to be biased when event-dependent observation periods are ignored. When they are allowed for, antipsychotic use remains strongly positively associated with stroke in patients with dementia, but not in patients without dementia. Two detailed simulation studies are included as Supplemental Material. "], ["Predictive Macro-Finance With Dynamic Partition Models", " Dynamic partition models are used to predict movements in the term structure of interest rates. This allows one to understand historic cycles in the performance of how interest rates behave, and to offer policy makers guidance regarding future expectations on their evolution. Our approach allows for a random number of possible change points in the term structure of interest rates. We use particle learning to learn about the unobserved state variables in a new class of dynamic product partition models that relate macro-variables to term structures. The empirical results, using data from 1970 to 2000, clearly identifies some of the key shocks to the economy, such as recessions. We construct a time series of Bayes factors that, surprisingly, could serve as a leading indicator of economic activity, validated via a Granger causality test. Finally, the in-sample and out-of-sample forecasts from our model are quite robust regardless of the time to maturity of interest rates. "], ["Malaria in Northwest India: Data Analysis via Partially Observed Stochastic Differential Equation Models Driven by L\u00e9vy Noise", " An online supplement presents details of the methodology implemented and two additional figures. "], ["Global Warming and Local Dimming: The Statistical Evidence", null], ["Rejoinder", null], ["Comment", null], ["Modeling Partial Compliance Through Copulas in a Principal Stratification Framework", " Within the principal stratification framework for causal inference, modeling partial compliance is challenging because the continuous nature of the principal strata raises subtle specification issues. In this context, we propose an approach based on the assumption that the joint distribution of the degree of compliance to the treatment and the degree of compliance to the control follows a Plackett copula, so that their association is modeled in a flexible way through a single parameter. Moreover, given the two compliances, the distribution of the outcomes is parameterized in a flexible way through a regression model which may include interaction and quadratic terms and may also be heteroscedastic. In order to estimate the parameters of the resulting model, and then the causal effect of the treatment, we adopt a maximum likelihood approach via the EM algorithm. In applying this approach, the marginal distributions of the two compliances are estimated by their empirical distribution functions, so that no constraints are posed on these distributions. Since the two compliances cannot be jointly observed, there is not direct empirical support for the association parameter. We describe a strategy for studying this parameter by a profile likelihood method and discuss an analysis of the sensitivity of the causal inference results to its value. We apply the proposed approach to data from a study about a drug for lowering cholesterol levels previously analyzed by Efron and Feldman and by Jin and Rubin. Estimated causal effects are in line with those of previous analyses, but the pattern of association between the compliances is qualitatively different, apparently due to the flexibility of the copula and to allowing regression equations in the proposed method to include interactions and heteroscedasticity. "], ["Cocaine Dependence Treatment Data: Methods for Measurement Error Problems With Predictors Derived From Stationary Stochastic Processes", " In a cocaine dependence treatment study, we use linear and nonlinear regression models to model posttreatment cocaine craving scores and first cocaine relapse time. A subset of the covariates are summary statistics derived from baseline daily cocaine use trajectories, such as baseline cocaine use frequency and average daily use amount. These summary statistics are subject to estimation error and can therefore cause biased estimators for the regression coefficients. Unlike classical measurement error problems, the error we encounter here is heteroscedastic with an unknown distribution, and there are no replicates for the error-prone variables or instrumental variables. We propose two robust methods to correct for the bias: a computationally efficient method-of-moments-based method for linear regression models and a subsampling extrapolation method that is generally applicable to both linear and nonlinear regression models. Simulations and an application to the cocaine dependence treatment data are used to illustrate the efficacy of the proposed methods. Asymptotic theory and variance estimation for the proposed subsampling extrapolation method and some additional simulation results are described in the online supplementary material. "], ["Identifying Risk Factors for Severe Childhood Malnutrition by Boosting Additive Quantile Regression", " We investigated the risk factors for childhood malnutrition in India based on the 2005/2006 Demographic and Health Survey by applying a novel estimation technique for additive quantile regression. Ordinary linear and generalized linear regression models relate the mean of a response variable to a linear combination of covariate effects, and, as a consequence, focus on average properties of the response. The use of such a regression model for analyzing childhood malnutrition in developing or transition countries implies that the estimated effects describe the average nutritional status. However, it is of even greater interest to analyze quantiles of the response distribution, such as the 5% or 10% quantile, which relate to the risk of extreme malnutrition. Our investigation is based on a semiparametric extension of quantile regression models where different types of nonlinear effects are included in the model equation, leading to additive quantile regression. We addressed the variable selection and model choice problems associated with estimating such an additive quantile regression model using a novel boosting approach. Our proposal allows for data-driven determination of the amount of smoothness required for the nonlinear effects and combines model choice with an automatic variable selection property. In an empirical evaluation, we compared our boosting approach with state-of-the-art methods for additive quantile regression. The results suggest that boosting is an appropriate tool for estimation and variable selection in additive quantile regression models and helps to identify yet unknown risk factors for childhood malnutrition. This article has supplementary material online. "], ["Using Split Samples and Evidence Factors in an Observational Study of Neonatal Outcomes", " During a few years around the turn of the millennium, a series of local hospitals in Philadelphia closed their obstetrics units, with the consequence that many mothers-to-be arrived unexpectedly at the city\u2019s large, regional teaching hospitals whose obstetrics units remained open. Nothing comparable happened in other United States cities, where there were only sporadic changes in the availability of obstetrics units. What effect did these closures have on mothers and their newborns? We study this question by comparing Philadelphia before and after the closures to a control Philadelphia constructed from elsewhere in Pennsylvania, California, and Missouri, matching mothers for 59 observed covariates including year of birth. The analysis focuses on the period 1995\u20131996, when there were no closures, and the period 1997\u20131999 when five hospitals abruptly closed their obstetrics units. Using a new sensitivity analysis for difference-in-differences with binary outcomes, we examine the possibility that Philadelphia mothers differed from control mothers in terms of some covariate not measured, and perhaps the distribution of that unobserved covariate changed in a different way in Philadelphia and control\u2013Philadelphia in the years before and after the closures. We illustrate two recently proposed techniques for the design and analysis of observational studies, namely split samples and evidence factors. To boost insensitivity to unmeasured bias, we drew a small random planning sample of about 26,000 mothers in 13,000 pairs and used them to frame hypotheses that promised to be less sensitive to bias; then these hypotheses were tested on the large, independent complementary analysis sample of nearly 240,000 mothers in 120,000 pairs. The splitting was successful twice over: (i) it successfully identified an interesting and moderately insensitive conclusion, (ii) by comparison of the planning and analysis samples, it is clearly seen to have avoided a exaggerated claim of insensitivity to unmeasured bias that might have occurred by focusing on the least sensitive of many findings. Also, we identified two approximate evidence factors and one test for unmeasured bias: (i) factor 1 compared Philadelphia to control before and after the closures, (ii) factor 2 focused on the years 1997\u20131999 of abrupt closures and compared zip codes with closures to zip codes without closures, (iii) and the test for bias focused on the years 1995\u20131996 prior to closures and compared zip codes which would have closures in 1997\u20131999 to zip codes without closures in 1997\u20131999\u2014any ostensible effect found in that last comparison is surely bias from the characteristics of Philadelphia zip codes in which closures took place. Approximate evidence factors provide nearly independent tests of a null hypothesis such that the evidence in each factor would be unaffected by certain biases that would invalidate the other factor. "], ["Inference for Quantitation Parameters in Polymerase Chain Reactions via Branching Processes With Random Effects", " The quantitative polymerase chain reaction (qPCR) is a widely used tool for gene quantitation and has been applied extensively in several scientific areas. The current methods used for analyzing qPCR data fail to account for multiple sources of variability present in the PCR dynamics, leading to biased estimates and incorrect inference. In this article, we introduce a branching process model with random effects to account for within-reaction and between-reaction variability in PCR experiments. We describe, in terms of the observed fluorescence data, new statistical methodology for gene quantitation. Using simulations, PCR experiments, and asymptotic theory we demonstrate the improvements achieved by our methodology compared to existing methods. This article has supplemental materials online. "], ["A Direct Bootstrap Method for Complex Sampling Designs From a Finite Population", " In complex designs, classical bootstrap methods result in a biased variance estimator when the sampling design is not taken into account. Resampled units are usually rescaled or weighted in order to achieve unbiasedness in the linear case. In the present article, we propose novel resampling methods that may be directly applied to variance estimation. These methods consist of selecting subsamples under a completely different sampling scheme from that which generated the original sample, which is composed of several sampling designs. In particular, a portion of the subsampled units is selected without replacement, while another is selected with replacement, thereby adjusting for the finite population setting. We show that these bootstrap estimators directly and precisely reproduce unbiased estimators of the variance in the linear case in a time-efficient manner, and eliminate the need for classical adjustment methods such as rescaling, correction factors, or artificial populations. Moreover, we show via simulation studies that our method is at least as efficient as those currently existing, which call for additional adjustment. This methodology can be applied to classical sampling designs, including simple random sampling with and without replacement, Poisson sampling, and unequal probability sampling with and without replacement. "], ["Nonparametric Independence Screening in Sparse Ultra-High-Dimensional Additive Models", " A variable screening procedure via correlation learning was proposed by Fan and Lv (2008) to reduce dimensionality in sparse ultra-high-dimensional models. Even when the true model is linear, the marginal regression can be highly nonlinear. To address this issue, we further extend the correlation learning to marginal nonparametric learning. Our nonparametric independence screening (NIS) is a specific type of sure independence screening. We propose several closely related variable screening procedures. We show that with general nonparametric models, under some mild technical conditions, the proposed independence screening methods have a sure screening property. The extent to which the dimensionality can be reduced by independence screening is also explicitly quantified. As a methodological extension, we also propose a data-driven thresholding and an iterative nonparametric independence screening (INIS) method to enhance the finite- sample performance for fitting sparse additive models. The simulation results and a real data analysis demonstrate that the proposed procedure works well with moderate sample size and large dimension and performs better than competing methods. "], ["Estimating the Jump Activity Index Under Noisy Observations Using High-Frequency Data", " It is widely accepted that the high-frequency data are contaminated by microstructure noise, whose effect on the statistical inference has been of increasing interest in the literature. Much of it, however, has focused on the integrated volatility. In this article, we investigate another important characteristic, namely, the jump activity index (JAI) of a discretely sampled semi-martingale corrupted by microstructure noise. We point out that ignoring the microstructure noise can have a disastrous effect on the estimation of the JAI. Consequently, we propose a two-stage procedure to estimate the JAI. It first reduces the effect of noise by local smoothing and then estimates the index from the smoothed data. The asymptotic properties such as consistency and asymptotic normality are given. Simulations are conducted to evaluate the performance of the procedure. Finally, we implement our estimators to some real datasets. "], ["Nonparametric Evaluation of Biomarker Accuracy Under Nested Case-Control Studies", " To evaluate the clinical utility of new risk markers, a crucial step is to measure their predictive accuracy with prospective studies. However, it is often infeasible to obtain marker values for all study participants. The nested case-control (NCC) design is a useful cost-effective strategy for such settings. Under the NCC design, markers are only ascertained for cases and a fraction of controls sampled randomly from the risk sets. The outcome dependent sampling generates a complex data structure and therefore a challenge for analysis. Existing methods for analyzing NCC studies focus primarily on association measures. Here, we propose a class of nonparametric estimators for commonly used accuracy measures. We derived asymptotic expansions for accuracy estimators based on both finite population and Bernoulli sampling and established asymptotic equivalence between the two. Simulation results suggest that the proposed procedures perform well in finite samples. The new procedures were illustrated with data from the Framingham Offspring study. "], ["Randomization-Based Inference Within Principal Strata", " In randomized studies, treatment comparisons conditional on intermediate postrandomization outcomes using standard analytic methods do not have a causal interpretation. An alternative approach entails treatment comparisons within principal strata defined by the potential outcomes for the intermediate outcome that would be observed under each treatment assignment. In this article we develop methods for randomization-based inference within principal strata. We compare our proposed methods with existing large-sample methods as well as traditional intent-to-treat approaches. This research is motivated by HIV prevention studies, where few infections are expected and inference is desired within the always-infected principal stratum, that is, all individuals who would become infected regardless of randomization assignment. "], [null, null], ["Bootstrapping Lasso Estimators", " In this article, we consider bootstrapping the Lasso estimator of the regression parameter in a multiple linear regression model. It is known that the standard bootstrap method fails to be consistent. Here, we propose a modified bootstrap method, and show that it provides valid approximation to the distribution of the Lasso estimator, for all possible values of the unknown regression parameter vector, including the case where some of the components are zero. Further, we establish consistency of the modified bootstrap method for estimating the asymptotic bias and variance of the Lasso estimator. We also show that the residual bootstrap can be used to consistently estimate the distribution and variance of the adaptive Lasso estimator. Using the former result, we formulate a novel data-based method for choosing the optimal penalizing parameter for the Lasso using the modified bootstrap. A numerical study is performed to investigate the finite sample performance of the modified bootstrap. The methodology proposed in the article is illustrated with a real data example. "], ["Outlier Detection Using Nonconvex Penalized Regression", null], ["Nonparametric Regression Analysis for Group Testing Data", null], ["Do-Validation for Kernel Density Estimation", " Bandwidth selection in kernel density estimation is one of the fundamental model selection problems of mathematical statistics. The study of this problem took major steps forward with the articles of Hall and Marron (1987) and Hall and Johnstone (1992). Since then, the focus seems to have been on various versions of implementing the so-called plug-in method aimed at estimating the minimum mean integrated squared error (MISE). The most successful of these efforts still seems to be the plug-in method of Sheather and Jones (1991) or Park and Marron (1990) that we also use as a benchmark in this article. In this article we derive a new theorem deriving the asymptotic theory for linear combinations of bandwidths obtained from different selectors as, for example, direct and indirect cross-validation and plug-in, where we take advantage of recent advances in the study of indirect cross-validation; see Hart and Yi (1998), Hart and Lee (2005), and Savchuk, Hart, and Sheather (2008, 2010). We conclude that the slow convergence of data-driven bandwidths implies that once asymptotic theory is close to that of the plug-in, then it is the practical implementation that counts. This insight led us to a bandwidth selector search with the symmetrized version of one-sided cross-validation as a clear winner. "], ["Projection Estimators for Generalized Linear Models", null], ["Adaptive Thresholding for Sparse Covariance Matrix Estimation", " In this article we consider estimation of sparse covariance matrices and propose a thresholding procedure that is adaptive to the variability of individual entries. The estimators are fully data-driven and demonstrate excellent performance both theoretically and numerically. It is shown that the estimators adaptively achieve the optimal rate of convergence over a large class of sparse covariance matrices under the spectral norm. In contrast, the commonly used universal thresholding estimators are shown to be suboptimal over the same parameter spaces. Support recovery is discussed as well. The adaptive thresholding estimators are easy to implement. The numerical performance of the estimators is studied using both simulated and real data. Simulation results demonstrate that the adaptive thresholding estimators uniformly outperform the universal thresholding estimators. The method is also illustrated in an analysis on a dataset from a small round blue-cell tumor microarray experiment. A supplement to this article presenting additional technical proofs is available online. "], ["A Semiparametric Threshold Model for Censored Longitudinal Data Analysis", " Motivated by an investigation of the relationship between blood pressure change and progression of microalbuminuria (MA) among individuals with type I diabetes, we propose a new semiparametric threshold model for censored longitudinal data analysis. We also study a new semiparametric Bayes information criterion-type criterion for identifying the parametric component of the proposed model. Cluster effects in the model are implemented as unknown fixed effects. Asymptotic properties are established for the proposed estimators. A quadratic approximation used to implement the estimation procedure makes the method very easy to implement by avoiding the computation of multiple integrals and the need for iterative algorithms. Simulation studies show that the proposed methods work well in practice. An illustration using the Wisconsin Diabetes Registry dataset suggests some interesting findings. "], ["The Degrees of Freedom of Partial Least Squares Regression", " The derivation of statistical properties for partial least squares regression can be a challenging task. The reason is that the construction of latent components from the predictor variables also depends on the response variable. While this typically leads to good performance and interpretable models in practice, it makes the statistical analysis more involved. In this work, we study the intrinsic complexity of partial least squares regression. Our contribution is an unbiased estimate of its degrees of freedom. It is defined as the trace of the first derivative of the fitted values, seen as a function of the response. We establish two equivalent representations that rely on the close connection of partial least squares to matrix decompositions and Krylov subspace techniques. We show that the degrees of freedom depend on the collinearity of the predictor variables: The lower the collinearity, the higher the complexity. In particular, they are typically higher than the naive approach that defines the degrees of freedom as the number of components. Further, we illustrate how our degrees of freedom estimate can be used for the comparison of different regression methods. In the experimental section, we show that our degrees of freedom estimate in combination with information criteria is useful for model selection. "], ["Robust and Efficient One-Way MANOVA Tests", " We propose robust tests as alternatives to the classical Wilks\u2019 Lambda test in one-way MANOVA. The robust tests use highly robust and efficient multisample multivariate S-estimators or MM-estimators instead of the empirical covariances. The properties of several robust test statistics are compared. Under the null hypothesis, the distribution of the test statistics is proportional to a chi-square distribution. As an alternative to the asymptotic distribution, we develop a fast robust bootstrap method to estimate the distribution under the null hypothesis. We show when it is asymptotically correct to estimate the null distribution in this way and we use simulations to verify the performance of the bootstrap based tests in finite samples. We also investigate the power of the new tests, as well as their robustness against outliers. Finally, we illustrate the use of these robust test statistics on a real data example. Some additional results are provided as supplemental material. "], ["An Outlier-Robust Fit for Generalized Additive Models With Applications to Disease Outbreak Detection", " We are interested in a class of unsupervised methods to detect possible disease outbreaks, that is, rapid increases in the number of cases of a particular disease that deviate from the pattern observed in the past. The motivating application for this article deals with detecting outbreaks using generalized additive models (GAMs) to model weekly counts of certain infectious diseases. We can use the distance between the predicted and observed counts for a specific week to determine whether an important departure has occurred. Unfortunately, this approach may not work as desired because GAMs can be very sensitive to the presence of a small proportion of observations that deviate from the assumed model. Thus, the outbreak may affect the predicted values causing these to be close to the atypical counts, and thus mask the outliers by having them appear not to be too extreme or atypical. We illustrate this phenomenon with influenza-like-illness doctor-visits data from the United States for the 2006\u20132008 flu seasons. One way to avoid this masking problem is to derive an algorithm to fit GAM models that can resist the effect of a small number of atypical observations. In this article we discuss such an outlier-robust fit for GAMs based on the backfitting algorithm. The basic idea is to replace the maximum likelihood based weights used in the generalized local scoring algorithm with those derived from robust quasi-likelihood equations (Cantoni and Ronchetti 2001b). These robust estimators for generalized linear models work well for the Poisson family of distributions, and also for binomial distributions with relatively large numbers of trials. We show that the resulting estimated mean function is resistant to the presence of outliers in the response variable and that it also remains close to the usual GAM estimator when the data do not contain atypical observations. We illustrate the use of this approach on the detection of the recent outbreak of H1N1 flu by looking at the weekly counts of influenza-like-illness (ILI) doctor visits, as reported through the U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet), and also apply our method to the numbers of requested isolates in Canada. Weeks with a sudden increase in ILI visits or requested isolates are much more clearly identified as atypical by the robust fit because the observed counts are far from the ones predicted by the fitted GAM model. "], ["Best Predictive Small Area Estimation", " We derive the best predictive estimator (BPE) of the fixed parameters under two well-known small area models, the Fay\u2013Herriot model and the nested-error regression model. This leads to a new prediction procedure, called observed best prediction (OBP), which is different from the empirical best linear unbiased prediction (EBLUP). We show that BPE is more reasonable than the traditional estimators derived from estimation considerations, such as maximum likelihood (ML) and restricted maximum likelihood (REML), if the main interest is estimation of small area means, which is a mixed-model prediction problem. We use both theoretical derivations and empirical studies to demonstrate that the OBP can significantly outperform EBLUP in terms of the mean squared prediction error (MSPE), if the underlying model is misspecified. On the other hand, when the underlying model is correctly specified, the overall predictive performance of the OBP is very similar to that of the EBLUP if the number of small areas is large. A general theory about OBP, including its exact MSPE comparison with the BLUP in the context of mixed-model prediction, and asymptotic behavior of the BPE, is developed. A real data example is considered. A supplementary appendix is available online. "], ["Making and Evaluating Point Forecasts", " Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance. "], ["Book Reviews", null], ["Correction", null], ["Statistics: A Key to Innovation in a Data-Centric World!", null], ["Bayesian Spatial Quantile Regression", " Tropospheric ozone is one of the six criteria pollutants regulated by the United States Environmental Protection Agency under the Clean Air Act and has been linked with several adverse health effects, including mortality. Due to the strong dependence on weather conditions, ozone may be sensitive to climate change and there is great interest in studying the potential effect of climate change on ozone, and how this change may affect public health. In this paper we develop a Bayesian spatial model to predict ozone under different meteorological conditions, and use this model to study spatial and temporal trends and to forecast ozone concentrations under different climate scenarios. We develop a spatial quantile regression model that does not assume normality and allows the covariates to affect the entire conditional distribution, rather than just the mean. The conditional distribution is allowed to vary from site-to-site and is smoothed with a spatial prior. For extremely large datasets our model is computationally infeasible, and we develop an approximate method. We apply the approximate version of our model to summer ozone from 1997\u20132005 in the Eastern U.S., and use deterministic climate models to project ozone under future climate conditions. Our analysis suggests that holding all other factors fixed, an increase in daily average temperature will lead to the largest increase in ozone in the Industrial Midwest and Northeast. "], ["Statistical Estimation of Word Acquisition With Application to Readability Prediction", " Models of language learning play a central role in a wide range of applications: from psycholinguistic theories of how people acquire new word knowledge, to information systems that can automatically match content to users\u2019 reading ability. Traditional methods for estimating word acquisition ages or content readability are typically based on linear regression over a small number of summary features derived from time-consuming user studies or costly expert judgments. With the increasing amounts of content available from the web and other sources, however, new statistical approaches are possible that can exploit this easily acquired data to learn more flexible, fine-grained models of language usage. We present a novel statistical model for document readability that is based on the logistic Rasch model and the quantiles of word acquisition age distributions. We use this model to estimate the distributions of word acquisition ages from empirical readability data collected from the web. We then demonstrate that the estimated acquisition distributions are very effective in predicting both global and local document readability. We also compare the estimated distributions with word acquisition data from existing oral studies, revealing interesting historical trends as well as differences between oral and written word acquisition grade levels. "], ["A Hierarchical Model for Quantifying Forest Variables Over Large Heterogeneous Landscapes With Uncertain Forest Areas", null], ["Online Model-Based Clustering for Crisis Identification in Distributed Computing", " In the periods between crises we perform full Bayesian inference for the past crises, and as a new crisis occurs we apply fast approximate Bayesian updating. These inferences allow real-time expected-cost-minimizing decision making that fully accounts for uncertainty in the crisis labels and other parameters. We apply and validate our methods using simulated data and data from a production computing center with hundreds of servers running a 24/7 email-related application. "], ["Modeling Three-Dimensional Chromosome Structures Using Gene Expression Data", " Recent genomic studies have shown that significant chromosomal spatial correlation exists in gene expression of many organisms. Interestingly, coexpression has been observed among genes separated by a fixed interval in specific regions of a chromosome chain, which is likely caused by three-dimensional (3D) chromosome folding structures. Modeling such spatial correlation explicitly may lead to essential understandings of 3D chromosome structures and their roles in transcriptional regulation. In this paper, we explore chromosomal spatial correlation induced by 3D chromosome structures, and propose a hierarchical Bayesian method based on helical structures to formally model and incorporate the correlation into the analysis of gene expression microarray data. It is the first study to quantify and infer 3D chromosome structures in vivo using expression microarrays. Simulation studies show computing feasibility of the proposed method and that, under the assumption of helical chromosome structures, it can lead to precise estimation of structural parameters and gene expression levels. Real data applications demonstrate an intriguing biological phenomenon that functionally associated genes, which are far apart along the chromosome chain, are brought into physical proximity by chromosomal folding in 3D space to facilitate their coexpression. It leads to important biological insight into relationship between chromosome structure and function. "], ["Multiple Testing for Pattern Identification, With Applications to Microarray Time-Course Experiments", null], ["Changepoints in the North Atlantic Tropical Cyclone Record", null], ["Self-Exciting Point Process Modeling of Crime", " Highly clustered event sequences are observed in certain types of crime data, such as burglary and gang violence, due to crime-specific patterns of criminal behavior. Similar clustering patterns are observed by seismologists, as earthquakes are well known to increase the risk of subsequent earthquakes, or aftershocks, near the location of an initial event. Space\u2013time clustering is modeled in seismology by self-exciting point processes and the focus of this article is to show that these methods are well suited for criminological applications. We first review self-exciting point processes in the context of seismology. Next, using residential burglary data provided by the Los Angeles Police Department, we illustrate the implementation of self-exciting point process models in the context of urban crime. For this purpose we use a fully nonparametric estimation methodology to gain insight into the form of the space\u2013time triggering function and temporal trends in the background rate of burglary. "], ["Dynamic Trees for Learning and Design", " Dynamic regression trees are an attractive option for automatic regression and classification with complicated response surfaces in online application settings. We create a sequential tree model whose state changes in time with the accumulation of new data, and provide particle learning algorithms that allow for the efficient online posterior filtering of tree states. A major advantage of tree regression is that it allows for the use of very simple models within each partition. The model also facilitates a natural division of labor in our sequential particle-based inference: tree dynamics are defined through a few potential changes that are local to each newly arrived observation, while global uncertainty is captured by the ensemble of particles. We consider both constant and linear mean functions at the tree leaves, along with multinomial leaves for classification problems, and propose default prior specifications that allow for prediction to be integrated over all model parameters conditional on a given tree. Inference is illustrated in some standard nonparametric regression examples, as well as in the setting of sequential experiment design, including both active learning and optimization applications, and in online classification. We detail implementation guidelines and problem specific methodology for each of these motivating applications. Throughout, it is demonstrated that our practical approach is able to provide better results compared to commonly used methods at a fraction of the cost. "], ["Meta Analysis of Functional Neuroimaging Data via Bayesian Spatial Point Processes", " As the discipline of functional neuroimaging grows there is an increasing interest in meta analysis of brain imaging studies. A typical neuroimaging meta analysis collects peak activation coordinates (foci) from several studies and identifies areas of consistent activation. Most imaging meta analysis methods only produce null hypothesis inferences and do not provide an interpretable fitted model. To overcome these limitations, we propose a Bayesian spatial hierarchical model using a marked independent cluster process. We model the foci as offspring of a latent study center process, and the study centers are in turn offspring of a latent population center process. The posterior intensity function of the population center process provides inference on the location of population centers, as well as the interstudy variability of foci about the population centers. We illustrate our model with a meta analysis consisting of 437 studies from 164 publications, show how two subpopulations of studies can be compared and assess our model via sensitivity analyses and simulation studies. Supplemental materials are available online. "], ["Improved Inference for Respondent-Driven Sampling Data With Application to HIV Prevalence Estimation", " We present a successive-sampling based estimator for population means based on respondent-driven sampling data, and demonstrate its superior performance when the size of the hidden population is known. We present sensitivity analyses for unknown population sizes. In addition, we note that like other existing estimators, our new estimator is subject to bias induced by the selection of the initial sample. Using data collected among three populations in two countries, we illustrate the application of this approach to populations with varying characteristics. We conclude that the successive sampling estimator improves on existing estimators, and can also be used as a diagnostic tool when population size is not known. This article has supplementary material online. "], ["Saddlepoint Test in Measurement Error Models", " We develop second-order hypothesis testing procedures in functional measurement error models for small or moderate sample sizes, where the classical first-order asymptotic analysis often fails to provide accurate results. In functional models no distributional assumptions are made on the unobservable covariates and this leads to semiparametric models. Our testing procedure is derived using saddlepoint techniques and is based on an empirical distribution estimation subject to the null hypothesis constraints, in combination with a set of estimating equations which avoid a distribution approximation. The validity of the method is proved in theorems for both simple and composite hypothesis tests, and is demonstrated through simulation and a farm size data analysis. "], ["A Semiparametric Estimation of Mean Functionals With Nonignorable Missing Data", " In this paper, based on the exponential tilting model, we propose a semiparametric estimation method of mean functionals with nonignorable missing data. A semiparametric logistic regression model is assumed for the response probability and a nonparametric regression approach for missing data discussed in Cheng (1994) is used in the estimator. By adopting nonparametric components for the model, the estimation method can be made robust. Variance estimation is also discussed and results from a simulation study are presented. The proposed method is applied to real income data from the Korean Labor and Income Panel Survey. "], ["Hard or Soft Classification? Large-Margin Unified Machines", null], ["Inverse Regression Estimation for Censored Data", " An inverse regression methodology for assessing predictor performance in the censored data setup is developed along with inference procedures and a computational algorithm. The technique developed here allows for conditioning on the unobserved failure time along with a weighting mechanism that accounts for the censoring. The implementation is nonparametric and computationally fast. This provides an efficient methodological tool that can be used especially in cases where the usual modeling assumptions are not applicable to the data under consideration. It can also be a good diagnostic tool that can be used in the model selection process. We have provided theoretical justification of consistency and asymptotic normality of the methodology. Simulation studies and two data analyses are provided to illustrate the practical utility of the procedure. "], ["Testing and Estimating Shape-Constrained Nonparametric Density and Regression in the Presence of Measurement Error", null], ["Fast Robust Model Selection in Large Datasets", null], ["Adaptive Probability-Based Latin Hypercube Designs", " Adaptive sampling is an effective method developed mainly for regular regions. However, experimental regions in irregular shapes are commonly observed in practice. Motivated by a data center thermal management study, a new class of adaptive designs is proposed to accommodate a specific type of irregular region. Because the adaptive procedure introduces biases into conventional estimators, several design-unbiased estimators are given for estimating the population mean. Efficient and easy-to-compute unbiased estimators are also introduced. The proposed method is applied to obtain an adaptive sensor placement plan to monitor and study the thermal distribution in a data center. All of the supplemental materials used in this work are available online. "], ["Testing for Threshold Effects in Regression Models", " In this article, we develop a general method for testing threshold effects in regression models, using sup-likelihood-ratio (LR)-type statistics. Although the sup-LR-type test statistic has been considered in the literature, our method for establishing the asymptotic null distribution is new and nonstandard. The standard approach in the literature for obtaining the asymptotic null distribution requires that there exist a certain quadratic approximation to the objective function. The article provides an alternative, novel method that can be used to establish the asymptotic null distribution, even when the usual quadratic approximation is intractable. We illustrate the usefulness of our approach in the examples of the maximum score estimation, maximum likelihood estimation, quantile regression, and maximum rank correlation estimation. We establish consistency and local power properties of the test. We provide some simulation results and also an empirical application to tipping in racial segregation. This article has supplementary materials online. "], ["VIF Regression: A Fast Regression Algorithm for Large Data", null], ["On Propagated Scoring for Semisupervised Additive Models", null], ["Tests for High-Dimensional Regression Coefficients With Factorial Designs", null], ["Stringing High-Dimensional Data for Functional Analysis", null], ["Some Approximate Evidence Factors in Observational Studies", null], ["EEBoost: A General Method for Prediction and Variable Selection Based on Estimating Equations", null], ["Nonparametric Regression With Predictors Missing at Random", " Nonparametric regression with predictors missing at random (MAR), where the probability of missing depends only on observed variables, is considered. Univariate predictor is the primary case of interest. A new adaptive orthogonal series estimator is developed. Large sample theory shows that the estimator is rate-minimax and it is also sharp-minimax whenever predictors are missing completely at random (MCAR). Furthermore, confidence bands, estimation of nuisance functions, including conditional probability of observing the predictor, design density and scale, and multiple regression are also considered. Numerical study and a real example show feasibility of the proposed methodology for small samples. Supplementary materials, containing results of the numerical study, are available online. "], ["Confidence Distributions and a Unifying Framework for Meta-Analysis", null], [null, null], ["Multivariate Matching Methods That Are Monotonic Imbalance Bounding", " We introduce a new \u201cMonotonic Imbalance Bounding\u201d (MIB) class of matching methods for causal inference with a surprisingly large number of attractive statistical properties. MIB generalizes and extends in several new directions the only existing class, \u201cEqual Percent Bias Reducing\u201d (EPBR), which is designed to satisfy weaker properties and only in expectation. We also offer strategies to obtain specific members of the MIB class, and analyze in more detail a member of this class, called Coarsened Exact Matching, whose properties we analyze from this new perspective. We offer a variety of analytical results and numerical simulations that demonstrate how members of the MIB class can dramatically improve inferences relative to EPBR-based matching methods. "], ["Model Selection by Testing for the Presence of Small-Area Effects, and Application to Area-Level Data", " The models used in small-area inference often involve unobservable random effects. While this can significantly improve the adaptivity and flexibility of a model, it also increases the variability of both point and interval estimators. If we could test for the existence of the random effects, and if the test were to show that they were unlikely to be present, then we would arguably not need to incorporate them into the model, and thus could significantly improve the precision of the methodology. In this article we suggest an approach of this type. We develop simple bootstrap methods for testing for the presence of random effects, applicable well beyond the conventional context of the natural exponential family. If the null hypothesis that the effects are not present is not rejected then our general methodology immediately gives us access to estimators of unknown model parameters and estimators of small-area means. Such estimators can be substantially more effective, for example, because they enjoy much faster convergence rates than their counterparts when the model includes random effects. If the null hypothesis is rejected then the next step is either to make the model more elaborate (our methodology is available quite generally) or to turn to existing random effects models. This article has supplementary material online. "], ["Book Reviews", null], ["Letter to the Editor", null], ["Correction", null], ["Correction", null]]}