{"1984": [["Report by the Editors", null], ["A Century of Methodological Progress at the U.S. Bureau of Labor Statistics", "This article describes progress at the Bureau of Labor Statistics during its first 100 years by tracing developments in selected methodological areas as shown in three major programs. The importance of clear conceptualization is shown by examples from the Producer Price Index and other programs. The Consumer Price Index program has demonstrated initiatives in sampling and estimation methods. The problems and opportunities of a federal-state cooperative environment are discussed in the context of the survey of employment, hours, and earnings."], ["Latent Structure Analysis of a Set of Multidimensional Contingency Tables", null], ["Inference from Nonrandomly Missing Categorical Data: An Example from a Genetic Study on Turner's Syndrome", "The process leading to partial classification with categorical data is sometimes nonrandom. A particular model accounting for incomplete data, which allows the probability of uncertain classification to depend on category identity, is utilized for an analysis of data obtained from a genetic study on Turner's syndrome. Estimates of population proportions are obtained from maximum likelihood. A method for handling nonrandomly missing data arrayed in contingency tables is discussed. Sensitivity analyses incorporating parameters related to the missing-data mechanism are recommended for estimation and testing."], ["An Investigation of Some Estimators of Variance for Systematic Sampling", "This article provides to the survey practitioner some guidance about the problem of variance estimation for equal probability single-start systematic samples. An investigation of eight alternative estimators of the variance of the sample mean is presented. In the first half of the investigation, the theoretical properties of the estimators are compared using several superpopulation models; in the second half, the comparison is empirical, based on several real populations. Recommendations are made about the appropriateness of the various estimators."], ["Comparing Non-Nested Linear Models", null], ["Sample Analogs to Multidimensional Coverages with Applications to Prediction", null], ["The Many Faces of a Scatterplot", null], ["On the Behavior of Some Estimators from Probability Plots", "Probability plots are popular graphical methods used to assess distributional assumptions. Under a location-scale model, the plot tends to lie on a straight line. A common practice in this situation is to fit a line through the plot and use the intercept and slope of the fitted line as estimates of the location and scale parameters. What are the properties of these estimators? Estimators from weighted least squares lines are considered, and their asymptotic, finite-sample, robustness, and optimality properties are discussed. Included among these are the ordinary least squares estimators and estimators from least squares lines fitted after trimming or Winsorizing some of the extreme order statistics."], ["Cross-Validated Spline Methods for the Estimation of Three-Dimensional Tumor Size Distributions from Observations on Two-Dimensional Cross Sections", "We study the problem of estimating the distribution of the three-dimensional radiuses of a collection of spheres, given measurements of the two-dimensional radiuses of a sample of planar cross sections. This problem arises in the estimation of the tumor size distribution of spherical microtumors induced in mouse livers following injection of a carcinogen. We first convert this problem to a form suitable for the application of cross-validated spline methods for the solution of ill-posed integral equations given noisy data. Then we develop special numerical techniques that will allow the spline methods to be accurately applied to integral equations like those associated with the present problem. We apply the resulting method to some mouse-liver data. The subject mouse liver has been completely dissected, allowing a rare comparison of the estimate with the \u201ctruth.\u201d The statistical properties of the estimate are explored via Monte Carlo methods. The interplay between statistical and numerical analytic methods for problems like this are explored and the use of eigensequence plots for studying \u201cill posedness\u201d is described."], ["Approximate Inference in Location\u2014Scale Regression Models", null], ["Approximations for Standard Errors of Estimators of Fixed and Random Effects in Mixed Linear Models", "Best linear unbiased estimators of the fixed and random effects of mixed linear models are available when the true values of the variance ratios are known. If the true values are replaced by estimated values, the mean squared errors of the estimators of the fixed and random effects increase in size. The magnitude of this increase is investigated, and a general approximation is proposed. The performance of this approximation is investigated in the context of (a) the estimation of the effects of the balanced one-way random model and (b) the estimation of treatment contrasts for balanced incomplete block designs."], ["On Simultaneous Pairwise Comparisons in Analysis of Covariance", "Procedures for multiple comparisons among treatment means, in analysis of covariance with a random concomitant, are considered. A Tukey-Kramer (TK)-type procedure is introduced for a conditional analysis and is compared with Thigpen and Paulson's (1974) unconditional procedure. It is first established (by simulation) that the TK procedure controls the unconditional familywise error rate at the nominal level set for the conditional procedure. In the second part of this work, the confidence interval lengths obtained by the two procedures are compared. It is found that the expected ratio of the confidence interval length obtained by the conditional method to that obtained by the unconditional method is generally less than 1. Moreover, the probability that the length of a TK interval will be larger than that of the Thigpen and Paulson procedure is (roughly) between .2 and .3."], ["Global Sensitivity Results for Generalized Least Squares Estimates", null], ["Least Median of Squares Regression", null], ["A Likelihood Ratio Test regarding Two Nested but Oblique Order-Restricted Hypotheses", "Cadoret, Woolson, and Winokur (1977) considered two theories regarding the genetic makeup of patients suffering from unipolar affective disorder. These two theories imply nested but oblique order restrictions on the parameters of a statistical model. A likelihood ratio test for these two restrictions is studied and found to be dominated by a test that neglects some of the information in the model."], ["Selecting the Best Population, Provided it is Better than a Standard: The Unequal Variance Case", null], [null, null], ["Linear Discriminant Analysis with Misallocation in Training Samples", "Linear discriminant analysis for a two-class case is studied in the presence of misallocation in training samples. A general approach to modeling of misallocation is formulated, and the mean vectors and covariance matrices of the mixture distributions are derived. The asymptotic distribution of the discriminant boundary is obtained, and the asymptotic first two moments of the error rates are given. Certain numerical results for the error rates are presented by considering the random and two nonrandom misallocation models."], ["Measures of Conditional Linear Dependence and Feedback between Time Series", "Measures of linear dependence and feedback for two multiple time series conditional on a third are defined. The measure of conditional linear dependence is the sum of linear feedback from the first to the second conditional on the third, linear feedback from the second to the first conditional on the third, and instantaneous linear feedback between the first and second series conditional on the third. The measures are non-negative and may be expressed in terms of measures of unconditional feedback between various combinations of the three series. The measures of conditional linear feedback can be additively decomposed by frequency. Estimates of these measures are straightforward to compute, and their distribution can be routinely approximated by bootstrap methods. An empirical example involving real output, money, and interest rates is presented."], ["The Order of Differencing in ARIMA Models", "A Lagrange multiplier test is derived for testing for the order of differencing in an autoregressive integrated moving average (ARIMA) model. The procedure is illustrated with an example."], ["Hypothesis Tests for Markov Process Models Estimated from Aggregate Frequency Data", null], ["Small-Sample Comparisons for the Power Divergence Goodness-of-Fit Statistics", null], ["Classification of Probability Laws by Tail Behavior", "In this article I refine Parzen's density-quantile tail exponent classification of probability laws by tail behavior by subdividing his medium-tailed class into medium-short, medium-medium, and medium-long. I give a physical interpretation of short-, medium-, and long-tailed distributions in terms of the limiting size of the extreme spacings in a random sample from the distribution. I show that this classification, by the limiting size of extreme spacings, fits nicely within the framework of my refinement of Parzen's density-quantile classification."], ["Book Reviews", null], ["Corrigenda", null], ["Editorial Board Page", "This article has no abstract"], ["Probability Forecasting in Meteorology", "Efforts to quantify the uncertainty in weather forecasts began more than 75 years ago, and many studies and experiments involving objective and subjective probability forecasting have been conducted in meteorology in the intervening period. Moreover, the U.S. National Weather Service (NWS) initiated a nationwide program in 1965 in which precipitation probability forecasts were formulated on an operational basis and routinely disseminated to the general public. In addition, the NWS now prepares objective probability forecasts for many variables, using statistical procedures. Hence probability forecasting in meteorology is unique in that very large sets of probability forecasts that have been subjected to detailed evaluation are available. This article has four objectives: (a) to review the history of probability forecasting in meteorology to acquaint statisticians with this body of literature; (b) to describe recent methodological, experimental, and operational activities in this field; (c) to examine current issues in probability forecasting; and (d) to discuss briefly the relationship between probability forecasting in meteorology and probability forecasting in other fields. Results of operational and experimental weather forecasting programs are presented, methods of verifying and evaluating probability forecasts in common use in meteorology are discussed, and an extensive list of references is provided."], ["Improvement by Planned Multistage Selection", "An outline of the problem of multistage selection is given, followed by examples of applications during the past 40 years. Optimal policies for successive selections among strains of crop plants are discussed. Animal breeding has analogous problems. The screening of chemical compounds for therapeutic uses has different features, notably in the formulation of a cost structure. The problems of selection from human populations are mentioned\u2014those not selected now cannot be discarded. In this article I concentrate on the relation of the statistical formulation and analysis to the applications. Much has been done, and much remains to be done, in mathematical theory and computation."], ["Monitoring the 1982 Spanish Socialist Victory: A Bayesian Analysis", "An estimate of how people intended to vote was obtained four weeks before the Spanish general elections of October 1982. The evolution of opinion was followed during the campaign. Using a small sample of polling stations, the victory of the Spanish Socialist Party was predicted with great accuracy only two hours after the polls closed. A Bayesian hierarchical model was used."], ["Reducing Bias in Observational Studies Using Subclassification on the Propensity Score", "The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Previous theoretical arguments have shown that subclassification on the propensity score will balance all observed covariates. Subclassification on an estimated propensity score is illustrated, using observational data on treatments for coronary artery disease. Five subclasses defined by the estimated propensity score are constructed that balance 74 covariates, and thereby provide estimates of treatment effects using direct adjustment. These subclasses are applied within sub-populations, and model-based adjustments are then used to provide estimates of treatment effects within these sub-populations. Two appendixes address theoretical issues related to the application: the effectiveness of subclassification on the propensity score in removing bias, and balancing properties of propensity scores with incomplete data."], ["A Demographic Analogy for Shareowner Accounts", "A demographic analogy suggests categories for statistical analysis of the shareowner accounts of a large corporation, using data from routine record-keeping systems. The study reveals unexpected stability of account closing rates over time, extremely high proportions of accounts closing soon after opening, and other features unnoticed in previous studies of stock ownership, most of which have focused on the market or the investor."], ["Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods", null], ["Telephone Interview and Mail Questionnaire Applications of the Randomized Response Model", "Randomization devices and methodologies for self-administered mail questionnaire and telephone interview applications of the randomized response method are described. The results of field tests of both techniques indicate that if the serious problems of nonresponse and failure to follow instructions can be overcome, the techniques may serve as less expensive alternatives to personal interview randomized response studies. A multivariate version of Warner's linear model is also described."], ["Conditional Permutation Tests and the Propensity Score in Observational Studies", "In observational studies, the distribution of treatment assignments is unknown, and therefore randomization tests are not generally applicable. However, permutation tests that condition on sample information about the treatment assignment mechanism can be applicable in observational studies, provided treatment assignment is strongly ignorable. These tests use the conditional distribution of the treatment assignments given a sufficient statistic for the unknown parameter of the propensity score. Several tests that are commonly used in observational studies are particular instances of this general procedure. Moreover, conditional permutation tests and covariance adjustment are closely related, in the sense that the conditional permutation distribution of the covariance adjusted difference leads to the same inferences as the conditional permutation distribution of the unadjusted difference of sample means. A backtrack algorithm is developed to permit efficient calculation of the exact conditional significance level, and two approximations are discussed. A clinical study of treatments for lung cancer is used to illustrate the technique. Conditional permutation tests extend previous large sample results on the propensity score by providing a basis for exact tests and confidence intervals in small observational studies when treatment assignment is strongly ignorable."], ["Cross-Validation of Regression Models", "A methodolgy for assessment of the predictive ability of regression models is presented. Attention is given to models obtained via subset selection procedures, which are extremely difficult to evaluate by standard techniques. Cross-validatory assessments of predictive ability are obtained and their use illustrated in examples."], ["A Fast and Efficient Cross-Validation Method for Smoothing Parameter Choice in Spline Regression", "The spline smoothing approach to nonparametric regression is considered, with particular reference to the problem of choosing how much to smooth. Both computational and statistical aspects of the method of cross-validation are discussed. An approximate cross-validation method is proposed that requires a small linear amount of computer time. A simulation study indicates that the method has very good statistical properties. Some mathematical justification and motivation are also provided."], ["A Comparison of Minimum Distance and Maximum Likelihood Estimation of a Mixture Proportion", "The estimation of mixing proportions in the mixture model is discussed, with emphasis on the mixture of two normal components with all five parameters unknown. Simulations are presented that compare minimum distance (MD) and maximum likelihood (ML) estimation of the parameters of this mixture-of-normals model. Some practical issues of implementation of these results are also discussed. Simulation results indicate that ML techniques are superior to MD when component distributions actually are normal, but MD techniques provide better estimates than ML under symmetric departures from component normality. Interestingly, an ad hoc starting value for the iterative procedures occasionally outperformed both the ML and MD techniques. Results are presented that establish strong consistency and asymptotic normality of the MD estimator under conditions that include the mixture-of-normals model. Asymptotic variances and relative efficiencies are obtained for further comparison of the MD and ML estimators."], ["Projection Pursuit Density Estimation", "The projection pursuit methodology is applied to the multivariate density estimation problem. The resulting nonparametric procedure is often less biased than the kernel and near-neighbor methods. In addition, graphical information is produced that can be used to help gain geometric insight into the multivariate data distribution."], ["Smoothness Priors and Nonlinear Regression", null], ["A Refined Method of Robust Smoothing", null], ["Design-Consistent versus Model-Dependent Estimation for Small Domains", "For small domains, we have a choice between design-consistent estimators that make the strongest possible use of auxiliary information and model-dependent design-biased alternatives such as the synthetic estimator. In this article, a design-consistent method is developed that borrows strength in the same way that the design-biased synthetic estimator does. The latter has a design-based mean squared error (MSE) advantage in small samples under the supposition that domains resemble each other. Under deviations from that model, however, and with moderate to large samples, the design-consistent method gives a smaller MSE. It also has the advantage that design-based variance estimators and design-based confidence intervals can be easily obtained. This article emphasizes that the choice of estimation method depends on a complex interaction of factors that include sample size, sampling fraction, domain size, and departures from the model."], ["Maximum Likelihood Estimation in the Mover-Stayer Model", "The discrete time mover-stayer model, a special mixture of two independent Markov chains, has been widely used in modeling the dynamics of social processes. The problem of maximum likelihood estimation of its parameters from the data, however, which consist of a sample of independent realizations of this process, has not been considered in the literature. I present a maximum likelihood procedure for the estimation of the parameters of the mover-stayer model and develop a recursive method of computation of maximum likelihood estimators that is very simple to implement. I also verify that obtained maximum likelihood estimators are strongly consistent. I show that the two estimators of the parameters of the mover-stayer model previously proposed in the literature are special cases of the maximum likelihood estimator derived in this article, that is, they coincide with the maximum likelihood estimator under special conditions. I thus explain the interconnection between existing estimators. I also present a numerical comparison of the three estimators. Finally, I illustrate the application of the maximum likelihood estimators to testing the hypothesis that the Markov chain describes the data against the hypothesis that the mover-stayer model describes the data."], ["Tests for Differences in Tumor Incidence Based on Animal Carcinogenesis Experiments", "In animal carcinogenesis studies, statistical methods that compare the production of occult tumors should compare tumor incidence rates. In general, the incidence rate cannot be identified even approximately without both survival and sacrifice information. If information from frequent sacrifices is available, however, a function that approximates the incidence rate can be identified without assumptions about the relationship between tumor incidence and death. Some earlier proposals that based comparisons on tumor prevalence rates and other functions also compared tumor incidence rates under special conditions, but these tests can be biased when the conditions are violated. Tests that compare functions approximating tumor incidence rates can be developed using survival/sacrifice information, but these tests may be seriously inefficient in special cases in which sacrifices are not necessary to identify the tumor incidence rate."], ["Testing Goodness of Fit for Proportional Hazards Model with Censored Observations", "A numerical omnibus test of fit for the two-sample proportional hazards model is proposed for randomly censored observations. The test is derived from Cox's partial likelihood and does not need a dummy time-dependent covariate or any partition of the time-axis. Consistency of the test is established. Examples are provided for illustration."], ["Two-Sample Asymptotically Distribution-Free Tests for Incomplete Multivariate Observations", "Asymptotically distribution-free tests for equality of two multivariate distributions based on censored observations are proposed. By choosing appropriate test scores, we obtain the multivariate versions of the Gehan and log-rank tests. An important application of the proposed tests is in the area in which multiple observations are collected on study subjects, some of whom may have missing observations. The patterns of missing observations are allowed to differ for the two groups to be compared. These tests are particularly useful for longitudinal studies\u2014for example, the analysis of repeated measurements and growth curves. Real examples are also provided for illustration."], ["Required Sample Size for Categorical Matching", "This article investigates the sampling effort required to obtain matches for all members of a given sample. The exact distribution of the sample size required to complete the match quotas in all categories of the matching variable(s) is derived; the first two moments are also derived recursively, using a Markov chain approach. For problems with equal quotas in equiprobable categories, these moments may be computed as functions of gamma and normal order statistic moments. Numerical results are given to characterize the feasibility of matching in relation to the study size, the number of matching categories, and the distributions of the category probabilities and quotas."], [null, null], ["Comparison of Asymptotically Distribution-Free Procedures for the Analysis of Complete Blocks", null], ["A Likelihood Ratio Test for Stochastic Ordering", "This article presents a method of extending the likelihood ratio principle to a nonparametric setting. The problem dealt with is a test for stochastic ordering. The resulting test is shown, via simulation, to have good power. It is shown to be consistent and unbiased."], ["The G-Spectral Estimator", "In this article a modified definition of the G-spectral estimator is given. It is shown that the resulting estimator is a method of moments autoregressive moving average (ARMA) spectral estimator that does not require an estimate of the moving average parameters. As a result, a new formula for the power spectrum of an ARMA process is given that does not explicitly involve the moving average (MA) parameters. This formula then leads to a closed-form expression for the MA parameters and their corresponding moment estimators."], [null, null], ["An Algorithm for Exact Logistic Regression", "An efficient algorithm is given for the exact logistic analysis of a single parameter. Hypothesis tests and confidence intervals for a logistic regression parameter are obtained. The method provides for the control of the effect of a stratification factor when making inferences concerning the regression parameter."], ["Range Preserving Unbiased Estimators in the Multinomial Case", null], ["One-Sided Simultaneous Bounds in Linear Regression", "This article discusses the one-sided simultaneous tolerance and confidence bounds in the linear regression model. We approach the problem from the area of test theory. The bounds are constructed by projecting a region of the test statistics, and we then propose a necessary and sufficient condition for obtaining a taut tolerance bound by the method. Finally, for illustrative purposes, we give several examples of one-sided simultaneous bounds with various shapes."], ["A Note on Ordinary Least Squares Methods for Two-Stage Sampling", "A class of linear models for two-stage sampling problems is considered. Ordinary least squares estimates are optimal for these models. For some parameters the usual least squares estimates of standard errors are shown to be appropriate."], ["On the Inverse of a Patterned Covariance Matrix", "The inversion of a patterned covariance matrix is discussed. A general form of the correlation matrix is noted, and inversion by the method of tearing is applied. For certain correlation structures, the calculations are considerably simplified."], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Goals: Where are We and Where Should We be Going?", null], ["Abraham Wald's Work on Aircraft Survivability", "While he was a member of the Statistical Research Group (SRG), Abraham Wald worked on the problem of estimating the vulnerability of aircraft, using data obtained from survivors. This work was published as a series of SRG memoranda and was used in World War II and in the wars in Korea and Vietnam. The memoranda were recently reissued by the Center for Naval Analyses. This article is a condensation and exposition of Wald's work, in which his ideas and methods are described. In the final section, his main results are reexamined in the light of classical statistical theory and more recent work."], ["Comment", null], ["Rejoinder", null], ["New Estimates of Child Mortality in the United States at the Turn of the Century", "This article estimates levels of childhood mortality on the basis of new data derived from a nationally representative sample of manuscripts of the 1900 U.S. census. The data are responses to census questions on numbers of children ever born and numbers surviving. The results for a subsample corresponding to the small death registration area (DRA) in 1900/02 validate the procedures used. Among the principal findings are that the 1900/02 DRA life tables very seriously overestimate national child mortality among blacks, that there is a small overestimate for whites, but that the combined figures are accurate because of an underrepresentation of blacks in the DRA. Evidence also indicates that child mortality was declining at a moderate pace in the late 19th century, but that little decline was occurring among blacks. The results suggest the need for revising accounts of American black demographic history, including birth rates. They also imply that 20th-century progress in narrowing black-white mortality differentials has been smaller than is commonly believed."], ["A Comparison of Population Estimation Methods: Housing Unit versus Component II, Ratio Correlation, and Administrative Records", "The housing unit (HU) method is often characterized as inferior to other methods for estimating the population of states and local areas. We believe this characterization must be challenged. In this article we evaluate population estimates produced by the housing unit method and by three other commonly used methods: component II, ratio correlation, and administrative records. Basing our analysis on 1980 census data from 67 counties in Florida and testing for precision, bias, and the distribution of errors, we find our application of the HU method performs at least as well as the more highly acclaimed methods of local population estimation."], ["Nonparametric Estimation of the Hazard Ratio", "A nonparametric estimate of the hazard ratio function is developed and applied to the time-dependent survival advantage of having achieved an objective response to treatment for a given disease. The method computes the total exposure to risk of failure, or dying, among patients in either of two disease states (nonresponder/responder). By assuming constant hazard rates between observed failure times, and using smoothing techniques of exploratory data analysis, the log hazard ratio function is computed and plotted versus time. The method is applicable to any binary, time-dependent covariate, of which disease state is a particular example. Continuous-valued covariates may be handled by a plot of updated covariate percentiles for failures. The methods are applied to data on leukemia patients and to a simulated example and compared with standard Cox regression results."], ["On the Behavior of Certain Maximum Likelihood Estimators from Large, Randomly Censored Samples", null], ["The Analysis of Transformed Data", "Recently it was suggested (Bickel and Doksum 1981) that when data are used to select a transformation, the post-transformation analysis of those data may need to be modified considerably from standard form so as to allow for the selection. We argue that common sense and the work of Box and Cox (1964) point to a contrary conclusion. Our argument is based on considerations of parameter interpretation and subsequent Bayesian analysis, within the context of fitting normal-error linear models. Numerical examples are used to illustrate the main points."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Power Transformations When Fitting Theoretical Models to Data", "We investigate power transformations in nonlinear regression problems when there is a physical model for the response but little understanding of the underlying error structure. In such circumstances, and unlike the ordinary power transformation model, both the response and the model must be transformed simultaneously and in the same way. We show by an asymptotic theory and a small Monte Carlo study that for estimating the model parameters there is little cost for not knowing the correct transform a priori; this is in dramatic contrast to the results for the usual case where only the response is transformed. Possible applications of the theory are illustrated by examples."], ["Leverage in Least Squares Additive-Plus-Multiplicative Fits for Two-Way Tables", "An additive-plus-multiplicative model can describe both main effects and row x column interactions in two-way tables of data. When each cell contains exactly one observation, a least squares fit for this nonlinear model calculates the main effects, using means of rows and columns, and then fits a multiplicative term to the additive residuals, using the singular value decomposition. A natural extension of the hat matrix for a linear model yields a definition of leverage that provides insights about the impact of erroneous data values on the fit. Theoretical and numerical investigations reveal the complex nature of leverage for this nonlinear model."], ["Regression Diagnostics for General Linear Regression Models", null], ["The Significance Attained by the Best-Fitting Regressor Variable", null], ["On Tests Applied to Residuals", null], ["Testing for Unit Roots in Seasonal Time Series", "Regression estimators of coefficients in seasonal autoregressive models are described. The percentiles of the distributions for time series that have unit roots at the seasonal lag are computed by Monte Carlo integration for finite samples and by analytic techniques and Monte Carlo integration for the limit case. The tabled distributions may be used to test the hypothesis that a time series has a seasonal unit root."], ["A Method for Estimating Distributed Lags When Observations are Randomly Missing", null], [null, "We show that the General Partial Autocorrelation Function (GPAC), which has recently been suggested to be used as one of a set of convenient tools for order identification in ARMA models, has unstable behavior when applied to time series of moderate length. Its use in detecting the order of MA components in real series is very limited and can only be recommended as a means to confirm a pure AR fit to the data."], ["A Smoothness Priors\u2013State Space Modeling of Time Series with Trend and Seasonality", "A smoothness priors modeling of time series with trends and seasonalities is shown. An observed time series is decomposed into local polynomial trend, seasonal, globally stationary autoregressive and observation error components. Each component is characterized by an unknown variance\u2013white noise perturbed difference equation constraint. The constraints or Bayesian smoothness priors are expressed in state space model form. Trading day factors are also incorporated in the model. A Kalman predictor yields the likelihood for the unknown variances (hyperparameters). Likelihoods are computed for different constraint order models in different subsets of constraint equation model classes. Akaike's minimum AIC procedure is used to select the best model fitted to the data within and between the alternative model classes. Smoothing is achieved by using a fixed-interval smoother algorithm. Examples are shown."], ["A Note on Bayesian Least Squares Inference for Finite Population Models", "This note presents some distribution-free results for incorporating prior information in finite population models. The formulas given are based on modifications of existing Bayesian least squares estimation results and require only the specification of first and second moments of the distributions involved."], ["Estimating a Population of Parameter Values Using Bayes and Empirical Bayes Methods", "In standard Bayes and empirical Bayes component decision problems, estimating inidividual parameters is the primary goal. In multiple comparison problems and in comparisons of histograms of estimates, however, the primary goal is to produce parameter estimates that can be considered as an ensemble. For example, the histogram of estimates should be a good estimate of the histogram of parameters. Standard methods of estimating by the posterior expectation do minimize symmetric, componentwise losses such as squared error, but they produce ensembles of estimates with a sample variance smaller than the posterior expected sample variance for parameters. In this article we propose new Bayes and empirical Bayes estimates that minimize a distance function between the empirical cdf of the estimates and the true parameters. These estimators are weighted averages of the prior mean and the data, with weight on the data being approximately the square root of that for the posterior expectation. We give theoretical and applied examples, including subgroup analysis in a clinical trial."], ["Selection through an Associated Characteristic, with Applications to the Random Effects Model", null], ["Estimation and Prediction in a Multivariate Random Effects Generalized Linear Model", "Estimation of an individual's regression coefficients in a multivariate random effects linear model is considered. The problem of prediction of future responses of an individual based on the random effects model is also considered, and the prediction mean squared errors of various predictors are derived and compared for the practical situation where all parameters of the model are unknown and estimates must be used."], ["Adaptive Classification Procedures", "An explicitly computable, necessary, and sufficient condition for the existence of an adaptive classification procedure is obtained. By definition, an adaptive procedure, which classifies a sample as coming from one of alternative distributions known only up to a finite-valued nuisance parameter, is required to have the same asymptotic behavior of error probability for these families as asymptotically optimal rules for each of the families. We investigate the conditions under which the overall maximum likelihood procedure is adaptive and derive a rule that is adaptive if any procedure is. We study the consistency of these procedures. Several exponential-family examples illustrate their form, and a small-sample study of error probabilities is performed."], ["On the Distributions of Scan Statistics", "This article considers the distribution of the scan statistic over some interval of the real line, both for a fixed number of independent uniform points and for a Poisson process. It provides a new series method of calculating this distribution in the Poisson case and a simple self-contained recursive approximation in the fixed case. These results work best at low densities, which is precisely the case when the closed formulas are computationally impractical. The results are compared with values tabulated by Neff and Naus (1980) and with a recent approximation due to Naus (1982). There is also an explanation of why Naus's approximation is good in the Poisson case and a derivation of recursive bounds in the fixed case."], ["Optimal Sequential Selection Based on Relative Ranks with Renewable Call Options", "Sequential sampling problems may be affected significantly by the presence of sampling costs and the ability to recall historical observations. In the context of the classical secretary problem, we incorporate these two notions into the decision maker's action set, thereby creating a stopped decision process. Whenever a desirable applicant appears, we may consider purchasing an option to recall it subsequently. The problem is solved for the best-choice criterion, either reduced or discounted by the option costs incurred."], ["Probability Inequalities for Multivariate Distributions with Dependence Structures", null], ["Understanding Cox's Regression Model: A Martingale Approach", "An informal discussion is given of how martingale techniques can be used to extend Cox's regression model and to derive its large sample properties."], ["Sigmoidally Constrained Maximum Likelihood Estimation in Quantal Bioassay", null], ["A Truncated Maximum Likelihood Estimator of a Constrained Bivariate Linear Regression Coefficient", null], ["Approximate Tolerance Intervals, Based on Maximum Likelihood Estimates", null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"], ["Linear Mixtures: A New Approach to Bivariate Trend Lines", "This article describes a new, biologically motivated model for bivariate data containing a trend line. The model is characterized by a mixture in which the component means lie on a line. Consistent methods for estimating the parameters of the mixture line are derived, as are procedures for obtaining a confidence interval for the slope. The methods are proved valid within a large class of models for bivariate data, including an errors-invariables model, and are illustrated with data on Canadian herring. This work provides one possible objective resolution of a (sometimes heated) debate in allometry and elsewhere on how to define and estimate a linear trend through bivariate data from a natural population."], ["On the Nature and Discovery of Structure", "A condition for consistent estimation stated in virtually every book on econometrics is meaningless in one common form, impossible to satisfy in another. The distinction between exogenous and other predetermined variables is irrelevant to consistent estimation of the effects of predetermined variables. The relation of predetermined to excluded variables is irrelevant to consistent estimation of the effects of endogenous variables."], ["Comment: Causal Inference from Messy Data", null], ["Comment: The Indispensable Art of Econometrics", null], ["Comment: Estimating the Effects Caused by Treatments", null], ["Rejoinder", null], ["Dynamic Representation of Multivariate Time Series Data", "In this article we describe a procedure for representing multivariate time series data by means of interactive, computer-generated dynamic imagery with computer-music accompaniment. This innovation conveys the novel insights that dynamic imagery can provide; yet, the imagery is developed from principles that make the representation useful when examined either statically or dynamically. This is because the development of the dynamic representation is guided by the same perceptual and technical principles used in making a motion picture. The particular implementation we describe is evaluated by a formal psychophysics experiment in which we measure the threshold correlation that can be perceived in our dynamic representation, and in each of three different types of graphical portrayals."], ["From Association to Causation in Observational Studies: The Role of Tests of Strongly Ignorable Treatment Assignment", "If treatment assignment is strongly ignorable, then adjustment for observed covariates is sufficient to produce consistent estimates of treatment effects in observational studies. A general approach to testing this critical assumption is developed and applied to a study of the effects of nuclear fallout on the risk of childhood leukemia. R.A. Fisher's advice on the interpretation of observational studies was \u201cMake your theories elaborate\u201d; formally, make causal theories sufficiently detailed that, under the theory, strongly ignorable assignment has testable consequences."], ["The Accuracy of Peizer Approximations to the Hypergeometric Distribution, with Comparisons to Some other Approximations", "Results of an extensive empirical study of the accuracy of 12 normal and 3 binomial approximations to the hypergeometric distribution are presented in terms of maximum absolute error under various conditions on the variables. The most useful conditions employ the minimum cell in the given or complementary 2 \u00d7 2 table and the tail probability itself. Of the normal approximations, the best by far are of a heretofore unpublished type originated by Peizer. Especially detailed results on both absolute and relative errors are given for one Peizer approximation. Its absolute error is at most .0001, for example, if the minimum cell is at least 4."], ["Graphical Methods for Assessing Logistic Regression Models", "In ordinary linear regression, graphical diagnostic displays can be very useful for detecting and examining anomalous features in the fit of a model to data. For logistic regression models, the discreteness of binary data makes it difficult to interpret such displays. Modifications and extensions of linear model displays lead to three methods for diagnostic checking of logistic regression models. Local mean deviance plots are useful for detecting overall lack of fit. Empirical probability plots help point out isolated departures from the fitted model. Partial residual plots, when smoothed to show underlying structure, help identify specific causes of lack of fit. These methods are illustrated through the analyses of simulated and real data."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment: Assessing the Fit of Logistic Regressions Using the Implied Discriminant Analysis", null], ["Rejoinder", null], ["Consistent Estimates of Autoregressive Parameters and Extended Sample Autocorrelation Function for Stationary and Nonstationary ARMA Models", "A unified approach for the tentative specification of the order of mixed stationary and nonstationary ARMA models is proposed. For the ARMA models, an iterative regression procedure is given to produce consistent estimates of the autoregressive parameters. An extended sample autocorrelation function based on these consistent estimates is then defined and used for order determination. One of the advantages of this new approach is that it eliminates the need to determine, usually rather arbitrarily, the order of differencing to produce stationarity in modeling time series. Comparisons with other existing identification methods are discussed, and several samples are given."], ["Bootstrapping a Regression Equation: Some Empirical Results", "The bootstrap, like the jackknife, is a technique for estimating standard errors. The idea is to use Monte Carlo simulation based on a nonparametric estimate of the underlying error distribution. The main object of this article is to present the bootstrap in the context of an econometric equation describing the demand for energy by industry. As it turns out, the conventional asymptotic formulas for estimating standard errors are too optimistic by factors of nearly three, when applied to a particular finite-sample problem. In a simpler context, this finding can be given a mathematical proof."], ["Choosing a Symmetrizing Power Transformation", "It is demonstrated that a method for choosing power transformations for symmetry suggested by Emerson and Stoto (1982) behaves poorly for highly skewed data."], ["Rejoinder", null], ["Efficiency of a Kernel Density Estimator under an Autoregressive Dependence Model", "The problem of estimating the probability density function of a strictly stationary process is considered. To study the effect of a dependence structure on the efficiency of a kernel density estimator, the mean integrated squared error (MISE) of the Fourier integral estimator (FIE) is derived on the assumption that the observed data are generated by a first-order autoregressive process. Numerical results for the normal and Cauchy densities show that even moderate departures from independence can lead to a considerable loss in efficiency of the FIE. In addition to efficiency considerations, the issue of determining an optimal smoothing parameter for the FIE under the autoregressive model is addressed."], ["Regression Models with Time Series Errors", "The time series regression models in which the errors of regression equations follow stationary or nonstationary autoregressive moving average models are considered. Convergence properties of the sample autocorrelation function of observed series and the least squares estimates of the linear regression parameters are shown. Based upon these results, a procedure for specifying the tentative order of the mixed ARMA errors is proposed. Two examples are given."], ["Estimating Missing Observations in Economic Time Series", "Two related problems are considered. The first concerns the maximum likelihood estimation of the parameters in an ARIMA model when some of the observations are missing or subject to temporal aggregation. The second concerns the estimation of the missing observations. Both problems can be solved by setting up the model in state space form and applying the Kalman filter."], ["Analysis of Grouped Data from Multinomial Populations", null], ["Rank-One round Robin Designs", "A class of analysis of variance models is proposed for analyzing round robin interaction data, where the actor and partner effects of any member are linearly related, thus forcing the covariance matrix of these effects to have rank one. Such data arise frequently in applications, for instance, in sociometric studies on structures of social relationships. A factor analytic approach is proposed for studying these models. Statistical inference about the linear effects is discussed and a convergent algorithm is presented for obtaining the maximum likelihood estimates of the covariance components. A data set is analyzed using the methodology developed."], ["An Alternative to Eisenhart's Model II and Mixed Model in the Case of Negative Variance Estimates", "An alternative model useful for addressing the problem of negative variance component estimators is proposed. Specifically, it is possible to view certain of the parameters as covariances rather than as variances. Thus, the problems of the negative variance component estimator may be resolved by using a different model. Distributional results relating to both estimation and tests of hypotheses are also given."], ["Robustness of Clevenson-Zidek-Type Estimators", "For estimating the means of several independent Poisson distributions, Clevenson and Zidek (1975) were the first to propose a class of estimators that are better than the usual one under the normalized squared error loss (1.1). This class of estimators was subsequently enlarged by others. This article examines the sensitivity of the superiority of the Clevenson-Zidek (CZ)-type means estimators over the usual one with respect to the exactness of the Poisson distribution assumption. It is shown that many of the CZ-type means estimators dominate the usual estimator even when the underlying distributions are negative binomial, whether they are close to the Poisson or not. This broadens the scope of use of CZ-type estimators."], ["Testing Hypotheses concerning Unions of Linear Subspaces", "The likelihood ratio test (LRT) for hypotheses concerning unions of linear subspaces is derived for the normal theory linear model. A more powerful test, an intersection-union test, is proposed for the case in which the subspaces are not all of the same dimension. A theorem is proved that may be used to identify hypotheses that concern unions of linear subspaces. Some hypotheses about the spacings between normal means are shown to concern unions of linear subspaces and therefore can be tested using the LRT. Finally, the computation of the LRT statistic is discussed."], [null, null], ["Data-Based Nonparametric Estimation of the Hazard Function with Applications to Model Diagnostics and Exploratory Analysis", null], ["Estimation of the Weibull Parameters under Type I Censoring", "The authors propose estimators for the shape and the scale parameters in the Weibull distribution under Type I censoring. They derive exact moments of the estimator for the shape parameter and establish the joint asymptotic normality of both estimators. The proposed estimators, compared with the corresponding maximum likelihood estimators, provide a simpler procedure while maintaining high asymptotic relative efficiencies for a wide range of censoring levels. Estimation of the reliability function and the mean life and the relationship to the Type II case are also discussed."], ["Modeling and Inference for Multivariate Binary Data with Positive Dependence", "A model is proposed for multivariate binary data that incorporates positive dependence among components in a natural way. The model is derived from reliability-theoretic concepts, but is regarded as appropriate for analysis of multivariate binary data in any field when positive dependence is an appropriate assumption. Maximum likelihood estimation by iterative solution of likelihood equations is discussed for the general model, and asymptotically efficient estimates are obtained in closed form for the fully parameterized (saturated) model. The estimation procedures are illustrated on a data set from Martin and Bradley (1972)."], ["A Goodness-of-Fit Test for One-Sample Life Table Data", "Grouped censored data are common in life testing, medical trials, and other fields. Individuals, subject to a disease, have their lifetimes recorded as belonging to a certain interval. If the individuals are censored or removed before death, then it is only known that their lifetimes were at least as great as the beginning of the interval in which the individuals were censored. A goodness-of-fit problem is to test the agreement of a hypothesized survival curve with the observed data. Several tests have been proposed in the literature that require various assumptions about the censoring distribution. It is shown in this article that if these conditions are relaxed, then the tests may no longer have the stated properties, such as the correct size. To derive a test under minimal conditions, the maximum likelihood estimates of the probability of death or censoring in an interval are found, given that the probability of death in the absence of censoring is a certain quantity. The estimates are used to construct the maximum likelihood test of the fit of the postulated survival curve. The test does not require any assumptions about the censoring marginal distribution. If the assumptions for the more specific tests hold, however, then there may be some loss of power."], ["On Inverting Permutation Tests", "Polynomial time algorithms are presented for inverting permutation tests. The one-sample permutation test is inverted to make confidence statements about a location parameter, and the two-sample permutation test is inverted to make confidence statements concerning a shift in location or scale. These algorithms require polynomial time, as opposed to complete enumeration algorithms, which require exponential time. A computational method for the inversion of rank tests is also suggested. The algorithms extend to stratified experiments."], ["A Note on Two-Sided Distribution-Free Treatment versus Control Multiple Comparisons", "Distribution-free treatment versus control multiple comparison procedures using joint ranking and pairwise ranking are compared. The pairwise ranking method is extended to allow for all contrast comparisons among the treatments and the control. The author shows by counterexample that the joint ranking method does not control the maximum type 1 error rate."], ["A Distribution-Free Rank Test for Ordered Alternatives in Randomized Complete Block Designs", null], ["The Exact-Approximation Method for Generating Random Variables in a Computer", null], ["Book Reviews", null], ["Editorial Board Page", "This article has no abstract"]]}