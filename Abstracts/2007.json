{"2007": [["Statistical Analysis of Diffusion Tensors in Diffusion-Weighted Magnetic Resonance Imaging Data", "Diffusion tensor imaging has been widely used to reconstruct the structure and orientation of fibers in biological tissues, particularly in the white matter of the brain, because it can track the effective diffusion of water along those fibers. The raw diffusion-weighted images from which diffusion tensors are estimated, however, inherently contain noise. Noise in the images produces uncertainty in the estimation of the tensors (which are 3 \u00d7 3 positive-definite matrices) and of their derived quantities, including eigenvalues, eigenvectors, and the fiber pathways that are reconstructed based on those tensor elements. The aim of this article is to provide a comprehensive theoretical framework of statistical inference for quantifying the effects of noise on diffusion tensors, on their eigenvalues and eigenvectors, and on their morphological classification. We propose a semiparametric model to account for noise in diffusion-weighted images. We then develop a one-step, weighted least squares estimate of the tensors and justify use of the one-step estimates based on our theoretical framework and computational results. We also quantify the effects of noise on the eigenvalues and eigenvectors of the estimated tensors by establishing their limiting distributions. We construct pseudo-likelihood ratio statistics to classify tensor morphologies. Simulation studies show that our theoretical results can accurately predict the stochastic behavior of the estimated eigenvalues and eigenvectors, as well as the bias that is introduced by sorting the eigenvalues by their magnitudes. Implementation of these methods is illustrated in a diffusion-weighted dataset from seven healthy human subjects."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Gait-Based Human Recognition by Classification of Cyclostationary Processes on Nonlinear Shape Manifolds", "We study the problem of analyzing and classifying human gait by modeling it as a stochastic process on a shape space. We consider gait as a evolution of human silhouettes as seen in video sequences, and focus on their shapes. More specifically, we define a shape space of planar, closed curves and model a human gait as a stochastic process on this space. Due to the periodic nature of human walk, this process is naturally constrained to be cyclostationary, that is, its mean path is assumed to be cyclic. We compare two subjects using a metric that quantifies differences between average gait cycles of each subject. This computation uses several tools from differential geometry of the shape space, including computation of geodesics, estimation of means of observed shapes, interpolation between observed shapes, and temporal registration of two gait cycles. Finally, we apply a nearest-neighbor classifier, using the gait metric, to perform human recognition, and present results from an experiment involving 26 subjects."], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Local Smoothing Image Segmentation for Spotted Microarray Images", "Gene microarray data are used in a wide variety of applications, including pharmaceutical and clinical research. By comparing gene expression in normal and abnormal cells, microarrays can be used to identify genes involved in particular diseases, and these genes then can be targeted by therapeutic drugs. Most gene expression data are produced from spotted microarray images. A spotted microarray image consists of thousands of spots, with individual DNA sequences first printed at each spot and then equal amounts of probes (e.g., cDNA samples) from treatment and control cells mixed and hybridized with the printed DNA sequences. To obtain gene expression data, the image first must be segmented to separate foregrounds from backgrounds for individual spots, after which averages of foreground pixels are used to compute the gene expression data. Thus image segmentation of microarray images is directly related to the reliability of gene expression data. Several image segmentation procedures have been suggested in the literature and included in software packages handling gene microarray data. This article proposes a new image segmentation methodology based on local smoothing. Theoretical arguments and numerical studies show that it has good statistical properties and should perform well in applications."], ["A Multiresolution Hazard Model for Multicenter Survival Studies", "In multicenter studies, one often needs to make inference about a population survival curve based on multiple, possibly heterogeneous survival data from individual centers. We investigate a flexible Bayesian method for estimating a population survival curve based on a semiparametric multiresolution hazard model that can incorporate covariates and account for center heterogeneity. The method yields a smooth estimate of the survival curve for \u201cmultiple resolutions\u201d or time scales of interest. The Bayesian model used has the capability to accommodate general forms of censoring and a priori smoothness assumptions. We develop a model checking and diagnostic technique based on the posterior predictive distribution and use it to identify departures from the model assumptions. The hazard estimator is used to analyze data from 110 centers that participated in a multicenter randomized clinical trial to evaluate tamoxifen in the treatment of early stage breast cancer. Of particular interest are the estimates of center heterogeneity in the baseline hazard curves and in the treatment effects, after adjustment for a few key clinical covariates. Our analysis suggests that the treatment effect estimates are rather robust, even for a collection of small trial centers, despite variations in center characteristics."], ["Bayesian Multivariate Isotonic Regression Splines", "In many applications, interest focuses on assessing the relationship between a predictor and a multivariate outcome variable, and there may be prior knowledge about the shape of the regression curves. For example, regression functions that relate dose of a possible risk factor to different adverse outcomes can often be assumed to be nondecreasing. In such cases, interest focuses on (1) assessing evidence of an overall adverse effect, (2) determining which outcomes are most affected, and (3) estimating outcome-specific regression curves. This article proposes a Bayesian approach for addressing this problem, motivated by multisite tumor data from carcinogenicity experiments. A multivariate smoothing spline model is specified, that accommodates dependency in the multiple curves through a hierarchical Markov random field prior for the basis coefficients, while also allowing for residual correlation. A Gibbs sampler is proposed for posterior computation, and the approach is applied to data on body weight and tumor occurrence."], ["Testing Forecast Optimality Under Unknown Loss", "Empirical tests of forecast optimality have traditionally been conducted under the assumption of mean squared error loss or some other known loss function. In this article we establish new testable properties that hold when the forecaster's loss function is unknown but testable restrictions can be imposed on the data-generating process, trading off conditions on the data-generating process against conditions on the loss function. We propose flexible estimation of the forecaster's loss function in situations where the loss depends not only on the forecast error, but also on other state variables, such as the level of the target variable. We apply our results to the problem of evaluating the Federal Reserve's forecasts of output growth. Forecast optimality is rejected if the Fed's loss depends only on the forecast error. However, the empirical findings are consistent with forecast optimality provided that overpredictions of output growth are costlier to the Fed than underpredictions, particularly during periods of low economic growth."], ["Bayesian Forecasting of an Inhomogeneous Poisson Process With Applications to Call Center Data", "A call center is a centralized hub where customer and other telephone calls are handled by an organization. In today's economy, call centers have become the primary points of contact between customers and businesses. Thus accurate predictions of call arrival rates are indispensable to help call center practitioners staff their call centers efficiently and cost-effectively. This article proposes a multiplicative model for modeling and forecasting within-day arrival rates to a U.S. commercial bank's call center. Markov chain Monte Carlo sampling methods are used to estimate both latent states and model parameters. One-day-ahead density forecasts for the rates and counts are provided. The calibration of these predictive distributions is evaluated through probability integral transforms. Furthermore, 1-day-ahead forecasts comparisons with classical statistical models are given. Our predictions show significant improvements of up to 25% over these standards. A sequential Monte Carlo algorithm is also proposed for sequential estimation and forecasts of the model parameters and rates."], ["On the Estimation of Disability-Free Life Expectancy", "A rapidly aging population, such as the United States today, is characterized by the increased prevalence of chronic impairment. Robust estimation of disability-free life expectancy (DFLE), or healthy life expectancy, is essential for examining whether additional years of life are spent in good health and whether life expectancy is increasing faster than the decline of disability rates. Over 30 years since its publication, Sullivan's method remains the most widely used method to estimate DFLE. Therefore, it is surprising to note that Sullivan did not provide any formal justification of his method. Debates in the literature have centered around the properties of Sullivan's method and have yielded conflicting results regarding the assumptions required for Sullivan's method. In this article we establish a statistical foundation of Sullivan's method. We prove that, under stationarity assumptions, Sullivan's estimator is unbiased and consistent. This resolves the debate in the literature, which has generally concluded that additional assumptions are necessary. We also show that the standard variance estimator is consistent and approximately unbiased. Finally, we demonstrate that Sullivan's method can be extended to estimate DFLE without stationarity assumptions. Such an extension is possible whenever a cohort life table and either consecutive cross-sectional disability surveys or a longitudinal survey are available. Our empirical analysis of the 1907 and 1912 U.S. birth cohorts suggests that while mortality rates remain approximately stationary, disability rates decline during this time period."], ["Incorporating Historical Control Data When Comparing Tumor Incidence Rates", null], ["High-Resolution Space\u2013Time Ozone Modeling for Assessing Trends", "This article proposes a space\u2013time model for daily 8-hour maximum ozone levels to provide input for regulatory activities: detection, evaluation, and analysis of spatial patterns and temporal trend in ozone summaries. The model is applied to the analysis of data from the state of Ohio that contains a mix of urban, suburban, and rural ozone monitoring sites. The proposed space\u2013time model is autoregressive and incorporates the most important meteorological variables observed at a collection of ozone monitoring sites as well as at several weather stations where ozone levels have not been observed. This misalignment is handled through spatial modeling. In so doing we adopt a computationally convenient approach based on the successive daily increments in meteorological variables. The resulting hierarchical model is specified within a Bayesian framework and is fitted using Markov chain Monte Carlo techniques. Full inference with regard to model unknowns as well as for predictions in time and space, evaluation of annual summaries, and assessment of trends are presented."], ["Inference of Tamoxifen's Effects on Prevention of Breast Cancer From a Randomized Controlled Trial", "The largest randomized, double-blind, placebo-controlled chemoprevention trial, the National Surgical Adjuvant Breast and Bowel Project's Breast Cancer Prevention Trial (NSABP\u2013BCPT), evaluated the efficacy of tamoxifen in preventing breast cancer among women at high risk of developing the disease. The trial has reported a reduction of breast cancer incidence for the tamoxifen group; however, the effect of tamoxifen on the time to diagnosis of the disease over the 6-year follow-up of the trial has not been fully explored in the literature. We propose a flexible semiparametric model to assess the effects of tamoxifen on the incidence of breast cancer as well as time to diagnosis of the disease separately in the framework of a cure rate model. We used an estimating equation approach to estimating the unknown parameters and assessed the semiparametric model assumption with a test based on the area between two survival curves. In the NSABP\u2013BCPT data, we found that tamoxifen has a substantial effect in reducing invasive breast cancer events in estrogen receptor (ER)-positive tumors, but has no effect on ER-negative tumors. Among women diagnosed with ER-positive breast cancer during study follow-up, there was little difference in terms of time to diagnosis between the two arms. However, tamoxifen may advance the time to breast cancer diagnosis for ER-negative breast cancer, whereas the incidence of ER-negative tumors is similar in the two treatment arms."], ["Bayesian Analysis of Frequency of Allelic Loss Data", "One objective of allelic-loss studies is to identify chromosomal locations that may harbor tumor-suppressor genes. An instability-selection model has been developed for allelic-loss data in which the loss events are available for each tumor and each marker (allelotypes). In performing pooled analyses of published allelic-loss experiments, however, only summaries of the frequency of allelic loss (FAL) may be available. The instability-selection model can be applied to these summary data, but naive computational approaches are prohibitive. A hidden Markov model (HMM) maximum likelihood approach has recently been proposed for FAL data, but the computation remains challenging. Moreover, precise methods for hypothesis testing and location inference are not available. We propose an alternative Bayesian treatment of the instability-selection model for FAL data. Advantages of the Bayesian approach include the availability of (1) natural imputation approaches to handle missing data, (2) hypothesis testing using Bayes factors, and (3) interpretable posterior intervals for tumor-suppressor locations. We apply our Bayesian approach to four previously reported allelic-loss studies."], ["Estimating Time to Event From Longitudinal Categorical Data", "The expanded disability status scale (EDSS) is an ordinal score that measures progression in multiple sclerosis (MS). Progression is defined as reaching EDSS of a certain level (absolute progression) or increasing EDSS by one point (relative progression). Survival methods for time to progression are not adequate for such data because they do not exploit the EDSS level at the end of follow-up. Instead, we suggest a Markov transitional model applicable for repeated categorical or ordinal data. This approach enables derivation of covariate-specific survival curves, obtained after estimation of the regression coefficients and manipulations of the resulting transition matrix. Large-sample theory and resampling methods are employed to derive pointwise confidence intervals, which perform well in simulation. Methods for generating survival curves for time to EDSS of a certain level, time to increase EDSS by at least one point, and time to two consecutive visits with EDSS greater than 3 are described explicitly. The regression models described are easily implemented using standard software packages. Survival curves are obtained from the regression results using packages that support simple matrix calculation. We present and demonstrate our method on data collected at the Partners Multiple Sclerosis Center in Boston. We apply our approach to progression defined by time to two consecutive visits with EDSS greater than 3 and calculate crude (without covariates) and covariate-specific curves."], ["Distance-Weighted Discrimination", "High-dimension low\u2013sample size statistical analysis is becoming increasingly important in a wide range of applied contexts. In such situations, the popular support vector machine suffers from \"data piling\" at the margin, which can diminish generalizability. This leads naturally to the development of distance-weighted discrimination, which is based on second-order cone programming, a modern computationally intensive optimization method."], ["False Discovery Rates for Spatial Signals", "The problem of multiple testing for the presence of signal in spatial data can involve numerous locations. Traditionally, each location is tested separately for signal presence, but then the findings are reported in terms of clusters of nearby locations. This is an indication that the units of interest for testing are clusters rather than individual locations. The investigator may know a priori these more natural units or an approximation to them. We suggest testing these cluster units rather than individual locations, thus increasing the signal-to-noise ratio within the unit tested as well as reducing the number of hypothesis tests conducted. Because the signal may be absent from part of each cluster, we define a cluster as containing a signal if the signal is present somewhere within the cluster. We suggest controlling the false discovery rate (FDR) on clusters (i.e., the expected proportion of clusters rejected erroneously out of all clusters rejected) or its extension to general weights (WFDR). We introduce a powerful two-stage testing procedure and show that it controls the WFDR. Once the cluster discoveries have been made, we suggest \"cleaning\" locations in which the signal is absent. For this purpose, we develop a hierarchical testing procedure that first tests clusters, then tests locations within rejected clusters. We show formally that this procedure controls the desired location error rate asymptotically, and conjecture that this is also so for realistic settings by extensive simulations. We discuss an application to functional neuroimaging that motivated this research and demonstrate the advantages of the proposed methodology on an example."], [null, null], ["Robust Linear Model Selection Based on Least Angle Regression", "In this article we consider the problem of building a linear prediction model when the number of candidate predictors is large and the data possibly contain anomalies that are difficult to visualize and clean. We want to predict the nonoutlying cases; therefore, we need a method that is simultaneously robust and scalable. We consider the stepwise least angle regression (LARS) algorithm which is computationally very efficient but sensitive to outliers. We introduce two different approaches to robustify LARS. The plug-in approach replaces the classical correlations in LARS by robust correlation estimates. The cleaning approach first transforms the data set by shrinking the outliers toward the bulk of the data (which we call multivariate Winsorization) and then applies LARS to the transformed data. We show that the plug-in approach is time-efficient and scalable and that the bootstrap can be used to stabilize its results. We recommend using bootstrapped robustified LARS to sequence a number of candidate predictors to form a reduced set from which a more refined model can be selected."], ["Weighted Repeated Median Smoothing and Filtering", null], ["Sieve Maximum Likelihood Estimation for Regression Models With Covariates Missing at Random", "Missing covariates are common in regression problems. We propose a new semiparametric method based on a fully nonparametric distribution for the missing covariates that are assumed to be missing at random. The method of sieve maximum likelihood estimation is used to obtain the estimators of the regression coefficients. These estimators are shown to be consistent and asymptotically normal with their asymptotic covariance matrix that achieves the semiparametric efficiency bound. A bootstrap approach is used to estimate the asymptotic covariance matrix. Some practical modeling approaches for high-dimensional covariates are proposed. Extensive simulation studies are conducted to examine the finite-sample properties of the estimates, and a real data set from a liver cancer clinical trial is analyzed using the proposed method."], ["Unbalanced Haar Technique for Nonparametric Function Estimation", null], ["A Note on Penalized Spline Smoothing With Correlated Errors", "We investigate the behavior of data-driven smoothing parameters for penalized spline regression in the presence of correlated data. It has been shown for other smoothing methods that mean squared error minimizers, such as (generalized) cross-validation or the Akaike information criterion, are extremely sensitive to misspecifications of the correlation structure resulting in over- or (under-)fitting the data. In contrast to this, we show that a maximum likelihood\u2013based choice of the smoothing parameter is more robust and that for a moderately misspecified correlation structure over- or (under-)fitting does not occur. This is demonstrated in simulations and data examples and is supported by theoretical investigations."], ["Portmanteau Test of Independence for Functional Observations", "In various fields, observations are curves over some natural time interval. These curves often arise from finely spaced measurements (e.g., in physical sciences and finance) or from smoothing unequally spaced sparse measurements. Recent years have seen the development of tools for analyzing such data in the growing field of functional data analysis. To validate the assumptions underlying these tools, it is important to verify that the functional observations form a simple random sample. If the curves form a (functional) time series, then model validation typically relies on checking whether model residuals are independent and identically distributed. We propose a test for independence and identical distribution of functional observations. To reduce dimension, curves are projected on the most important functional principal components, then a test statistic based on lagged cross-covariances of the resulting vectors is constructed. We show that this dimension-reduction step introduces asymptotically negligible terms; that is, the projections behave asymptotically as iid vector\u2013valued observations. A complete asymptotic theory based on correlations of random matrices, functional principal component expansions, and Hilbert space techniques is developed. The test statistic has a chi-squared asymptotic null distribution and can be readily computed using the R package fda. The test has good empirical size and power, which in our simulations and examples is not affected by the choice of the functional basis. Its application is illustrated on two data sets: credit card sales activity and geomagnetic records."], ["Multi-Scale Jump and Volatility Analysis for High-Frequency Financial Data", "The wide availability of high-frequency data for many financial instruments stimulates an upsurge interest in statistical research on the estimation of volatility. Jump-diffusion processes observed with market microstructure noise are frequently used to model high-frequency financial data. Yet existing methods are developed for either noisy data from a continuous-diffusion price model or data from a jump-diffusion price model without noise. We propose methods to cope with both jumps in the price and market microstructure noise in the observed data. These methods allow us to estimate both integrated volatility and jump variation from the data sampled from jump-diffusion price processes, contaminated with the market microstructure noise. Our approach is to first remove jumps from the data and then apply noise-resistant methods to estimate the integrated volatility. The asymptotic analysis and the simulation study reveal that the proposed wavelet methods can successfully remove the jumps in the price processes and the integrated volatility can be estimated as accurately as in the case with no presence of jumps in the price processes. In addition, they have outstanding statistical efficiency. The methods are illustrated by applications to two high-frequency exchange rate data sets."], ["Rank-Based Extensions of the Brock, Dechert, and Scheinkman Test", null], ["A Thinned Block Bootstrap Variance Estimation Procedure for Inhomogeneous Spatial Point Patterns", null], ["Efficient Estimation for the Accelerated Failure Time Model", "The accelerated failure time model provides a natural formulation of the effects of covariates on potentially censored response variable. The existing semiparametric estimators are computationally intractable and statistically inefficient. In this article we propose an approximate nonparametric maximum likelihood method for the accelerated failure time model with possibly time-dependent covariates. We estimate the regression parameters by maximizing a kernel-smoothed profile likelihood function. The maximization can be achieved through conventional gradient-based search algorithms. The resulting estimators are consistent and asymptotically normal. The limiting covariance matrix attains the semiparametric efficiency bound and can be consistently estimated. We also provide a consistent estimator for the error distribution. Extensive simulation studies demonstrate that the asymptotic approximations are accurate in practical situations and the new estimators are considerably more efficient than the existing ones. Illustrations with clinical and epidemiologic studies are provided."], ["Regression Analysis of Longitudinal Data in the Presence of Informative Observation and Censoring Times", "Longitudinal data frequently occur in many studies, such as longitudinal follow-up studies. To develop statistical methods and theory for the analysis of these data, independent or noninformative observation and censoring times are typically assumed, which naturally leads to inference procedures conditional on observation and censoring times. But in many situations this may not be true or realistic; that is, longitudinal responses may be correlated with observation times as well as censoring times. This article considers the analysis of longitudinal data where these correlations may exist and proposes a joint modeling approach that uses some latent variables to characterize the correlations. For inference about regression parameters, estimating equation approaches are developed and both large-sample and final-sample properties of the proposed estimators are established. In addition, some graphical and numerical procedures are presented for model checking. The methodology is applied to a bladder cancer study that motivated this investigation."], ["Nonparametric Association Analysis of Bivariate Competing-Risks Data", "Although nonparametric analyses of bivariate failure times under independent censoring have been widely studied, nonparametric analyses of bivariate competing-risks data have not yet been investigated. Such analyses are important in familial association studies, where multiple interacting failure types may violate the independent censoring assumption. We develop nonparametric estimators for the bivariate cause-specific hazards function and the bivariate cumulative incidence function that are natural analogs of their univariate counterparts and make no assumptions about the dependence of the risks. The estimators are shown to be uniformly consistent and to converge weakly to Gaussian processes. They provide the basis for novel time-dependent association measures, with the associated inferences yielding tests of cause-specific independence in pairs. The methodology performs well in simulations with realistic sample sizes. Its practical utility is illustrated in an analysis of dementia in the Cache County Study, where the nonparametric methods indicate that mother\u2013child disease associations are strongly time-varying."], ["Nonparametric Regression Estimation in the Heteroscedastic Errors-in-Variables Problem", "In the classical errors-in-variables problem, the goal is to estimate a regression curve from data in which the explanatory variable is measured with error. In this context, nonparametric methods have been proposed that rely on the assumption that the measurement errors are identically distributed. Although there are many situations in which this assumption is too restrictive, nonparametric estimators in the more realistic setting of heteroscedastic errors have not been studied in the literature. We propose an estimator of the regression function in such a setting and show that it is optimal. We give estimators in cases in which the error distributions are unknown and replicated observations are available. Practical methods, including an adaptive bandwidth selector for the errors-in-variables regression problem, are suggested, and their finite-sample performance is illustrated through simulated and real data examples."], ["Small-Area Estimation Under Informative Probability Sampling of Areas and Within the Selected Areas", "In this article we show how to predict small-area means and obtain valid mean squared error estimators and confidence intervals when the areas represented in the sample are sampled with unequal probabilities possibly related to the true (unknown) area means and the sampling of units within the selected areas is with probabilities possibly related to the outcome values. Ignoring the effects of the sampling process on the distribution of the observed outcomes in such cases may bias the inference very severely. Classical design-based inference that uses the randomization distribution of probability-weighted estimators cannot be applied for predicting the means of nonsampled areas. We propose simple test statistics for testing the informativeness of the selection of areas and sampling of units within the selected areas. We illustrate the proposed procedures by a simulation study and a real application of estimating mean body mass index in U.S. counties, using data from the Third National Health and Nutrition Examination Survey."], ["A Robust-Likelihood Cumulative Sum Chart", "In practice, the cumulative sum (CUSUM) control chart is often used to detect small shifts in the mean of a normally distributed process, but it performs poorly for thick-tailed processes and for large shifts. This article provides a robust-likelihood cumulative sum (RLCUSUM) chart that discounts outliers and yet has the ability to detect large shifts quickly. The new chart is motivated by the likelihood underpinnings of the CUSUM. It is based on the likelihood of a variate constructed to ensure robust performance of the resulting chart. The new chart is compared with the conventional CUSUM in terms of average run length, with the finding that the RLCUSUM is much better than the conventional CUSUM, especially for large shifts. For small shifts, the two charts are essentially equivalent. We study the properties of the conditional limiting distribution. A final application of the RLCUSUM in assessing livestock disease shows that the method is applicable in process control."], ["Sequential Implementation of Stepwise Procedures for Identifying the Maximum Tolerated Dose", "This article considers the problem of finding the maximum tolerated dose (MTD) of a drug in human trials. The MTD is defined as the maximum test dose with toxicity probability less than or equal to a target toxicity rate. We adopt the multiple test framework, with step-down tests used in an escalation stage and step-up tests used in a deescalation stage, to allow sequential dose assignments for ethical purposes. By formulating the estimation problem as a testing problem, the proposed procedures formally control the error probability of selecting an unsafe dose. In addition, we can control the probability of correctly selecting the MTD under a parameter subspace where no toxicity probability lies in an interval bracketed by the target toxicity rate and an unacceptably high toxicity rate, the so-called \u201cindifference zone.\u201d This frequentist property, which is currently lacking in the conduct of dose-finding trials in humans, is appealing from a regulatory standpoint. We give the general expressions of the selection probabilities and apply some common statistical tests to the stepwise procedure. The design parameters are calibrated so that the average number of patients receiving an overdose is kept low. From a practical viewpoint, stepwise tests are simple and easy to understand, and the sequential implementation operates in a manner similar to the traditional algorithm familiar to clinicians. Extensive simulations illustrate that our methods yield good, competitive operating characteristics under a wide range of scenarios with realistic sample size and performs well even in situations in which other existing methods may fail, namely when the dose\u2013toxicity curve is flat up to the targeted MTD."], ["The Multiple Adaptations of Multiple Imputation", "Multiple imputation was first conceived as a tool that statistical agencies could use to handle nonresponse in large-sample public use surveys. In the last two decades, the multiple-imputation framework has been adapted for other statistical contexts. For example, individual researchers use multiple imputation to handle missing data in small samples, statistical agencies disseminate multiply-imputed data sets for purposes of protecting data confidentiality, and survey methodologists and epidemiologists use multiple imputation to correct for measurement errors. In some of these settings, Rubin's original rules for combining the point and variance estimates from the multiply-imputed data sets are not appropriate, because what is known\u2014and thus the conditional expectations and variances used to derive inferential methods\u2014differs from that in the missing-data context. These applications require new combining rules and methods of inference. In fact, more than 10 combining rules exist in the published literature. This article describes some of the main adaptations of the multiple-imputation framework, including missing data in large and small samples, data confidentiality, and measurement error. It reviews the combining rules for each setting and explains why they differ. Finally, it highlights research topics in extending the multiple-imputation framework."], ["Book Reviews", null], ["Models for Intensive Longitudinal Data", null], ["The Statistical Analysis of Interval-Censored Failure Time Data", null], ["Dynamic Regression Models for Survival Data", null], ["Introductory Statistical Inference", null], ["DNA, Words and Models: Statistics of Exceptional Words", null], ["Reliability and Risk: A Bayesian Perspective", null], ["Operational Risks: Modeling Analytics", null], ["Probabilistic Applications of Tauberian Theorems", null], ["Stochastic Processes in Science, Engineering, and Finance", null], ["Statistical Analysis of Environmental Space-Time Processes", null], ["Extending the Linear Model With R: Generalized Linear, Mixed Effects and Nonparametric Regression Models", null], ["Analysis of Phylogenetics and Evolution With R. Emmanuel Paradis", null], ["Bootstrap Techniques for Signal Processing", null], ["The Theory of Response-Adaptive Randomization in Clinical Trials", null], ["Latent Curve Models: A Structural Equation Approach", null], ["Social Choice With Partial Knowledge of Treatment Response", null], ["Telegraphic Reviews", null], ["A Few Remarks on \u201cFixed-Width Output Analysis for Markov Chain Monte Carlo\u201d by Jones et al", "Our aim is to relax assumptions and simplify proofs in results given by Jones, Haran, Caffo, and Neath in the recent paper \u201cFixed-Width Output Analysis for Markov Chain Monte Carlo.\u201d"], ["2007 Editorial Collaborators", null], ["Subjective Likelihood for the Assessment of Trends in the Ocean's Mixed-Layer Depth", "This article describes a Bayesian statistical analysis of long-term changes in the depth of the ocean's mixed layer. The data are thermal profiles recorded by ships. For these data, there is no good sampling model and thus no obvious likelihood function. Our approach is to elicit posterior distributions for training data directly from the expert. We then infer the likelihood function and use it on large datasets."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Oscillations and Time Trends in Stratospheric Ozone Levels", "A functional data analysis approach is presented to study altitude-dependent patterns of ozone variation in data from a sequence of ozonesonde flights. Ozonesondes are balloon-based instruments that measure ozone as the balloon ascends through the troposphere and lower stratosphere. This article concentrates on variation in the altitude range of 15.5\u201330.5 km, in January\u2013July in 1967\u20131998. Ozonesonde flights originating at a mid-latitude site, Hohenpeissenberg in Germany, are studied. Estimates are obtained of altitude-dependent nonlinear time trends in ozone partial pressures, together with ozone variation associated with the quasi-biennial oscillation (QBO), an atmospheric process thought to influence global ozone transport. Both methodological and scientific contributions are made. The data analysis approach combines dimension-reduction basis function approximations with low-dimensional spline-based models on the basis function coefficients. This provides an efficient and flexible approach for studying complex time/altitude variation in the ozone partial pressure profiles, including nonlinear time trends. In contrast, the standard approach uses multiple linear regression models to estimate linear or piecewise-linear ozone time trends separately for each altitude level. Scientific results include identifying clear bimodalities (in altitude) in the estimated QBO components of variation in certain months. These may relate to planetary wave transport of ozone from the tropics to mid-latitudes. Additional empirical evidence of an 11-year cycle in ozone levels also is provided, possibly linked with a solar cycle. However, ozone peaks at Hohenpeissenberg do not always coincide with the timing of the maxima of the 11-year solar cycle. This may indicate a transport-related lag in ozone maxima at some altitudes. The estimated QBO features are robust to the presence or absence of the data quality \u201ccorrection factors\u201d commonly used in ozonesonde studies. However, the nonlinear time trend components show greater sensitivity to these."], ["Semiparametric Mixed Models for Increment-Averaged Data With Application to Carbon Sequestration in Agricultural Soils", "Adoption of conservation tillage practice in agriculture offers the potential to mitigate greenhouse gas emissions. Studies comparing conservation tillage methods to traditional tillage pair fields under the two management systems and obtain soil core samples from each treatment. Cores are divided into multiple increments, and matching increments from one or more cores are aggregated and analyzed for carbon stock. These data represent not the actual value at a specific depth, but rather the total or average over a depth increment. A semiparametric mixed model is developed for such increment-averaged data. The model uses parametric fixed effects to represent covariate effects, random effects to capture correlation within studies, and an integrated smooth function to describe effects of depth. The depth function is specified as an additive model, estimated with penalized splines using standard mixed model software. Smoothing parameters are automatically selected using restricted maximum likelihood. The methodology is applied to the problem of estimating a change in carbon stock due to a change in tillage practice."], ["An Analysis of the New York City Police Department's \u201cStop-and-Frisk\u201d Policy in the Context of Claims of Racial Bias", "Recent studies by police departments and researchers confirm that police stop persons of racial and ethnic minority groups more often than whites relative to their proportions in the population. However, it has been argued that stop rates more accurately reflect rates of crimes committed by each ethnic group, or that stop rates reflect elevated rates in specific social areas, such as neighborhoods or precincts. Most of the research on stop rates and police\u2013citizen interactions has focused on traffic stops, and analyses of pedestrian stops are rare. In this article we analyze data from 125,000 pedestrian stops by the New York Police Department over a 15-month period. We disaggregate stops by police precinct and compare stop rates by racial and ethnic group, controlling for previous race-specific arrest rates. We use hierarchical multilevel models to adjust for precinct-level variability, thus directly addressing the question of geographic heterogeneity that arises in the analysis of pedestrian stops. We find that persons of African and Hispanic descent were stopped more frequently than whites, even after controlling for precinct variability and race-specific estimates of crime participation."], ["Bayesian Spatial Modeling of Extreme Precipitation Return Levels", null], ["A Statistical Framework of Optimal Workload Consolidation With Application to Capacity Planning for On-Demand Computing", "In on-demand computing services, the customer pays based on actual usage, and the service provider is free to allocate unused capacity to other customers. Capacity planning becomes an important issue in such a shared environment. It is desirable that a portfolio effect occurs so that less total capacity is required in a shared system than in a dedicated system. In this article a quantile-based statistical approach is proposed to quantify the portfolio effect and the associated risks. It is shown that the portfolio effect may or may not exist for a given set of workloads, depending crucially on their joint distribution and the assumed risk levels. For Gaussian workloads, the portfolio effect almost always exists regardless of the statistical correlation and risk level. For non-Gaussian workloads, the portfolio effect is not guaranteed and may even be negative, even if the workloads are negatively correlated or statistically independent. However, when simultaneously recorded workload history is available, the portfolio effect can be estimated directly from the data. Based on the data-driven approach, an optimization problem is formulated for capacity planning with the aim of maximizing the portfolio effect for a given set of workloads. This problem calls for consolidation of the workloads into one or more portfolios so that each portfolio can be served satisfactorily by a dedicated system and the total capacity requirement is minimized. Iterative algorithms are proposed to solve the optimization problem numerically. The method is applied to a server consolidation problem with real workload data."], ["Structured Measurement Error in Nutritional Epidemiology", "Preterm birth, defined as delivery before 37 completed weeks' gestation, is a leading cause of infant morbidity and mortality. Identifying factors related to preterm delivery is an important goal of public health professionals who wish to identify etiologic pathways to target for prevention. Validation studies are often conducted in nutritional epidemiology in order to study measurement error in instruments that are generally less invasive or less expensive than \u201cgold standard\u201d instruments. Data from such studies are then used in adjusting estimates based on the full study sample. However, measurement error in nutritional epidemiology has recently been shown to be complicated by correlated error structures in the study-wide and validation instruments. Investigators of a study of preterm birth and dietary intake designed a validation study to assess measurement error in a food frequency questionnaire (FFQ) administered during pregnancy and with the secondary goal of assessing whether a single administration of the FFQ could be used to describe intake over the relatively short pregnancy period, in which energy intake typically increases. Here, we describe a likelihood-based method via Markov chain Monte Carlo to estimate the regression coefficients in a generalized linear model relating preterm birth to covariates, where one of the covariates is measured with error and the multivariate measurement error model has correlated errors among contemporaneous instruments (i.e., FFQs, 24-hour recalls, and biomarkers). Because of constraints on the covariance parameters in our likelihood, identifiability for all the variance and covariance parameters is not guaranteed, and, therefore, we derive the necessary and sufficient conditions to identify the variance and covariance parameters under our measurement error model and assumptions. We investigate the sensitivity of our likelihood-based model to distributional assumptions placed on the true folate intake by employing semiparametric Bayesian methods through the mixture of Dirichlet process priors framework. We exemplify our methods in a recent prospective cohort study of risk factors for preterm birth. We use long-term folate as our error-prone predictor of interest, the FFQ and 24-hour recall as two biased instruments, and the serum folate biomarker as the unbiased instrument. We found that folate intake, as measured by the FFQ, led to a conservative estimate of the estimated odds ratio of preterm birth (.76) when compared to the odds ratio estimate from our likelihood-based approach, which adjusts for the measurement error (.63). We found that our parametric model led to similar conclusions to the semiparametric Bayesian model."], ["Variable Selection in Regression Mixture Modeling for the Discovery of Gene Regulatory Networks", "The profusion of genomic data through genome sequencing and gene expression microarray technology has facilitated statistical research in determining gene interactions regulating a biological process. Current methods generally consist of a two-stage procedure: clustering gene expression measurements and searching for regulatory \u201cswitches,\u201d typically short, conserved sequence patterns (motifs) in the DNA sequence adjacent to the genes. This process often leads to misleading conclusions as incorrect cluster selection may lead to missing important regulatory motifs or making many false discoveries. Treating cluster memberships as known, rather than estimated, introduces bias into analyses, preventing uncertainty about cluster parameters. Further, there is underutilization of the available data, as the sequence information is ignored for purposes of expression clustering and vice versa. We propose a way to address these issues by combining gene clustering and motif discovery in a unified framework, a mixture of hierarchical regression models, with unknown components representing the latent gene clusters, and genomic sequence features linked to the resultant gene expression through a multivariate hierarchical regression. We demonstrate a Monte Carlo method for simultaneous variable selection (for motifs) and clustering (for genes). The selection of the number of components in the mixture is addressed by computing the analytically intractable Bayes factor through a novel multistage mixture importance sampling approach. This methodology is used to analyze a yeast cell cycle dataset to determine an optimal set of motifs that discriminates between groups of genes and simultaneously finds the most significant gene clusters."], ["Implementation of Estimating Function-Based Inference Procedures With Markov Chain Monte Carlo Samplers", "Under a semiparametric or nonparametric setting, inferences about the unknown parameter are often made based on a nonsmooth estimating function. Resampling methods are quite handy for obtaining good approximations to the distribution of the consistent estimator when the estimating equation and its resampled counterparts are not difficult to solve numerically. In this article we propose a simple, flexible procedure that provides such approximations through the standard Markov chain Monte Carlo sampler without solving any equations. More generally, the procedure may locate all possible roots of the estimating equation and provides an approximation to the distribution of each root. We illustrate our proposed procedure extensively with three examples and evaluate its performance comprehensively through a simulation study."], ["Comment", null], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Oracle and Adaptive Compound Decision Rules for False Discovery Rate Control", null], ["Detecting Sparse Signals in Random Fields, With an Application to Brain Mapping", null], [null, null], ["Optimal Tests of Noncorrelation Between Multivariate Time Series", null], ["Longitudinal Studies With Outcome-Dependent Follow-up", "We propose Bayesian parametric and semiparametric partially linear regression methods to analyze the outcome-dependent follow-up data when the random time of a follow-up measurement of an individual depends on the history of both observed longitudinal outcomes and previous measurement times. We begin with the investigation of the simplifying assumptions of Lipsitz, Fitzmaurice, Ibrahim, Gelber, and Lipshultz, and present a new model for analyzing such data by allowing subject-specific correlations for the longitudinal response and by introducing a subject-specific latent variable to accommodate the association between the longitudinal measurements and the follow-up times. An extensive simulation study shows that our Bayesian partially linear regression method facilitates accurate estimation of the true regression line and the regression parameters. We illustrate our new methodology using data from a longitudinal observational study."], ["Bayesian Curve Classification Using Wavelets", "We propose classification models for binary and multicategory data where the predictor is a random function. We use Bayesian modeling with wavelet basis functions that have nice approximation properties over a large class of functional spaces and can accommodate a wide variety of functional forms observed in real life applications. We develop an unified hierarchical model to encompass both the adaptive wavelet-based function estimation model and the logistic classification model. We couple together these two models are to borrow strengths from each other in a unified hierarchical framework. The use of Gibbs sampling with conjugate priors for posterior inference makes the method computationally feasible. We compare the performance of the proposed model with other classification methods, such as the existing naive plug-in methods, by analyzing simulated and real data sets."], ["Robust Truncated Hinge Loss Support Vector Machines", "The support vector machine (SVM) has been widely applied for classification problems in both machine learning and statistics. Despite its popularity, however, SVM has some drawbacks in certain situations. In particular, the SVM classifier can be very sensitive to outliers in the training sample. Moreover, the number of support vectors (SVs) can be very large in many applications. To circumvent these drawbacks, we propose the robust truncated hinge loss SVM (RSVM), which uses a truncated hinge loss. The RSVM is shown to be more robust to outliers and to deliver more accurate classifiers using a smaller set of SVs than the standard SVM. Our theoretical results show that the RSVM is Fisher-consistent, even when there is no dominating class, a scenario that is particularly challenging for multicategory classification. Similar results are obtained for a class of margin-based classifiers."], ["Functional Principal Component Regression and Functional Partial Least Squares", null], ["On Directional Regression for Dimension Reduction", null], ["Optimal Geostatistical Model Selection", "In many fields of science, predicting variables of interest over a study region based on noisy data observed at some locations is an important problem. Two popular methods for the problem are kriging and smoothing splines. The former assumes that the underlying process is stochastic, whereas the latter assumes it is purely deterministic. Kriging performs better than smoothing splines in some situations, but is outperformed by smoothing splines in others. However, little is known regarding selecting between kriging and smoothing splines. In addition, how to perform variable selection in a geostatistical model has not been well studied. In this article we propose a general methodology for selecting among arbitrary spatial prediction methods based on (approximately) unbiased estimation of mean squared prediction errors using a data perturbation technique. The proposed method accounts for estimation uncertainty in both kriging and smoothing spline predictors, and is shown to be optimal in terms of two mean squared prediction error criteria. A simulation experiment is performed to demonstrate the effectiveness of the proposed methodology. The proposed method is also applied to a water acidity data set by selecting important variables responsible for water acidity based on a spatial regression model. Moreover, a new method is proposed for estimating the noise variance that is robust and performs better than some well-known methods."], ["Variable Selection in Finite Mixture of Regression Models", "In the applications of finite mixture of regression (FMR) models, often many covariates are used, and their contributions to the response variable vary from one component to another of the mixture model. This creates a complex variable selection problem. Existing methods, such as the Akaike information criterion and the Bayes information criterion, are computationally expensive as the number of covariates and components in the mixture model increases. In this article we introduce a penalized likelihood approach for variable selection in FMR models. The new method introduces penalties that depend on the size of the regression coefficients and the mixture structure. The new method is shown to be consistent for variable selection. A data-adaptive method for selecting tuning parameters and an EM algorithm for efficient numerical computations are developed. Simulations show that the method performs very well and requires much less computing power than existing methods. The new method is illustrated by analyzing two real data sets."], ["Unified LASSO Estimation by Least Squares Approximation", "We propose a method of least squares approximation (LSA) for unified yet simple LASSO estimation. Our general theoretical framework includes ordinary least squares, generalized linear models, quantile regression, and many others as special cases. Specifically, LSA can transfer many different types of LASSO objective functions into their asymptotically equivalent least squares problems. Thereafter, the standard asymptotic theory can be established and the LARS algorithm can be applied. In particular, if the adaptive LASSO penalty and a Bayes information criterion\u2013type tuning parameter selector are used, the resulting LSA estimator can be as efficient as the oracle. Extensive numerical studies confirm our theory."], ["Sensitivity Analysis for Instrumental Variables Regression With Overidentifying Restrictions", "Instrumental variables regression (IV regression) is a method for making causal inferences about the effect of a treatment based on an observational study in which there are unmeasured confounding variables. The method requires one or more valid instrumental variables (IVs); a valid IV is a variable that is associated with the treatment, is independent of unmeasured confounding variables, and has no direct effect on the outcome. Often there is uncertainty about the validity of the proposed IVs. When a researcher proposes more than one IV, the validity of these IVs can be tested through the \u201coveridentifying restrictions test.\u201d Although the overidentifying restrictions test does provide some information, the test has no power versus certain alternatives and can have low power versus many alternatives due to its omnibus nature. To fully address uncertainty about the validity of the proposed IVs, we argue that a sensitivity analysis is needed. A sensitivity analysis examines the impact of plausible amounts of invalidity of the proposed IVs on inferences for the parameters of interest. We develop a method of sensitivity analysis for IV regression with overidentifying restrictions that makes full use of the information provided by the overidentifying restrictions test but provides more information than the test by exploring sensitivity to violations of the validity of the proposed IVs in directions for which the test has low power. Our sensitivity analysis uses interpretable parameters that can be discussed with subject matter experts. We illustrate our method using a study of food demand among rural households in the Philippines."], ["Robust Modeling for Inference From Generalized Linear Model Classes", "Generalized linear models (GLMs) are widely used for data analysis; however, their maximum likelihood estimators can be sensitive to outliers. We propose new statistical models that allow robust inferences from the GLM class of models, including Poisson and binomial GLMs, and their extension to generalized linear mixed models. The likelihood score equations from the new models give estimators with bounded influence, so that the resulting estimators are robust against outliers while maintaining high efficiency in the absence of outliers."], ["Book Reviews", null], ["Encyclopedia of Statistical Sciences", null], ["Longitudinal Data Analysis", null], ["Applied MANOVA and Discriminant Analysis", null], ["Statistical Matching: Theory and Practice", null], ["Nonparametric Functional Data Analysis: Theory and Practice", null], ["Point Process Theory and Applications: Marked Point and Piecewise Deterministic Processes", null], ["Measure Theory and Probability Theory", null], ["Probabilistic Symmetries and Invariance Principles", null], ["Time Series Analysis and Its Applications: With R Examples", null], ["Automatic Nonuniform Random Variate Generation", null], ["Using Statistical Methods for Water Quality Management: Issues, Problems, and Solutions", null], ["Telegraphic Reviews", null], ["From Data to Policy", "Science, engineering, technology, and people\u2014these are the ingredients that must come together to support the growing complexity of today's global challenges, ranging from international security to space exploration. As scientists and engineers, it is essential that we develop the means to put our work into a decision context for policy makers; otherwise, our efforts will only inform the writers of textbooks and not the leaders who shape the world within which we live. Statisticians must step up to that challenge! Scientific and technical progress requires interdisciplinary teams, because it is impossible for a single individual to have enough knowledge to solve many of today's problems, for example, mapping the genome, modeling the spread of a pandemic, and developing diagnostic and treatment devices for developing countries. A principal role of the statistician is to bring the cutting edge of statistical sciences to these problems. By the nature of our training, statisticians are well poised to assume the role of science and technology integrator. To be successful, this must place statisticians closer to policy pressures and politics. This address will focus on the growing expectations facing statistical sciences and how we, as statisticians, must take responsibility for separating the scientific method from the politics of the scientific process to guarantee that scientific excellence and impact is communicated to decision makers."], ["Model-Assisted Estimation of Forest Resources With Generalized Additive Models", "Multiphase surveys are often conducted in forest inventories, with the goal of estimating forested area and tree characteristics over large regions. This article describes how design-based estimation of such quantities, based on information gathered during ground visits of sampled plots, can be made more precise by incorporating auxiliary information available from remote sensing. The relationship between the ground visit measurements and the remote sensing variables is modeled using generalized additive models. Nonparametric estimators for these models are discussed and applied to forest data collected in the mountains of northern Utah. Model-assisted estimators that use the nonparametric regression fits are proposed for these data. The design context of this study is two-phase systematic sampling from a spatial continuum, under which properties of model-assisted estimators are derived. Difficulties with the standard variance estimation approach, which assumes simple random sampling in each phase, are described. An alternative assessment of estimator performance based on a synthetic population is implemented and shows that using the model predictions in a model-assisted survey estimation procedure results in substantial efficiency improvements over current estimation approaches."], ["Comment", null], ["Comment", null], ["Comment", null], ["Rejoinder", null], ["Spatial Bayesian Variable Selection With Application to Functional Magnetic Resonance Imaging", "We propose a procedure to undertake Bayesian variable selection and model averaging for a series of regressions located on a lattice. For those regressors that are in common in the regressions, we consider using an Ising prior to smooth spatially the indicator variables representing whether or not the variable is zero or nonzero in each regression. This smooths spatially the probabilities that each independent variable is nonzero in each regression and indirectly smooths spatially the regression coefficients. We discuss how single-site sampling schemes can be used to evaluate the joint posterior distribution. The approach is applied to the problem of functional magnetic resonance imaging in medical statistics, where massive datasets arise that require prompt processing. Here the Ising prior with a three-dimensional neighborhood structure is used to smooth spatially activation maps from regression models of blood oxygenation. The Ising prior also has the advantage of allowing incorporation of anatomic prior information through the external field. Using a visual experiment, we show how a single-site sampling scheme can provide rapid evaluation of the posterior activation maps and activation amplitudes. The approach is shown to result in maps that are superior to those produced by a recent Bayesian approach using a continuous Markov random field for the activation amplitude."], ["Disability and Employment", "Measurement error in health and disability status has been widely accepted as a central problem in social science research. Long-standing debates about the prevalence of disability, the role of health in labor market outcomes, and the influence of federal disability policy on declining employment rates have all emphasized issues regarding the reliability of self-reported disability. In addition to random error, inaccuracy in survey datasets may be produced by a host of economic, social, and psychological factors that can lead respondents to misreport work capacity. We develop a nonparametric foundation for assessing how assumptions on the reporting error process affect inferences on the employment gap between the disabled and nondisabled. Rather than imposing the strong assumptions required to obtain point identification, we derive sets of bounds that formalize the identifying power of primitive nonparametric assumptions that appear to share broad consensus in the literature. Within this framework, we introduce a finite-sample correction for the analog estimator of the monotone instrumental variable (MIV) bound. Our empirical results suggest that conclusions derived from conventional latent variable reporting error models may be driven largely by ad hoc distributional and functional form restrictions. We also find that under relatively weak nonparametric assumptions, nonworkers appear to systematically overreport disability."], ["Spatiotemporal Models for Region of Interest Analyses of Functional Neuroimaging Data", "In vivo functional neuroimaging technology enables the evaluation of behavior-related changes in measured brain activity within specific cortical regions of interest (ROIs). When sufficient neurophysiologic evidence exists to restrict attention to a defined cortical region, an ROI analysis can provide powerful insights regarding neural representations of cognition, emotions, behaviors, and the neuropathology of psychiatric disorders. Given the complexity and abundance of data from neuroimaging experiments, anatomically focused research questions allow statisticians to explore models that more accurately reflect neurophysiologic characteristics of the data than global activation studies. Neural processing characteristics of particular interest in this article are spatial correlations stemming from the interplay between spatially distinct brain locations and temporal correlations between serial measures of brain activity. Despite the simplified data structure of ROI studies, challenges remain in modeling spatial correlations due to, for example, the fact that the correlations do not necessarily decrease as a function of increasing separation between the measurement locations. This article presents a spatiotemporal model that incorporates a functionally defined distance metric into a parametric structure for spatial correlations and includes temporal correlations between repeated scans. We demonstrate the use of the spatiotemporal model using experimental data from a study of the effects of ethanol administration on brain activity in the cerebellum, which largely controls balance and posture. We further illustrate our model using data simulated from a study evaluating neural processing alterations in the right prefrontal cortex associated with mental arithmetic."], ["An \u201cUnfolding\u201d Latent Variable Model for Likert Attitude Data", "Likert attitude data consist of responses to favorable and unfavorable statements about an entity, where responses fall into ordered categories ranging from disagreement to agreement. Social science and marketing researchers frequently use data of this type to measure attitudes toward an entity such as a policy or product. We focus on data on American and British attitudes toward their respective nations (\u201cnational pride\u201d). We introduce a multidimensional unfolding model (MUM) to describe the relationship between the data and the attitudes underlying them. Unlike most existing models, the MUM allows the data to reflect not just attitudes, but also response style, which is defined as a consistent and content-independent pattern of response category selection such as a tendency to agree with all statements. The MUM can be used to model multiple attitudes, which allows researchers to expand their analysis of the data of interest to include all available Likert data so as to increase information on response style. For example, we include additional data on immigration attitudes to help distinguish the effects of response style and national pride on our data. The MUM can be used to fit linear models for the effects of background variables on attitudes. Resulting inferences about attitudes are adjusted for response style and should be less biased. Simulation results strongly suggest that, unlike Likert's popular scoring model, the MUM yields unbiased inferences even when there are unequal proportions of favorable and unfavorable statements."], ["Accounting for Spatial Dependence in the Analysis of SPECT Brain Imaging Data", null], ["Combining Information From Two Surveys to Estimate County-Level Prevalence Rates of Cancer Risk Factors and Screening", "Cancer surveillance research requires estimates of the prevalence of cancer risk factors and screening for small areas such as counties. Two popular data sources are the Behavioral Risk Factor Surveillance System (BRFSS), a telephone survey conducted by state agencies, and the National Health Interview Survey (NHIS), an area probability sample survey conducted through face-to-face interviews. Both data sources have advantages and disadvantages. The BRFSS is a larger survey and almost every county is included in the survey, but it has lower response rates as is typical with telephone surveys and it does not include subjects who live in households with no telephones. On the other hand, the NHIS is a smaller survey, with the majority of counties not included; but it includes both telephone and nontelephone households, and has higher response rates. A preliminary analysis shows that the distributions of cancer screening and risk factors are different for telephone and nontelephone households. Thus, information from the two surveys may be combined to address both nonresponse and noncoverage errors. A hierarchical Bayesian approach that combines information from both surveys is used to construct county-level estimates. The proposed model incorporates potential noncoverage and nonresponse biases in the BRFSS as well as complex sample design features of both surveys. A Markov chain Monte Carlo method is used to simulate draws from the joint posterior distribution of unknown quantities in the model that uses design-based direct estimates and county-level covariates. Yearly prevalence estimates at the county level for 49 states, as well as for the entire state of Alaska and the District of Columbia, are developed for six outcomes using BRFSS and NHIS data from the years 1997\u20132000. The outcomes include smoking and use of common cancer screening procedures. The NHIS/BRFSS combined county-level estimates are substantially different from those based on the BRFSS alone."], ["Multiple Testing of General Contrasts", null], ["Estimating the Null and the Proportion of Nonnull Effects in Large-Scale Multiple Comparisons", "An important issue raised by Efron in the context of large-scale multiple comparisons is that in many applications, the usual assumption that the null distribution is known is incorrect, and seemingly negligible differences in the null may result in large differences in subsequent studies. This suggests that a careful study of estimation of the null is indispensable. In this article we consider the problem of estimating a null normal distribution, and a closely related problem, estimation of the proportion of nonnull effects. We develop an approach based on the empirical characteristic function and Fourier analysis. The estimators are shown to be uniformly consistent over a wide class of parameters. We investigate the numerical performance of the estimators using both simulated and real data. In particular, we apply our procedure to the analysis of breast cancer and human immunodeficiency virus microarray datasets. The estimators perform favorably compared with existing methods."], [null, null], ["Multiple Hypothesis Testing by Clustering Treatment Effects", null], [null, null], ["Partially Linear Hazard Regression for Multivariate Survival Data", null], ["Smoothed Rank Regression With Censored Data", "A weighted rank estimating function is proposed to estimate the regression parameter vector in an accelerated failure time model with right censored data. In general, rank estimating functions are discontinuous in the regression parameter, creating difficulties in determining the asymptotic distribution of the estimator. A local distribution function is used to create a rank based estimating function that is continuous and monotone in the regression parameter vector. A weight is included in the estimating function to produce a bounded influence estimate. The asymptotic distribution of the regression estimator is developed and simulations are performed to examine its finite sample properties. A lung cancer dataset is used to illustrate the methodology."], ["Flexible Cure Rate Modeling Under Latent Activation Schemes", "With rapid improvements in medical treatment and health care, many datasets dealing with time to relapse or death now reveal a substantial portion of patients who are cured (i.e., who never experience the event). Extended survival models called cure rate models account for the probability of a subject being cured and can be broadly classified into the classical mixture models of Berkson and Gage (BG type) or the stochastic tumor models pioneered by Yakovlev and extended to a hierarchical framework by Chen, Ibrahim, and Sinha (YCIS type). Recent developments in Bayesian hierarchical cure models have evoked significant interest regarding relationships and preferences between these two classes of models. Our present work proposes a unifying class of cure rate models that facilitates flexible hierarchical model building while including both existing cure model classes as special cases. This unifying class enables robust modeling by accounting for uncertainty in underlying mechanisms leading to cure. Issues such as regressing on the cure fraction and propriety of the associated posterior distributions under different modeling assumptions are also discussed. Finally, we offer a simulation study and also illustrate with two datasets (on melanoma and breast cancer) that reveal our framework's ability to distinguish among underlying mechanisms that lead to relapse and cure."], ["Sensitivity Analyses Comparing Time-to-Event Outcomes Existing Only in a Subset Selected Postrandomization", null], [null, null], ["Scan Statistics With Weighted Observations", null], ["Determining the Number of Factors in the General Dynamic Factor Model", null], ["Dynamic Integration of Time- and State-Domain Methods for Volatility Estimation", "Time- and state-domain methods are two common approaches to nonparametric prediction. Whereas the former uses data predominantly from recent history, the latter relies mainly on historical information. Combining these two pieces of valuable information is an interesting challenge in statistics. We surmount this problem by dynamically integrating information from both the time and state domains. The estimators from these two domains are optimally combined based on a data-driven weighting strategy, which provides a more efficient estimator of volatility. Asymptotic normality is separately established for the time domain, the state domain, and the integrated estimators. By comparing the efficiency of the estimators, we demonstrate that the proposed integrated estimator uniformly dominates the other two estimators. The proposed dynamic integration approach is also applicable to other estimation problems in time series. Extensive simulations are conducted to demonstrate that the newly proposed procedure outperforms some popular ones, such as the RiskMetrics and historical simulation approaches, among others. In addition, empirical studies convincingly endorse our integration method."], ["Analysis of Longitudinal Data With Semiparametric Estimation of Covariance Function", "Improving efficiency for regression coefficients and predicting trajectories of individuals are two important aspects in the analysis of longitudinal data. Both involve estimation of the covariance function. Yet challenges arise in estimating the covariance function of longitudinal data collected at irregular time points. A class of semiparametric models for the covariance function by that imposes a parametric correlation structure while allowing a nonparametric variance function is proposed. A kernel estimator for estimating the nonparametric variance function is developed. Two methods for estimating parameters in the correlation structure\u2014a quasi-likelihood approach and a minimum generalized variance method\u2014are proposed. A semiparametric varying coefficient partially linear model for longitudinal data is introduced, and an estimation procedure for model coefficients using a profile weighted least squares approach is proposed. Sampling properties of the proposed estimation procedures are studied, and asymptotic normality of the resulting estimators is established. Finite-sample performance of the proposed procedures is assessed by Monte Carlo simulation studies. The proposed methodology is illustrated with an analysis of a real data example."], ["Empirical Likelihood for a Varying Coefficient Model With Longitudinal Data", "In this article local empirical likelihood-based inference for a varying coefficient model with longitudinal data is investigated. First, we show that the naive empirical likelihood ratio is asymptotically standard chi-squared when undersmoothing is employed. The ratio is self-scale invariant and the plug-in estimate of the limiting variance is not needed. Second, to enhance the performance of the ratio, mean-corrected and residual-adjusted empirical likelihood ratios are recommended. The merit of these two bias corrections is that without undersmoothing, both also have standard chi-squared limits. Third, a maximum empirical likelihood estimator (MELE) of the time-varying coefficient is defined, the asymptotic equivalence to the weighted least-squares estimator (WLSE) is provided, and the asymptotic normality is shown. By the empirical likelihood ratios and the normal approximation of the MELE/WLSE, the confidence regions of the time-varying coefficients are constructed. Fourth, when some components are of particular interest, we suggest using mean-corrected and residual-adjusted partial empirical likelihood ratios to construct the confidence regions/intervals. In addition, we also consider the construction of the simultaneous and bootstrap confidence bands. A simulation study is undertaken to compare the empirical likelihood, the normal approximation, and the bootstrap methods in terms of coverage accuracies and average areas/widths of confidence regions/bands. An example in epidemiology is used for illustration."], ["Optimal Incomplete Block Designs", "Optimal incomplete block designs are pursued through the E criterion of minimizing maximal variance. Methodology is developed for design choice based on graphs and an extensive catalog of downloadable designs is compiled. Along with the general methodology, a near complete solution for E-optimal block designs is provided for up to 15 treatments. E optimality is revealed to be a flexible criterion that, depending on the application, can offer many choices for good designs."], ["Robust Model-Based and Model-Assisted Predictors of the Finite Population Total", "The prediction approach to finite population inference has received considerable attention in recent years. Under this approach, the finite population is assumed to be a realization from a superpopulation described by a known probability model, usually a linear model. The prediction approach is often criticized for its lack of robustness against model misspecification. In this article we revisit this important issue and introduce a new robust prediction approach in which the superpopulation model is chosen adaptively from the well-known Box\u2013Cox class of probability distributions. The richness of the Box\u2013Cox class ensures robustness in our model-based prediction approach. We explain how our robust model-based predictor can be adjusted to handle zero observations for the study variable and to achieve the design-unbiasedness and benchmarking properties. We demonstrate the robustness of our proposed predictors using a Monte Carlo simulation study and a real life example."], ["Model Averaging and Dimension Selection for the Singular Value Decomposition", null], ["High-Dimension, Low\u2013Sample Size Perspectives in Constrained Statistical Inference", "High-dimensional categorical data models, often with inadequately large sample sizes, crop up in many fields of application. The SARS epidemic, originating in southern China in 2002, had an identified single-stranded and positive-sense RNA virus with large genome size and moderate mutation rate. The present genomic study is used as a prime illustration for motivating appropriate statistical methodology for comprehending the genomic variation in such high-dimensional categorical data models. Because of underlying restraints, a pseudomarginal approach based on Hamming distance is considered in a constrained statistical inference setup. The union-intersection principle and jackknifing methods are incorporated in exploring appropriate statistical procedures."], [null, "We propose a novel bootstrap hypothesis testing approach for the problem of testing a null hypothesis of a common mean direction, mean polar axis, or mean shape across several populations of real unit vectors (the directional case) or complex unit vectors (the two-dimensional shape case). Multisample testing problems of this type arise frequently in directional statistics and shape analysis (as in other areas of statistics), but to date there has been relatively little discussion of nonparametric bootstrap approaches to this problem. The bootstrap approach described here is based on a statistic that can be expressed as the smallest eigenvalue of a certain positive definite matrix. We prove that this statistic has a limiting chi-squared distribution under the null hypothesis of equality of means across populations. Although we focus mainly on the version of the statistic in which neither isotropy within populations nor constant dispersion structure across populations is assumed, we explain how to modify the statistic so that either or both of these assumptions can be incorporated. Our numerical results indicate that the bootstrap approach proposed here may be expected to perform well in practice."], ["Nonparametric Tests for Perfect Judgment Rankings", "The ranked-set sampling literature includes both inference procedures that rely on the assumption of perfect rankings and inference procedures that are robust to violations of this assumption. Procedures that assume perfect rankings tend to be more efficient when rankings are in fact perfect, but they may be invalid when perfect rankings fail. As a result, users of ranked-set sampling must decide between efficiency and robustness, and there is at present little to guide their decision. In this article we introduce three rank-based goodness-of-fit tests that may be consulted in making these decisions. Our strategy in producing these tests is to think of the judgment order statistic classes as separate samples, compute the ranks of the units from each sample within the combined sample, and use these ranks to test whether the judgment rankings are perfect. Consideration of both power and ease of use leads us to recommend use of a test that rejects when the concordance between the vector of mean ranks and its null expectation is small. Tables of critical values and appropriate asymptotic theory for applying this test are provided, and we illustrate the use of the tests by applying them to a biological dataset."], ["Smooth Location-Dependent Bandwidth Selection for Local Polynomial Regression", "A bandwidth function for local polynomial models is commonly obtained by optimizing a pointwise penalty criterion, such as an estimated mean squared error (MSE), over a grid of predictor locations. A resultant regression estimate may suffer from irregularities, such as discontinuities, and contextual information over nearby predictor locations is not used. To mediate these difficulties, ad hoc postprocessing is sometimes carried out in the form of smoothing of the penalty criterion and/or the bandwidth estimates. In this work a technique is developed for choosing a smooth bandwidth function that uses a smoothing spline selected based on new \u201cfit\u201d and \u201croughness\u201d penalties. The fit penalty pushes the bandwidth estimate to adhere to the chosen pointwise criterion, whereas the roughness penalty is imposed on the fitted regression estimate as opposed to the bandwidth estimate, which usually is not of direct interest. The technique can be used in conjunction with various adaptive bandwidth selection methods and provides a systematic way of incorporating contextual information into bandwidth estimation. To justify a spline bandwidth function, we show that under mild regularity conditions, there exists a smooth, asymptotically optimal bandwidth function. We also demonstrate empirically that the technique outperforms the empirical-bias bandwidth selector (EBBS) of Ruppert when using an EBBS MSE pointwise penalty estimate."], ["Semiparametric Estimation of Spectral Density With Irregular Observations", null], ["A Nonparametric Assessment of Properties of Space\u2013Time Covariance Functions", "We propose a unified framework for testing various assumptions commonly made for covariance functions of stationary spatio-temporal random fields. The methodology is based on the asymptotic normality of space\u2013time covariance estimators. We focus on tests for full symmetry and separability in this article, but our framework naturally covers testing for isotropy and Taylor's hypothesis. Our test successfully detects the asymmetric and nonseparable features in two sets of wind speed data. We perform simulation experiments to evaluate our test and conclude that our method is reliable and powerful for assessing common assumptions on space\u2013time covariance functions."], ["Jump Surface Estimation, Edge Detection, and Image Restoration", "Surface estimation is important in many applications. When conventional smoothing procedures (e.g., running averages, local polynomial kernel smoothing procedures, smoothing spline procedures) are used for estimating jump surfaces from noisy data, jumps are blurred at the same time when noise is removed. In recent years, new smoothing methodologies have been proposed in the statistical literature for detecting jumps in surfaces and for estimating jump surfaces with jumps preserved. We provide a review of these methodologies. Because a monochrome image can be considered a jump surface of the image intensity function, with jumps at the outlines of objects, edge detection and image restoration problems in image processing are closely related to the jump surface estimation problem in statistics. We also review major methodologies on edge detection and image restoration, and discuss connections and differences among these methods and related methods in the statistical literature."], ["Book Reviews", null], ["A First Course in Monte Carlo", null], ["Simulation Techniques in Financial Risk Management", null], ["Pattern Recognition Algorithms for Data Mining", null], ["Robust Statistical Methods with R", null], ["Generalized Additive Models: An Introduction With R", null], ["Nonparametric Regression Methods for Longitudinal Data Analysis. Mixed-effects Modeling Approaches", null], ["Theory of Preliminary Test and Stein-Type Estimation With Applications", null], ["Test Equating, Scaling, and Linking: Methods and Practices", null], ["Advanced Distance Sampling: Estimating Abundance of Biological Populations", null], ["Exact Analysis of Discrete Data", null], ["A Modern Theory of Factorial Design", null], ["Introductory Stochastic Analysis for Finance and Insurance", null], ["Measures, Integrals and Martingales", null], ["Advanced Statistics From an Elementary Point of View", null], ["Applied Statistics for Engineers and Scientists", null], ["Biostatistics: a Bayesian Introduction", null], ["Telegraphic Reviews", null], [null, null], ["Prediction of U.S. Cancer Mortality Counts Using Semiparametric Bayesian Techniques", "We present two models for the short-term prediction of the number of deaths arising from common cancers in the United States. The first is a local linear model, in which the slope of the segment joining the number of deaths for any two consecutive time periods is assumed to be random with a nonparametric distribution, which has a Dirichlet process prior. For slightly longer prediction periods, we present a local quadratic model. This extension of the local linear model includes an additional \u201cacceleration\u201d term that allows it to quickly adjust to sudden changes in the time series. The proposed models can be used to obtain the predictive distributions of the future number of deaths, as well their means and variances through Markov chain Monte Carlo techniques. We illustrate our methods by runs on data from selected cancer sites."], ["Periodic Seasonal Reg-ARFIMA\u2013GARCH Models for Daily Electricity Spot Prices", "Novel periodic extensions of dynamic long-memory regression models with autoregressive conditional heteroscedastic errors are considered for the analysis of daily electricity spot prices. The parameters of the model with mean and variance specifications are estimated simultaneously by the method of approximate maximum likelihood. The methods are implemented for time series of 1,200\u20134,400 daily price observations in four European power markets. Apart from persistence, heteroscedasticity, and extreme observations in prices, a novel empirical finding is the importance of day-of-the-week periodicity in the autocovariance function of electricity spot prices. In particular, the very persistent daily log prices from the Nord Pool power exchange of Norway are effectively modeled by our framework, which is also extended with explanatory variables to capture supply-and-demand effects. The daily log prices of the other three electricity markets\u2014EEX in Germany, Powernext in France, and APX in The Netherlands\u2014are less persistent, but periodicity is also highly significant. The dynamic behavior differs from market to market and depends primarily on the method of power generation: hydro power, power generated from fossil fuels, or nuclear power. The article improves on existing models in capturing the memory characteristics, which are important in derivative pricing and real option analysis."], ["Cost (or Price) Forecasting in the Face of Technological Advance", "The problem considered involves forecasting the future costs of hard drives of various capacities and speeds of revolution or, more generally, forecasting the future costs of various quantifiably different versions of a commodity that is subject to technological advance. In the primary development, it is supposed that the data consist of past and present costs. A model is proposed in which the past, present, and future costs of each version are related with each other and also with the costs of the other versions. The model encompasses a stochastic version of an empirical relationship known as Moore's law. A forecasting methodology was developed by adopting a Bayesian approach and taking the prior distribution to be of a relatively tractable form. An implementation of the Gibbs sampler was devised for making draws from the posterior distribution of the future costs; the forecasts are based on those draws. The proposed methodology was used to obtain forecasts retrospectively from data accumulated (over a 5-year period) on the quarterly costs of hard drives. The accuracy of the longer-term forecasts compared favorably with those of certain benchmark forecasts, whereas the accuracy of the shorter-term forecasts compared less favorably. Greater accuracy can be achieved through enhancements to the proposed methodology that provide for the use of supplementary information (i.e., information that is relevant but not fully reflected in the past and present costs)."], ["Spatial Analyses of Periodontal Data Using Conditionally Autoregressive Priors Having Two Classes of Neighbor Relations", "Attachment loss, the extent of a tooth's root (in millimeters) that is no longer attached to surrounding bone by periodontal ligament, is often used to measure the current state of a patient's periodontal disease and monitor disease progression. Attachment loss data can be analyzed using a conditionally autoregressive (CAR) prior distribution that smooths fitted values toward neighboring values. However, it may be desirable to have more than one class of neighbor relation in the spatial structure, so the different classes of neighbor relations can induce different degrees of smoothing. For example, we may wish to allow smoothing of neighbor pairs bridging the gap between teeth to differ from smoothing of pairs that do not bridge such gaps. Adequately modeling the spatial structure may improve the monitoring of periodontal disease progression. This article develops a two-neighbor-relation CAR model to handle this situation and presents associated theory to help explain the sometimes unusual posterior distributions of the parameters controlling the different types of smoothing. The posterior of these smoothing parameters often has long upper tails, and its shape can change dramatically depending on the spatial structure. Like previous authors, we show that the prior distribution on these parameters has little effect on the posterior of the fixed effects but has a marked influence on the posterior of both the random effects and the smoothing parameters. Our analysis of attachment loss data also suggests that the spatial structure itself varies between individuals."], ["A Unified Semiparametric Framework for Quantitative Trait Loci Analyses, With Application to Spike Phenotypes", null], ["Episodic Nonlinear Event Detection in the Canadian Exchange Rate", "This article uses daily observations for the Canadian dollar\u2013U.S. dollar nominal exchange rate over the recent flexible exchange rate period and a new statistical technique, recently developed by Hinich, to detect major political and economic events that have affected the exchange rate."], ["Minimum Distance Matched Sampling With Fine Balance in an Observational Study of Treatment for Ovarian Cancer", null], ["Inference for Stereological Extremes", "In the production of clean steels, the occurrence of imperfections\u2014so-called \u201cinclusions\u201d\u2014is unavoidable. The strength of a clean steel block is largely dependent on the size of the largest imperfection that it contains, so inference on extreme inclusion size forms an important part of quality control. Sampling is generally done by measuring imperfections on planar slices, leading to an extreme value version of a standard stereological problem: how to make inference on large inclusions using only the sliced observations. Under the assumption that inclusions are spherical, this problem has been tackled previously using a combination of extreme value models, stereological calculations, a Bayesian hierarchical model, and standard Markov chain Monte Carlo (MCMC) techniques. Our objectives in this article are twofold: (1) to assess the robustness of such inferences with respect to the assumption of spherical inclusions, and (2) to develop an inference procedure that is valid for nonspherical inclusions. We investigate both of these aspects by extending the spherical family for inclusion shapes to a family of ellipsoids. We then address the issue of robustness by assessing the performance of the spherical model when fitted to measurements obtained from a simulation of ellipsoidal inclusions. The issue of inference is more difficult, because likelihood calculation is not feasible for the ellipsoidal model. To handle this aspect, we propose a modification to a recently developed likelihood-free MCMC algorithm. After verifying the viability and accuracy of the proposed algorithm through a simulation study, we analyze a real inclusion dataset, comparing the inference obtained under the ellipsoidal inclusion model with that previously obtained assuming spherical inclusions."], ["Correlation and Large-Scale Simultaneous Significance Testing", null], ["Detecting Differential Expressions in GeneChip Microarray Studies", "In this article we consider testing for differentially expressed genes in GeneChip studies by modeling and analyzing the quantiles of gene expression through probe level measurements. By developing a robust rank score test for linear quantile models with a random effect, we propose a reliable test for detecting differences in certain quantiles of the intensity distributions. By using a genomewide adjustment to the test statistic to account for within-array correlation, we demonstrate that the proposed rank score test is highly effective even when the number of arrays is small. Our empirical studies with real experimental data show that detecting differences in the quartiles for the probe level data is a valuable complement to the usual mixed model analysis based on Gaussian likelihood. The methodology proposed in this article is a first attempt to develop inferential tools for quantile regression in mixed models."], ["Optimal Shrinkage Estimation of Variances With Applications to Microarray Data Analysis", null], ["Efficient Estimation of Population-Level Summaries in General Semiparametric Regression Models", "This article considers a wide class of semiparametric regression models in which interest focuses on population-level quantities that combine both the parametric and the nonparametric parts of the model. Special cases in this approach include generalized partially linear models, generalized partially linear single-index models, structural measurement error models, and many others. For estimating the parametric part of the model efficiently, profile likelihood kernel estimation methods are well established in the literature. Here our focus is on estimating general population-level quantities that combine the parametric and nonparametric parts of the model (e.g., population mean, probabilities, etc.). We place this problem in a general context, provide a general kernel-based methodology, and derive the asymptotic distributions of estimates of these population-level quantities, showing that in many cases the estimates are semiparametric efficient. For estimating the population mean with no missing data, we show that the sample mean is semiparametric efficient for canonical exponential families, but not in general. We apply the methods to a problem in nutritional epidemiology, where estimating the distribution of usual intake is of primary interest and semiparametric methods are not available. Extensions to the case of missing response data are also discussed."], ["A Simple Risk-Adjusted Exponentially Weighted Moving Average", "In such contexts as medical monitoring there is a need for a simple procedure to estimate the level of a smoothly changing dynamic process while adjusting for risk factors associated with heterogeneous nonnormal observations. Standard methods exist for when the data are normal, and simple methods have been developed for nonnormal data without covariates. The dynamic generalized linear model (DGLM) may be considered a standard for nonnormal data with covariates, but perhaps one that is overcomplicated for many contexts. We propose adapting the standard exponentially weighted moving average (EWMA) to take into account the effect of risk factors. By approximating the correct exponential family likelihood, we derive a risk-adjusted EWMA (RA\u2013EWMA) that is essentially a standard EWMA applied to \u201cpseudo-observations,\u201d which are the original observations adjusted for differential risk. The RA\u2013EWMA can be expressed as a type of filter, in which current estimate = previous estimate + discounted predictive error. We review Bayesian state-space models related to the EWMA and examine the properties of each. The RA\u2013EWMA and DGLM are compared algebraically, through simulation and an example. In the example, the expected mortality after surgery by a particular surgeon is allowed to vary over time and is estimated through the RA\u2013EWMA. Each binary observation is adjusted for the effect of patient-specific risk factors to standardize information fed into the estimator for the expected mortality for a \u201cbaseline\u201d patient. The tool used for adjustment can be used to provide future patient-specific preoperative risk assessments. We conclude that the RA\u2013EWMA performs similarly to the DGLM, is conceptually and computationally simpler by virtue of having fewer and simpler stages, and has an intuitive appeal to a wide variety of stakeholders."], ["Additive Expectancy Regression", "Regression is a useful tool for studying association between an outcome variable and its covariates. In a classical linear regression model, the outcome's expectation is usually a function of the covariates and some regression parameters, and the underlying distribution is often normal. In this article we propose a new class of additive expectancy regression models in which the outcomes tend to be positively skewed. In the new regression models, regression parameters are practically meaningful and also useful with inferences on the means of skewed outcomes. Parametric and semiparametric methods are developed for model estimation and inferences. Model-based prediction and model adequacy assessment are discussed. The proposed methodologies are demonstrated by two real data analyses."], ["Semiparametric Transformation Models With Random Effects for Recurrent Events", "In this article we study a class of semiparametric transformation models with random effects for the intensity function of the counting process. These models provide considerable flexibility in formulating the effects of possibly time-dependent covariates on the developments of recurrent events while accounting for the dependence of the recurrent event times within the same subject. We show that the nonparametric maximum likelihood estimators (NPMLEs) for the parameters of these models are consistent and asymptotically normal. The limiting covariance matrices for the estimators of the regression parameters achieve the semiparametric efficiency bounds and can be consistently estimated. The limiting covariance function for the estimator of any smooth functional of the cumulative intensity function also can be consistently estimated. We develop a simple and stable EM algorithm to compute the NPMLEs as well as the variance and covariance estimators. Simulation studies demonstrate that the proposed methods perform well in practical situations. Two medical studies are provided for illustrations."], ["Minimum Area Confidence Set Optimality for Confidence Bands in Simple Linear Regression", "The average width of a simultaneous confidence band has been used by several authors (e.g., Naiman and Piegorsch) as a criterion for the comparison of different confidence bands. In this article the area of the confidence set that corresponds to a confidence band is used as a new criterion. For simple linear regression, comparisons have been carried out under this new criterion between hyperbolic bands, two-segment bands, and three-segment bands, which include constant width bands as special cases. It is found that if one requires a confidence band over the whole range of the covariate, then the best confidence band is given by the Working and Hotelling hyperbolic band. Furthermore, if one needs a confidence band over a finite interval of the covariate, then a restricted hyperbolic band can again be recommended, although a three-segment band may be very slightly superior in certain cases."], ["Interference Between Units in Randomized Experiments", "In a randomized experiment comparing two treatments, there is interference between units if applying the treatment to one unit may affect other units. Interference implies that treatment effects are not comparisons of two potential responses that a unit may exhibit, one under treatment and the other under control, but instead are inherently more complex. Interference is common in social settings where people communicate, compete, or spread disease; in studies that treat one part of an organism using a symmetrical part as control; in studies that apply different treatments to the same organism at different times; and in many other situations. Available statistical tools are limited. For instance, Fisher's sharp null hypothesis of no treatment effect implicitly entails no interference, and so his randomization test may be used to test no effect, but conventional ways of inverting the test to obtain confidence intervals, say for an additive effect, are not applicable with interference. Another commonly used approach assumes that interference is of a simple parametric form confined to units that are near one another in time or space; this is useful when applicable but is of little use when interference may be widespread and of uncertain form. Exact, nonparametric methods are developed for inverting randomization tests to obtain confidence intervals for magnitudes of effect assuming nothing at all about the structure of the interference between units. The limitations of these methods are discussed. To illustrate the general approach, two simple methods and two simple empirical examples are discussed. Extension to randomization based covariance adjustment is briefly described."], ["Mixed Hidden Markov Models", "Hidden Markov models (HMMs) are a useful tool for capturing the behavior of overdispersed, autocorrelated data. These models have been applied to many different problems, including speech recognition, precipitation modeling, and gene finding and profiling. Typically, HMMs are applied to individual stochastic processes; HMMs for simultaneously modeling multiple processes\u2014as in the longitudinal data setting\u2014have not been widely studied. In this article I present a new class of models, mixed HMMs (MHMMs), where I use both covariates and random effects to capture differences among processes. I define the models using the framework of generalized linear mixed models and discuss their interpretation. I then provide algorithms for parameter estimation and illustrate the properties of the estimators via a simulation study. Finally, to demonstrate the practical uses of MHMMs, I provide an application to data on lesion counts in multiple sclerosis patients. I show that my model, while parsimonious, can describe the heterogeneity among such patients."], ["Transition Models for Multivariate Longitudinal Binary Data", "In many settings with longitudinal binary data, interest lies in modeling covariate effects on transition probabilities of an underlying stochastic process. When data from two or more processes are available, the scientific focus may be on the degree to which changes in one process are associated with changes in another process. Analysis based on independent Markov models permits separate examination of covariate effects on the transition probabilities for each process, but no insight into between-process associations is obtained. We propose a method of estimation and inference based on joint transitional models for multivariate longitudinal binary data using GEE2 or alternating logistic regression that allows modeling of covariate effects on marginal transition probabilities as well as the association parameters. Consistent estimates of regression coefficients and association parameters are obtained, and efficiency gains for the parameters governing the marginal transition probabilities are realized when the association between processes is strong. Extensions to deal with multivariate longitudinal categorical data are indicated."], ["Implementing Optimal Allocation in Sequential Binary Response Experiments", null], ["Controlling Variable Selection by the Addition of Pseudovariables", "We propose a new approach to variable selection designed to control the false selection rate (FSR), defined as the proportion of uninformative variables included in selected models. The method works by adding a known number of pseudovariables to the real dataset, running a variable selection procedure, and monitoring the proportion of pseudovariables falsely selected. Information obtained from bootstrap-like replications of this process is used to estimate the proportion of falsely selected real variables and to tune the selection procedure to control the FSR."], ["Extending the Akaike Information Criterion to Mixture Regression Models", "We examine the problem of jointly selecting the number of components and variables in finite mixture regression models. We find that the Akaike information criterion is unsatisfactory for this purpose because it overestimates the number of components, which in turn results in incorrect variables being retained in the model. Therefore, we derive a new information criterion, the mixture regression criterion (MRC), that yields marked improvement in model selection due to what we call the \u201cclustering penalty function.\u201d Moreover, we prove the asymptotic efficiency of the MRC. We show that it performs well in Monte Carlo studies for the same or different covariates across components with equal or unequal sample sizes. We also present an empirical example on sales territory management to illustrate the application and efficacy of the MRC. Finally, we generalize the MRC to mixture quasi-likelihood and mixture autoregressive models, thus extending its applicability to non-Gaussian models, discrete responses, and dependent data."], ["Quantile Regression in Reproducing Kernel Hilbert Spaces", "In this article we consider quantile regression in reproducing kernel Hilbert spaces, which we call kernel quantile regression (KQR). We make three contributions: (1) we propose an efficient algorithm that computes the entire solution path of the KQR, with essentially the same computational cost as fitting one KQR model; (2) we derive a simple formula for the effective dimension of the KQR model, which allows convenient selection of the regularization parameter; and (3) we develop an asymptotic theory for the KQR model."], ["Power Transformation Toward a Linear Regression Quantile", "In this article we consider the linear quantile regression model with a power transformation on the dependent variable. Like the classical Box\u2013Cox transformation approach, it extends the applicability of linear models without resorting to nonparametric smoothing, but transformations on the quantile models are more natural due to the equivariance property of the quantiles under monotone transformations. We propose an estimation procedure and establish its consistency and asymptotic normality under some regularity conditions. The objective function employed in the estimation can also be used to check inadequacy of a power-transformed linear quantile regression model and to obtain inference on the transformation parameter. The proposed approach is shown to be valuable through illustrative examples."], [null, null], ["Variance Reduction in Multiparameter Likelihood Models", "Local likelihood modeling is a unified and effective approach to establishing the dependence of a response variable, which can be of various types, on independent variables. Therefore, these models have become popular in a wide range of applications. There is an increasing interest in employing multiparameter local likelihood models to investigate trends of sample extremes in environmental statistics. When sample maxima are modeled by a generalized extreme value distribution, the sample size is small in general and local likelihood estimation exhibits a large variation. In this article variance reduction techniques are employed to improve the efficiency of the inference. A simulation study and an application to annual maximum temperatures show that our methods are very effective in finite samples."], ["Stochastic Approximation in Monte Carlo Computation", "The Wang\u2013Landau (WL) algorithm is an adaptive Markov chain Monte Carlo algorithm used to calculate the spectral density for a physical system. A remarkable feature of the WL algorithm is that it is not trapped by local energy minima, which is very important for systems with rugged energy landscapes. This feature has led to many successful applications of the algorithm in statistical physics and biophysics; however, there does not exist rigorous theory to support its convergence, and the estimates produced by the algorithm can reach only a limited statistical accuracy. In this article we propose the stochastic approximation Monte Carlo (SAMC) algorithm, which overcomes the shortcomings of the WL algorithm. We establish a theorem concerning its convergence. The estimates produced by SAMC can be improved continuously as the simulation proceeds. SAMC also extends applications of the WL algorithm to continuum systems. The potential uses of SAMC in statistics are discussed through two classes of applications, importance sampling and model selection. The results show that SAMC can work as a general importance sampling algorithm and a model selection sampler when the model space is complex."], ["Approximate Likelihood for Large Irregularly Spaced Spatial Data", null], ["Empirical Likelihood Inference in Nonlinear Errors-in-Covariables Models With Validation Data", "In this article we study inference in parametric\u2013nonparametric errors-in-covariables regression models using an empirical likelihood approach based on validation data. It is shown that the asymptotic behavior of the proposed estimator depends on the ratio of the sizes of the primary sample and the validation sample, respectively. Unlike cases without measurement errors, the limit distribution of the estimator is no longer tractable and cannot be used for constructing confidence regions. Monte Carlo approximations are employed to simulate the limit distribution. To increase the coverage accuracy of confidence regions, two adjusted empirical likelihood estimators are recommended, which in the limit have a standard chi-squared distribution. A simulation study is carried out to compare the proposed methods with other existing methods. The new methods outperform the least squares method, and one of them works better than simulation\u2013extrapolation (SIMEX) estimation, even when the restrictive model assumptions needed for SIMEX are satisfied. An application to a real dataset illustrates our new approach."], ["Robust Tests in Regression Models With Omnibus Alternatives and Bounded Influence", "A robust approach for testing the parametric form of a regression function versus an omnibus alternative is introduced. This generalizes existing robust methods for testing subhypotheses in a regression model. The new test is motivated by developments in modern smoothing-based testing procedures and can be viewed as a robustification of a smoothing-based conditional moment test. It is asymptotically normal under both the null hypothesis and local alternatives. The robustified test retains the \u201comnibus\u201d property of the corresponding smoothing test; that is, it is consistent for any fixed smooth alternative in an infinite-dimensional space. It is shown that the bias of the asymptotic level under shrinking local contamination is bounded only if the second-order Hampel's influence function is bounded. The test's performance is demonstrated through both Monte Carlo simulations and application to an agricultural dataset."], ["Strictly Proper Scoring Rules, Prediction, and Estimation", null], ["Book Reviews", null], ["Statistical Methods in the Atmospheric Sciences", null], ["Weather Derivative Valuation: The Meteorological, Statistical, Financial and Mathematical Foundations", null], ["Statistics for Fission Track Analysis", null], ["Stochastic Processes and Models", null], ["Statistical Analysis of Stochastic Processes in Time", null], ["Fractal-Based Point Processes", null], ["Partial Identification of Probability Distributions", null], ["Vertical Density Representations and Its Applications", null], ["A Kalman Filter Primer", null], ["Introduction to Regression Modeling", null], ["Introduction to Linear Models and Statistical Inference", null], ["Functional Approach to Optimal Experimental Design", null], ["Statistical Matching: A Frequentist Theory, Practical Applications and Alternative Bayesian Approaches", null], ["Scientific Reasoning: The Bayesian Approach", null], ["Modeling in Medical Decision Making: A Bayesian Approach", null], ["Computational Genome Analysis: An Introduction", null], ["Bioinformatics and Computational Biology Solutions Using R and Bioconductor", null], ["All of Nonparametric Statistics", null], ["Analysis of Integrated and Co-Integrated Time Series With R", null], ["Applied Choice Analysis: A Primer", null], ["Telegraphic Reviews", null], ["Letter to the Editor", null]]}